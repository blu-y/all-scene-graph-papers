id,abstract,authorships.author.display_name,authorships.institutions.display_name,cited_by_count,title,doi,fwci,is_retracted,language,is_oa,oa_status,primary_location.source.display_name,primary_location.source.type,publication_date,type
https://openalex.org/W2250378130,"Semantically complex queries which include attributes of objects and relations between objects still pose a major challenge to image retrieval systems.Recent work in computer vision has shown that a graph-based semantic representation called a scene graph is an effective representation for very detailed image descriptions and for complex queries for retrieval.In this paper, we show that scene graphs can be effectively created automatically from a natural language scene description.We present a rule-based and a classifierbased scene graph parser whose output can be used for image retrieval.We show that including relations and attributes in the query graph outperforms a model that only considers objects and that using the output of our parsers is almost as effective as using human-constructed scene graphs (Recall@10 of 27.1% vs. 33.4%).Additionally, we demonstrate the general usefulness of parsing to scene graphs by showing that the output can also be used to generate 3D scenes.",Sebastian Schuster|Ranjay Krishna|Anne Lynn S. Chang|Li Fei-Fei|Christopher D. Manning,Stanford University|Stanford University|Stanford University|Stanford University|Stanford University,352,Generating Semantically Precise Scene Graphs from Textual Descriptions for Improved Image Retrieval,https://doi.org/10.18653/v1/w15-2812,11.89798758,FALSE,en,TRUE,gold,,,2015-01-01,article
https://openalex.org/W2077069816,,Justin Johnson|Ranjay Krishna|Michael Stark|Li-Jia Li|David A. Shamma|Michael S. Bernstein|Li Fei-Fei,Stanford University|Stanford University|Max Planck Institute for Informatics|Yahoo (United States)|Snap (United States)|Yahoo (United States)|Stanford University|Stanford University,1093,Image retrieval using scene graphs,https://doi.org/10.1109/cvpr.2015.7298990,37.57259236,FALSE,en,FALSE,closed,,,2015-06-01,article
https://openalex.org/W2201056710,,Satoshi Ikehata|Hang Yang|Yasutaka Furukawa,Washington University in St. Louis|Washington University in St. Louis|Washington University in St. Louis,153,Structured Indoor Modeling,https://doi.org/10.1109/iccv.2015.156,10.01935796,FALSE,en,FALSE,closed,,,2015-12-01,article
https://openalex.org/W2576665611,,Zhiqiang Sui|Lingzhu Xiang|Odest Chadwicke Jenkins|Karthik Desingh,University of Michigan–Ann Arbor|University of Toronto|Institute for Christian Studies|University of Michigan–Ann Arbor|University of Michigan–Ann Arbor,35,Goal-directed robot manipulation through axiomatic scene estimation,https://doi.org/10.1177/0278364916683444,3.59686679,FALSE,en,FALSE,closed,The International Journal of Robotics Research,journal,2017-01-01,article
https://openalex.org/W2763231886,,Fei Fang|Miao Yi|Hui Feng|Shenghong Hu|Chunxia Xiao,Wuhan University|Wuhan University|Wuhan University|Wuhan University|Wuhan University,17,Narrative Collage of Image Collections by Scene Graph Recombination,https://doi.org/10.1109/tvcg.2017.2759265,0.63555111,FALSE,en,FALSE,closed,IEEE Transactions on Visualization and Computer Graphics,journal,2017-10-04,article
https://openalex.org/W2790781626,,Xinru Wei|Yonggang Qi|Jun Liu|Fang Liu,Beijing University of Posts and Telecommunications|Beijing University of Posts and Telecommunications|Beijing University of Posts and Telecommunications|Beijing University of Posts and Telecommunications,14,Image retrieval by dense caption reasoning,https://doi.org/10.1109/vcip.2017.8305157,0.50844089,FALSE,en,FALSE,closed,,,2017-12-01,article
https://openalex.org/W2958256323,"In modern industrial environments, human-robot collaboration is a trend in automation to improve performance and productivity. Instead of isolating robot from human to guarantee safety, collaborative robotics allows human and robot working in the same area at the same time. New hazards and risks, such as the collision between robot and human, arise in this situation. Safety analysis is necessary to protect both human and robot when using a collaborative robot.To perform safety analysis, robots need to perceive the surrounding environment in realtime. This surrounding environment is perceived and stored in the form of scene graph, which is a direct graph with semantic representation of the environment, the relationship between the detected objects and properties of these objects. In order to generate the scene graph, a simulated warehouse is used: robots and humans work in a common area for transferring products between shelves and conveyor belts. Each robot generates its own scene graph from the attached camera sensor. In the graph, each detected object is represented by a node and edges are used to denote the relationship among the identified objects. The graph node includes values like velocity, bounding box sizes, orientation, distance and directions between the object and the robot.We generate scene graph in a simulated warehouse scenario with the frequency of 7 Hz and present a study of Mask R-CNN based on the qualitative comparison. Mask R-CNN is a method for object instance segmentation to get the properties of the objects. It uses ResNetFPN for feature extraction and adds a branch to Faster R-CNN for predicting segmentation mask for each object. And its results outperform almost all existing, single-model entries on instance segmentation and bounding-box object detection. With the help of this method, the boundaries of the detected object are extracted from the camera images. We initialize Mask R-CNN model using three different types of weights: COCO pre-trained weight, ImageNet pre-trained weight and random weight, and the results of these three different weights are compared w.r.t. precision and recall.Results showed that Mask R-CNN is also suitable for simulated environments and can meet requirements in both detection precision and speed. Moreover, the model trained used the COCO pre-trained weight outperformed the model with ImageNet and randomly assigned initial weights. The calculated Mean Average Precision (mAP) value for validation dataset reaches 0.949 with COCO pre-trained weights and execution speed of 11.35 fps.",Shaolei Wang,,0,Scene Recognition for Safety Analysis in Collaborative Robotics,,0.0,FALSE,en,TRUE,green,KTH Publication Database DiVA (KTH Royal Institute of Technology),repository,2018-01-01,article
https://openalex.org/W2893908566,"We present a new methodology to teach spatial augmented reality in a practical assignment to large audiences. Our approach does not require specific equipment such as video projectors while teaching the principal topics and difficulties involved in spatial augmented reality applications, and especially calibration and tracking. The key idea is to set up a scene graph consisting of a 3D scene with a simulated projector that ""projects"" content onto a virtual representation of the real-world object. For illustrating the calibration, we simplify the intrinsic parameters to using the field of view, both for the camera and the projector. For illustrating the tracking, instead of relying on specific hardware or software, we exploit the relative transformations in the scene graph. We implemented our teaching methodology in Unity3D and tested it within a three-hour assignment to 24 and 20 master-level students in two consecutive years. We show the positive feedback that we received and discuss our plans for further improvement.",Brett Ridel|Patrick Reuter|Nadine Couture,Laboratoire Bordelais de Recherche en Informatique|École Supérieure des Technologies Industrielles Avancées|École Supérieure des Technologies Industrielles Avancées,1,Teaching Spatial Augmented Reality: a Practical Assignment for Large Audiences,https://doi.org/10.2312/eged.20181004,,FALSE,en,TRUE,gold,,,2018-01-01,preprint
https://openalex.org/W2828129428,,Taliesin L. Smith|Jesse Greenberg|Sam Reid|Emily B. Moore,University of Colorado Boulder|University of Colorado Boulder|University of Colorado Boulder|University of Colorado Boulder,11,Parallel DOM Architecture for Accessible Interactive Simulations,https://doi.org/10.1145/3192714.3192817,2.7482865,FALSE,en,FALSE,closed,,,2018-04-23,article
https://openalex.org/W2804009377,,Rui Li|Xiaodong Zhang|Hanzhe Li|Liming Zhang|Zhufeng Lu|Jiangcheng Chen,Xi'an Jiaotong University|Xi'an Jiaotong University|Xi'an Jiaotong University|Xi'an Jiaotong University|Xi'an Jiaotong University|University of Hong Kong,19,An approach for brain-controlled prostheses based on Scene Graph Steady-State Visual Evoked Potentials,https://doi.org/10.1016/j.brainres.2018.05.018,1.88217262,FALSE,en,FALSE,closed,Brain Research,journal,2018-05-16,article
https://openalex.org/W2889569452,,Jikai Wang|Zonghai Chen,University of Science and Technology of China|University of Science and Technology of China,15,A Novel Hybrid Map Based Global Path Planning Method,https://doi.org/10.1109/acirs.2018.8467225,1.15506032,FALSE,en,FALSE,closed,,,2018-07-01,article
https://openalex.org/W2808399177,"This paper focuses on scene graph completion which aims at predicting new relations between two entities utilizing existing scene graphs and images. By comparing with the well-known knowledge graph, we first identify that each scene graph is associated with an image and each entity of a visual triple in a scene graph is composed of its entity type with attributes and grounded with a bounding box in its corresponding image. We then propose an end-to-end model named Representation Learning via Jointly Structural and Visual Embedding (RLSV) to take advantages of structural and visual information in scene graphs. In RLSV model, we provide a fully-convolutional module to extract the visual embeddings of a visual triple and apply hierarchical projection to combine the structural and visual embeddings of a visual triple. In experiments, we evaluate our model on two scene graph completion tasks: link prediction and visual triple classification, and further analyze by case studies. Experimental results demonstrate that our model outperforms all baselines in both tasks, which justifies the significance of combining structural and visual information for scene graph completion.",Hai Wan|Yonghao Luo|Bo Peng|Wei‐Shi Zheng,China Guangzhou Analysis and Testing Center|Sun Yat-sen University|Sun Yat-sen University|Sun Yat-sen University|Sun Yat-sen University,33,Representation Learning for Scene Graph Completion via Jointly Structural and Visual Embedding,https://doi.org/10.24963/ijcai.2018/132,4.3683741,FALSE,en,TRUE,gold,,,2018-07-01,article
https://openalex.org/W2899483913,,Rafia Inam|Klaus Raizer|Alberto Hata|Ricardo Gabbay de Souza|Elena Forsman|Enyu Cao|Shaolei Wang,Ericsson (Sweden)|Ericsson (Sweden)|Ericsson (Sweden)|Ericsson (Sweden)|Ericsson (Sweden)|KTH Royal Institute of Technology|KTH Royal Institute of Technology,67,Risk Assessment for Human-Robot Collaboration in an automated warehouse scenario,https://doi.org/10.1109/etfa.2018.8502466,2.66908417,FALSE,en,FALSE,closed,,,2018-09-01,article
https://openalex.org/W2910766642,"We introduce UNDERWORLDS, a novel lightweight framework for cascading spatio-temporal situation assessment in robotics. UNDERWORLDS allows programmers to represent the robot's environment as real-time distributed data structures, containing both scene graphs (for representation of 3D geometries) and timelines (for representation of temporal events). UNDERWORLDS supports cascading representations: the environment is viewed as a set of worlds that can each have different spatial and temporal granularities, and may inherit from each other. UNDERWORLDS also provides a set of high-level client libraries and tools to introspect and manipulate the environment models.\n\nThis article presents the design and architecture of this open-source tool, and explores some applications, along with examples of use.",Séverin Lemaignan|Yoan Sallami|Christopher D. Wallbridge|Aurélie Clodic|Tony Belpaeme|Rachid Alami,University of the West of England|Bristol Robotics Laboratory|Laboratoire d'Analyse et d'Architecture des Systèmes|Centre National de la Recherche Scientifique|Université Fédérale de Toulouse Midi-Pyrénées|University of Plymouth|Centre National de la Recherche Scientifique|Université Fédérale de Toulouse Midi-Pyrénées|Laboratoire d'Analyse et d'Architecture des Systèmes|University of Plymouth|Université Fédérale de Toulouse Midi-Pyrénées|Laboratoire d'Analyse et d'Architecture des Systèmes|Centre National de la Recherche Scientifique,15,UNDERWORLDS: Cascading Situation Assessment for Robots,https://doi.org/10.1109/iros.2018.8594094,4.12435992,FALSE,en,TRUE,green,,,2018-10-01,preprint
https://openalex.org/W2907562548,,Valber Lemes Zacarkim|Eduardo Todt|Felipe Gustavo Bombardelli,Universidade Federal do Paraná|Universidade Federal do Paraná|Universidade Federal do Paraná,2,Evaluation of IGFTT Keypoints Detector in Indoor Visual SLAM,https://doi.org/10.1109/lars/sbr/wre.2018.00025,0.37494181,FALSE,en,FALSE,closed,,,2018-11-01,article
https://openalex.org/W2903754102,,Lars Kunze|Tom Bruls|Tarlan Suleymanov|Paul Newman,University of Oxford|University of Oxford|University of Oxford|University of Oxford,36,Reading between the Lanes: Road Layout Reconstruction from Partially Segmented Scenes,https://doi.org/10.1109/itsc.2018.8569270,3.51500443,FALSE,en,FALSE,closed,,,2018-11-01,article
https://openalex.org/W2902539442,"We introduce a novel framework for using natural language to generate and edit 3D indoor scenes, harnessing scene semantics and text-scene grounding knowledge learned from large annotated 3D scene databases. The advantage of natural language editing interfaces is strongest when performing semantic operations at the sub-scene level, acting on groups of objects. We learn how to manipulate these sub-scenes by analyzing existing 3D scenes. We perform edits by first parsing a natural language command from the user and transforming it into a semantic scene graph that is used to retrieve corresponding sub-scenes from the databases that match the command. We then augment this retrieved sub-scene by incorporating other objects that may be implied by the scene context. Finally, a new 3D scene is synthesized by aligning the augmented sub-scene with the user's current scene, where new objects are spliced into the environment, possibly triggering appropriate adjustments to the existing scene arrangement. A suggestive modeling interface with multiple interpretations of user commands is used to alleviate ambiguities in natural language. We conduct studies comparing our approach against both prior text-to-scene work and artist-made scenes and find that our method significantly outperforms prior work and is comparable to handmade scenes even when complex and varied natural sentences are used.",Rui Ma|Akshay Gadi Patil|Matthew Fisher|Manyi Li|Sören Pirk|Binh‐Son Hua|Sai-Kit Yeung|Xin Tong|Leonidas Guibas|Hao Zhang,|Simon Fraser University|Adobe Systems (United States)|Shandong University|Simon Fraser University|Stanford University|The University of Tokyo|Hong Kong University of Science and Technology|University of Hong Kong|Microsoft Research Asia (China)|Stanford University|Simon Fraser University,94,Language-driven synthesis of 3D scenes from scene databases,https://doi.org/10.1145/3272127.3275035,5.4865365,FALSE,en,TRUE,bronze,ACM Transactions on Graphics,journal,2018-11-28,article
https://openalex.org/W2965288565,"We propose Scene Graph Auto-Encoder (SGAE) that incorporates the language inductive bias into the encoder-decoder image captioning framework for more human-like captions. Intuitively, we humans use the inductive bias to compose collocations and contextual inference in discourse. For example, when we see the relation `person on bike', it is natural to replace `on' with `ride' and infer `person riding bike on a road' even the `road' is not evident. Therefore, exploiting such bias as a language prior is expected to help the conventional encoder-decoder models less likely overfit to the dataset bias and focus on reasoning. Specifically, we use the scene graph --- a directed graph ($\mathcal{G}$) where an object node is connected by adjective nodes and relationship nodes --- to represent the complex structural layout of both image ($\mathcal{I}$) and sentence ($\mathcal{S}$). In the textual domain, we use SGAE to learn a dictionary ($\mathcal{D}$) that helps to reconstruct sentences in the $\mathcal{S}\rightarrow \mathcal{G} \rightarrow \mathcal{D} \rightarrow \mathcal{S}$ pipeline, where $\mathcal{D}$ encodes the desired language prior; in the vision-language domain, we use the shared $\mathcal{D}$ to guide the encoder-decoder in the $\mathcal{I}\rightarrow \mathcal{G}\rightarrow \mathcal{D} \rightarrow \mathcal{S}$ pipeline. Thanks to the scene graph representation and shared dictionary, the inductive bias is transferred across domains in principle. We validate the effectiveness of SGAE on the challenging MS-COCO image captioning benchmark, e.g., our SGAE-based single-model achieves a new state-of-the-art $127.8$ CIDEr-D on the Karpathy split, and a competitive $125.5$ CIDEr-D (c40) on the official server even compared to other ensemble models.",Xu Yang|Kaihua Tang|Hanwang Zhang|Jianfei Cai,,2,Auto-Encoding Graphical Inductive Bias for Descriptive Image Captioning,,0.28876508,FALSE,en,TRUE,green,arXiv (Cornell University),repository,2018-12-06,article
https://openalex.org/W2902899766,"Scene graphs -- objects as nodes and visual relationships as edges -- describe the whereabouts and interactions of the things and stuff in an image for comprehensive scene understanding. To generate coherent scene graphs, almost all existing methods exploit the fruitful visual context by modeling message passing among objects, fitting the dynamic nature of reasoning with visual context, eg, person on bike can help to determine the relationship ride, which in turn contributes to the category confidence of the two objects. However, we argue that the scene dynamics is not properly learned by using the prevailing cross-entropy based supervised learning paradigm, which is not sensitive to graph inconsistency: errors at the hub or non-hub nodes are unfortunately penalized equally. To this end, we propose a Counterfactual critic Multi-Agent Training (CMAT) approach to resolve the mismatch. CMAT is a multi-agent policy gradient method that frames objects as cooperative agents, and then directly maximizes a graph-level metric as the reward. In particular, to assign the reward properly to each agent, CMAT uses a counterfactual baseline that disentangles the agent-specific reward by fixing the dynamics of other agents. Extensive validations on the challenging Visual Genome benchmark show that CMAT achieves a state-of-the-art by significant performance gains under various settings and metrics.",Long Chen|Hanwang Zhang|Jun Xiao|Xiangnan He|Shiliang Pu|Shih-Fu Chang,,12,Scene Dynamics: Counterfactual Critic Multi-Agent Training for Scene Graph Generation.,,1.58820793,FALSE,en,TRUE,green,arXiv (Cornell University),repository,2018-12-06,article
https://openalex.org/W2904763822,"Generation of scene graphs and natural language captions from images for deep image understanding is an ongoing research problem. Scene graphs and natural language captions have a common characteristic in that they are generated by considering the objects in the images and the relationships between the objects. This study proposes a deep neural network model named the Context-based Captioning and Scene Graph Generation Network (C2SGNet), which simultaneously generates scene graphs and natural language captions from images. The proposed model generates results through communication of context information between these two tasks. For effective communication of context information, the two tasks are structured into three layers: the object detection, relationship detection, and caption generation layers. Each layer receives related context information from the lower layer. In this study, the proposed model was experimentally assessed using the Visual Genome benchmark data set. The performance improvement effect of the context information was verified through various experiments. Further, the high performance of the proposed model was confirmed through performance comparison with existing models.",Donghyeop Shin|Incheol Kim,Kyonggi University|Kyonggi University,8,Deep Image Understanding Using Multilayered Contexts,https://doi.org/10.1155/2018/5847460,0.7219127,FALSE,en,TRUE,hybrid,Mathematical Problems in Engineering,journal,2018-12-10,article
https://openalex.org/W2904993015,,Ning Xu|An-An Liu|Jing Liu|Weizhi Nie|Yuting Su,Tianjin University|Tianjin University|Tianjin University|Tianjin University|Tianjin University,93,Scene graph captioner: Image captioning based on structural visual representation,https://doi.org/10.1016/j.jvcir.2018.12.027,6.2084492,FALSE,en,FALSE,closed,Journal of Visual Communication and Image Representation,journal,2018-12-14,article
https://openalex.org/W2921465855,"Scene graphs, as found in many visualization systems are a well-established concept for modeling virtual scenes in computer graphics. State-of-the-art approaches typically issue appropriate draw commands while traversing the graph. Equipped with a functional programming mindset we take a different approach and utilize attribute grammars as a central concept for modeling the problem domain declaratively. Instead of issuing draw commands imperatively, we synthesize first class objects describing appropriate draw commands. In order to make this approach practical in the face of dynamic changes to the scene graph, we utilize incremental evaluation, and thereby avoid repeated evaluation of unchanged parts. Our application prototypically demonstrates how complex systems benefit from domain-specific languages, declarative problem solving and the implications thereof. Besides from being concise and expressive, our solution demonstrates a real-world use case of self-adjusting computation which elegantly extends scene graphs with well-defined reactive semantics and efficient, incremental execution.",Harald Steinlechner|Georg Haaser|Stefan Maierhofer|Robert F. Tobler,VRVis GmbH (Austria)|VRVis GmbH (Austria)|VRVis GmbH (Austria)|VRVis GmbH (Austria),2,Attribute Grammars for Incremental Scene Graph Rendering,https://doi.org/10.5220/0007372800770088,0.21377151,FALSE,en,TRUE,gold,,,2019-01-01,article
https://openalex.org/W2930097295,,Moshiko Raboh|Roei Herzig|Gal Chechik|Jonathan Berant|Amir Globerson,,2,Learning Latent Scene-Graph Representations for Referring Relationships.,,0.30723549,FALSE,en,FALSE,closed,,,2019-01-01,article
https://openalex.org/W2989377923,"Scene graphs represent semantic information in images, which can help image captioning system to produce more descriptive outputs versus using only the image as context. Recent captioning approaches rely on ad-hoc approaches to obtain graphs for images. However, those graphs introduce noise and it is unclear the effect of parser errors on captioning accuracy. In this work, we investigate to what extent scene graphs can help image captioning. Our results show that a state-of-the-art scene graph parser can boost performance almost as much as the ground truth graphs, showing that the bottleneck currently resides more on the captioning models than on the performance of the scene graph parser.",Dalin Wang|Daniel Beck|Trevor Cohn,,29,On the Role of Scene Graphs in Image Captioning,https://doi.org/10.18653/v1/d19-6405,1.60328636,FALSE,en,TRUE,gold,,,2019-01-01,article
https://openalex.org/W3119360229,,Lateef Idan Subuh,,0,Placement of digital technology in the production of Scene graph Tarzan as a- symbol –,,0.0,FALSE,en,FALSE,closed,"LARK  JOURNAL FOR  PHILOSOPHY , LINGUISTICS AND SOCIAL SCIENCES",journal,2019-01-01,article
https://openalex.org/W3048317568,"This report covers a master’s thesis project done at the University Of Utah for theOpenSpace project. OpenSpace is a open-source astronomy visualization software and thethesis focus was to visualize the ever-increasing number of man-made space debris. Twodifferent visualization methods have been used in this thesis. One was a volume renderingand it was evaluated how it works in relation to an orbital trail representation, which wasthe other method. If the volumetric representation would reduce cluttering, is one of theaspects that will be evaluated, as well as a more open ended exploratory question whichis if the volumetric representation can provide any new insights about the data. In short,will a volumetric representation give anything that an orbital representation cannot? Avolume rendering can use different types of grids. The thesis evaluates the pros and consof a cartesian- and spherical grid, as well as the different resolution of the grid and tweaksin the transfer function.An orbital trail representation was previously implemented in OpenSpace (which will becalled the individual scene graph node implementation in this report) that had its pros.One con, however, was that it did not scale very well with increasing number of data elements.Visualizing all the data sets containing each trackable piece of space debris simultaneouslyusing this implementation causes the software to slow down significantly. Analternative implementation (which will be called single draw call implementation in thisreport) was therefore tested in hopes to solve this issue. To see the performance difference,tests were performed where frame time for the whole scene was measured.",Jonathan Fransson|Elon Olsson,,0,Visualization of Space Debris using Orbital Representation and Volume Rendering,,0.0,FALSE,en,TRUE,green,KTH Publication Database DiVA (KTH Royal Institute of Technology),repository,2019-01-01,article
https://openalex.org/W2913618459,,Xiangyang Li|Shuqiang Jiang,University of Chinese Academy of Sciences|University of Chinese Academy of Sciences,193,Know More Say Less: Image Captioning Based on Scene Graphs,https://doi.org/10.1109/tmm.2019.2896516,13.04006239,FALSE,en,FALSE,closed,IEEE Transactions on Multimedia,journal,2019-01-30,article
https://openalex.org/W2922136209,,Ji Zhang|Kevin J. Shih|Ahmed Elgammal|Andrew Tao|Bryan Catanzaro,,25,Graphical Contrastive Losses for Scene Graph Generation.,,2.35148666,FALSE,en,TRUE,green,arXiv (Cornell University),repository,2019-03-07,article
https://openalex.org/W2953783113,,Subarna Tripathi|Anahita Bhiwandiwalla|Alexei Bastidas|Hanlin Tang,,13,Heuristics for Image Generation from Scene Graphs,,1.38951484,FALSE,en,FALSE,closed,,,2019-03-20,article
https://openalex.org/W2936250137,,Wen-Jing Gao|Yonghua Zhu|Wenjun Zhang|Ke Zhang|Honghao Gao,Shanghai Open University|Shanghai University|Shanghai Open University|Shanghai University|Shanghai University|Shanghai Open University|Shanghai University|Shanghai Open University|Shanghai University|Shanghai Institute of Computing Technology,18,A hierarchical recurrent approach to predict scene graphs from a visual‐attention‐oriented perspective,https://doi.org/10.1111/coin.12202,1.38951484,FALSE,en,FALSE,closed,Computational Intelligence,journal,2019-03-22,article
https://openalex.org/W2970039170,,Lili Yu|Yunming Du|Xiaoguang Su|Fanbo Meng,Jiamusi University|Jiamusi University|Jiamusi University|Jiamusi University,0,Computation of Terrestrial Visible Area in Three-Dimensional GIS,https://doi.org/10.1109/iccar.2019.8813336,0.0,FALSE,en,FALSE,closed,,,2019-04-01,article
https://openalex.org/W2930983013,"Scene graph construction / visual relationship detection from an image aims to give a precise structural description of the objects (nodes) and their relationships (edges). The mutual promotion of object detection and relationship detection is important for enhancing their individual performance. In this work, we propose a new framework, called semantics guided graph relation neural network (SGRN), for effective visual relationship detection. First, to boost the object detection accuracy, we introduce a source-target class cognoscitive transformation that transforms the features of the co-occurent objects to the target object domain to refine the visual features. Similarly, source-target cognoscitive transformations are used to refine features of objects from features of relations, and vice versa. Second, to boost the relation detection accuracy, besides the visual features of the paired objects, we embed the class probability of the object and subject separately to provide high level semantic information. In addition, to reduce the search space of relationships, we design a semantics-aware relationship filter to exclude those object pairs that have no relation. We evaluate our approach on the Visual Genome dataset and it achieves the state-of-the-art performance for visual relationship detection. Additionally, Our approach also significantly improves the object detection performance (i.e. 4.2\% in mAP accuracy).",Wentong Liao|Cuiling Lan|Wenjun Zeng|Michael Ying Yang|Bodo Rosenhahn,,5,Exploring the Semantics for Visual Relationship Detection.,,0.42754303,FALSE,en,TRUE,green,arXiv (Cornell University),repository,2019-04-03,article
https://openalex.org/W2954469985,,Xiaolan Xie|Jianwei Wang|Hanlin Qin|Xiaochun Cheng,Guilin University of Technology|Guilin University of Technology|Guilin University of Technology|Middlesex University,2,The Simulation and Research of Fire Spread Situation Based on OSG,https://doi.org/10.1145/3335656.3335703,0.10688576,FALSE,en,FALSE,closed,,,2019-04-28,article
https://openalex.org/W2990821859,,Zhendong Li|Gaoyun An|Songhe Feng|Qiuqi Ruan,Beijing Jiaotong University|Beijing Jiaotong University|Beijing Jiaotong University|Beijing Jiaotong University,1,Dual Attention Message Passing Model for Scene Graph Generation,https://doi.org/10.1109/ddcls.2019.8908830,0.0,FALSE,en,FALSE,closed,2019 IEEE 8th Data Driven Control and Learning Systems Conference (DDCLS),conference,2019-05-01,article
https://openalex.org/W2979481854,,Phillip E. Pope|Soheil Kolouri|Mohammad Rostami|Charles E. Martin|H. Hoffmann,HRL Laboratories (United States)|HRL Laboratories (United States)|HRL Laboratories (United States)|HRL Laboratories (United States)|HRL Laboratories (United States),483,Explainability Methods for Graph Convolutional Neural Networks,https://doi.org/10.1109/cvpr.2019.01103,28.72651841,FALSE,en,FALSE,closed,,,2019-06-01,article
https://openalex.org/W2955988340,,Wenbin Wang|Ruiping Wang|Shiguang Shan|Xilin Chen,Institute of Computing Technology|University of Chinese Academy of Sciences|Chinese Academy of Sciences|Institute of Computing Technology|University of Chinese Academy of Sciences|Chinese Academy of Sciences|Chinese Academy of Sciences|University of Chinese Academy of Sciences|Institute of Computing Technology|Peng Cheng Laboratory|Institute of Computing Technology|Chinese Academy of Sciences|University of Chinese Academy of Sciences,99,Exploring Context and Visual Pattern of Relationship for Scene Graph Generation,https://doi.org/10.1109/cvpr.2019.00838,7.16134574,FALSE,en,FALSE,closed,,,2019-06-01,article
https://openalex.org/W2966369713,,Monica Haurilet|Alina Roitberg|Rainer Stiefelhagen,Kentucky imaging Technologies (United States)|Karlsruhe Institute of Technology|Kentucky imaging Technologies (United States)|Karlsruhe Institute of Technology|Karlsruhe Institute of Technology,18,It's Not About the Journey; It's About the Destination: Following Soft Paths Under Question-Guidance for Visual Reasoning,https://doi.org/10.1109/cvpr.2019.00203,1.38951484,FALSE,en,FALSE,closed,,,2019-06-01,article
https://openalex.org/W3022778813,,Mengshi Qi|Yunhong Wang|Annan Li|Jiebo Luo,Beihang University|Beihang University|Beihang University|University of Rochester,73,Sports Video Captioning via Attentive Motion Representation and Group Relationship Modeling,https://doi.org/10.1109/tcsvt.2019.2921655,3.95477302,FALSE,en,FALSE,closed,IEEE Transactions on Circuits and Systems for Video Technology,journal,2019-06-07,article
https://openalex.org/W2948087948,"We focus on task of grounding referring expressions images, e.g., localizing the white truck front of a one. To resolve this task fundamentally, one should first find out contextual objects (e.g., yellow truck) and then exploit them to disambiguate referent from other similar objects, by using attributes and relationships (e.g., white, yellow, in front of). However, it is extremely challenging to train such a model as ground-truth of contextual objects and their relationships are usually missing due to prohibitive annotation cost. Therefore, nearly all existing methods attempt to evade above joint grounding and reasoning process, but resort to a holistic association between sentence and region feature. As a result, they suffer from heavy parameters of fully-connected layers, poor interpretability, and limited generalization to unseen expressions. In this paper, we tackle this challenge by training and inference with proposed Marginalized Scene Graph Likelihood (MSGL). Specifically, we use scene graph: a graphical representation parsed from referring expression, where nodes are objects with attributes and edges are relationships. Thanks to conditional random field (CRF) built on scene graph, we can ground every object to its corresponding region, and perform reasoning with unlabeled contexts by marginalizing out them using sum-product belief propagation. Overall, our proposed MSGL is effective and interpretable, e.g., on three benchmarks, MSGL consistently outperforms state-of-the-arts while offers a complete grounding of all objects a sentence.",Daqing Liu|Hanwang Zhang|Zheng-Jun Zha|Fanglin Wang,,7,Referring Expression Grounding by Marginalizing Scene Graph Likelihood,,,FALSE,en,TRUE,green,arXiv (Cornell University),repository,2019-06-09,preprint
https://openalex.org/W2950601690,"The proliferation of high resolution cameras on embedded devices along with the growing maturity of deep neural networks (DNNs) has spawned powerful mobile vision applications. To enable applications on mobile devices, the offloading approach processes live video streams using DNNs on server-class GPU accelerators. However, their use in latency constrained applications is particularly challenging because of the large and unpredictable round-trip latency from mobile devices to the cloud computing resources. As a consequence, system designers routinely look for ways to offload to local servers at the cloud edge, known as the cloudlet. This paper explores the potential of serving multiple DNNs using the cloudlet model to implement complex vision applications on mobile devices. We present DeepQuery, a new mobile offloading system that is capable to serve DNNs with different structures for a wide range of tasks including object detection and tracking, scene graph detection, and video description. DeepQuery provides application programming interfaces to offload applications programed as Directed Acyclic Graphs of DNN queries, and employs data parallelization and input batching techniques to reduce processing delays. To improve GPU utilization, it co-locates real-time and delay-tolerant tasks on shared GPUs, and exploits a predictive and plan-ahead approach to alleviate resource contention caused by co-locating. We evaluate DeepQuery and demonstrate its effectiveness using several real world applications.",Zhou Fang|Dezhi Hong|Rajesh K. Gupta,"University of California, San Diego|University of California, San Diego|University of California, San Diego",36,Serving deep neural networks at the cloud edge for vision applications on mobile platforms,https://doi.org/10.1145/3304109.3306221,2.13771515,FALSE,en,TRUE,gold,,,2019-06-18,article
https://openalex.org/W2966204614,,Yunian Chen|Yanjie Wang|Yang Zhang|Yanwen Guo,Nanjing University|Zhejiang University of Science and Technology|Nanjing University|Nanjing University,25,PANet: A Context Based Predicate Association Network for Scene Graph Generation,https://doi.org/10.1109/icme.2019.00094,1.17574333,FALSE,en,FALSE,closed,,,2019-07-01,article
https://openalex.org/W2963393614,,Eath Manith|Sokchomrern Ean|Kwan‐Hee Yoo,|Chungbuk National University|Chungbuk National University,0,X3D Modeling and Animation for Human Respiratory Organ,https://doi.org/10.1145/3329714.3338139,0.0,FALSE,en,FALSE,closed,,,2019-07-22,article
https://openalex.org/W2963247770,,Jun Yu|Yezhou Yang|Fionn Murtagh|Xinbo Gao,Hangzhou Dianzi University|Arizona State University|University of Huddersfield|Xidian University,2,Fine-grained visual understanding and reasoning,https://doi.org/10.1016/j.neucom.2019.07.055,0.38832944,FALSE,en,FALSE,closed,Neurocomputing,journal,2019-07-25,article
https://openalex.org/W2964727037,"Image-text matching is a vital cross-modality task in artificial intelligence and has attracted increasing attention in recent years. Existing works have shown that learning semantic concepts is useful to enhance image representation and can significantly improve the performance of both image-to-text and text-to-image retrieval. However, existing models simply detect semantic concepts from a given image, which are less likely to deal with long-tail and occlusion concepts. Frequently co-occurred concepts in the same scene, e.g. bedroom and bed, can provide common-sense knowledge to discover other semantic-related concepts. In this paper, we develop a Scene Concept Graph (SCG) by aggregating image scene graphs and extracting frequently co-occurred concept pairs as scene common-sense knowledge. Moreover, we propose a novel model to incorporate this knowledge to improve image-text matching. Specifically, semantic concepts are detected from images and then expanded by the SCG. After learning to select relevant contextual concepts, we fuse their representations with the image embedding feature to feed into the matching module. Extensive experiments are conducted on Flickr30K and MSCOCO datasets, and prove that our model achieves state-of-the-art results due to the effectiveness of incorporating the external SCG.",Botian Shi|Lei Ji|Pan Lu|Zhendong Niu|Nan Duan,"Beijing Institute of Technology|Institute of Computing Technology|Microsoft Research Asia (China)|University of California, Los Angeles|Beijing Institute of Technology|Microsoft Research Asia (China)",74,Knowledge Aware Semantic Concept Expansion for Image-Text Matching,https://doi.org/10.24963/ijcai.2019/720,3.95477302,FALSE,en,TRUE,gold,,,2019-07-28,article
https://openalex.org/W2965198885,"In this paper, we propose Continuous Graph Flow, a generative continuous flow based method that aims to model distributions of graph-structured complex data. The model is formulated as an ordinary differential equation system with shared and reusable functions that operate over the graph structure. This leads to a new type of neural graph message passing scheme that performs continuous message passing over time. This class of models offer several advantages: (1) modeling complex graphical distributions without rigid assumptions on the distributions; (2) not limited to modeling data of fixed dimensions and can generalize probability evaluation and data generation over unseen subset of variables; (3) the underlying continuous graph message passing process is reversible and memory-efficient. We demonstrate the effectiveness of our model on two generation tasks, namely, image puzzle generation, and layout generation from scene graphs. Compared to unstructured and structured latent-space VAE models, we show that our proposed model achieves significant performance improvement (up to 400% in negative log-likelihood).",Zhiwei Deng|Megha Nawhal|Lili Meng|Greg Mori,,1,Continuous Graph Flow for Flexible Density Estimation.,,,FALSE,en,TRUE,green,arXiv (Cornell University),repository,2019-08-07,preprint
https://openalex.org/W2965572476,"Visual Question Answering (VQA) is a challenging task for evaluating the ability of comprehensive understanding of the world. Existing benchmarks usually focus on the reasoning abilities either only on the vision or mainly on the knowledge with relatively simple abilities on vision. However, the ability of answering a question that requires alternatively inferring on the image content and the commonsense knowledge is crucial for an advanced VQA system. In this paper, we introduce a VQA dataset that provides more challenging and general questions about Compositional Reasoning on vIsion and Commonsense, which is named as CRIC. To create this dataset, we develop a powerful method to automatically generate compositional questions and rich annotations from both the scene graph of a given image and some external knowledge graph. Moreover, this paper presents a new compositional model that is capable of implementing various types of reasoning functions on the image content and the knowledge graph. Further, we analyze several baselines, state-of-the-art and our model on CRIC dataset. The experimental results show that the proposed task is challenging, where state-of-the-art obtains 52.26% accuracy and our model obtains 58.38%.",Difei Gao|Ruiping Wang|Shiguang Shan|Xilin Chen,,6,From Two Graphs to N Questions: A VQA Dataset for Compositional Reasoning on Vision and Commonsense.,,,FALSE,en,TRUE,green,arXiv (Cornell University),repository,2019-08-08,preprint
https://openalex.org/W2969091962,,Donghyeop Shin|Incheol Kim,,0,Modelling Dynamics of Indoor Environments with 3D Scene Graphs,https://doi.org/10.5302/j.icros.2019.19.0105,0.0,FALSE,en,FALSE,closed,Journal of Institute of Control Robotics and Systems,journal,2019-08-13,article
https://openalex.org/W2970216698,,Arces Talavera|Daniel Stanley Tan|Arnulfo P. Azcarraga|Kai‐Lung Hua,National Taiwan University of Science and Technology|National Taiwan University of Science and Technology|De La Salle University|National Taiwan University of Science and Technology,10,Layout and Context Understanding for Image Synthesis with Scene Graphs,https://doi.org/10.1109/icip.2019.8803182,0.64131454,FALSE,en,FALSE,closed,,,2019-08-26,article
https://openalex.org/W2970778297,,Rui Li|Dong Gong|Jinqiu Sun|Yu Zhu|Ziwei Wei|Yanning Zhang,Northwestern Polytechnical University|University of Adelaide|Northwestern Polytechnical University|Northwestern Polytechnical University|Northwestern Polytechnical University|Northwestern Polytechnical University,2,Robust and Accurate Hybrid Structure-From-Moti,https://doi.org/10.1109/icip.2019.8803814,0.21377151,FALSE,en,FALSE,closed,,,2019-08-26,article
https://openalex.org/W2980916669,,Alberto Hata|Rafia Inam|Klaus Raizer|Shaolei Wang|Enyu Cao,Ericsson (Sweden)|Ericsson (Sweden)|Ericsson (Sweden)|KTH Royal Institute of Technology|KTH Royal Institute of Technology,16,AI-based Safety Analysis for Collaborative Mobile Robots,https://doi.org/10.1109/etfa.2019.8869263,0.92170647,FALSE,en,FALSE,closed,,,2019-09-01,article
https://openalex.org/W3004010675,,Shreya Goyal|Vishesh Mistry|Chiranjoy Chattopadhyay|Gaurav Bhatnagar,Indian Institute of Technology Jodhpur|Indian Institute of Technology Jodhpur|Indian Institute of Technology Jodhpur|Indian Institute of Technology Jodhpur,26,"BRIDGE: Building Plan Repository for Image Description Generation, and Evaluation",https://doi.org/10.1109/icdar.2019.00174,0.96197182,FALSE,en,FALSE,closed,,,2019-09-01,article
https://openalex.org/W3010715592,,Soohyeong Lee|Ju-Whan Kim|Youngmin Oh|Joo Hyuk Jeon,Samsung (South Korea)|Samsung (South Korea)|Samsung (South Korea)|Samsung (South Korea),26,Visual Question Answering over Scene Graph,https://doi.org/10.1109/gc46384.2019.00015,1.28262909,FALSE,en,FALSE,closed,,,2019-09-01,article
https://openalex.org/W2972895044,,Nicola Messina|Giuseppe Amato|Fabio Carrara|Fabrizio Falchi|Claudio Gennaro,"Istituto di Scienza e Tecnologie dell'Informazione ""Alessandro Faedo""|Istituto di Scienza e Tecnologie dell'Informazione ""Alessandro Faedo""|Istituto di Scienza e Tecnologie dell'Informazione ""Alessandro Faedo""|Istituto di Scienza e Tecnologie dell'Informazione ""Alessandro Faedo""|Istituto di Scienza e Tecnologie dell'Informazione ""Alessandro Faedo""",15,Learning visual features for relational CBIR,https://doi.org/10.1007/s13735-019-00178-7,1.38951484,FALSE,en,FALSE,closed,International Journal of Multimedia Information Retrieval,journal,2019-09-14,article
https://openalex.org/W2988486847,,Siyu Yu|Lin Qi|Yun Tie,Zhengzhou University|Zhengzhou University|Zhengzhou University,1,A Calibration Algorithm for Real-time Scene-aware Portable Augmented Reality,https://doi.org/10.1109/isne.2019.8896496,0.0,FALSE,en,FALSE,closed,,,2019-10-01,article
https://openalex.org/W2992028798,,Catherine Potts|Liping Yang|Diane Oyen|Brendt Wohlberg,Montana State University|Los Alamos National Laboratory|Los Alamos National Laboratory|Los Alamos National Laboratory,3,A Topological Graph-Based Representation for Denoising Low Quality Binary Images,https://doi.org/10.1109/iccvw.2019.00222,0.64258355,FALSE,en,FALSE,closed,,,2019-10-01,article
https://openalex.org/W2992195701,,Nikolaos Gkanatsios|Vassilis Pitsikalis|Petros Koutras|Petros Maragos,National Technical University of Athens||National Technical University of Athens|National Technical University of Athens,29,Attention-Translation-Relation Network for Scalable Scene Graph Generation,https://doi.org/10.1109/iccvw.2019.00218,1.17574333,FALSE,en,FALSE,closed,,,2019-10-01,article
https://openalex.org/W2981750021,,Florin Brad,,3,Scene Graph Contextualization in Visual Commonsense Reasoning,https://doi.org/10.1109/iccvw.2019.00560,0.32065727,FALSE,en,FALSE,closed,,,2019-10-01,article
https://openalex.org/W2995775106,,Daniel Dorda|Moin Nabi,,0,SynthRel0: Towards a Diagnostic Dataset for Relational Representation Learning,https://doi.org/10.1109/iccvw.2019.00219,0.0,FALSE,en,FALSE,closed,,,2019-10-01,article
https://openalex.org/W3003443782,,Panida Khuphiran|Supasit Kajkamhaeng|Chantana Chantrapornchai,Kasetsart University|Kasetsart University|Kasetsart University,0,Thai Scene Graph Generation from Images and Applications,https://doi.org/10.1109/icsec47112.2019.8974667,0.0,FALSE,en,FALSE,closed,,,2019-10-01,article
https://openalex.org/W2998146745,,Oh Weon Geun|Jong-Gook Ko,Electronics and Telecommunications Research Institute|Electronics and Telecommunications Research Institute,1,Visual Narrative Technology of Paintings Based on Image Objects,https://doi.org/10.1109/ictc46691.2019.8939893,0.10688576,FALSE,en,FALSE,closed,,,2019-10-01,article
https://openalex.org/W2953365189,"Scene graph prediction --- classifying the set of objects and predicates in avisual scene --- requires substantial training data. The long-taileddistribution of relationships can be an obstacle for such approaches, however,as they can only be trained on the small set of predicates that carrysufficient labels. We introduce the first scene graph prediction model thatsupports few-shot learning of predicates, enabling scene graph approaches togeneralize to a set of new predicates. First, we introduce a new model ofpredicates as functions that operate on object features or image locations.Next, we define a scene graph model where these functions are trained asmessage passing protocols within a new graph convolution framework. We trainthe framework with a frequently occurring set of predicates and show that ourapproach outperforms those that use the same amount of supervision by 1.78 atrecall@50 and performs on par with other scene graph models. Next, we extractobject representations generated by the trained predicate functions to trainfew-shot predicate classifiers on rare predicates with as few as 1 labeledexample. When compared to strong baselines like transfer learning from existingstate-of-the-art representations, we show improved 5-shot performance by 4.16recall@1. Finally, we show that our predicate functions generate interpretablevisualizations, enabling the first interpretable scene graph model.",Apoorva Dornadula|Austin Narcomey|Ranjay Krishna|Michael S. Bernstein|Li Fei-Fei,Stanford University|Stanford University|Stanford University|Stanford University|Stanford University,6,Visual Relationships as Functions:Enabling Few-Shot Scene Graph Prediction,https://doi.org/10.1109/iccvw.2019.00214,0.53442879,FALSE,en,TRUE,green,,,2019-10-01,article
https://openalex.org/W2981384185,,Jiawei Liu|Zheng-Jun Zha|Richang Hong|Meng Wang|Yongdong Zhang,University of Science and Technology of China|University of Science and Technology of China|Hefei University of Technology|Hefei University of Technology|University of Science and Technology of China,110,Deep Adversarial Graph Attention Convolution Network for Text-Based Person Search,https://doi.org/10.1145/3343031.3350991,4.91674483,FALSE,en,FALSE,closed,,,2019-10-15,article
https://openalex.org/W2981519940,,Lejian Ren|Si Liu|Han Huang|Jizhong Han|Shuicheng Yan|Bo Li,|Beihang University|Beihang University|||Beihang University,0,Finding Images by Dialoguing with Image,https://doi.org/10.1145/3343031.3350907,0.0,FALSE,en,FALSE,closed,,,2019-10-15,article
https://openalex.org/W2989503266,,Xiaoqiang Teng|Deke Guo|Yulan Guo|Pengfei Xu|Yiping Meng|Ruibo Hu|Hua Chai|Zhong Liu,National University of Defense Technology|National University of Defense Technology|National University of Defense Technology|Didi Chuxing (China)|Didi Chuxing (China)|Didi Chuxing (China)|Didi Chuxing (China)|National University of Defense Technology,1,SeMap,https://doi.org/10.1145/3356994.3365500,0.11385237,FALSE,en,FALSE,closed,,,2019-11-01,article
https://openalex.org/W2987671777,,Yuxin Peng|Jingze Chi,Peking University|Peking University,63,Unsupervised Cross-Media Retrieval Using Domain Adaptation With Scene Graph,https://doi.org/10.1109/tcsvt.2019.2953692,3.95477302,FALSE,en,FALSE,closed,IEEE Transactions on Circuits and Systems for Video Technology,journal,2019-11-15,article
https://openalex.org/W3016082038,,Chaoquan Chen|Yong Yang|Xiaolan Xie,Guilin University of Technology|Guilin University of Technology|Guilin University of Technology,0,Smoke Efficiency Simulation Based on OSG and Particle System,https://doi.org/10.1145/3378065.3378144,0.0,FALSE,en,FALSE,closed,,,2019-11-16,article
https://openalex.org/W2990369976,"Abstract. Mobile Augmented Reality (MAR) aligns toward current technological advances with more intuitive interfaces, realistic graphic content and flexible development processes. The case of overlaying precise 3D representations exploits their high penetration to induct users to a world where data are perceived as real counterparts. The work presented in this paper integrates web-like concepts with hybrid mobile tools to visualize high-quality and complex 3D geometry on the real environment. The implementation involves two different operational mechanisms: anchors and location-sensitive tracking. Three scenarios, for indoors and outdoors are developed using open-source and with no limit on distribution SDKs, APIs and rendering engines. The JavaScript-driven prototype consolidates some of the overarching principles of AR, such as pose estimation, registration and 3D tracking to an interactive User Interface under the scene graph concept. The 3D overlays are shown to the end user i) on top of an image target ii) on real-world planar surfaces and iii) at predefined points of interest (POI). The evaluation in terms of performance, rendering efficacy and responsiveness is made through various testing strategies: system and trace logs, profiling and ‗end-to-end‖ tests. The final benchmarking elucidates the slow and computationally intensive procedures induced by the big data rendering and optimization patterns are proposed to mitigate the performance impact to the non-native technologies.",A.-M. Boutsi|Charalabos Ioannidis|Sofia Soile,National Technical University of Athens|National Technical University of Athens|National Technical University of Athens,2,HYBRID MOBILE AUGMENTED REALITY: WEB-LIKE CONCEPTS APPLIED TO HIGH RESOLUTION 3D OVERLAYS,https://doi.org/10.5194/isprs-archives-xlii-2-w17-85-2019,0.10688576,FALSE,en,TRUE,diamond,"The international archives of the photogrammetry, remote sensing and spatial information sciences/International archives of the photogrammetry, remote sensing and spatial information sciences",journal,2019-11-29,article
https://openalex.org/W3024031618,,Pengfei Xu|Xiaojun Chang|Ling Guo|Po-Yao Huang|Xiaojiang Chen|Alex Hauptmann,,33,A Survey of Scene Graph: Generation and Application,https://doi.org/10.13140/rg.2.2.11161.57446,2.51921563,FALSE,en,TRUE,gold,,,2020-01-01,article
https://openalex.org/W3110014757,,Xu Yang|Hanwang Zhang|Jianfei Cai,Nanyang Technological University|Nanyang Technological University|Monash University,66,Auto-encoding and Distilling Scene Graphs for Image Captioning,https://doi.org/10.1109/tpami.2020.3042192,3.98875808,FALSE,en,FALSE,closed,IEEE Transactions on Pattern Analysis and Machine Intelligence,journal,2020-01-01,article
https://openalex.org/W3101632758,"A problem in automatically generated stories for image sequences is that they use overly generic vocabulary and phrase structure and fail to match the distributional characteristics of human-generated text. We address this problem by introducing explicit representations for objects and their relations by extracting scene graphs from the images. Utilizing an embedding of this scene graph enables our model to more explicitly reason over objects and their relations during story generation, compared to the global features from an object classifier used in previous work. We apply metrics that account for the diversity of words and phrases of generated stories as well as for reference to narratively-salient image features and show that our approach outperforms previous systems. Our experiments also indicate that our models obtain competitive results on reference-based metrics.",Xudong Hong|Rakshith Shetty|Asad Sayeed|Khushboo Mehra|Vera Demberg|Bernt Schiele,||University of Gothenburg|Saarland University|Language Science (South Korea)|Language Science (South Korea)|Saarland University|,13,Diverse and Relevant Visual Storytelling with Scene Graph Embeddings,https://doi.org/10.18653/v1/2020.conll-1.34,1.1546405,FALSE,en,TRUE,gold,,,2020-01-01,article
https://openalex.org/W4403602825,,Hong Lan|Qinyi Liu,,0,Image generation from scene graph with graph attention network,https://doi.org/10.11834/jig.190515,0.0,FALSE,en,TRUE,bronze,Journal of Image and Graphics,journal,2020-01-01,article
https://openalex.org/W3103588210,"Convolutional neural networks have been successfully used in action recognition but are usually restricted to operate on Euclidean data, such as images. In recent years there has been an increase in research devoted towards finding a generalized model operating on non-Euclidean data (e.g graphs) and manipulation action recognition on graphs is still a very novel subject. In this thesis a novel graph based deep neural network is developed for predicting manipulation actions and reconstructing graphs from a lower space representation. The network is trained on two manipulation action datasets and uses their, respective, previous works on action prediction as a baseline. In addition, a modular perception pipeline is developed that takes RGBD images as input and outputs a scene graph, consisting of objects and their spatial relations, which can then be fed to the network to lead to online action prediction. The network manages to outperform both baselines when training for action prediction and achieves comparable results when trained in an end-to-end manner performing both action prediction and graph reconstruction, simultaneously. Furthermore, to test the scalability of our model, the network is tested with input graphs deriving from our scene graph generator where the subject is performing 7 different demonstrations of the learned action types in a new scene context with novel objects.",Dawid Ejdeholm|Jacob Harsten,,1,Manipulation Action Recognition and Reconstruction using a Deep Scene Graph Network,,0.10496732,FALSE,en,TRUE,green,Hogskolan Ihalmstad (Halmstad University),repository,2020-01-01,article
https://openalex.org/W3080703222,"Scene graph parsing aims at understanding an image as a graph where vertices are visual objects (potentially with attributes) and edges are visual relationships among objects. This task is commonly seen as an extension to the object detection task where objects are detected individually, while the former requires recognizing relationships between object pairs. Therefore, scene graphs are usually seen as a better semantic representation of images for visual reasoning. In thesis we start with an inherent issue lying in scene graph parsing: the unbearable quadratic complexity of relationship detection. We develop an efficient model that effectively reduces the complexity from quadratic down to quasi-linear and show clear superiority over intuitive and strong baselines. Then we introduce two salient issues that naturally occur in scene graphs: Ambiguity in the language dimension and ambiguity in the visual dimension. The first happens when the vocabulary of objects and relationships are significantly large, and the second happens when multiple vertices or edges in a scene graph are from the same category and confuse the model to recognize the correct relational pairing. We propose two models that tackle these two problems separately, where the first model utilizes learnable embeddings to handle the ambiguity in the language dimension, while the second adds three types of losses that we design to for the model to learn to discriminate correct instances against confusing and hard negative instances. At last, with an accurately parsed scene graph, we discuss the topic of using scene graphs as richer feature and deeper knowledge of the input visual signals for better visual-semantic cross-modal reasoning. We design and develop a model that follows such logic and apply it on the video story understanding task, which achieves satisfying advantage over strong baseline models. In summary, we claim that scene graphs can be accurately and efficiently obtained by our models, and that we can build a sophisticated system that employs scene graphs for more explicit and interpretable cross-modal understanding.",Ji Zhang,,0,Scene graph parsing and its application in cross-modal reasoning tasks,https://doi.org/10.7282/t3-ka2q-b984,0.0,FALSE,en,TRUE,green,Rutgers University Community Repository (Rutgers University),repository,2020-01-01,article
https://openalex.org/W3100683605,"This paper focuses on machine reading comprehension for narrative passages. Narrative passages usually describe a chain of events. When reading this kind of passage, humans tend to restore a scene according to the text with their prior knowledge, which helps them understand the passage comprehensively. Inspired by this behavior of humans, we propose a method to let the machine imagine a scene during reading narrative for better comprehension. Specifically, we build a scene graph by utilizing Atomic as the external knowledge and propose a novel Graph Dimensional-Iteration Network (GDIN) to encode the graph. We conduct experiments on the ROCStories, a dataset of Story Cloze Test (SCT), and CosmosQA, a dataset of multiple choice. Our method achieves state-of-the-art.",Zhixing Tian|Yuanzhe Zhang|Kang Liu|Jun Zhao|Yantao Jia|Zhicheng Sheng,University of Chinese Academy of Sciences|Institute of Automation|Chinese Academy of Sciences|Chinese Academy of Sciences|Institute of Automation|Chinese Academy of Sciences|University of Chinese Academy of Sciences|Institute of Automation|University of Chinese Academy of Sciences|Institute of Automation|Chinese Academy of Sciences|Huawei Technologies (China)|Huawei Technologies (China),13,Scene Restoring for Narrative Machine Reading Comprehension,https://doi.org/10.18653/v1/2020.emnlp-main.247,1.61545503,FALSE,en,TRUE,gold,,,2020-01-01,article
https://openalex.org/W3208423679,,Son Nguyen|Ozgur S. Oguz|Valentin N. Hartmann|Marc Toussaint,,5,Self-Supervised Learning of Scene-Graph Representations for Robotic Sequential Manipulation Planning,,2.6373468,FALSE,en,FALSE,closed,Conference on Robot Learning,conference,2020-01-01,article
https://openalex.org/W3034757175,"Lack of labeled training samples is a big challenge for hyperspectral image (HSI) classification. In recent years, cross-scene classification has become a new research topic. In cross-scene classification, two closely related HSI scenes are considered, one contains adequate labeled samples, namely source scene, while the other one contains only a few labeled samples, namely target scene. The goal of cross-scene classification is utilizing the labeled samples in source scene to benefit the classification in target scene. In most cases, different HSIs are imaged by different sensors, leading to different feature dimensions (numbers of bands) in different scenes. In this situation, heterogeneous transfer learning is demanded. In this article, we propose a heterogeneous transfer learning algorithm namely semisupervised dual-dictionary nonnegative matrix factorization (SS-DDNMF). SS-DDNMF consists of two contributions. 1) Dual-dictionary nonnegative matrix factorization (DDNMF): DDNMF trains two dictionaries for source and target scenes, respectively, aiming at projecting the source and target features to a shared low-dimensional subspace, eliminating the difference between feature spaces. In DDNMF, within-scene and cross-scene graphs are built to maintain the similarities between pixels. 2) Semisupervised learning for target scene: as the limited number of labeled pixels in target scene will affect the graph building of DDNMF, semisupervised learning is adopted in target scene. In details, superpixel segmentation is adopted to generate pseudolabels for some unlabeled pixels, thus more &#x201C;labeled&#x201D; pixels can be considered for building better graphs. The effectiveness of SS-DDNMF is verified by experiments on cross-scene HSIs.",Chen Hong|Minchao Ye|Lei Ling|Huijuan Lu|Yuntao Qian,China Jiliang University|China Jiliang University|ORCID|China Jiliang University|China Jiliang University|ORCID|Zhejiang University,29,Semisupervised Dual-Dictionary Learning for Heterogeneous Transfer Learning on Cross-Scene Hyperspectral Images,https://doi.org/10.1109/jstars.2020.3000677,3.79124013,FALSE,en,TRUE,gold,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,journal,2020-01-01,article
https://openalex.org/W4387860213,"Traditionally, architects express their thoughts on the design of 3D architectural forms via perspective renderings and standardized 2D drawings.However, as architectural design is always multidimensional and intricate, it is difficult to make others understand the design intention, concrete form, and even spatial layout through simple language descriptions.Benefiting from the fast development of machine learning, especially natural language processing and convolutional neural networks, this paper proposes a Linguistics-based Architectural Form Generative Model (LAFGM) that could be trained to make 3D architectural form predictions based simply on language input.Several related works exist that focus on learning text-to-image generation, while others have taken a further step by generating simple shapes from the descriptions.However, the text parsing and output of these works still remain either at the 2D stage or confined to a single geometry.On the basis of these works, this paper used both Stanford Scene Graph Parser (Sebastian et al. 2015) and graph convolutional networks (Kipf and Welling 2016) to compile the analytic semantic structure for the input texts, then generated the 3D architectural form expressed by the language descriptions, which is also aided by several optimization algorithms.To a certain extent, the training results approached the 3D form intended in the textual description, not only indicating the tremendous potential of LAFGM from linguistic input to 3D architectural form, but also innovating design expression and communication regarding 3D spatial information.",Hang Zhang,California University of Pennsylvania,2,Text-to-Form,https://doi.org/10.52842/conf.acadia.2020.1.238,0.0,FALSE,en,TRUE,bronze,ACADIA quarterly,journal,2020-01-01,article
https://openalex.org/W3124213046,"This paper introduces the first computational model of a proposed set of cognitive structures and processes involved in interpreting visual narratives (specifically, comics). The study of cognitive processes that occur when readers interpret comics has been theorized and experimentally validated to include spatial semantics—a mental model of the physical environment depicted in each image— and event structure semantics—a hierarchy of narrative events that take place over time. These semantic models are interconnected, developing in parallel as readers interpret a panel sequence. Our computational model aims to bring clarity to the cognitive theory of visual narrative sensemaking and poses new questions for the cognitive science community. Towards this end, we present a prototype computational model in which event structures are represented as hierarchical plans and spatial structures are represented as relational scene graphs. Domain knowledge about narrative events and how they relate to underlying scene structure is encoded in a standard Hierarchical Task Network (HTN) planning domain representation. Using a standard implementation of HTN planning, we demonstrate how to search the space of possible HTN solutions for ones that match with comic panel sequences.",Chris Martens|Rogelio E. Cardona-Rivera|Neil Cohn,North Carolina State University|University of Utah|,6,The Visual Narrative Engine: A Computational Model of the Visual Narrative Parallel Architecture.,,0.44057865,FALSE,en,TRUE,green,Data Archiving and Networked Services (DANS),repository,2020-01-01,article
https://openalex.org/W3046003264,"Scene graph is a graph representation that explicitly represents high-level semantic knowledge of an image such as objects, attributes of objects and relationships between objects. Various tasks have been proposed for the scene graph, but the problem is that they have a limited vocabulary and biased information due to their own hypothesis. Therefore, results of each task are not generalizable and difficult to be applied to other down-stream tasks. In this paper, we propose Entity Synset Alignment(ESA), which is a method to create a general scene graph by aligning various semantic knowledge efficiently to solve this bias problem. The ESA uses a large-scale lexical database, WordNet and Intersection of Union (IoU) to align the object labels in multiple scene graphs/semantic knowledge. In experiment, the integrated scene graph is applied to the image-caption retrieval task as a down-stream task. We confirm that integrating multiple scene graphs helps to get better representations of images.",Woo Suk Choi|Kyoung-Woon On|Yu‐Jung Heo|Byoung‐Tak Zhang,|Seoul National University|Seoul National University|Seoul National University,2,Toward General Scene Graph: Integration of Visual Semantic Knowledge with Entity Synset Alignment,https://doi.org/10.18653/v1/2020.alvr-1.2,0.20993464,FALSE,en,TRUE,gold,,,2020-01-01,article
https://openalex.org/W3114427170,"Visual Question Answering (VQA) remains algorithmically challenging while it is effortless for humans. Humans combine visual observations with general and commonsense knowledge to answer questions about a given image. In this paper, we address the problem of incorporating general knowledge into VQA models while leveraging the visual information. We propose a model that captures the interactions between objects in a visual scene and entities in an external knowledge source. Our model is a graph-based approach that combines scene graphs with concept graphs, which learns a question-adaptive graph representation of related knowledge instances. We use Graph Attention Networks to set higher importance to key knowledge instances that are mostly relevant to each question. We exploit ConceptNet as the source of general knowledge and evaluate the performance of our model on the challenging OK-VQA dataset.",Maryam Ziaeefard|Freddy Lécué,Thales (Canada)|McGill University|Institut national de recherche en informatique et en automatique|Thales (Canada)|Institut national de recherche en informatique et en automatique|McGill University,23,Towards Knowledge-Augmented Visual Question Answering,https://doi.org/10.18653/v1/2020.coling-main.169,1.46954245,FALSE,en,TRUE,gold,,,2020-01-01,article
https://openalex.org/W3092314908,"In the field of robotics, it is crucial to obtain a comprehensive semantic understanding of a scene for many applications. Based on the behavioral topological map and scene graph, we propose to employ a semantic map named Topological Scene Map (TSM) for representation in indoor environment understanding. The behavioral topological map we constructed expresses the spatial connection relations and semantically describes the navigation behavior between adjacent topological nodes. The scene graph promotes the TSM to record the objects that appear in the scene and the relations between objects. The addition of spatial and semantic relations makes the expression of the scene more specific, which improves the robot's abilities of scene understanding and human-robotic interaction. In this article, we design a method for topological map construction and apply a novel approach to generate a scene graph from RGB-D data. The semantic representation of the environment generated in the experiments verifies that the TSM construction framework models the scene efficiently and the TSM is conducive to the realization of human-robotic interaction.",Zhiyong Liao|Yu Zhang|Junren Luo|Weilin Yuan,National University of Defense Technology|National University of Defense Technology|National University of Defense Technology|National University of Defense Technology,8,TSM: Topological Scene Map for Representation in Indoor Environment Understanding,https://doi.org/10.1109/access.2020.3029324,1.4933031,FALSE,en,TRUE,gold,IEEE Access,journal,2020-01-01,article
https://openalex.org/W3022386773,"Understanding an image goes beyond recognizing and locating the objects in it, the relationships between objects also very important in image understanding. Most previous methods have focused on recognizing local predictions of the relationships. But real-world image relationships often determined by the surrounding objects and other contextual information. In this work, we employ this insight to propose a novel framework to deal with the problem of visual relationship detection. The core of the framework is a relationship inference network, which is a recurrent structure designed for combining the global contextual information of the object to infer the relationship of the image. Experimental results on Stanford VRD and Visual Genome demonstrate that the proposed method achieves a good performance both in efficiency and accuracy. Finally, we demonstrate the value of visual relationship on two computer vision tasks: image retrieval and scene graph generation.",Yugang Li Yongbin Wang|Yuting Zhu,Academy of Broadcasting Science|Communication University of China|Nanyang Technological University,5,Visual Relationship Detection with Contextual Information,https://doi.org/10.32604/cmc.2020.07451,0.31490195,FALSE,en,TRUE,diamond,"Computers, materials & continua/Computers, materials & continua (Print)",journal,2020-01-01,article
https://openalex.org/W3025428140,,Jezia Zakraoui|Moutaz Saleh|Usman Asghar|Jihad Mohamad Alja’am|Somaya Al-Máadeed,Qatar University|Qatar University|Qatar University|Lebanese University|Qatar University,5,Generating Images from Arabic Story-Text using Scene Graph,https://doi.org/10.1109/iciot48696.2020.9089495,0.24493827,FALSE,en,FALSE,closed,"2020 IEEE International Conference on Informatics, IoT, and Enabling Technologies (ICIoT)",conference,2020-02-01,article
https://openalex.org/W3035994942,,Yuki Taya|Atsushi Keyaki|Hiroshi Tsukahara|Ichiro Kobayashi,,0,A User Study for Verification of the Effectiveness of Scene Graph Features on Image Captioning,,0.0,FALSE,en,FALSE,closed,The Japanese Society for Artificial Intelligence,journal,2020-04-01,article
https://openalex.org/W3033515201,,Hassam Riaz|Ahmad Terra|Klaus Raizer|Rafia Inam|Alberto Hata,Ericsson (Sweden)|Ericsson (Sweden)|Ericsson (Sweden)|Ericsson (Sweden)|Ericsson (Sweden),12,Scene Understanding for Safety Analysis in Human-Robot Collaborative Operations,https://doi.org/10.1109/iccar49639.2020.9108083,0.73477123,FALSE,en,FALSE,closed,,,2020-04-01,article
https://openalex.org/W3014867697,"Image scene graph is a semantic structural representation which can not only show what objects are in the image, but also infer the relationships and interactions among them. Despite the recent success in object detection using deep neural networks, automatically recognizing social relations of objects in images remains a challenging task due to the significant gap between the domains of visual content and social relation. In this work, we translate the scene graph into an Attentive Gated Graph Neural Network which can propagate a message by visual relationship embedding. More specifically, nodes in gated neural networks can represent objects in the image, and edges can be regarded as relationships among objects. In this network, an attention mechanism is applied to measure the strength of the relationship between objects. It can increase the accuracy of object classification and reduce the complexity of relationship classification. Extensive experiments on the widely adopted Visual Genome Dataset show the effectiveness of the proposed method.",Shuohao Li|Min Tang|Jun Zhang|Lincheng Jiang,National University of Defense Technology|Tohoku University|National University of Defense Technology|University of Alberta|National University of Defense Technology|National University of Defense Technology,6,Attentive Gated Graph Neural Network for Image Scene Graph Generation,https://doi.org/10.3390/sym12040511,0.31490195,FALSE,en,TRUE,gold,Symmetry,journal,2020-04-02,article
https://openalex.org/W2997514790,"We propose a new algorithm, called Deep Generative Probabilistic Graph Neural Networks (DG-PGNN), to generate a scene graph for an image. The input to DG-PGNN is an image, together with a set of region-grounded captions and object bounding-box proposals for the image. To generate the scene graph, DG-PGNN constructs and updates a new model, called a Probabilistic Graph Network (PGN). A PGN can be thought of as a scene graph with uncertainty: it represents each node and each edge by a CNN feature vector and defines a probability mass function (PMF) for node-type (object category) of each node and edge-type (predicate class) of each edge. The DG-PGNN sequentially adds a new node to the current PGN by learning the optimal ordering in a Deep Q-learning framework, where states are partial PGNs, actions choose a new node, and rewards are defined based on the ground-truth. After adding a node, DG-PGNN uses message passing to update the feature vectors of the current PGN by leveraging contextual relationship information, object co-occurrences, and language priors from captions. The updated features are then used to fine-tune the PMFs. Our experiments show that the proposed algorithm significantly outperforms the state-of-the-art results on the Visual Genome dataset for scene graph generation. We also show that the scene graphs constructed by DG-PGNN improve performance on the visual question answering task, for questions that need reasoning about objects and their interactions in the scene context.",Mahmoud Khademi|Oliver Schulte,Simon Fraser University|Simon Fraser University,55,Deep Generative Probabilistic Graph Neural Networks for Scene Graph Generation,https://doi.org/10.1609/aaai.v34i07.6783,3.26584362,FALSE,en,TRUE,diamond,Proceedings of the AAAI Conference on Artificial Intelligence,conference,2020-04-03,article
https://openalex.org/W2998116350,"This paper studies the task of image captioning with novel objects, which only exist in testing images. Intrinsically, this task can reflect the generalization ability of models in understanding and captioning the semantic meanings of visual concepts and objects unseen in training set, sharing the similarity to one/zero-shot learning. The critical difficulty thus comes from that no paired images and sentences of the novel objects can be used to help train the captioning model. Inspired by recent work (Chen et al. 2019b) that boosts one-shot learning by learning to generate various image deformations, we propose learning meta-networks for deforming features for novel object captioning. To this end, we introduce the feature deformation meta-networks (FDM-net), which is trained on source data, and learn to adapt to the novel object features detected by the auxiliary detection model. FDM-net includes two sub-nets: feature deformation, and scene graph sentence reconstruction, which produce the augmented image features and corresponding sentences, respectively. Thus, rather than directly deforming images, FDM-net can efficiently and dynamically enlarge the paired images and texts by learning to deform image features. Extensive experiments are conducted on the widely used novel object captioning dataset, and the results show the effectiveness of our FDM-net. Ablation study and qualitative visualization further give insights of our model.",Tingjia Cao|Ke Han|Xiaomei Wang|Lin Ma|Yanwei Fu|Yu–Gang Jiang|Xiangyang Xue,Fudan University|Fudan University|Fudan University|Tencent (China)|Fudan University|Fudan University|Fudan University,15,Feature Deformation Meta-Networks in Image Captioning of Novel Objects,https://doi.org/10.1609/aaai.v34i07.6620,0.97975309,FALSE,en,TRUE,diamond,Proceedings of the AAAI Conference on Artificial Intelligence,conference,2020-04-03,article
https://openalex.org/W3037675012,"We introduce a simple yet expressive image generation method. On the one hand, it does not require the user to paint the masks or define a bounding box of the various objects, since the model does it by itself. On the other hand, it supports defining a coarse location and size of each object. Based on this, we offer a simple, interactive GUI, that allows a layman user to generate diverse images effortlessly.From a technical perspective, we introduce a dual embedding of layout and appearance. In this scheme, the location, size, and appearance of an object can change independently of each other. This way, the model is able to generate innumerable images per scene graph, to better express the intention of the user.In comparison to previous work, we also offer better quality and higher resolution outputs. This is due to a superior architecture, which is based on a novel set of discriminators. Those discriminators better constrain the shape of the generated mask, as well as capturing the appearance encoding in a counterfactual way.Our code is publicly available at https://www.github.com/ashual/scene_generation.",Oron Ashual|Lior Wolf,Tel Aviv University|Tel Aviv University|Meta (Israel),4,Interactive Scene Generation via Scene Graphs with Attributes,https://doi.org/10.1609/aaai.v34i09.7112,0.08164609,FALSE,en,TRUE,diamond,Proceedings of the AAAI Conference on Artificial Intelligence,conference,2020-04-03,article
https://openalex.org/W2996941455,"Visual storytelling aims at generating a story from an image stream. Most existing methods tend to represent images directly with the extracted high-level features, which is not intuitive and difficult to interpret. We argue that translating each image into a graph-based semantic representation, i.e., scene graph, which explicitly encodes the objects and relationships detected within image, would benefit representing and describing images. To this end, we propose a novel graph-based architecture for visual storytelling by modeling the two-level relationships on scene graphs. In particular, on the within-image level, we employ a Graph Convolution Network (GCN) to enrich local fine-grained region representations of objects on scene graphs. To further model the interaction among images, on the cross-images level, a Temporal Convolution Network (TCN) is utilized to refine the region representations along the temporal dimension. Then the relation-aware representations are fed into the Gated Recurrent Unit (GRU) with attention mechanism for story generation. Experiments are conducted on the public visual storytelling dataset. Automatic and human evaluation results indicate that our method achieves state-of-the-art.",Ruize Wang|Zhongyu Wei|Piji Li|Qi Zhang|Xuanjing Huang,Fudan University|Fudan University|Tencent (China)|Fudan University|Fudan University,55,Storytelling from an Image Stream Using Scene Graphs,https://doi.org/10.1609/aaai.v34i05.6455,3.67407407,FALSE,en,TRUE,diamond,Proceedings of the AAAI Conference on Artificial Intelligence,conference,2020-04-03,article
https://openalex.org/W3023511145,,Ning Xu|An-An Liu|Yongkang Wong|Weizhi Nie|Yuting Su|Mohan Kankanhalli,Tianjin University|Tianjin University|National University of Singapore|Tianjin University|Tianjin University|Tianjin University,34,Scene Graph Inference via Multi-Scale Context Modeling,https://doi.org/10.1109/tcsvt.2020.2990989,2.41424831,FALSE,en,FALSE,closed,IEEE Transactions on Circuits and Systems for Video Technology,journal,2020-04-28,article
https://openalex.org/W3021746718,"Scene graph generation is to recognise objects and their semantic relationships in an image and can help computers understand visual scene. To improve relationship prediction, geometry information is essential and usually incorporated into relationship features. Existing methods use coordinates of objects to encode their spatial layout. However, in this way, they neglect the context of objects. In this study, to take full use of spatial knowledge efficiently, the authors propose a novel subgraph and object context‐masked network (SOCNet) consisting of spatial mask relation inference (SMRI) and hierarchical message passing (HMP) modules to address the scene graph generation task. In particular, to take advantage of spatial knowledge, SMRI masks partial context of object features depending on their spatial layout of objects and corresponding subgraph to facilitate their relationship recognition. To refine the features of objects and subgraphs, they also propose HMP that passes highly correlated messages from both microcosmic and macroscopic aspects through a triple‐path structure including subgraph–subgraph, object–object, and subgraph–object paths. Finally, statistical co‐occurrence probability is used to regularise relationship prediction. SOCNet integrates HMP and SMRI into a unified network, and comprehensive experiments on visual relationship detection and visual genome datasets indicate that SOCNet outperforms several state‐of‐the‐art methods on two common tasks.",Zhenxing Zheng|Zhendong Li|Gaoyun An|Songhe Feng,Beijing Information Science & Technology University|Beijing Jiaotong University|Beijing Information Science & Technology University|Beijing Jiaotong University|Beijing Jiaotong University|Beijing Information Science & Technology University|Beijing Jiaotong University,2,Subgraph and object context‐masked network for scene graph generation,https://doi.org/10.1049/iet-cvi.2019.0896,0.0,FALSE,en,TRUE,green,IET Computer Vision,journal,2020-04-30,article
https://openalex.org/W3021735172,,Guanghui Ren|Lejian Ren|Yue Liao|Si Liu|Bo Li|Jizhong Han|Shuicheng Yan,Chinese Academy of Sciences|Institute of Information Engineering|Shenzhen Institutes of Advanced Technology|Chinese Academy of Sciences|Institute of Information Engineering|Beihang University|Beihang University|Beihang University|Chinese Academy of Sciences|Institute of Information Engineering|,43,Scene Graph Generation With Hierarchical Context,https://doi.org/10.1109/tnnls.2020.2979270,3.46392149,FALSE,en,FALSE,closed,IEEE Transactions on Neural Networks and Learning Systems,journal,2020-05-01,article
https://openalex.org/W3096232590,"Description: The dataset consists of of a large number of realistic synthetic images that feature a number of objects on a table-top, of three classes: staplers, mugs and bananas. These are taken at a variety of lighting, viewpoint and object configuration conditions. In addition, the dataset includes a set of annotated real images that were manually taken to feature a number of objects of the considered classes. The dataset includes over 22000 realistic synthetic images that can be used for training and testing, and 135 annotated real images for testing. All datasets include object annotations and their masks. Image resolution is 256 x 256. Synthetic datasets include all the latent variables of the 3D scene (scene graph). The synthetic scenes were rendered using the Blender software: www.blender.org. For each object its associated latent variables are its position, scaling factor, azimuthal rotation, shape (1-of-K encoding) and colour (RGB). The ground plane has a random RGB colour. The camera is taken to be at a random height above the origin and to be looking down with a random angle of elevation. The illumination model is uniform lighting plus a directional source (specified by the strength, azimuth and elevation of the source). Real dataset: for each object we annotated its class, instance mask, and the contact point using the LabelMe software.",Łukasz Romaszko,,0,Dataset for: Learning Direct Optimization for Scene Understanding,https://doi.org/10.17632/gr62b6d33h.1,,FALSE,en,TRUE,green,,,2020-05-03,article
https://openalex.org/W3027378356,,Randall Hess|Matthew J. Morris|Alex Thompson|Nicolas Durland,Northrop Grumman (United States)|Northrop Grumman (United States)|Northrop Grumman (United States)|Northrop Grumman (United States),0,"A low-latency, low-throughput graphical architecture for multi-sensor pilotage displays",https://doi.org/10.1117/12.2561719,0.0,FALSE,en,FALSE,closed,,,2020-05-18,article
https://openalex.org/W3030462281,,Jie Luo|Jia Zhao|Bin Wen|Yuhang Zhang,Beihang University|Beihang University|Beihang University|Beihang University,14,Explaining the semantics capturing capability of scene graph generation models,https://doi.org/10.1016/j.patcog.2020.107427,1.1546405,FALSE,en,FALSE,closed,Pattern Recognition,journal,2020-05-30,article
https://openalex.org/W3034305926,,Yongzhi Li|Duo Zhang|Yadong Mu,||Peking University,35,Visual-Semantic Matching by Exploring High-Order Attention and Distraction,https://doi.org/10.1109/cvpr42600.2020.01280,2.62418295,FALSE,en,FALSE,closed,,,2020-06-01,article
https://openalex.org/W3033697199,,Yutian Guo|Jingjing Chen|Hao Zhang|Yu–Gang Jiang,Fudan University|Fudan University|City University of Hong Kong|Fudan University,16,Visual Relations Augmented Cross-modal Retrieval,https://doi.org/10.1145/3372278.3390709,1.57450977,FALSE,en,FALSE,closed,,,2020-06-02,article
https://openalex.org/W3037757389,"Natural language provides an intuitive and effective interaction interface between human beings and robots. Currently, multiple approaches are presented to address natural language visual grounding for human-robot interaction. However, most of the existing approaches handle the ambiguity of natural language queries and achieve target objects grounding via dialogue systems, which make the interactions cumbersome and time-consuming. In contrast, we address interactive natural language grounding without auxiliary information. Specifically, we first propose a referring expression comprehension network to ground natural referring expressions. The referring expression comprehension network excavates the visual semantics via a visual semantic-aware network, and exploits the rich linguistic contexts in expressions by a language attention network. Furthermore, we combine the referring expression comprehension network with scene graph parsing to achieve unrestricted and complicated natural language grounding. Finally, we validate the performance of the referring expression comprehension network on three public datasets, and we also evaluate the effectiveness of the interactive natural language grounding architecture by conducting extensive natural language query groundings in different household scenarios.",Jinpeng Mi|Jianzhi Lyu|Song Tang|Qingdu Li|Jianwei Zhang,Universität Hamburg|University of Shanghai for Science and Technology|Universität Hamburg|University of Shanghai for Science and Technology|Universität Hamburg|University of Shanghai for Science and Technology|Universität Hamburg,12,Interactive Natural Language Grounding via Referring Expression Comprehension and Scene Graph Parsing,https://doi.org/10.3389/fnbot.2020.00043,0.73477123,FALSE,en,TRUE,gold,Frontiers in Neurorobotics,journal,2020-06-25,article
https://openalex.org/W3040708944,,Fan Lyu|Wei Feng|Song Wang,Tianjin University|Tianjin University|Tianjin University|University of South Carolina,18,vtGraphNet: Learning weakly-supervised scene graph for complex visual grounding,https://doi.org/10.1016/j.neucom.2020.06.091,1.1546405,FALSE,en,FALSE,closed,Neurocomputing,journal,2020-07-07,article
https://openalex.org/W3042063280,"The Audio-Visual Scene-aware Dialog (AVSD) task requires an agent to indulge in a natural conversation with a human about a given video. Specifically, apart from the video frames, the agent receives the audio, brief captions, and a dialog history, and the task is to produce the correct answer to a question about the video. Due to the diversity in the type of inputs, this task poses a very challenging multimodal reasoning problem. Current approaches to AVSD either use global video-level features or those from a few sampled frames, and thus lack the ability to explicitly capture relevant visual regions or their interactions for answer generation. To this end, we propose a novel spatio-temporal scene graph representation (STSGR) modeling fine-grained information flows within videos. Specifically, on an input video sequence, STSGR (i) creates a two-stream visual and semantic scene graph on every frame, (ii) conducts intra-graph reasoning using node and edge convolutions generating visual memories, and (iii) applies inter-graph aggregation to capture their temporal evolutions. These visual memories are then combined with other modalities and the question embeddings using a novel semantics-controlled multi-head shuffled transformer, which then produces the answer recursively. Our entire pipeline is trained end-to-end. We present experiments on the AVSD dataset and demonstrate state-of-the-art results. A human evaluation on the quality of our generated answers shows 12% relative improvement against prior methods.",Shijie Geng|Peng Gao|Chiori Hori|Jonathan Le Roux|Anoop Cherian,,7,Spatio-Temporal Scene Graphs for Video Dialog.,,,FALSE,en,TRUE,green,arXiv (Cornell University),repository,2020-07-08,preprint
https://openalex.org/W3041393943,"Inferring objects and their relationships from an image is useful in many applications at the intersection of vision and language. Due to a long tail data distribution, the task is challenging, with the inevitable appearance of zero-shot compositions of objects and relationships at test time. Current models often fail to properly understand a scene in such cases, as during training they only observe a tiny fraction of the distribution corresponding to the most frequent compositions. This motivates us to study whether increasing the diversity of the training distribution, by generating replacement for parts of real scene graphs, can lead to better generalization? We employ generative adversarial networks (GANs) conditioned on scene graphs to generate augmented visual features. To increase their diversity, we propose several strategies to perturb the conditioning. One of them is to use a language model, such as BERT, to synthesize plausible yet still unlikely scene graphs. By evaluating our model on Visual Genome, we obtain both positive and negative results. This prompts us to make several observations that can potentially lead to further improvements.",B. A. Knyazev|Harm de Vries|Cătălina Cangea|Graham W. Taylor|Aaron Courville|Eugene Belilovsky,,1,Generative Graph Perturbations for Scene Graph Prediction.,,,FALSE,en,TRUE,green,arXiv (Cornell University),repository,2020-07-11,preprint
https://openalex.org/W3044298711,,Kunal Goyal|Utkarsh Gupta|Abir De|Soumen Chakrabarti,Indian Institute of Technology Bombay|Indian Institute of Technology Bombay|Indian Institute of Technology Bombay|Indian Institute of Technology Bombay,5,Deep Neural Matching Models for Graph Retrieval,https://doi.org/10.1145/3397271.3401216,0.44057865,FALSE,en,FALSE,closed,,,2020-07-25,article
https://openalex.org/W2608029649,,Hatem M. Wasfy|Tamer M. Wasfy|Jeanne M. Peters,Indiana University – Purdue University Indianapolis|Columbia University|Columbia University|Indiana University – Purdue University Indianapolis|Columbia University|Indiana University – Purdue University Indianapolis,2,Flexible Multibody Dynamics Explicit Solver for Real-time Simulation of an Online Virtual Dynamics Lab,https://doi.org/10.18260/1-2--21398,0.29309591,FALSE,en,FALSE,closed,,,2020-09-11,article
https://openalex.org/W4242995582,"Graph-based scene model has been receiving increasing attention as a flexible and descriptive scene model for visual robot self-localization. In a typical self-localization application, objects, object features, and object relationship in an environment map are described respectively by nodes, node features, and edges in a scene graph, which are then matched against a query scene graph by a graph matching engine. However, its overhead for computation, storage, and communication, is proportional to the number and feature dimensionality of graph nodes, and can be significant in large-scale applications. In this study, we observe that graph-convolutional neural network (GCN) has a potential to become an efficient tool to train and predict with a graph matching engine. However, it is non-trivial to translate a given visual feature to a proper graph feature that contributes to good self-localization performance. To address this issue, we introduce a new knowledge transfer (KT) framework, which introduces an arbitrary self-localization model as a teacher to train the student, GCN-based self-localization system. Our KT framework enables lightweight storage/communication by using compact teacher's output signals as training data. Results on RobotCar datasets show that the proposed method outperforms existing comparing methods as well as the teacher self-localization system.",koji takeda|Tanaka Kanji,,0,Knowledge Transfer from Map to DNN: Use of Graph Convolutional Neural Network for Augmenting Visual Robot Self-localization System,https://doi.org/10.31224/osf.io/738bq,0.0,FALSE,en,TRUE,gold,,,2020-09-14,preprint
https://openalex.org/W3028602002,"The goal of this project is to develop a complete, fully detailed 3D interactive model of the human body and systems in the human body, and allow the user to interacts in 3D with all the elements of that system, to teach students about human anatomy. Some organs, which contain a lot of details about a particular anatomy, need to be accurately and fully described in minute detail, such as the brain, lungs, liver and heart. These organs are need have all the detailed descriptions of the medical information needed to learn how to do surgery on them, and should allow the user to add careful and precise marking to indicate the operative landmarks on the surgery location. Adding so many different items of information is challenging when the area to which the information needs to be attached is very detailed and overlaps with all kinds of other medical information related to the area. Existing methods to tag areas was not allowing us sufficient locations to attach the information to. Our solution combines a variety of tagging methods, which use the marking method by selecting the RGB color area that is drawn in the texture, on the complex 3D object structure. Then, it relies on those RGB color codes to tag IDs and create relational tables that store the related information about the specific areas of the anatomy. With this method of marking, it is possible to use the entire set of color values (R, G, B) to identify a set of anatomic regions, and this also makes it possible to define multiple overlapping regions.",Chung Văn Lê|Gia Nhu Nguyen|Tri Huu Nguyen|Tung Nguyen|Dac‐Nhuong Le,Duy Tan University|Duy Tan University|Hue University|Hue University|Hai Phong University of Management and Technology,5,An effective RGB color selection for complex 3D object structure in scene graph systems,https://doi.org/10.11591/ijece.v10i6.pp5951-5964,0.9336144,FALSE,en,TRUE,diamond,International Journal of Power Electronics and Drive Systems/International Journal of Electrical and Computer Engineering,journal,2020-09-18,article
https://openalex.org/W3089709427,,Zhuoqian Yang|Zengchang Qin|Jing Yu|Tao Wan,Carnegie Mellon University|Beihang University|Institute of Information Engineering|Beihang University,22,Prior Visual Relationship Reasoning For Visual Question Answering,https://doi.org/10.1109/icip40778.2020.9190771,1.46954245,FALSE,en,FALSE,closed,,,2020-09-30,article
https://openalex.org/W3089515120,"Research in image captioning has mostly focused on English because of the availability of image-caption paired datasets in this language. However, building vision-language systems only for English deprives a large part of the world population of AI technologies' benefit. On the other hand, creating image-caption paired datasets for every target language is expensive. In this work, we present a novel unsupervised cross-lingual method to generate image captions in a target language without using any image-caption corpus in the source or target languages. Our method relies on (i) a cross-lingual scene graph to sentence translation process, which learns to decode sentences in the target language from a cross-lingual encoding space of scene graphs using a sentence parallel (bitext) corpus, and (ii) an unsupervised cross-modal feature mapping which seeks to map an encoded scene graph features from image modality to language modality. We verify the effectiveness of our proposed method on the Chinese image caption generation task. The comparisons against several existing methods demonstrate the effectiveness of our approach.",Jiahui Gao|Yi Zhou|Philip L. H. Yu|Jiuxiang Gu,University of Hong Kong|University of Hong Kong||Adobe Systems (United States),9,Unsupervised Cross-lingual Image Captioning.,,,FALSE,en,TRUE,green,arXiv (Cornell University),repository,2020-10-03,preprint
https://openalex.org/W3124782988,"To make autonomous robots ""taskable"" so that they function properly and interact fluently with human partners, they must be able to perceive and understand the semantic aspects of their environments. More specifically, they must know what objects exist and where they are in the unstructured human world. Progresses in robot perception, especially in deep learning, have greatly improved for detecting and localizing objects. However, it still remains a challenge for robots to perform a highly reliable scene estimation in unstructured environments that is determined by robustness, adaptability and scale. In this dissertation, we address the scene estimation problem under uncertainty, especially in unstructured environments. We enable robots to build a reliable object-oriented representation that describes objects present in the environment, as well as inter-object spatial relations. Specifically, we focus on addressing following challenges for reliable scene estimation: 1) robust perception under uncertainty results from noisy sensors, objects in clutter and perceptual aliasing, 2) adaptable perception in adverse conditions by combined deep learning and probabilistic generative methods, 3) scalable perception as the number of objects grows and the structure of objects becomes more complex (e.g. objects in dense clutter). \n\nTowards realizing robust perception, our objective is to ground raw sensor observations into scene states while dealing with uncertainty from sensor measurements and actuator control . Scene states are represented as scene graphs, where scene graphs denote parameterized axiomatic statements that assert relationships between objects and their poses. To deal with the uncertainty, we present a pure generative approach, Axiomatic Scene Estimation (AxScEs). AxScEs estimates a probabilistic distribution across plausible scene graph hypotheses describing the configuration of objects. By maintaining a diverse set of possible states, the proposed approach demonstrates the robustness to the local minimum in the scene graph state space and effectiveness for manipulation-quality perception based on edit distance on scene graphs. \n\nTo scale up to more unstructured scenarios and be adaptable to adversarial scenarios, we present Sequential Scene Understanding and Manipulation (SUM), which estimates the scene as a collection of objects in cluttered environments. SUM is a two-stage method that leverages the accuracy and efficiency from convolutional neural networks (CNNs) with probabilistic inference methods. Despite the strength from CNNs, they are opaque in understanding how the decisions are made and fragile for generalizing beyond overfit training samples in adverse conditions (e.g., changes in illumination). The probabilistic generative method complements these weaknesses and provides an avenue for adaptable perception. \n \nTo scale up to densely cluttered environments where objects are physically touching with severe occlusions, we present GeoFusion, which fuses noisy observations from multiple frames by exploring geometric consistency at object level. Geometric consistency characterizes geometric compatibility between objects and geometric similarity between observations and objects. It reasons about geometry at the object-level, offering a fast and reliable way to be robust to semantic perceptual aliasing. The proposed approach demonstrates greater robustness and accuracy than the state-of-the-art pose estimation approach.",Zhiqiang Sui,,0,Robust Scene Estimation for Goal-directed Robotic Manipulation in Unstructured Environments,,,FALSE,en,TRUE,green,Deep Blue (University of Michigan),repository,2020-10-04,article
https://openalex.org/W3092596539,,Jai G. Singla|Kirti Padia,Indian Space Research Organisation|Indian Space Research Organisation,24,A Novel Approach for Generation and Visualization of Virtual 3D City Model Using Open Source Libraries,https://doi.org/10.1007/s12524-020-01191-8,2.67640419,FALSE,en,FALSE,closed,Journal of the Indian Society of Remote Sensing,journal,2020-10-08,article
https://openalex.org/W3093329502,,Xu Yang|Chongyang Gao|Hanwang Zhang|Jianfei Cai,Nanyang Technological University|Dartmouth College|Nanyang Technological University|Monash University,24,Hierarchical Scene Graph Encoder-Decoder for Image Paragraph Captioning,https://doi.org/10.1145/3394171.3413859,1.46954245,FALSE,en,FALSE,closed,,,2020-10-12,article
https://openalex.org/W3092867601,"The scene graph which can be represented by a set of visual triples is composed of objects and the relations between object pairs. It is vital for image captioning, visual question answering, and many other applications. However, there is a long tail distribution on the scene graph dataset, and the tail relation cannot be accurately identified due to the lack of training samples. The problem of the nonstandard label and feature overlap on the scene graph affects the extraction of discriminative features and exacerbates the effect of data imbalance on the model. For these reasons, we propose a novel scene graph generation model that can effectively improve the detection of low-frequency relations. We use the method of memory features to realize the transfer of high-frequency relation features to low-frequency relation features. Extensive experiments on scene graph datasets show that our model significantly improved the performance of two evaluation metrics R@K and mR@K compared with state-of-the-art baselines.",Weitao Wang|Ruyang Liu|Meng Wang|Sen Wang|Xiaojun Chang|Yang Chen,Southeast University|Southeast University|Southeast University|University of Queensland|Monash University|Southeast University,11,Memory-Based Network for Scene Graph with Unbalanced Relations,https://doi.org/10.1145/3394171.3413507,1.04967318,FALSE,en,TRUE,green,,,2020-10-12,article
https://openalex.org/W3092668800,,Hongshuo Tian|Ning Xu|An-An Liu|Yongdong Zhang,Tianjin University|Tianjin University|Tianjin University|University of Science and Technology of China,12,Part-Aware Interactive Learning for Scene Graph Generation,https://doi.org/10.1145/3394171.3413501,0.94470586,FALSE,en,FALSE,closed,,,2020-10-12,article
https://openalex.org/W3093140812,,Fan Yu|Haonan Wang|Tongwei Ren|Jinhui Tang|Gangshan Wu,Nanjing University|Nanjing University|Nanjing University|Nanjing University of Science and Technology|Nanjing University,11,Visual Relation of Interest Detection,https://doi.org/10.1145/3394171.3413566,1.04967318,FALSE,en,FALSE,closed,,,2020-10-12,article
https://openalex.org/W3093017735,,Chenchen Jing|Yuwei Wu|Mingtao Pei|Yao Hu|Yunde Jia|Qi Wu,Beijing Institute of Technology|Beijing Institute of Technology|Beijing Institute of Technology|Alibaba Group (China)|Beijing Institute of Technology|University of Adelaide,35,Visual-Semantic Graph Matching for Visual Grounding,https://doi.org/10.1145/3394171.3413902,2.41424831,FALSE,en,FALSE,closed,,,2020-10-12,article
https://openalex.org/W3093432214,,Junjie Jiang|Zaixing He|Shuyou Zhang|Xinyue Zhao|Jianrong Tan,Zhejiang University|Zhejiang University|Zhejiang University|Zhejiang University|Zhejiang University,24,Learning to transfer focus of graph neural network for scene graph parsing,https://doi.org/10.1016/j.patcog.2020.107707,1.99437904,FALSE,en,FALSE,closed,Pattern Recognition,journal,2020-10-17,article
https://openalex.org/W3117141303,,Weon Geun Oh|Jong-Gook Ko,Electronics and Telecommunications Research Institute|Electronics and Telecommunications Research Institute,0,Guidelines for Evaluation of Visual Narrative Technology Based on Image Objects,https://doi.org/10.1109/ictc49870.2020.9289398,0.0,FALSE,en,FALSE,closed,,,2020-10-21,article
https://openalex.org/W3094005538,"Aging in place is a notion which supports the independent living of older adults at their own place of residence for as long as possible. To support this alternative living which can be in contrast to various other types of assisted living options, modes of monitoring technology need to be explored and studied in order to determine a balance between the preservation of privacy and adequacy of sensed information for better estimation and visualization of movements and activities. In this paper, we explore such monitoring paradigm on how a network of RGB-D sensors can be utilized for this purpose. This type of sensor offers both visual and depth sensing modalities from the scene where the information can be fused and coded for better protection of privacy. For this purpose, we introduce the novel notion of passive observer. This observer is only triggered by detecting the absence of movements of older adults in the scene. This is accomplished by classifying and localizing objects in the monitoring scene from both before and after the detection of movements. A deep learning tool is utilized for visual classification of known objects in the physical scene followed by virtual reality reconstructing of the scene where the shape and location of objects are recreated. Such reconstruction can be used as a visual summary in order to identify objects which were handled by an older adult in-between observation. The simplified virtual scene can be used, for example, by caregivers or monitoring personnel in order to assist in detecting any anomalies. This virtual visualization can offer a high level of privacy protection without having any direct visual access to the monitoring scene. In addition, using the scene graph representation, an automatic decision-making tool is proposed where spatial relationships between the objects can be used to estimate the expected activities. The results of this paper are demonstrated through two case studies.",Shahram Payandeh|Jim Park,Simon Fraser University|Simon Fraser University,3,Passive Observer of Activities for Aging in Place Using a Network of RGB-D Sensors,https://doi.org/10.1155/2020/8867926,0.31490195,FALSE,en,TRUE,gold,International Journal of Telemedicine and Applications,journal,2020-10-23,article
https://openalex.org/W3094082687,,Varshanth Rao|Peng Dai|Sidharth Singla,Huawei Technologies (Canada)|Huawei Technologies (Canada)|Huawei Technologies (Canada),1,Structural fragmentation in scene graphs,https://doi.org/10.1016/j.knosys.2020.106504,0.10496732,FALSE,en,FALSE,closed,Knowledge-Based Systems,journal,2020-10-23,article
https://openalex.org/W3119206490,,Franklin Kenghagho Kenfack|F.A. Siddiky|Ferenc Bálint-Benczédi|Michael Beetz,University of Bremen|University of Bremen|University of Bremen|University of Bremen,23,RobotVQA — A Scene-Graph- and Deep-Learning-based Visual Question Answering System for Robot Manipulation,https://doi.org/10.1109/iros45743.2020.9341186,1.25960781,FALSE,en,FALSE,closed,,,2020-10-24,article
https://openalex.org/W3119540085,,Zhixuan Zhang|Chi Zhang|Yuehu Liu|Yuanqi Su|Ping Li|Jinzi Zheng,Xi'an Jiaotong University|Xi'an Jiaotong University|Xi'an Jiaotong University|Xi'an Jiaotong University|China Academy of Railway Sciences|China Academy of Railway Sciences,8,A Bottom-up Paradigm for Traffic Scene Graph Representation,https://doi.org/10.1145/3436369.3437437,0.41986927,FALSE,en,FALSE,closed,,,2020-10-30,article
https://openalex.org/W3108792048,,Eftychia Lakka|Don Brutzman|Richard Puk|Athanasios G. Malamos,University of South Wales|Naval Postgraduate School||Hellenic Mediterranean University,3,"Extending X3D Realism with Audio Graphs, Acoustic  Properties and 3D Spatial Sound",https://doi.org/10.1145/3424616.3424709,0.31490195,FALSE,en,FALSE,closed,,,2020-11-09,article
https://openalex.org/W3099790176,"This paper proposes a novel visual experience-based question answering problem (VEQA) and the corresponding dataset for embodied intelligence research that requires an agent to do actions, understand 3D scenes from successive partial input images, and answer natural language questions about its visual experiences in real time. Unlike the conventional visual question answering (VQA), the VEQA problem assumes both partial observability and dynamics of a complex multimodal environment. To address this VEQA problem, we propose a hybrid visual question answering system, VQAS, integrating a deep neural network-based scene graph generation model and a rule-based knowledge reasoning system. The proposed system can generate more accurate scene graphs for dynamic environments with some uncertainty. Moreover, it can answer complex questions through knowledge reasoning with rich background knowledge. Results of experiments using a photo-realistic 3D simulated environment, AI2-THOR, and the VEQA benchmark dataset prove the high performance of the proposed system.",Incheol Kim,Kyonggi University,1,Visual Experience-Based Question Answering with Complex Multimodal Environments,https://doi.org/10.1155/2020/8567271,0.0,FALSE,en,TRUE,hybrid,Mathematical Problems in Engineering,journal,2020-11-19,article
https://openalex.org/W3119113982,"This study proposes a novel deep neural network model that can accurately detect objects and their relationships in an image and represent them as a scene graph. The proposed model utilizes several multimodal features, including linguistic features and visual context features, to accurately detect objects and relationships. In addition, in the proposed model, context features are embedded using graph neural networks to depict the dependencies between two related objects in the context feature vector. This study demonstrates the effectiveness of the proposed model through comparative experiments using the Visual Genome benchmark dataset.",Gayoung Jung|Incheol Kim,,2,Multimodal Context Embedding for Scene Graph Generation,https://doi.org/10.3745/jips.02.0147,0.20993464,FALSE,en,TRUE,green,Journal of Information Processing Systems,journal,2020-12-01,article
https://openalex.org/W3162646375,,Xiaolin Li|Yuwei Gao,Chongqing University of Posts and Telecommunications|Chongqing University of Posts and Telecommunications,8,Research on Text to Image Based on Generative Adversarial Network,https://doi.org/10.1109/itca52113.2020.00077,0.10496732,FALSE,en,FALSE,closed,,,2020-12-01,article
https://openalex.org/W3111813289,"As one of the fundamental tasks in remote sensing (RS) image understanding, multi-label remote sensing image scene classification (MLRSSC) is attracting increasing research interest. Human beings can easily perform MLRSSC by examining the visual elements contained in the scene and the spatio-topological relationships of these visual elements. However, most of existing methods are limited by only perceiving visual elements but disregarding the spatio-topological relationships of visual elements. With this consideration, this paper proposes a novel deep learning-based MLRSSC framework by combining convolutional neural network (CNN) and graph neural network (GNN), which is termed the MLRSSC-CNN-GNN. Specifically, the CNN is employed to learn the perception ability of visual elements in the scene and generate the high-level appearance features. Based on the trained CNN, one scene graph for each scene is further constructed, where nodes of the graph are represented by superpixel regions of the scene. To fully mine the spatio-topological relationships of the scene graph, the multi-layer-integration graph attention network (GAT) model is proposed to address MLRSSC, where the GAT is one of the latest developments in GNN. Extensive experiments on two public MLRSSC datasets show that the proposed MLRSSC-CNN-GNN can obtain superior performance compared with the state-of-the-art methods.",Yansheng Li|Ruixian Chen|Yongjun Zhang|Mi Zhang|Ling Chen,Wuhan University|Wuhan University|Wuhan University|Wuhan University|Zhejiang University of Science and Technology,90,Multi-Label Remote Sensing Image Scene Classification by Combining a Convolutional Neural Network and a Graph Neural Network,https://doi.org/10.3390/rs12234003,11.17418145,FALSE,en,TRUE,gold,Remote Sensing,journal,2020-12-07,article
https://openalex.org/W3128220517,,Dong Cao|Qunhe Zhao|Yunbin Fu,Deepblue Technology (China)|Deepblue Technology (China)|Deepblue Technology (China),3,Using Spatial Temporal Graph Convolutional Network Dynamic Scene Graph for Video Captioning of Pedestrians Intention,https://doi.org/10.1145/3443279.3443285,0.20993464,FALSE,en,FALSE,closed,,,2020-12-18,article
https://openalex.org/W3113685523,,Hai Wan|Jin Rui Liang|Jianfeng Du|Yanan Liu|Jialing Ou|Baoyi Wang|Jeff Z. Pan|Juan Zeng,Sun Yat-sen University|Sun Yat-sen University|Guangdong University of Foreign Studies|Sun Yat-sen University|Sun Yat-sen University|Sun Yat-sen University|University of Edinburgh|Sun Yat-sen University,3,Iterative Visual Relationship Detection via Commonsense Knowledge Graph,https://doi.org/10.1016/j.bdr.2020.100175,0.20993464,FALSE,en,FALSE,closed,Big Data Research,journal,2020-12-21,article
https://openalex.org/W3116367518,,Saketh Vishnubhatla|Nishant Sinha,Birla Institute of Technology and Science - Hyderabad Campus|,2,Image Captioning with Pretrained Language Generators,https://doi.org/10.1145/3430984.3431059,0.10496732,FALSE,en,FALSE,closed,,,2020-12-28,article
https://openalex.org/W3141484220,,Federico Tombari,,0,3D Indoor Scene Understanding with Scene Graphs and Self-supervision.,,0.0,FALSE,en,FALSE,closed,,,2021-01-01,article
https://openalex.org/W3164612019,,Federico Tombari,,0,3D Scene Understanding with Scene Graphs and Self-Supervision.,,0.0,FALSE,en,FALSE,closed,,,2021-01-01,article
https://openalex.org/W3207464756,"Synthetic aperture radar (SAR) image target recognition technology is aimed at automatically determining the presence or absence of target information from the input SAR image and improving the efficiency and accuracy of SAR image interpretation. Based on big data analysis, dirty data is removed, clean data is returned, and standardized processing of SAR image data is realized. At the same time, by establishing a statistical model of coherent speckles, the convolutional autoencoder is used to denoise the SAR image. Finally, the network model modified by softmax cross‐entropy loss and Fisher loss is used for automatic target recognition. Based on the MSTAR data set, two scene graphs containing the target synthesized by the background image and the target slice are used for experiments. Several comparative experiments have verified the effectiveness of the classification and recognition model in this paper.",Xiang Chen|Xing Wang|You Chen|Haihan Wang,Air Force Engineering University|Air Force Engineering University|Air Force Engineering University|Air Force Engineering University,1,A Novel SAR Image Target Recognition Algorithm under Big Data Analysis,https://doi.org/10.1155/2021/4556157,0.30723359,FALSE,en,TRUE,hybrid,Wireless Communications and Mobile Computing,journal,2021-01-01,article
https://openalex.org/W3213937552,"Deriving and modifying graphs from natural language text has become a versatile basis technology for information extraction with applications in many subfields, such as semantic parsing or knowledge graph construction. A recent work used this technique for modifying scene graphs (He et al. 2020), by first encoding the original graph and then generating the modified one based on this encoding. In this work, we show that we can considerably increase performance on this problem by phrasing it as graph extension instead of graph generation. We propose the first model for the resulting graph extension problem based on autoregressive sequence labelling. On three scene graph modification data sets, this formulation leads to improvements in accuracy over the state-of-the-art between 13 and 24 percentage points. Furthermore, we introduce a novel data set from the biomedical domain which has much larger linguistic variability and more complex graphs than the scene graph modification data sets. For this data set, the state-of-the art fails to generalize, while our model can produce meaningful predictions.",Leon Weber|Jannes Münchmeyer|Samuele Garda|Ulf Leser,Max Delbrück Center|Humboldt-Universität zu Berlin|Humboldt-Universität zu Berlin|Humboldt-Universität zu Berlin|Humboldt-Universität zu Berlin,0,"Extend, don’t rebuild: Phrasing conditional graph modification as autoregressive sequence labelling",https://doi.org/10.18653/v1/2021.emnlp-main.93,0.0,FALSE,en,TRUE,hybrid,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,conference,2021-01-01,article
https://openalex.org/W3171015458,"&lt;p style='text-indent:20px;'&gt;The recent progress in learning image feature representations has opened the way for tasks such as label-to-image or text-to-image synthesis. However, one particular challenge widely observed in existing methods is the difficulty of synthesizing fine-grained textures and small-scale instances. In this paper, we propose a novel Global-Affine and Local-Specific Generative Adversarial Network (GALS-GAN) to explicitly construct global semantic layouts and learn distinct instance-level features. To achieve this, we adopt the graph convolutional network to calculate the instance locations and spatial relationships from scene graphs, which allows our model to obtain the high-fidelity semantic layouts. Also, a local-specific generator, where we introduce the feature filtering mechanism to separately learn semantic maps for different categories, is utilized to disentangle and generate specific visual features. Moreover, we especially apply a weight map predictor to better combine the global and local pathways considering the highly complementary between these two generation sub-networks. Extensive experiments on the COCO-Stuff and Visual Genome datasets demonstrate the superior generation performance of our model against previous methods, our approach is more capable of capturing photo-realistic local characteristics and rendering small-sized entities with more details.&lt;/p&gt;",Susu Zhang|Jiancheng Ni|Lijun Hou|Zili Zhou|Jie Hou|Feng Gao,,0,Global-Affine and Local-Specific Generative Adversarial Network for semantic-guided image generation,https://doi.org/10.3934/mfc.2021009,0.0,FALSE,en,TRUE,diamond,Mathematical Foundations of Computing,journal,2021-01-01,article
https://openalex.org/W4411245124,,Juyong Song|Sunghyun Choi,,0,Image-Text Alignment using Adaptive Cross-attention with Transformer Encoder for Scene Graphs,https://doi.org/10.5244/c.35.30,0.0,FALSE,en,FALSE,closed,,,2021-01-01,article
https://openalex.org/W3160816591,"Recent vision-language understanding approaches adopt a multi-modal transformer pre-training and finetuning paradigm. Prior work learns representations of text tokens and visual features with cross-attention mechanisms and captures the alignment solely based on indirect signals. In this work, we propose to enhance the alignment mechanism by incorporating image scene graph structures as the bridge between the two modalities, and learning with new contrastive objectives. In our preliminary study on the challenging compositional visual question answering task, we show the proposed approach achieves improved results, demonstrating potentials to enhance vision-language understanding.",Han Ding|Li Erran Li|Zhiting Hu|Yi Xu|Dilek Hakkani-Tür|Zheng Du|Belinda Zeng,,2,Semantic Aligned Multi-modal Transformer for Vision-LanguageUnderstanding: A Preliminary Study on Visual QA,https://doi.org/10.18653/v1/2021.maiworkshop-1.11,0.20443896,FALSE,en,TRUE,gold,,,2021-01-01,article
https://openalex.org/W3120016398,,Gal Sadeh Kenigsfield|Ran El‐Yaniv,Technion – Israel Institute of Technology|Technion – Israel Institute of Technology,3,TranstextNet: Transducing Text for Recognizing Unseen Visual Relationships,https://doi.org/10.1109/wacv48630.2021.00200,0.30665844,FALSE,en,FALSE,closed,,,2021-01-01,article
https://openalex.org/W3187037671,"When humans make sense of the world, they do not understand it as a cascade of observations; rather, from a cascade of observations, humans assemble a holistic narrative, connecting their observations using prior knowledge and inference. The final product of observations connected with prior knowledge and inference may be modeled as a knowledge graph. The process of sensemaking described above is one we seek to emulate in the realm of image understanding through a computational system. Starting from observed objects and relationships in a sequence of images (from Visual Genome Scene Graphs), the system we are building consults a commonsense knowledge network (ConceptNet), over-generates a set of hypothesized narrative-based connections between observations, and evaluates and trims its hypotheses through Multi-Objective Optimization to create a consistent set. The resultant knowledge graph reflects the system’s consistent speculations, beyond the directly observable, of what is happening in, and across, the images.",Zev Battad|Mei Si,,0,Understanding Image Sequences Via Narrative Sensemaking,,0.0,FALSE,en,TRUE,green,eScholarship (California Digital Library),repository,2021-01-01,article
https://openalex.org/W3192255200,,Eleonora Giunchiglia|Maxime Kayser|Bowen Li|Thomas Lukasiewicz,,0,Visual Question Answering using Scene Graphs,,0.0,FALSE,en,FALSE,closed,,,2021-01-01,article
https://openalex.org/W3163567431,,Ikuto Kurosawa|Tetsunori Kobayashi|Yoshihiko Hayashi,Waseda University|Waseda University|Waseda University,0,Exploring and Exploiting the Hierarchical Structure of a Scene for Scene Graph Generation,https://doi.org/10.1109/icpr48806.2021.9413251,0.0,FALSE,en,FALSE,closed,,,2021-01-10,article
https://openalex.org/W3160576096,,Shabnam Sadegharmaki|Marc A. Kastner|Shin’ichi Satoh,Technical University of Munich|National Institute of Informatics|National Institute of Informatics,3,FashionGraph: Understanding fashion data using scene graph generation,https://doi.org/10.1109/icpr48806.2021.9412662,0.30665844,FALSE,en,FALSE,closed,,,2021-01-10,article
https://openalex.org/W3159629595,,Neil Hallonquist|Donald German|Laurent Younès,Johns Hopkins University Applied Physics Laboratory|Johns Hopkins University|Johns Hopkins University,0,Graph Discovery for Visual Test Generation,https://doi.org/10.1109/icpr48806.2021.9412848,0.0,FALSE,en,FALSE,closed,,,2021-01-10,article
https://openalex.org/W3162835725,,Anurag Tripathi|Siddharth Srivastava|Brejesh Lall|Santanu Chaudhury,Indian Institute of Technology Delhi|Indian Institute of Technology Delhi|Indian Institute of Technology Delhi|Indian Institute of Technology Delhi,2,Using Scene Graphs for Detecting Visual Relationships,https://doi.org/10.1109/icpr48806.2021.9412337,0.0,FALSE,en,FALSE,closed,,,2021-01-10,article
https://openalex.org/W3122572429,"Over the last several years, in parallel with the general global advancement in mobile technology and a rise in social media network content consumption, multimedia content production and reproduction has increased exponentially. Therefore, enabled by the rapid recent advancements in deep learning technology, research on scene graph generation is being actively conducted to more efficiently search for and classify images desired by users within a large amount of content. This approach lets users accurately find images they are searching for by expressing meaningful information on image content as nodes and edges of a graph. In this study, we propose a scene graph generation method based on using the Resource Description Framework (RDF) model to clarify semantic relations. Furthermore, we also use convolutional neural network (CNN) and recurrent neural network (RNN) deep learning models to generate a scene graph expressed in a controlled vocabulary of the RDF model to understand the relations between image object tags. Finally, we experimentally demonstrate through testing that our proposed technique can express semantic content more effectively than existing approaches.",Seong-Yong Kim|Tae Hyeon Jeon|Ilsun Rhiu|Jinhyun Ahn|Dong-Hyuk Im,Hoseo University|Hoseo University|Dongduk Women's University|Jeju National University|Kwangwoon University,11,Semantic Scene Graph Generation Using RDF Model and Deep Learning,https://doi.org/10.3390/app11020826,0.71553635,FALSE,en,TRUE,gold,Applied Sciences,journal,2021-01-17,article
https://openalex.org/W3126726381,,Xiaodong Wang|Fangmei Liu|Yonghong Hu,,0,On Simulative Training of Competitive Sports Based on Virtual Reality Technique,,0.0,FALSE,en,FALSE,closed,Solid State Technology,journal,2021-01-21,article
https://openalex.org/W3129130152,,An-An Liu|Yanhui Wang|Ning Xu|Shan Liu|Xuanya Li,Tianjin University|Tianjin University|Tianjin University||Baidu (China),12,Scene-Graph-Guided message passing network for dense captioning,https://doi.org/10.1016/j.patrec.2021.01.024,0.91997531,FALSE,en,FALSE,closed,Pattern Recognition Letters,journal,2021-02-04,article
https://openalex.org/W3126589408,"This paper presents a new framework for training image-based classifiers from a combination of texts and images with very few labels. We consider a classification framework with three modules: a backbone, a relational reasoning component, and a classification component. While the backbone can be trained from unlabeled images by self-supervised learning, we can fine-tune the relational reasoning and the classification components from external sources of knowledge instead of annotated images. By proposing a transformer-based model that creates structured knowledge from textual input, we enable the utilization of the knowledge in texts. We show that, compared to the supervised baselines with 1% of the annotated images, we can achieve ~8x more accurate results in scene graph classification, ~3x in object classification, and ~1.5x in predicate classification.",Sahand Sharifzadeh|Sina Moayed Baharlou|Martin Schmitt|Hinrich Schütze|Volker Tresp,,2,Improving Visual Reasoning by Exploiting The Knowledge in Texts.,,,FALSE,en,TRUE,green,arXiv (Cornell University),repository,2021-02-09,preprint
https://openalex.org/W3127546394,"The mainstream image captioning models rely on Convolutional Neural Network (CNN) image features with an additional attention to salient regions and objects to generate captions via recurrent models. Recently, scene graph representations of images have been used to augment captioning models so as to leverage their structural semantics, such as object entities, relationships and attributes. Several studies have noted that naive use of scene graphs from a black-box scene graph generator harms image caption-ing performance, and scene graph-based captioning mod-els have to incur the overhead of explicit use of image features to generate decent captions. Addressing these challenges, we propose a framework, SG2Caps, that utilizes only the scene graph labels for competitive image caption-ing performance. The basic idea is to close the semantic gap between two scene graphs - one derived from the input image and the other one from its caption. In order to achieve this, we leverage the spatial location of objects and the Human-Object-Interaction (HOI) labels as an additional HOI graph. Our framework outperforms existing scene graph-only captioning models by a large margin (CIDEr score of 110 vs 71) indicating scene graphs as a promising representation for image captioning. Direct utilization of the scene graph labels avoids expensive graph convolutions over high-dimensional CNN features resulting in 49%fewer trainable parameters.",Subarna Tripathi|Kien Nguyen|Tanaya Guha|Bang Du|Truong Q. Nguyen,,5,SG2Caps: Revisiting Scene Graphs for Image Captioning.,,,FALSE,en,TRUE,green,arXiv (Cornell University),repository,2021-02-09,preprint
https://openalex.org/W3129263794,,Shi Chen|Kazuyuki Demachi,The University of Tokyo|The University of Tokyo,70,Towards on-site hazards identification of improper use of personal protective equipment using deep learning-based geometric relationships and hierarchical scene graph,https://doi.org/10.1016/j.autcon.2021.103619,10.10566001,FALSE,en,FALSE,closed,Automation in Construction,journal,2021-02-17,article
https://openalex.org/W3154933149,,Vinod Kumar|Deepanshu Aggarwal|Vinamra Bathwal|Saurabh Singh,Delhi Technological University|Delhi Technological University|Delhi Technological University|Delhi Technological University,3,A Novel Approach to Scene Graph Vectorization,https://doi.org/10.1109/icccis51004.2021.9397230,0.19003548,FALSE,en,FALSE,closed,"2021 International Conference on Computing, Communication, and Intelligent Systems (ICCCIS)",conference,2021-02-19,article
https://openalex.org/W3133684245,,Mitra Tajrobehkar|Kaihua Tang|Hanwang Zhang|Joo‐Hwee Lim,Nanyang Technological University|Nanyang Technological University|Nanyang Technological University|Institute for Infocomm Research,9,Align R-CNN: A Pairwise Head Network for Visual Relationship Detection,https://doi.org/10.1109/tmm.2021.3062543,0.91997531,FALSE,en,FALSE,closed,IEEE Transactions on Multimedia,journal,2021-02-26,article
https://openalex.org/W3185848774,"<p indent=0mm>Autonomous furniture arrangement plays an important role in many computer vision and graphics applications, such as indoor scene design and dynamic scene generation. The traditional approaches leverage the common spatial, semantical and functional object-object relationships to understand the inner structure of the indoor scene and facilitate the scene generation task. Benefited from the large-scale indoor scene dataset, we propose to embed the unstructured furniture into the graph structure, and leverage the graph neural network to iteratively learn the distribution of scene layout. In order to satisfy the diversity of furniture arrangement, we propose to incorporate the graph neural network into a conditional variational autoencoder. It leverages an encoder to input the scene information into a latent vector that represent the Gaussian distribution, and employ a generator to decode the scene layout from the sampled Gaussian noise for conditional new scene generation. Experimentally, we observe better quality of our algorithm compared to various baselines via the minimum matching distance on the public Fu-floor benchmark. The proposed algorithm is important for many practical applications, including scene completion, interior design based on scene graph and so on.",Miao Yang|Qingnan Fan|Yujie Wang|Yueqi Duan|Baoquan Chen,Shandong University of Science and Technology|Stanford University|Shandong University of Science and Technology|Stanford University|Peking University,2,Graph Neural Network for Generative Furniture Arrangement,https://doi.org/10.3724/sp.j.1089.2021.18457,0.29113014,FALSE,en,TRUE,bronze,Journal of Computer-Aided Design & Computer Graphics,journal,2021-03-01,article
https://openalex.org/W3158981562,,Xiaoyi Zhang|Zheng Wang|Xing Xu|Jiwei Wei|Yang Yang,University of Electronic Science and Technology of China|University of Electronic Science and Technology of China|University of Electronic Science and Technology of China|University of Electronic Science and Technology of China|University of Electronic Science and Technology of China,2,Scene graph generation via multi-relation classification and cross-modal attention coordinator,https://doi.org/10.1145/3444685.3446276,0.10221948,FALSE,en,FALSE,closed,,,2021-03-07,article
https://openalex.org/W3134991666,,Junzhong Ji|Zhuoran Du|Xiaodan Zhang,Beijing University of Technology|Beijing University of Technology|Beijing University of Technology,39,Divergent-convergent attention for image captioning,https://doi.org/10.1016/j.patcog.2021.107928,3.57768176,FALSE,en,FALSE,closed,Pattern Recognition,journal,2021-03-09,article
https://openalex.org/W3138534878,,Peng Tian|Hongwei Mo|Laihao Jiang,Harbin Engineering University|Harbin Engineering University|Harbin Engineering University,8,Scene graph generation by multi-level semantic tasks,https://doi.org/10.1007/s10489-020-02115-2,0.71553635,FALSE,en,FALSE,closed,Applied Intelligence,journal,2021-03-17,article
https://openalex.org/W3139586422,"Scene graph is a structured representation of a scene that can clearly express the objects, attributes, and relationships between objects in the scene. As computer vision technology continues to develop, people are no longer satisfied with simply detecting and recognizing objects in images; instead, people look forward to a higher level of understanding and reasoning about visual scenes. For example, given an image, we want to not only detect and recognize objects in the image, but also know the relationship between objects (visual relationship detection), and generate a text description (image captioning) based on the image content. Alternatively, we might want the machine to tell us what the little girl in the image is doing (Visual Question Answering (VQA)), or even remove the dog from the image and find similar images (image editing and retrieval), etc. These tasks require a higher level of understanding and reasoning for image vision tasks. The scene graph is just such a powerful tool for scene understanding. Therefore, scene graphs have attracted the attention of a large number of researchers, and related research is often cross-modal, complex, and rapidly developing. However, no relatively systematic survey of scene graphs exists at present. To this end, this survey conducts a comprehensive investigation of the current scene graph research. More specifically, we first summarized the general definition of the scene graph, then conducted a comprehensive and systematic discussion on the generation method of the scene graph (SGG) and the SGG with the aid of prior knowledge. We then investigated the main applications of scene graphs and summarized the most commonly used datasets. Finally, we provide some insights into the future development of scene graphs. We believe this will be a very helpful foundation for future research on scene graphs.",Xiaojun Chang|Pengzhen Ren|Pengfei Xu|Zhihui Li|Xiaojiang Chen|Alex Hauptmann,,11,Scene Graphs: A Survey of Generations and Applications.,,,FALSE,en,TRUE,green,arXiv (Cornell University),repository,2021-03-17,preprint
https://openalex.org/W3156197669,,Jingwen Duan|Weidong Min|Deyu Lin|Jianfeng Xu|Xin Xiong,Nanchang University|Education Department of Jiangxi Province|Nanchang University|Nanchang University|Nanchang University|Nanchang University,6,Multimodal graph inference network for scene graph generation,https://doi.org/10.1007/s10489-021-02304-7,0.30665844,FALSE,en,FALSE,closed,Applied Intelligence,journal,2021-04-13,article
https://openalex.org/W3153500265,,Jiancheng Ni|Susu Zhang|Zili Zhou|Lijun Hou|Jie Hou|Feng Gao,Qufu Normal University|Qufu Normal University|Qufu Normal University|Qufu Normal University|Qufu Normal University|Qufu Normal University,7,Background and foreground disentangled generative adversarial network for scene image synthesis,https://doi.org/10.1016/j.cag.2021.04.003,0.71553635,FALSE,en,FALSE,closed,Computers & Graphics,journal,2021-04-19,article
https://openalex.org/W3157307310,"Video scene graph generation (ViDSGG), the creation of video scene graphs that helps in deeper and better visual scene understanding, is a challenging task. Segment-based and sliding-window based methods have been proposed to perform this task. However, they all have certain limitations. This study proposes a novel deep neural network model called VSGG-Net for video scene graph generation. The model uses a sliding window scheme to detect object tracklets of various lengths throughout the entire video. In particular, the proposed model presents a new tracklet pair proposal method that evaluates the relatedness of object tracklet pairs using a pretrained neural network and statistical information. To effectively utilize the spatio-temporal context, low-level visual context reasoning is performed using a spatio-temporal context graph and a graph neural network as well as high-level semantic context reasoning. To improve the detection performance for sparse relationships, the proposed model applies a class weighting technique that adjusts the weight of sparse relationships to a higher level. This study demonstrates the positive effect and high performance of the proposed model through experiments using the benchmark dataset VidOR and VidVRD.",Gayoung Jung|Jonghun Lee|Incheol Kim,Kyonggi University|Kyonggi University|Kyonggi University,9,Tracklet Pair Proposal and Context Reasoning for Video Scene Graph Generation,https://doi.org/10.3390/s21093164,0.71553635,FALSE,en,TRUE,gold,Sensors,journal,2021-05-02,article
https://openalex.org/W3123505479,,Fei Deng|Zhuo Zhi|Donghun Lee|Sungjin Ahn,"Rutgers Sexual and Reproductive Health and Rights|University of California, San Diego|Electronics and Telecommunications Research Institute|Rutgers Sexual and Reproductive Health and Rights",10,Generative Scene Graph Networks,,1.02219479,FALSE,en,FALSE,closed,,,2021-05-03,article
https://openalex.org/W3120919044,,Guoshun Nan|Jiaqi Zeng|Rui Qiao|Wei Lu,Singapore University of Technology and Design|Shanghai Jiao Tong University|Singapore University of Technology and Design|Singapore University of Technology and Design,0,Counterfactual Thinking for Long-tailed Information Extraction,,0.0,FALSE,en,FALSE,closed,,,2021-05-04,article
https://openalex.org/W3107392300,"Scene graph (SG) generation has been gaining a lot of traction recently. Current SG generation techniques, however, rely on the availability of expensive and limited number of labeled datasets. Synthetic data offers a viable alternative as labels are essentially free. However, neural network models trained on synthetic data, do not perform well on real data because of the domain gap. To overcome this challenge, we propose Sim2SG, a scalable technique for sim-to-real transfer for scene graph generation. Sim2SG addresses the domain gap by decomposing it into appearance, label and prediction discrepancies between the two domains. We handle these discrepancies by introducing pseudo statistic based self-learning and adversarial techniques. Sim2SG does not require costly supervision from the real-world dataset. Our experiments demonstrate significant improvements over baselines in reducing the domain gap both qualitatively and quantitatively. We validate our approach on toy simulators, as well as realistic simulators evaluated on real-world data.",Aayush Prakash|Shoubhik Debnath|Jean Francois Lafleche|Eric Cameracci|Gavriel State|Marc T. Law,Nvidia (United Kingdom)|Nvidia (United Kingdom)|Nvidia (United Kingdom)|Nvidia (United Kingdom)|Nvidia (United Kingdom)|Nvidia (United Kingdom),1,Sim2SG: Sim-to-Real Scene Graph Generation for Transfer Learning,,,FALSE,en,TRUE,green,arXiv (Cornell University),repository,2021-05-04,preprint
https://openalex.org/W3110903909,,Kyoung-Woon On|Eun‐Sol Kim|Il-Jae Kwon|Sangwoong Yoon|Byoung‐Tak Zhang,Seoul National University|Brain (Germany)|Kao Corporation (Japan)|Seoul National University|Seoul National University|Seoul National University,0,Spectrally Similar Graph Pooling,,0.0,FALSE,en,FALSE,closed,,,2021-05-04,article
https://openalex.org/W3133440195,,Yizhou Zhang|Zhaoheng Zheng|Yan Liu,Southern California University for Professional Studies|University of Southern California|Southern California University for Professional Studies|University of Southern California|University of Southern California|Southern California University for Professional Studies,0,Weakly Supervised Scene Graph Grounding,,0.0,FALSE,en,FALSE,closed,,,2021-05-04,article
https://openalex.org/W3176313262,"Visual storytelling is a task of creating a short story based on photo streams. Different from visual captions, stories contain not only factual descriptions, but also imaginary concepts that do not appear in the images. In this paper, we propose a novel imagine-reason-write generation framework (IRW) for visual storytelling, inspired by the logic of humans when they write the story. First, an imagine module is leveraged to learn the imaginative storyline explicitly, improving the coherence and reasonability of the generated story. Second, we employ a reason module to fully exploit the external knowledge (commonsense knowledge base) and task-specific knowledge (scene graph and event graph) with relational reasoning method based on the storyline. In this way, we can effectively capture the most informative commonsense and visual relationships among objects in images, which enhances the diversity and informativeness of the generated story. Finally, we integrate the imaginary concepts and relational knowledge to generate human-like story based on the original semantics of images. Extensive experiments on a benchmark dataset (i.e., VIST) demonstrate that the proposed IRW framework significantly outperforms the state-of-the-art methods across multiple evaluation metrics.",Chunpu Xu|Min Yang|Chengming Li|Ying Shen|Xiang Ao|Ruifeng Xu,Shenzhen Institutes of Advanced Technology|Shenzhen Institutes of Advanced Technology|Shenzhen Institutes of Advanced Technology||Institute of Computing Technology|Harbin Institute of Technology,26,"Imagine, Reason and Write: Visual Storytelling with Graph Knowledge and Relational Reasoning",https://doi.org/10.1609/aaai.v35i4.16410,1.39359356,FALSE,en,TRUE,diamond,Proceedings of the AAAI Conference on Artificial Intelligence,conference,2021-05-18,article
https://openalex.org/W3164115078,"Abstract The automatic generation of realistic images directly from a story text is a very challenging problem, as it cannot be addressed using a single image generation approach due mainly to the semantic complexity of the story text constituents. In this work, we propose a new approach that decomposes the task of story visualization into three phases: semantic text understanding, object layout prediction, and image generation and refinement. We start by simplifying the text using a scene graph triple notation that encodes semantic relationships between the story objects. We then introduce an object layout module to capture the features of these objects from the corresponding scene graph. Specifically, the object layout module aggregates individual object features from the scene graph as well as averaged or likelihood object features generated by a graph convolutional neural network. All these features are concatenated to form semantic triples that are then provided to the image generation framework. For the image generation phase, we adopt a scene graph image generation framework as stage-I, which is refined using a StackGAN as stage-II conditioned on the object layout module and the generated output image from stage-I. Our approach renders object details in high-resolution images while keeping the image structure consistent with the input text. To evaluate the performance of our approach, we use the COCO dataset and compare it with three baseline approaches, namely, sg2im, StackGAN and AttnGAN, in terms of image quality and user evaluation. According to the obtained assessment results, our object layout guidance-based approach significantly outperforms the abovementioned baseline approaches in terms of the accuracy of semantic matching and realism of the generated images representing the story text sentences.",Jezia Zakraoui|Moutaz Saleh|Somaya Al-Máadeed|Jihad Mohammed Jaam,Qatar University|Qatar University|Qatar University|Qatar University,19,Improving text-to-image generation with object layout guidance,https://doi.org/10.1007/s11042-021-11038-0,1.32885323,FALSE,en,TRUE,hybrid,Multimedia Tools and Applications,journal,2021-05-20,article
https://openalex.org/W3205633134,,Xinghang Li|Di Guo|Huaping Liu|Fuchun Sun,Tsinghua University|Center for Information Technology|Tsinghua University|Center for Information Technology|Center for Information Technology|Tsinghua University|Center for Information Technology|Tsinghua University,11,Robotic Indoor Scene Captioning from Streaming Video,https://doi.org/10.1109/icra48506.2021.9560904,1.02219479,FALSE,en,FALSE,closed,,,2021-05-30,article
https://openalex.org/W3173399746,,Yifeng Zhang|Ming Jiang|Qi Zhao,University of Minnesota System|University of Minnesota System|University of Minnesota System,25,Explicit Knowledge Incorporation for Visual Reasoning,https://doi.org/10.1109/cvpr46437.2021.00141,2.55548697,FALSE,en,FALSE,closed,,,2021-06-01,article
https://openalex.org/W3174151851,,Sijin Wang|Ziwei Yao|Ruiping Wang|Zhongqin Wu|Xilin Chen,Institute of Computing Technology|University of Chinese Academy of Sciences|Chinese Academy of Sciences|University of Chinese Academy of Sciences|Institute of Computing Technology|Chinese Academy of Sciences|Institute of Computing Technology|Chinese Academy of Sciences|University of Chinese Academy of Sciences||Institute of Computing Technology|University of Chinese Academy of Sciences|Chinese Academy of Sciences,27,FAIEr: Fidelity and Adequacy Ensured Image Caption Evaluation,https://doi.org/10.1109/cvpr46437.2021.01383,2.35104802,FALSE,en,FALSE,closed,,,2021-06-01,article
https://openalex.org/W3178679506,,Cheng-Fu Yang|Wan-Cyuan Fan|Fu-En Yang|Yu-Chiang Frank Wang,National Taiwan University|National Taiwan University|Asus (Taiwan)|National Taiwan University|National Taiwan University|Asus (Taiwan),36,LayoutTransformer: Scene Layout Generation with Conceptual and Spatial Diversity,https://doi.org/10.1109/cvpr46437.2021.00373,2.96436489,FALSE,en,FALSE,closed,,,2021-06-01,article
https://openalex.org/W3191053189,,Amir Rahimi|Kevin Lee|Amit Agarwal|Hyukseong Kwon|Rajan Bhattacharyya,HRL Laboratories (United States)|HRL Laboratories (United States)|HRL Laboratories (United States)|HRL Laboratories (United States)|HRL Laboratories (United States),2,Toward Improving The Visual Characterization of Sport Activities With Abstracted Scene Graphs,https://doi.org/10.1109/cvprw53098.2021.00507,0.0,FALSE,en,FALSE,closed,,,2021-06-01,article
https://openalex.org/W3168680617,,Xian Zhong|Zhengwei Yang|Mang Ye|Wenxin Huang|Jingling Yuan|Chia‐Wen Lin,Wuhan University of Technology|Wuhan University of Technology|Wuhan University|Hubei University|Wuhan University of Technology|National Tsing Hua University,9,Auxiliary Bi-Level Graph Representation for Cross-Modal Image-Text Retrieval,https://doi.org/10.1109/icme51207.2021.9428380,0.91997531,FALSE,en,FALSE,closed,,,2021-06-09,article
https://openalex.org/W3168245224,,Yao Yang|Bo Gu,Sun Yat-sen University|Sun Yat-sen University,2,Multiple Hub-Driven Attention Graph Network for Scene Graph Generation,https://doi.org/10.1109/icme51207.2021.9428430,0.20443896,FALSE,en,FALSE,closed,,,2021-06-09,article
https://openalex.org/W3171522711,,Hao Zhou|Tingjin Luo|Jun Zhang|Jun Lei|Shuo Li,National University of Defense Technology|National University of Defense Technology|National University of Defense Technology|National University of Defense Technology|National University of Defense Technology,2,Relationship-Aware Primal-Dual Graph Attention Network For Scene Graph Generation,https://doi.org/10.1109/icme51207.2021.9428472,0.10221948,FALSE,en,FALSE,closed,,,2021-06-09,article
https://openalex.org/W3167621940,,Yao Yang|Bo Gu|Mamoun Alazab|Neeraj Kumar|Han Yu,Sun Yat-sen University|Sun Yat-sen University|Charles Darwin University|Asia University|University of Petroleum and Energy Studies|Thapar Institute of Engineering & Technology|Key Laboratory of Guangdong Province|Sun Yat-sen University,10,Integrating Multihub Driven Attention Mechanism and Big Data Analytics for Virtual Representation of Visual Scenes,https://doi.org/10.1109/tii.2021.3089689,1.02219479,FALSE,en,FALSE,closed,IEEE Transactions on Industrial Informatics,journal,2021-06-16,article
https://openalex.org/W3206790637,,Xinghang Li|Di Guo|Huaping Liu|Fuchun Sun,,7,Embodied Semantic Scene Graph Generation,,0.38007097,FALSE,en,FALSE,closed,5th Annual Conference on Robot Learning,conference,2021-06-19,article
https://openalex.org/W3176524055,,An-An Liu|Hongshuo Tian|Ning Xu|Weizhi Nie|Yongdong Zhang|Mohan Kankanhalli,China Machine Press|Tianjin University|Tianjin University|Tianjin University|Tianjin University|University of Science and Technology of China|Chinese Academy of Sciences|National University of Singapore,37,Toward Region-Aware Attention Learning for Scene Graph Generation,https://doi.org/10.1109/tnnls.2021.3086066,3.16880385,FALSE,en,FALSE,closed,IEEE Transactions on Neural Networks and Learning Systems,journal,2021-06-21,article
https://openalex.org/W3215994516,,Xutao Deng|Liang Xie|Yibo Cui|Meishan Zhang|Huijiong Yan|Ye Yan|Erwei Yin,Academy of Military Medical Sciences|Academy of Military Medical Sciences|Academy of Military Medical Sciences|Tianjin University|Academy of Military Medical Sciences|Academy of Military Medical Sciences|Academy of Military Medical Sciences,0,Modular Attention Network based on Language Model for Referring Expression,https://doi.org/10.1109/mlise54096.2021.00023,0.0,FALSE,en,FALSE,closed,,,2021-07-01,article
https://openalex.org/W3186819486,"A deep understanding of our visual world is more than an isolated perception on a series of objects, and the relationships between them also contain rich semantic information. Especially for those satellite remote sensing images, the span is so large that the various objects are always of different sizes and complex spatial compositions. Therefore, the recognition of semantic relations is conducive to strengthen the understanding of remote sensing scenes. In this paper, we propose a novel multi-scale semantic fusion network (MSFN). In this framework, dilated convolution is introduced into a graph convolutional network (GCN) based on an attentional mechanism to fuse and refine multi-scale semantic context, which is crucial to strengthen the cognitive ability of our model Besides, based on the mapping between visual features and semantic embeddings, we design a sparse relationship extraction module to remove meaningless connections among entities and improve the efficiency of scene graph generation. Meanwhile, to further promote the research of scene understanding in remote sensing field, this paper also proposes a remote sensing scene graph dataset (RSSGD). We carry out extensive experiments and the results show that our model significantly outperforms previous methods on scene graph generation. In addition, RSSGD effectively bridges the huge semantic gap between low-level perception and high-level cognition of remote sensing images.",Peng Li|Dezheng Zhang|Aziguli Wulamu|Xin Liu|Peng Chen,University of Science and Technology Beijing|University of Science and Technology Beijing|University of Science and Technology Beijing|University of Alberta|University of Science and Technology Beijing|Bank of China,14,Semantic Relation Model and Dataset for Remote Sensing Scene Understanding,https://doi.org/10.3390/ijgi10070488,0.91997531,FALSE,en,TRUE,gold,ISPRS International Journal of Geo-Information,journal,2021-07-17,article
https://openalex.org/W3199179967,,Haiyan Gao|Xin Tian|Yi Ji|Ying Li|Chunping Liu,Soochow University|Soochow University|Soochow University|Soochow University|Soochow University,0,Do We Really Reduce Bias for Scene Graph Generation?,https://doi.org/10.1109/ijcnn52387.2021.9533447,0.0,FALSE,en,FALSE,closed,,,2021-07-18,article
https://openalex.org/W3199100740,,Zhichao Zhang|Junyu Dong|Qilu Zhao|Lin Qi|Shu Zhang,Ocean University of China|Ocean University of China|Ocean University of China|Ocean University of China|Ocean University of China,0,Attention LSTM for Scene Graph Generation,https://doi.org/10.1109/icivc52351.2021.9526967,0.0,FALSE,en,FALSE,closed,,,2021-07-23,article
https://openalex.org/W3194910448,,Quoc-An Luong|Duc Minh Vo|Akihiro Sugimoto,"The Graduate University for Advanced Studies, SOKENDAI|Nippon Soken (Japan)|The University of Tokyo|National Institute of Informatics",0,Saliency based Subject Selection for Diverse Image Captioning,https://doi.org/10.23919/mva51890.2021.9511360,0.0,FALSE,en,FALSE,closed,,,2021-07-25,article
https://openalex.org/W3197904762,"Abstract Lots of machine learning tasks require dealing with graph data, and among them, scene graph generation is a challenging one that calls for graph neural networks’ potential ability. In this paper, we present a definition of graph neural network (GNN) consists of node, edge and global attribute, as well as their corresponding update and aggregate functions. Based on this, we then propose a realization of GNN model called Graph-LSTM and use it in scene graph generation. The model first extracts the item features in the image as the initial states of the node-LSTM representing subject/object and edge-LSTM representing predicate. Two LSTMs update the states via LSTM’s timestep and aggregate information via message passing. Repeat the update-aggregate until convergence. Meanwhile, the tag feature, i.e., the generated probability distribution of image’s semantic concepts is sent to the LSTM through a semantic compositional network (SCN). The SCN-LSTM is trained in an ensemble style, and hence allows the tag feature to serve as the global attribute providing context information to all individuals. The LSTMs’ final states are input to inference modules and generate the triplet ( subject, predicate, object ) of the scene graph. Experimental results show that Graph-LSTM outperforms the Message Passing and the attention Graph Covolutional Network methods, proving the effectiveness of the proposed scheme.",Tong Shao|Dapeng Wu,University of Florida|University of Florida,3,Graph-LSTM with Global Attribute for Scene Graph Generation,https://doi.org/10.1088/1742-6596/2003/1/012001,0.42331073,FALSE,en,TRUE,diamond,Journal of Physics Conference Series,journal,2021-08-01,article
https://openalex.org/W4300711121,"This paper presents an approach for modeling landmark sites such as the Statue of Liberty based on large-scale contaminated image collections gathered from the Internet. Our system combines 2D appearance and 3D geometric constraints to efficiently extract scene summaries, build 3D models, and recognize instances of the landmark in new test images. We start by clustering images using low-dimensional global “gist” descriptors. Next, we perform geometric verification to retain only the clusters whose images share a common 3D structure. Each valid cluster is then represented by a single iconic view, and geometric relationships between iconic views are captured by an iconic scene graph. In addition to serving as a compact scene summary, this graph is used to guide structure from motion to efficiently produce 3D models of the different aspects of the landmark. The set of iconic images is also used for recognition, i.e., determining whether new test images contain the landmark. Results on three data sets consisting of tens of thousands of images demonstrate the potential of the proposed approach.",Xiaowei li|Changchang Wu|Christopher Zach|Svetlana Lazebnik|Jan‐Michael Frahm,University of North Carolina at Chapel Hill|University of North Carolina at Chapel Hill|University of North Carolina at Chapel Hill|University of North Carolina at Chapel Hill|University of North Carolina at Chapel Hill,0,Modeling and Recognition of Landmark Image Collections Using Iconic Scene Graphs,https://doi.org/10.17615/gzka-wp71,0.0,FALSE,en,TRUE,green,UNC Libraries,repository,2021-08-14,article
https://openalex.org/W3213351830,,Xian Zhong|Tianyou Lu|Wenxuan Liu|Duxiu Feng|Qi Cui|Luo Zhong,Wuhan University of Technology|Wuhan University of Technology|Wuhan University of Technology||China State Construction Engineering (China)|Wuhan University of Technology,0,Visual Relationship Learning for Cross-Modal Retrieval,https://doi.org/10.1109/iccse51940.2021.9569264,0.0,FALSE,en,FALSE,closed,,,2021-08-17,article
https://openalex.org/W3196620563,,Yupan Huang|Zhaoyang Zeng|Yutong Lu,Sun Yat-sen University|Sun Yat-sen University|Sun Yat-sen University,4,"Be Specific, Be Clear: Bridging Machine and Human Captions by Scene-Guided Transformer",https://doi.org/10.1145/3463945.3469054,0.40887792,FALSE,en,FALSE,closed,,,2021-08-21,article
https://openalex.org/W3195588784,,Cong Bai|Anqi Zheng|Huang Yuan|Xiang Pan|Nan Chen,Zhejiang University of Technology|Zhejiang University of Technology|Zhejiang University of Technology|Zhejiang University of Technology|Qilu Normal University,42,Boosting convolutional image captioning with semantic content and visual relationship,https://doi.org/10.1016/j.displa.2021.102069,3.27102333,FALSE,en,FALSE,closed,Displays,journal,2021-08-23,article
https://openalex.org/W3196091473,,Haishun Chen|Ying Wang|Xin Yang|Jie Li,Xidian University|Xidian University|Xidian University|Xidian University,17,Captioning Transformer With Scene Graph Guiding,https://doi.org/10.1109/icip42928.2021.9506193,1.32885323,FALSE,en,FALSE,closed,,,2021-08-23,article
https://openalex.org/W3195149243,,Wonhee Lee|Sungeun Kim|Gunhee Kim,Samsung (South Korea)|Seoul National University|Seoul National University|Seoul National University,0,Contextual Label Transformation For Scene Graph Generation,https://doi.org/10.1109/icip42928.2021.9506213,0.0,FALSE,en,FALSE,closed,,,2021-08-23,article
https://openalex.org/W3194878172,,Toshal Patel|Alvin Yan Hong Yao|Qiang Yu|Wei Tsang Ooi|Roger Zimmermann,National University of Singapore|National University of Singapore|Seagate (United States)|National University of Singapore|National University of Singapore,5,Multi-Camera Video Scene Graphs for Surveillance Videos Indexing and Retrieval,https://doi.org/10.1109/icip42928.2021.9506713,0.30665844,FALSE,en,FALSE,closed,,,2021-08-23,article
https://openalex.org/W3197809352,,Yu Duan|Yun Xiong|Yao Zhang|Yuwei Fu|Yangyong Zhu,Fudan University|Fudan University|Fudan University|Fudan University|Fudan University,8,HSGMP,https://doi.org/10.1145/3460426.3463650,0.81775583,FALSE,en,FALSE,closed,,,2021-08-24,article
https://openalex.org/W3195797850,,An-An Liu|Zimu Lu|Ning Xu|Weizhi Nie|Wenhui Li,Tianjin University|Tianjin University|Tianjin University|Tianjin University|Tianjin University,5,Multi-type decision fusion network for visual Q&amp;A,https://doi.org/10.1016/j.imavis.2021.104281,0.51109739,FALSE,en,FALSE,closed,Image and Vision Computing,journal,2021-08-24,article
https://openalex.org/W3200680343,,Syefrida Yulina,Caltex Riau Polytechnic|Peninsula Library System,0,Design and Implementation: JavaFX Face Detection with Scene Builder and Netbeans IDE,,,FALSE,en,FALSE,closed,,,2021-08-25,article
https://openalex.org/W3208355706,,Zhijie Qin|Wei Zhong|Fei Hu|Xinyan Yang|Long Ye|Qin Zhang,Communication University of China|Communication University of China|Communication University of China|Communication University of China|Communication University of China|Communication University of China,1,Layout Structure Assisted Indoor Image Generation,https://doi.org/10.1109/mipr51284.2021.00061,0.0,FALSE,en,FALSE,closed,,,2021-09-01,article
https://openalex.org/W3204531320,"Aiming at the problem of ill-posed regions with blurred disparity edges,unsmooth disparity,discontinuous disparity of a single object,and holes in stereo matching,a lightweight real-time binocular depth estimation algorithm is proposed,which uses the semantic tags obtained by semantic segmentation of the scene graph and the edge detail images obtained by edge detection asauxi-liary loss,and the ground truth image as the main loss,to construct the joint loss function which can better supervise the generation of the disparity map.In addition,a lightweight feature extraction module is constructed to reduce the redundancy of the feature extraction stage,which can better simplify the feature extraction steps,and improve the real-time and lightness of the network.Finally,the idea of from coarse to fine is used to realize the gradual refinement process of the disparity map with fusion of low-resolution disparity map deformation and high-resolution feature map to generate disparity maps of different scales in stages,meanwhile,the detailed features are gradually enriched,thus obtaining the final accurate disparity map.The 3px error rate of 1.72% is obtained on the KITTI 2012 dataset,the Vintge error rate on the Middlebury 2014 dataset is 1.23%,the Playroom error rate is 2.23%,and the Recycle error rate is 1.65%.Meanwhile,the calculation time on the Scene Flow dataset reaches 0.76 s with 2.4 G memory occupation,which significantly improves the accuracy and computational efficiency of stereo matching algorithms in the ill-posed regions,meets the real-time requirements in engineering practice,and has important guiding significance for real-time 3D reconstruction tasks.",Peng Zhang|Wang Xinqing|Yi Xiao|Baoguo Duan|Honghui Xu,,1,Real-time Binocular Depth Estimation Algorithm Based on Semantic Edge Drive,https://doi.org/10.11896/jsjkx.200800203,0.09168142,FALSE,en,TRUE,green,SHILAP Revista de lepidopterología,journal,2021-09-01,article
https://openalex.org/W3198555355,,Bingqian Lin|Yi Zhu|Xiaodan Liang,Sun Yat-sen University|University of Chinese Academy of Sciences|Sun Yat-sen University,23,Atom correlation based graph propagation for scene graph generation,https://doi.org/10.1016/j.patcog.2021.108300,1.9421701,FALSE,en,FALSE,closed,Pattern Recognition,journal,2021-09-03,article
https://openalex.org/W3199006865,"In this paper, we address the problem of image sequence-based self-localization (ISS) from a new highly compressive scene representation called sequential semantic scene graph (S3G). Recent developments in deep graph convolutional neural networks (GCNs) have enabled a highly compressive visual place classifier (VPC) that can use a scene graph as the input modality. However, in such a highly compressive application, the amount of information lost in the image-to-graph mapping is significant and can damage the classification performance. To address this issue, we propose a pair of similarity-preserving mappings, image-to-nodes and image-to-edges, such that the nodes and edges act as absolute and relative features, respectively, that complement each other. Moreover, the proposed GCN-VPC is applied to a new task of viewpoint planning (VP) of the query image sequence, which contributes to further improvement in the VPC performance. Experiments using the public NCLT dataset validated the effectiveness of the proposed method.",Mitsuki Yoshida|Ryogo Yamamoto|Tanaka Kanji,||University of Fukui,1,S3G-ARM: Highly Compressive Visual Self-localization from Sequential Semantic Scene Graph Using Absolute and Relative Measurements.,,,FALSE,en,TRUE,green,arXiv (Cornell University),repository,2021-09-09,preprint
https://openalex.org/W3201890160,,Jing Shi|Yiwu Zhong|Ning Xu|Yin Li|Chenliang Xu,University of Rochester|University of Wisconsin–Madison|Adobe Systems (United States)|University of Wisconsin–Madison|University of Rochester,23,A Simple Baseline for Weakly-Supervised Scene Graph Generation,https://doi.org/10.1109/iccv48922.2021.01608,1.33024839,FALSE,en,FALSE,closed,2021 IEEE/CVF International Conference on Computer Vision (ICCV),conference,2021-10-01,article
https://openalex.org/W3208467797,,Nuno Pereira|Anthony Rowe|Michael W. Farb|Ivan Liang|Edward Lu|Eric Riebling,Carnegie Mellon University|Polytechnic Institute of Porto|Carnegie Mellon University|Carnegie Mellon University|Carnegie Mellon University|Carnegie Mellon University|Carnegie Mellon University,35,ARENA: The Augmented Reality Edge Networking Architecture,https://doi.org/10.1109/ismar52148.2021.00065,3.06658437,FALSE,en,FALSE,closed,,,2021-10-01,article
https://openalex.org/W3201861986,,Yichao Lu|Himanshu Rai|Jason S. Chang|B. A. Knyazev|Guangwei Yu|Shashank Shekhar|Graham W. Taylor|Maksims Volkovs,|||University of Guelph|Vector Institute||University of Guelph|Vector Institute|University of Guelph|Vector Institute|,78,Context-aware Scene Graph Generation with Seq2Seq Transformers,https://doi.org/10.1109/iccv48922.2021.01563,3.86405486,FALSE,en,FALSE,closed,2021 IEEE/CVF International Conference on Computer Vision (ICCV),conference,2021-10-01,article
https://openalex.org/W3201670572,,Markos Diomataris|Nikolaos Gkanatsios|Vassilis Pitsikalis|Petros Maragos,National Technical University of Athens|Carnegie Mellon University||National Technical University of Athens,6,Grounding Consistency: Distilling Spatial Common Sense for Precise Visual Relationship Detection,https://doi.org/10.1109/iccv48922.2021.01561,0.31672581,FALSE,en,FALSE,closed,2021 IEEE/CVF International Conference on Computer Vision (ICCV),conference,2021-10-01,article
https://openalex.org/W3215135886,,Kang Zhou|Chi Guo|Huyin Zhang,Wuhan University|Beijing Satellite Navigation Center|Wuhan University|Wuhan University,4,Visual Navigation via Reinforcement Learning and Relational Reasoning,https://doi.org/10.1109/swc50871.2021.00027,0.20443896,FALSE,en,FALSE,closed,,,2021-10-01,article
https://openalex.org/W3214782054,,Jia‐Qi Zhang|Xiang Xu|Zhi‐Meng Shen|Zehuan Huang|Yang Zhao|Yan‐Pei Cao|Pengfei Wan|Miao Wang,Beihang University|Beihang University|Beihang University|Beihang University|Beihang University|Guilford Technical Community College|Kuaishou (China)|Kuaishou (China)|Guilford Technical Community College|Beihang University,15,Write‐An‐Animation: High‐level Text‐based Animation Editing with Character‐Scene Interaction,https://doi.org/10.1111/cgf.14415,1.19549618,FALSE,en,FALSE,closed,Computer Graphics Forum,journal,2021-10-01,article
https://openalex.org/W3207531676,,Hao Zhou|Yazhou Yang|Tingjin Luo|Jun Zhang|Shuohao Li,National University of Defense Technology|National University of Defense Technology|National University of Defense Technology|National University of Defense Technology|National University of Defense Technology,29,A unified deep sparse graph attention network for scene graph generation,https://doi.org/10.1016/j.patcog.2021.108367,1.73773114,FALSE,en,FALSE,closed,Pattern Recognition,journal,2021-10-08,article
https://openalex.org/W3205857725,,Tianling Jiang|Hailin Shao|Xin Tian|Yi Ji|Chunping Liu,Soochow University|Soochow University|Soochow University|Soochow University|Soochow University,9,Aligning vision-language for graph inference in visual dialog,https://doi.org/10.1016/j.imavis.2021.104316,0.81775583,FALSE,en,FALSE,closed,Image and Vision Computing,journal,2021-10-13,article
https://openalex.org/W3201853009,,George Pantazopoulos|Jeremy Bruyere|Malvina Nikandrou|Thibaud Boissier|Supun Hemanthage|Binha Kumar Sachish|V.K. Shah|Christian Dondrup|Oliver Lemon,Heriot-Watt University|Heriot-Watt University|Heriot-Watt University|Heriot-Watt University|Heriot-Watt University|Heriot-Watt University|Heriot-Watt University|Heriot-Watt University|Heriot-Watt University,4,"ViCA: Combining visual, social, and task-oriented conversational AI in a healthcare setting",https://doi.org/10.1145/3462244.3479909,0.20443896,FALSE,en,FALSE,closed,,,2021-10-15,article
https://openalex.org/W3205398323,,Sitong Su|Lianli Gao|Junchen Zhu|Jie Shao|Jingkuan Song,University of Electronic Science and Technology of China|University of Electronic Science and Technology of China|University of Electronic Science and Technology of China|University of Electronic Science and Technology of China|University of Electronic Science and Technology of China,14,Fully Functional Image Manipulation Using Scene Graphs in A Bounding-Box Free Way,https://doi.org/10.1145/3474085.3475326,1.43107271,FALSE,en,FALSE,closed,,,2021-10-17,article
https://openalex.org/W3205377310,,Mengshi Qi|Jie Qin|Di Huang|Zhiqiang Shen|Yi Yang|Jiebo Luo,Beijing University of Posts and Telecommunications|Nanjing University of Aeronautics and Astronautics|Beihang University|Carnegie Mellon University|University of Technology Sydney|University of Rochester,17,Latent Memory-augmented Graph Transformer for Visual Storytelling,https://doi.org/10.1145/3474085.3475236,1.63551166,FALSE,en,FALSE,closed,,,2021-10-17,article
https://openalex.org/W3205822151,,Hongshuo Tian|Ning Xu|An-An Liu|Chenggang Yan|Zhendong Mao|Quan Zhang|Yongdong Zhang,Tianjin People's Hospital|Tianjin University|Tianjin University|Tianjin University|Hangzhou Dianzi University|University of Science and Technology of China|University of Science and Technology Beijing|Peking University|University of Science and Technology Beijing|University of Science and Technology of China,11,Mask and Predict,https://doi.org/10.1145/3474085.3475545,1.02219479,FALSE,en,FALSE,closed,,,2021-10-17,article
https://openalex.org/W3205866742,,Yanyuan Qiao|Qi Chen|Chaorui Deng|Ning Ding|Yuankai Qi|Mingkui Tan|Xincheng Ren|Qi Wu,University of Adelaide|University of Adelaide|University of Adelaide|South China University of Technology|University of Adelaide|South China University of Technology|Yanan University Affiliated Hospital|University of Adelaide,17,R-GAN: Exploring Human-like Way for Reasonable Text-to-Image Synthesis via Generative Adversarial Networks,https://doi.org/10.1145/3474085.3475363,1.63551166,FALSE,en,FALSE,closed,,,2021-10-17,article
https://openalex.org/W3206617243,,Zeming Liao|Qingbao Huang|Yu Liang|Mingyi Fu|Yi Cai|Qing Li,Guangxi University|South China University of Technology|Guangxi University|Guangxi University|Guangxi University|South China University of Technology|Hong Kong Polytechnic University,16,Scene Graph with 3D Information for Change Captioning,https://doi.org/10.1145/3474085.3475712,1.43107271,FALSE,en,FALSE,closed,,,2021-10-17,article
https://openalex.org/W3207942052,,Weizhi Nie|Jiesi Li|Ning Xu|An-An Liu|Xuanya Li|Yongdong Zhang,Tianjin People's Hospital|Tianjin University|Tianjin University|Tianjin University|Tianjin University|Baidu (China)|University of Science and Technology of China,7,Triangle-Reward Reinforcement Learning,https://doi.org/10.1145/3474085.3475604,0.71553635,FALSE,en,FALSE,closed,,,2021-10-17,article
https://openalex.org/W4200311227,,Yonghua Zhu|Jieyu Huang|Ning Ge|Yunwen Zhu|Binghui Zheng|Wenjun Zhang,Shanghai Open University|Shanghai University|Shanghai Open University|Shanghai University|Shanghai University|Shanghai Open University|Shanghai Open University|Shanghai University|Shanghai Open University|Shanghai University|Shanghai Jian Qiao University|Shanghai University,0,Text Pared into Scene Graph for Diverse Image Generation,https://doi.org/10.1145/3487075.3487158,0.0,FALSE,en,FALSE,closed,,,2021-10-19,article
https://openalex.org/W4200348116,,Soyeon Kim|Kyung-no Joo|Chan‐Hyun Youn,Korea Advanced Institute of Science and Technology|Korea Advanced Institute of Science and Technology|Korea Advanced Institute of Science and Technology,4,Graph Neural Network based Scene Change Detection Using Scene Graph Embedding with Hybrid Classification Loss,https://doi.org/10.1109/ictc52510.2021.9621009,1.57218255,FALSE,en,FALSE,closed,2021 International Conference on Information and Communication Technology Convergence (ICTC),conference,2021-10-20,article
https://openalex.org/W4241704080,,Gao Zheng Yang|Liu Pei|Kang Zeng Xin|Chu Zhong Yi,Beihang University|Beihang University|Beihang University|Beihang University,2,Manual assembly action segmentation method based on spatiotemporal features,https://doi.org/10.1109/cac53003.2021.9727556,0.12669032,FALSE,en,FALSE,closed,2021 China Automation Congress (CAC),conference,2021-10-22,article
https://openalex.org/W3195129957,"Visual question answering (VQA) is a challenging problem in machine perception, which requires a deep joint understanding of both visual and textual data. Recent research has advanced the automatic generation of high-quality scene graphs from images, while powerful yet elegant models like graph neural networks (GNNs) have shown great power in reasoning over graph-structured data. In this work, we propose to bridge the gap between scene graph generation and VQA by leveraging GNNs. In particular, we design a new model called Conditional Enhanced Graph ATtention network (CE-GAT) to encode pairs of visual and semantic scene graphs with both node and edge features, which is seamlessly integrated with a textual question encoder to generate answers through question-graph conditioning. Moreover, to alleviate the training difficulties of CE-GAT towards VQA, we enforce more useful inductive biases in the scene graphs through novel question-guided graph enriching and pruning. Finally, we evaluate the framework on one of the largest available VQA datasets (namely, GQA) with ground-truth scene graphs, achieving the accuracy of 77.87%, compared with the state of the art (namely, the neural state machine (NSM)), which gives 63.17%. Notably, by leveraging existing scene graphs, our framework is much lighter compared with end-to-end VQA methods (e.g., about 95.3% less parameters than a typical NSM).",Sai Vidyaranya Nuthalapati|Ramraj Chandradevan|Eleonora Giunchiglia|Bowen Li|Maxime Kayser|Thomas Lukasiewicz|Carl Yang,University of Oxford|Emory University|University of Oxford|University of Oxford|University of Oxford|University of Oxford|Emory University,20,Lightweight Visual Question Answering using Scene Graphs,https://doi.org/10.1145/3459637.3482218,1.53329218,FALSE,en,TRUE,gold,,,2021-10-26,article
https://openalex.org/W4200046362,,Yuming Sun|Hangyu Lin|Chen Liu|Yanwei Fu,Fudan University|Fudan University|University of Hong Kong|Fudan University,1,Learning Depth Information in Layout for Sketch Generation from Scene Graph,https://doi.org/10.1109/bigdia53151.2021.9619707,0.10221948,FALSE,en,FALSE,closed,,,2021-10-29,article
https://openalex.org/W3213646606,,Aiswarya S. Kumar|Jyothisha J. Nair,Amrita Vishwa Vidyapeetham|Amrita Vishwa Vidyapeetham,1,A novel SPLIT-SIM approach for efficient image retrieval,https://doi.org/10.1007/s00530-021-00864-9,0.10221948,FALSE,en,FALSE,closed,Multimedia Systems,journal,2021-11-08,article
https://openalex.org/W4205119235,"The animation construction of forest scene is a virtual stand scene visualization framework which uses the related technologies of virtual forest modeling and stand scene visualization, and uses the scene graph technology to manage. This paper studies the influence of digital media technology on the animation design of forest scene. In this paper, the model of virtual stand scene is mainly completed by Creator modeling software of MultiGen company. In order to reduce the number of scene patches and ensure realism, the tree model is designed with OpenFlight tree hierarchy. At the same time, the key technologies of Creator modeling and model optimization are analyzed. The virtual stand scene visualization framework uses the open source graphics rendering engine OpenSceneGraph (OSG) as the scene driver to realize the stand scene visualization. This paper provides a variety of roaming control methods. The experimental results show that the virtual forest scene visualization framework can better simulate the forest scene and has a strong sense of reality.",Lu Yu,Henan Polytechnic University,1,Influence of Digital Media Technology on Forest Scene Animation Design,https://doi.org/10.17762/jfcr.vi.184,0.0,FALSE,en,TRUE,bronze,Forest Chemicals Review,journal,2021-11-16,article
https://openalex.org/W3217281196,,Jie Lin|Yi Cai|Xin Wu|Jianwei Lu,South China University of Technology|South China University of Technology|South China University of Technology|South China University of Technology,6,Graph-Based Information Block Detection in Infographic With Gestalt Organization Principles,https://doi.org/10.1109/tvcg.2021.3130071,0.40887792,FALSE,en,FALSE,closed,IEEE Transactions on Visualization and Computer Graphics,journal,2021-11-23,article
https://openalex.org/W4250643848,,Bowen Zhao|Zhendong Mao|Shancheng Fang|Wenyu Zang|Yongdong Zhang,University of Science and Technology of China|University of Science and Technology of China|University of Science and Technology of China|Institute of Information Engineering|China Electronics Corporation (China)|Chinese Academy of Sciences|University of Science and Technology of China,17,Semantically Similarity-Wise Dual-Branch Network for Scene Graph Generation,https://doi.org/10.1109/tcsvt.2021.3130197,1.12441427,FALSE,en,FALSE,closed,IEEE Transactions on Circuits and Systems for Video Technology,journal,2021-11-23,article
https://openalex.org/W4200341733,,Xuan Hou|Huailin Zhao,Shanghai Institute of Technology|Shanghai Institute of Technology,0,Edge-cloud Collaborative Architecture for Scene Graph Application System,https://doi.org/10.1109/iciibms52876.2021.9651576,0.0,FALSE,en,FALSE,closed,,,2021-11-25,article
https://openalex.org/W4321383586,,Todor Stojanovski,KTH Royal Institute of Technology,5,Urban morphology and artificial intelligence,https://doi.org/10.51347/um26.0005,0.25864281,FALSE,en,FALSE,closed,Urban Morphology,journal,2021-11-28,article
https://openalex.org/W4210369563,,Zhiming Wang|Yuxiao Li|Danlan Huang|Yantian Luo|Ning Ge|Jianhua Lu,National Engineering Research Center for Information Technology in Agriculture|Tsinghua University|National Engineering Research Center for Information Technology in Agriculture|Tsinghua University|Tsinghua University|National Engineering Research Center for Information Technology in Agriculture|National Engineering Research Center for Information Technology in Agriculture|Tsinghua University|Tsinghua University|National Engineering Research Center for Information Technology in Agriculture|National Engineering Research Center for Information Technology in Agriculture|Tsinghua University,5,Deformable Geometry based Semantic Reconstruction from Scene Graphs,https://doi.org/10.1109/globecom46510.2021.9685640,0.19003548,FALSE,en,FALSE,closed,2021 IEEE Global Communications Conference (GLOBECOM),conference,2021-12-01,article
https://openalex.org/W4205393926,,Chao Zhang|Ignas Budvytis|Stephan Liwicki|Roberto Cipolla,Toshiba (United Kingdom)|University of Cambridge|Toshiba (United Kingdom)|University of Cambridge|Toshiba (United Kingdom),3,Lifted Semantic Graph Embedding for Omnidirectional Place Recognition,https://doi.org/10.1109/3dv53792.2021.00147,1.28293791,FALSE,en,FALSE,closed,2021 International Conference on 3D Vision (3DV),conference,2021-12-01,article
https://openalex.org/W4200013588,,Tian Zheng|Wenhua Qian|Rencan Nie|Jinde Cao|Dan Xu,Yunnan University|Yunnan University|Yunnan University|Yonsei University|Southeast University|Yunnan University,0,Graph Structural Attention and Increased Global Attention for Image Captioning,https://doi.org/10.1109/icicip53388.2021.9642211,0.0,FALSE,en,FALSE,closed,,,2021-12-03,article
https://openalex.org/W3214135126,,Minghao Xu|Meng Qu|Bingbing Ni|Jian Tang,Shanghai Jiao Tong University||Shanghai Jiao Tong University|HEC Montréal,5,Joint Modeling of Visual Objects and Relations for Scene Graph Generation,,3.14748201,FALSE,en,FALSE,closed,Neural Information Processing Systems,conference,2021-12-06,article
https://openalex.org/W3213320466,,Shoulong Zhang|Shuai Li|Aimin Hao|Hong Qin,"Beihang University|Beihang University|Beihang University|University at Albany, State University of New York|Albany State University",16,Knowledge-inspired 3D Scene Graph Prediction in Point Cloud,,5.7860327,FALSE,en,FALSE,closed,Neural Information Processing Systems,conference,2021-12-06,article
https://openalex.org/W4206837865,,Chuxue Cao|Yiming He|Yuzhen Chen|Chunli Song|Hao Ling|Renchu Guan|Xiaoyue Feng,Jilin University|Jilin University|Jilin University|Second Affiliated Hospital of Jilin University|Second Affiliated Hospital of Jilin University|Jilin University|Jilin University,0,Medical Scene Graphs and Reasoning,https://doi.org/10.1109/bibm52615.2021.9669882,0.0,FALSE,en,FALSE,closed,2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),conference,2021-12-09,article
https://openalex.org/W4210632034,,Elham Alighardash|Hassan Khotanlou|Vahid Pour Amin,Bu-Ali Sina University|Bu-Ali Sina University|,1,Conceptual Intelligent Model for Visual Question Answering Using Attention Mechanism and Relational Reasoning,https://doi.org/10.1109/ikt54664.2021.9685682,0.10221948,FALSE,en,FALSE,closed,,,2021-12-14,article
https://openalex.org/W4200464318,,Ga Young Jung|Incheol Kim,,0,Dynamic 3D Scene Graph Generation for Robotic Manipulation Tasks,https://doi.org/10.5302/j.icros.2021.21.0140,0.0,FALSE,en,FALSE,closed,Journal of Institute of Control Robotics and Systems,journal,2021-12-15,article
https://openalex.org/W4200261572,,Ping Li|Yu Zhou|Yibing Zhan,Hangzhou Dianzi University|Hangzhou Dianzi University|Jingdong (China)|Hangzhou Dianzi University,6,Deep relational self-Attention networks for scene graph generation,https://doi.org/10.1016/j.patrec.2021.12.013,0.51109739,FALSE,en,FALSE,closed,Pattern Recognition Letters,journal,2021-12-24,article
https://openalex.org/W4205395137,"To understand an image or a scene properly, it is necessary to identify objects participating in the scene, their relationships, and various attributes that describe their properties. A scene graph is a high-level representation that confines all these features in a structured manner. Scene graph generation includes multiple challenges like the semantics of relationships considered and the availability of a well-balanced dataset with sufficient training examples. We tried to mitigate these problems by extracting two subsets, VG-R10 and VG-A16, from the popular Visual Genome dataset. Also, a framework (S2G) is proposed for generating scene graphs directly from images using depth and spatial information of object pairs. Evaluations on the scene graph generation model reveal that the proposed framework achieves better results on our data than the state-of-the-art.",Aiswarya S. Kumar|Jyothisha J. Nair,Amrita Vishwa Vidyapeetham|Amrita Vishwa Vidyapeetham,9,"Scene Graph Generation Using Depth, Spatial, and Visual Cues in 2D Images",https://doi.org/10.1109/access.2021.3139000,0.71553635,FALSE,en,TRUE,gold,IEEE Access,journal,2021-12-27,article
https://openalex.org/W4313011309,"Many learning activities include working with graph data, which offers a wealth of relational information between parts.Modeling physical systems, learning molecular fingerprints, predicting protein interfaces, and diagnosing illnesses all need the use of a model that can learn from graph inputs.In other fields, such as learning from non-structural data such as texts and images, reasoning on extracted structures (such as phrase dependency trees and image scene graphs) is a major topic that requires graph reasoning models.Graph neural networks (GNNs) are neural models that use message transmission between graph nodes to represent graph dependency.Variants of GNNs have recently showed ground-breaking performance on a variety of deep learning tasks.This paper represents a review of the literature on Knowledge Graphs and Graph Neural Networks, with a particular focus on Graph Embeddings and Graph Neural Networks applications as a powerful tool for organizing structured data and making sense of unstructured data, which can be applied to a variety of real-world problems.",Elda Xhumari|Suela Maxhelaku|Endri Xhina,University of Tirana||,0,A REVIEW OF KNOWLEDGE GRAPH AND GRAPH NEURAL NETWORK APPLICATION,https://doi.org/10.20472/efc.2022.016.015,0.0,FALSE,en,TRUE,gold,,,2022-01-01,review
https://openalex.org/W4312390192,,Ryogo Yamamoto|Tota ISHIKAWA|Mitsuki Yoshida|Wakayama Kazuki|Tanaka Kanji,University of Fukui|University of Fukui|University of Fukui|University of Fukui|University of Fukui,0,Active SLAM: Place Classification and Viewpoint Planning from Domain-Invariant Scene Graphs,https://doi.org/10.1299/jsmermd.2022.2p1-h10,0.0,FALSE,en,FALSE,closed,The Proceedings of JSME annual Conference on Robotics and Mechatronics (Robomec),journal,2022-01-01,article
https://openalex.org/W4312435765,,Kanglong Fan|Wei Liu|Xiaowen Chen|Bharath Ramesh|Cheng Xiang,National University of Singapore|National University of Singapore|National University of Singapore|Western Sydney University|National University of Singapore,0,An Interpretable Scene Understanding Framework Via Graph Learning,https://doi.org/10.2139/ssrn.4238333,,FALSE,en,TRUE,green,SSRN Electronic Journal,repository,2022-01-01,article
https://openalex.org/W4291653579,,Hao Zhou|Jun Zhang|Tingjin Luo|Yazhou Yang|Jun Lei,National University of Defense Technology|Naval University of Engineering|National University of Defense Technology|National University of Defense Technology|National University of Defense Technology|National University of Defense Technology,10,Debiased Scene Graph Generation for Dual Imbalance Learning,https://doi.org/10.1109/tpami.2022.3198965,1.23791801,FALSE,en,FALSE,closed,IEEE Transactions on Pattern Analysis and Machine Intelligence,journal,2022-01-01,article
https://openalex.org/W4226020578,"The goal of scene graph generation (SGG) is to classify objects and their pair-wise relationships in a visual scene. Object occlusion is a critical challenge when generating scene graphs in complex scenes. However, this issue has rarely been explored in recent works. Accordingly, in this paper, we propose a subset matching network (SM-Net) that handles the above problem. First, we decompose SGG into two types of subset matching problems: node subset matching and edge subset matching. Each node/edge subset handles the occlusion between one node/edge pair, thereby reducing the difficulty of SGG in a &#x201C;divide and conquer&#x201D; manner. Second, we introduce a node subset prediction module that utilizes a subset-based message passing module to refine the node subset representation and a matching loss to supervise node subset prediction. Third, we propose an edge subset prediction module that applies a feature selection-based fusion function to obtain edge subset features and a matching loss to supervise edge subset predictions. Experiments on three popular datasets show that our model achieves state-of-the-art performance. The code of SM-Net will be released.",Xin Lin|Jinquan Zeng|Xingquan Li,South China University of Technology|South China University of Technology|Shenzhen Institute of Information Technology,3,Divide and Conquer: Subset Matching for Scene Graph Generation in Complex Scenes,https://doi.org/10.1109/access.2022.3165617,0.3713754,FALSE,en,TRUE,gold,IEEE Access,journal,2022-01-01,article
https://openalex.org/W4287887959,"Jianguo Mao, Wenbin Jiang, Xiangdong Wang, Zhifan Feng, Yajuan Lyu, Hong Liu, Yong Zhu. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",Jianguo Mao|Wenbin Jiang|Xiangdong Wang|Zhifan Feng|Yajuan Lyu|Hong Liu|Zhu Yong,Institute of Computing Technology|Chinese Academy of Sciences|University of Chinese Academy of Sciences|Baidu (China)|Chinese Academy of Sciences|Institute of Computing Technology|Baidu (China)|Baidu (China)|Institute of Computing Technology|Chinese Academy of Sciences|Baidu (China),6,Dynamic Multistep Reasoning based on Video Scene Graph for Video Question Answering,https://doi.org/10.18653/v1/2022.naacl-main.286,0.41426553,FALSE,en,TRUE,hybrid,Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,conference,2022-01-01,article
https://openalex.org/W4285265289,"Human-robot collaboration (HRC) plays a crucial role in agile, flexible, and human-centric manufacturing towards the mass personalization transition. Nevertheless, in today's HRC tasks, either humans or robots need to follow the partners' commands and instructions along collaborative activities progressing, instead of proactive, mutual engagement. The non-semantic perception of HRC scenarios impedes mutually needed, proactive planning and high-cognitive capabilities in existing HRC systems. To overcome the bottleneck, this research explores a dynamic scene graph-based method for mutual-cognition generation in Proactive HRC applications. Firstly, a spatial-attention object detector is utilized to dynamically perceive objects in industrial settings. Secondly, a linking prediction module is leveraged to construct HRC scene graphs. An attentional graph convolutional network (GCN) is utilized to capture relations between industrial parts, human operators, and robot operations and reason structural connections of human-robot collaborative processing as graph embedding, which links to mutual planners for human operation supports and robot proactive instructions. Lastly, the Proactive HRC implementation is demonstrated on disassembly tasks of aging electronic vehicle batteries (EVBs) and evaluate its mutual-cognition capabilities.",Shufei Li|Pai Zheng|Zuoxu Wang|Junming Fan|Lihui Wang,Hong Kong Polytechnic University|Hong Kong Polytechnic University|Beihang University|Hong Kong Polytechnic University|KTH Royal Institute of Technology,15,Dynamic Scene Graph for Mutual-Cognition Generation in Proactive Human-Robot Collaboration,https://doi.org/10.1016/j.procir.2022.05.089,2.37845625,FALSE,en,TRUE,diamond,Procedia CIRP,journal,2022-01-01,article
https://openalex.org/W4286830653,"The task of generating sequence images from paragraphs by generating confrontation networks can already generate higher quality images.However,when the input text involves multiple objects and relationships,the context information of the text sequence is difficult to extract,the object layout of the generated image is prone to confusion,and the generated object details are insufficient.To solve this problem,this paper proposes a method of generating sequence images based on scene graphs based on StoryGAN.First,the paragraph is converted into multiple scene graphs through graph convolution,each scene graph contains the object and relationship information of the corresponding text.Then,the bounding box and segmentation mask of the object are predicted to calculate the scene layout.Finally,according to the scene layout and the context information,a sequence of images more in line with the object and its relationship is generated.Tests on CLEVR-SV and CoDraw-SV data sets show that the me-thod in this paper can generate 64&times;64-pixel sequence images containing multiple objects and their relationships.Experimental results show that on the CLEVR-SV data set,the SSIM and FID of this method are improved by 1.34% and 9.49% respectively than StoryGAN.On the CoDraw-SV data set,the ACC of this method is 7.40% higher than that of StoryGAN.The proposed method improves the rationality of the layout of the generated scene,not only can generate an image sequence containing multiple objects and relationships,but also the generated image has higher quality and clearer details.",TANG Yi-feng ZHANG Wei-qi,Suzhou University of Science and Technology|Suzhou Institute of Trade & Commerce,0,Image Stream From Paragraph Method Based on Scene Graph,https://doi.org/10.11896/jsjkx.201100207,0.0,FALSE,en,TRUE,green,SHILAP Revista de lepidopterología,journal,2022-01-01,article
https://openalex.org/W4287854977,"In this work, we propose the application of abstract meaning representation (AMR) based semantic parsing models to parse textual descriptions of a visual scene into scene graphs, which is the first work to the best of our knowledge. Previous works examined scene graph parsing from textual descriptions using dependency parsing and left the AMR parsing approach as future work since sophisticated methods are required to apply AMR. Hence, we use pre-trained AMR parsing models to parse the region descriptions of visual scenes (i.e. images) into AMR graphs and pre-trained language models (PLM), BART and T5, to parse AMR graphs into scene graphs. The experimental results show that our approach explicitly captures high-level semantics from textual descriptions of visual scenes, such as objects, attributes of objects, and relationships between objects. Our textual scene graph parsing approach outperforms the previous state-of-the-art results by 9.3% in the SPICE metric score.",Woo Suk Choi|Yu‐Jung Heo|Dharani Punithan|Byoung‐Tak Zhang,|Seoul National University|Seoul National University|Seoul National University,7,Scene Graph Parsing via Abstract Meaning Representation in Pre-trained Language Models,https://doi.org/10.18653/v1/2022.dlg4nlp-1.4,0.86654261,FALSE,en,TRUE,gold,,,2022-01-01,article
https://openalex.org/W4312295020,,Tomoya OTA|Ryogo Yamamoto|Honoka OKUGUCHI|Yudai MORISHITA|Hiroki Tomoe|Tanaka Kanji,University of Fukui|University of Fukui|University of Fukui|University of Fukui|University of Fukui|University of Fukui,0,Scene graph representation robust against graph errors for scene graph classification,https://doi.org/10.1299/jsmermd.2022.2p1-h11,0.0,FALSE,en,FALSE,closed,The Proceedings of JSME annual Conference on Robotics and Mechatronics (Robomec),journal,2022-01-01,article
https://openalex.org/W4312240915,"Applications such as providing a preview of personal albums (e.g., Google Photos) or suggesting thematic collections based on user interests (e.g., Pinterest) require a semantically-enriched image representation, which should be more informative with respect to simple low-level visual features and image tags. To this aim, we propose an image collection summarization technique based on frequent subgraph mining. We represent images with a novel type of scene graphs including fine-grained relationship types between objects. These scene graphs are automatically derived by our method. The resulting summary consists of a set of frequent subgraphs describing the underlying patterns of the image dataset. Our results are interpretable and provide more powerful semantic information with respect to previous techniques, in which the summary is a subset of the collection in terms of images or image patches. The experimental evaluation shows that the proposed technique yields non-redundant summaries, with a high diversity of the discovered patterns.",Andrea Pasini|Flavio Giobergia|Eliana Pastor|Elena Baralis,Polytechnic University of Turin|Polytechnic University of Turin|Polytechnic University of Turin|Polytechnic University of Turin,6,Semantic Image Collection Summarization With Frequent Subgraph Mining,https://doi.org/10.1109/access.2022.3229654,0.74275081,FALSE,en,TRUE,gold,IEEE Access,journal,2022-01-01,article
https://openalex.org/W4285151875,,Zhiyuan Lin|Feng Zhu|Yanzi Kong|Qun Wang|Jianyu Wang,Chinese Academy of Sciences|Shenyang Institute of Automation|University of Chinese Academy of Sciences|Chinese Academy of Sciences|Shenyang Institute of Automation|Chinese Academy of Sciences|University of Chinese Academy of Sciences|Shenyang Institute of Automation|Shenyang Institute of Automation|University of Chinese Academy of Sciences|Chinese Academy of Sciences|Northeastern University|Shenyang Institute of Automation|Chinese Academy of Sciences,11,SRSG and S2SG: A Model and a Dataset for Scene Graph Generation of Remote Sensing Images From Segmentation Results,https://doi.org/10.1109/tgrs.2022.3185678,1.23791801,FALSE,en,FALSE,closed,IEEE Transactions on Geoscience and Remote Sensing,journal,2022-01-01,article
https://openalex.org/W4288768562,,Bo Sun|Yong Wu|Jun He|Lejun Yu,,0,Structured Coding Based on Semantic Disambiguation for Video Captioning,https://doi.org/10.2139/ssrn.4174916,,FALSE,en,TRUE,green,SSRN Electronic Journal,repository,2022-01-01,article
https://openalex.org/W4301193036,"With the development of oil and gas fields in deepwater areas, the risk of deepwater oil spill accidents is increasing. In order to improve the capacity of deepwater oil spill contingency response and management in the South China Sea, an user-friendly and robust 3D (three-dimensional) underwater oil spill simulation system is developed by using the MFC (Microsoft Foundation Classes) interface library and the 3D OSG (Open Scene Graph) rendering engine. Many key technologies including submarine topography drawing, sea surface emulation, three-dimensional current display and oil particle simulation are developed and integrated, and the functions including 3D simulations of marine environment and deepwater oil spill prediction are also implemented in the simulation software system. The 3D oil spill simulation system has been applied in the preparation of oil spill response plans (OSRP) for Liwan 3-1 Block, Liwan 43-11 Block, Chevron’s Block 42-05 and Baiyun Block in the South China Sea. Preliminary application shows that the system can provide convenient and intuitive visual interactive services for the exploitation of oil and gas resources in the deepwater area of the South China Sea. Meanwhile, the system also specifies underwater trajectories of oil spill in deep waters, the concentration distribution of oil spilled in water, and the time and place the spilled oil reaches the surface, thus providing technical support for on-scene oil spill emergency response and marine environmental impact assessment.",Jianwei Li|Wei An|Yuanshou He|Weiwei Jin|Shasha Song,Ocean University of China||||,0,Study on the Key Technology of Deepwater Oil Spill Simulation in the South China Sea,https://doi.org/10.11648/j.ijepp.20221003.13,0.0,FALSE,en,TRUE,diamond,International Journal of Environmental Protection and Policy,journal,2022-01-01,article
https://openalex.org/W4296912696,,Yuchen Zhou|Yue Zhang|Zhanwei Zhao|Kaidong Zhang|Chao Gou,Shenzhen University|Sun Yat-sen University|Sun Yat-sen University|Shenzhen University|Sun Yat-sen University|Shenzhen University|Shenzhen University|Sun Yat-sen University|Sun Yat-sen University|Shenzhen University,15,Toward Driving Scene Understanding: A Paradigm and Benchmark Dataset for Ego-Centric Traffic Scene Graph Representation,https://doi.org/10.1109/jrfid.2022.3207017,1.73308522,FALSE,en,FALSE,closed,IEEE Journal of Radio Frequency Identification,journal,2022-01-01,article
https://openalex.org/W4207058579,"Scene graph generation (SGG) aims to detect objects and their relationships in an image, thereby enabling a detailed understanding of a complex scene for various real-world applications. In SGG applications such as robot vision, it is important to correctly detect all objects without recognizing any object as another kind of object or ignoring it. However, previous studies on SGG do not consider unknown objects whose classes are unseen in training. Consequently, current SGG methods wrongly classify them as known object classes or overlook them. In this paper, we propose a new problem named &#x201C;open-set SGG&#x201D; with unknown objects, focusing on detecting even unknown objects and their relationships. Specifically, we formally define this new problem and propose an evaluation protocol, including an extended dataset with unknown objects and novel evaluation metrics designed for the open-set setting. We also build baseline methods by employing and extending existing SGG methods and compare them through experiments to establish the current baseline performance of open-set SGG. Finally, we discuss the limitations of the current SGG methodology in the open-set setting and point out future research directions.",Motoharu Sonogashira|Masaaki Iiyama|Yasutomo Kawanishi,Kyoto Seika University|RIKEN|Shiga University|Kyoto Seika University|RIKEN,5,Towards Open-Set Scene Graph Generation With Unknown Objects,https://doi.org/10.1109/access.2022.3145465,0.61895901,FALSE,en,TRUE,gold,IEEE Access,journal,2022-01-01,article
https://openalex.org/W4225333936,"Bug tracking systems are standard repositories that preserve a large number of uncovered bugs. Once a bug is reported in these repositories, developers search for appropriate changes to fix the bug. However, discovering the changes that can fix the bugs has a negative influence on the schedule and cost of projects. Mainly, fixing bugs could be done by performing some other maintenance changes. In this work, we study and examine the role of adaptive maintenance in the context of API-migration during bug fixing activities through a case study on KOffice, Extragear/graphics, and Open Scene Graph projects. Our goal is to direct developers towards potential bugs early in development which are more likely to be fixed by performing adaptive changes as opposed to other maintenance tasks. We examined the reports of fixed bugs from the bug tracking systems of the studied projects, then we explored several factors related to variant dimensions of the reports and their relevant version history commits, in order to evaluate their effectiveness to decide whether a bug is likely to be fixed by adaptive changes. Our case study results show that bug residency time, textual contents of the report, the component that the bug was found in, and reporter/commenter experience show significant differences between the bugs that are fixed by adaptive changes and other fixed bugs.",Nouh Alhindawi|Omar Meqdadi|Jamal Alsakran|Nader Mohammad Aljawarneh|Hatim S. Migdadi,Jadara University|Jordan University of Science and Technology|Higher Colleges of Technology|Jadara University|Hashemite University,4,Understanding and predicting bugs fixed by API-migrations,https://doi.org/10.5267/j.ijdns.2022.2.011,1.51996444,FALSE,en,TRUE,diamond,International Journal of Data and Network Science,journal,2022-01-01,article
https://openalex.org/W4312738140,"Visual relationship detection (VRD) is an important direction in the field of image processing, and it is a research task to explore object relationships based on object recognition and localization regression. At the same time, it is also one of the key contents of scene graph generation and construction of multimodal knowledge graph. In order to better improve the visual relationship detection effect, the fusion method of image location and feature information embedding is adopted on the vrd dataset. First, analyze the causes of errors in existing methods, and build a pre-training pre-dataset vrd&#x005F;P for data enhancement; then, combine the characteristics of the vrd dataset to establish an entities relationship network (Entities-net) to guide target recognition; finally, a visual relationship detection model of LTransE is proposed to achieve the joint representation of target features and target locations for relationship detection. The results show that the method of embedding fusion of image location and feature information can effectively improve the visual relationship detection effect.",Jinghui Peng|Ying Zhang|Weichun Huang,,3,Visual Relationship Detection With Image Position and Feature Information Embedding and Fusion,https://doi.org/10.1109/access.2022.3219207,0.3713754,FALSE,en,TRUE,gold,IEEE Access,journal,2022-01-01,article
https://openalex.org/W4312351435,,Yosuke Kawasaki|M. TAKAHASHI,Keio University|Keio University,0,World state dependent action graph based on knowledge representation using scene graph,https://doi.org/10.1299/jsmermd.2022.1a1-t08,0.0,FALSE,en,FALSE,closed,The Proceedings of JSME annual Conference on Robotics and Mechatronics (Robomec),journal,2022-01-01,article
https://openalex.org/W4225474437,"Toward solving the problems of low data scheduling efficiency and relative delay in rendering when constructing complex urban three-dimensional (3D) scenes, we propose a visualperception-driven strategy based on scene graphs.According to the spatial distribution characteristics of 3D scene data, this strategy uses a scene graph to organize the local 3D scene of a city.It uses a level-of-detail simplification algorithm to simplify the 3D model of the city into four resolution levels.On this basis, a visual-perception-driven strategy based on scene graphs is designed.This strategy utilizes the good relationship attributes between geographical entities provided by scene graphs to construct a visual-perception evaluation model and help constrain the adaptive scheduling of models at different detail levels.Experimental results show that this method can effectively improve the data scheduling efficiency and accelerate the construction of local 3D scenes.",Xiang Wang|Tao Shen|Liang Huo|Xiaoyong Zhang,Beijing University of Civil Engineering and Architecture|Beijing University of Civil Engineering and Architecture|Beijing University of Civil Engineering and Architecture|Chinese Academy of Fishery Sciences|Beijing University of Civil Engineering and Architecture,2,Visual-perception-driven Urban Three-dimensional Scene Data Scheduling Method,https://doi.org/10.18494/sam.2022.3669,0.2475836,FALSE,en,TRUE,gold,Sensors and Materials,journal,2022-01-04,article
https://openalex.org/W3214435038,"In autonomous vehicles (AVs), early warning systems rely on collision prediction to ensure occupant safety. However, state-of-the-art methods using deep convolutional networks either fail at modeling collisions or are too expensive/slow, making them less suitable for deployment on AV edge hardware. To address these limitations, we propose sg2vec, a spatio-temporal scene-graph embedding methodology that uses Graph Neural Network (GNN) and Long Short-Term Memory (LSTM) layers to predict future collisions via visual scene perception. We demonstrate that sg2vec predicts collisions 8.11% more accurately and 39.07% earlier than the state-of-the-art method on synthesized datasets, and 29.47% more accurately on a challenging real-world collision dataset. We also show that sg2vec is better than the state-of-the-art at transferring knowledge from synthetic datasets to real-world driving datasets. Finally, we demonstrate that sg2vec performs inference 9.3x faster with an 88.0% smaller model, 32.4% less power, and 92.8% less energy than the state-of-the-art method on the industry-standard Nvidia DRIVE PX 2 platform, making it more suitable for implementation on the edge.",Arnav Vaibhav Malawade|Shih-Yuan Yu|Brandon Hsu|Deepan Muthirayan|Pramod P. Khargonekar|Mohammad Abdullah Al Faruque,"University of California, Irvine|University of California, Irvine|University of California, Irvine|University of California, Irvine|University of California, Irvine|University of California, Irvine",0,Spatiotemporal Scene-Graph Embedding for Autonomous Vehicle Collision Prediction,https://doi.org/10.1109/jiot.2022.3141044,0.0,FALSE,en,TRUE,green,IEEE Internet of Things Journal,journal,2022-01-06,preprint
https://openalex.org/W4206126653,,Shril Mody|Janvi Thakkar,Indian Institute of Technology Gandhinagar|Indian Institute of Technology Gandhinagar,0,Analysis of Image generation using Scene graphs,https://doi.org/10.1145/3493700.3493749,0.0,FALSE,en,FALSE,closed,,,2022-01-07,article
https://openalex.org/W4220902619,,Jia Geng|Yunhui Yi|Wentao Kan,Xidian University|Xidian University|Xidian University,0,VTuckeR: Multimodal Tucker Fusion for Scene Graph Generation,https://doi.org/10.1145/3514105.3514111,0.0,FALSE,en,FALSE,closed,,,2022-01-11,article
https://openalex.org/W4206336297,,Yalin Miao|Wenfang Cheng|Shuyun He|Hui Jiang,Xi'an University of Technology|Xi'an University of Technology|Xi'an University of Technology|Xi'an University of Technology,19,Research on Visual Question Answering Based on GAT Relational Reasoning,https://doi.org/10.1007/s11063-021-10689-2,2.35204422,FALSE,en,FALSE,closed,Neural Processing Letters,journal,2022-01-13,article
https://openalex.org/W4205697054,,Kang Zhou|Chi Guo|Huyin Zhang,Wuhan University|Wuhan University|Wuhan University,3,Relational attention-based Markov logic network for visual navigation,https://doi.org/10.1007/s11227-021-04283-5,0.3713754,FALSE,en,FALSE,closed,The Journal of Supercomputing,journal,2022-01-20,article
https://openalex.org/W4206996644,"Abstract A 3D scene is more than the geometry and classes of the objects it comprises. An essential aspect beyond object-level perception is the scene context, described as a dense semantic network of interconnected nodes. Scene graphs have become a common representation to encode the semantic richness of images, where nodes in the graph are object entities connected by edges, so-called relationships. Such graphs have been shown to be useful in achieving state-of-the-art performance in image captioning, visual question answering and image generation or editing. While scene graph prediction methods so far focused on images, we propose instead a novel neural network architecture for 3D data, where the aim is to learn to regress semantic graphs from a given 3D scene. With this work, we go beyond object-level perception, by exploring relations between object entities. Our method learns instance embeddings alongside a scene segmentation and is able to predict semantics for object nodes and edges. We leverage 3DSSG , a large scale dataset based on 3RScan that features scene graphs of changing 3D scenes. Finally, we show the effectiveness of graphs as an intermediate representation on a retrieval task.",Johanna Wald|Nassir Navab|Federico Tombari,Technical University of Munich|Technical University of Munich|Google (Switzerland)|Technical University of Munich,26,Learning 3D Semantic Scene Graphs with Instance Embeddings,https://doi.org/10.1007/s11263-021-01546-9,3.09479503,FALSE,en,TRUE,hybrid,International Journal of Computer Vision,journal,2022-01-22,article
https://openalex.org/W4213352847,,Fanfan Wu|Feihu Yan|Weimin Shi|Zhong Zhou,Beihang University|Beihang University|Beihang University|Beihang University,10,3D scene graph prediction from point clouds,https://doi.org/10.1016/j.vrih.2022.01.005,2.53605893,FALSE,en,TRUE,diamond,Virtual Reality & Intelligent Hardware,journal,2022-02-01,article
https://openalex.org/W4213089501,"Abstract When describing an image, people can rapidly extract the topic from the image and find the main object, generating sentences that match the main idea of the image. However, most of the scene graph generation methods do not emphasise the importance of the topic of the image. Consequently, the captions generated by the scene graph‐based image captioning models cannot reflect the topic in the image then expressing the central idea of the image. In this paper, we propose a method for image captioning based on topic scene graphs (TSG). Firstly, we propose the structure of topic scene graphs that express images' topics and the relationships between objects. Then, combined with the topic scene graph, we utilise the salient object detection to generate the topic scene graph highlighting the salient objects of the image. Note that our framework is agnostic to any scene graph‐based image captioning model and thus can be widely applied in the community which seeks salient object predictions. We compare the performance of our topic scene graph with the state‐of‐the‐art scene graph generation models and mainstream image captioning models on MSCOCO and Visual Genome datasets, both achieving better performance.",Min Zhang|Jingxiang Chen|Pengfei Li|Ming Jiang|Zhe Zhou,Hangzhou Dianzi University|Hangzhou Dianzi University|Hangzhou Dianzi University|Hangzhou Dianzi University|Hangzhou Medical College,7,Topic scene graphs for image captioning,https://doi.org/10.1049/cvi2.12093,0.86654261,FALSE,en,TRUE,gold,IET Computer Vision,journal,2022-02-18,article
https://openalex.org/W4220789945,,Svetlana Orlova|А. В. Лопота,,0,Scene recognition for confined spaces in mobile robotics: current state and tendencies,https://doi.org/10.31776/rtcj.10102,0.0,FALSE,en,FALSE,closed,Robotics and Technical Cybernetics,journal,2022-03-01,article
https://openalex.org/W4214879921,,Yiming Li|Xiaoshan Yang|Xuhui Huang|Zhe Ma|Changsheng Xu,Zhengzhou University|Institute of Automation|Chinese Academy of Sciences|China Aerospace Science and Industry Corporation (China)|China Aerospace Science and Industry Corporation (China)|Chinese Academy of Sciences|Institute of Automation,11,Zero-Shot Predicate Prediction for Scene Graph Parsing,https://doi.org/10.1109/tmm.2022.3155928,1.36170981,FALSE,en,FALSE,closed,IEEE Transactions on Multimedia,journal,2022-03-03,article
https://openalex.org/W4302235895,,Simon Schneegans|Moritz Zeumer|Jonas Gilg|Andreas Gerndt,Deutsches Zentrum für Luft- und Raumfahrt e. V. (DLR)|Deutsches Zentrum für Luft- und Raumfahrt e. V. (DLR)|Deutsches Zentrum für Luft- und Raumfahrt e. V. (DLR)|University of Bremen|Deutsches Zentrum für Luft- und Raumfahrt e. V. (DLR),7,CosmoScout VR: A Modular 3D Solar System Based on SPICE,https://doi.org/10.1109/aero53065.2022.9843488,0.48330979,FALSE,en,FALSE,closed,2022 IEEE Aerospace Conference (AERO),conference,2022-03-05,article
https://openalex.org/W4220751672,,Yamin Han|Tao Zhuo|Peng Zhang|Wei Huang|Yufei Zha|Yanning Zhang|Mohan Kankanhalli,Northwestern Polytechnical University|Northwest A&F University|Shandong Academy of Sciences|Qilu University of Technology|Northwestern Polytechnical University|Nanchang University|Northwestern Polytechnical University|Northwestern Polytechnical University|National University of Singapore,12,One-shot Video Graph Generation for Explainable Action Reasoning,https://doi.org/10.1016/j.neucom.2022.02.069,1.48550161,FALSE,en,FALSE,closed,Neurocomputing,journal,2022-03-07,article
https://openalex.org/W4221053795,,Mohamed O. Elasri|Omar Elharrouss|Somaya Al-Máadeed|Hamid Tairi,Sidi Mohamed Ben Abdellah University|Qatar University|Qatar University|Sidi Mohamed Ben Abdellah University,122,Image Generation: A Review,https://doi.org/10.1007/s11063-022-10777-x,14.35984893,FALSE,en,FALSE,closed,Neural Processing Letters,journal,2022-03-11,review
https://openalex.org/W4362241031,"With the great success of artificial intelligence in recent years, graph learning is gaining attention from both academia and industry [1, 2]. The power of graph data is its capacity to represent numerous complicated structures in a broad spectrum of application domains including protein networks, social networks, food webs, molecular structures, knowledge graphs, sentence dependency trees, and scene graphs of images. However, designing an effective graph learning architecture on arbitrary graphs is still an on-going research topic because of two challenges of learning complex topological structures of graphs and their nature of isomorphism. In this work, we aim to summarize and discuss the latest methods in graph learning, with special attention to two aspects of structure learning and permutation invariance learning. The survey starts by reviewing basic concepts on graph theory and graph signal processing. Next, we provide systematic categorization of graph learning methods to address two aspects above respectively. Finally, we conclude our paper with discussions and open issues in research and practice.",Tuyen Thi Thanh Ho,,0,Graph Structure and Isomorphism Learning: A Survey,https://doi.org/10.32913/mic-ict-research.v2022.n1.1028,0.0,FALSE,en,TRUE,bronze,Research and Development on Information and Communication Technology,journal,2022-03-13,article
https://openalex.org/W4220974559,,Yong Zhang|Yingwei Pan|Ting Yao|Rui Huang|Tao Mei|Chang‐Wen Chen,"Chinese University of Hong Kong, Shenzhen|Jingdong (China)|Jingdong (China)|Chinese University of Hong Kong, Shenzhen|Jingdong (China)|Hong Kong Polytechnic University",26,Boosting Scene Graph Generation with Visual Relation Saliency,https://doi.org/10.1145/3514041,3.21858683,FALSE,en,FALSE,closed,ACM Transactions on Multimedia Computing Communications and Applications,journal,2022-03-17,article
https://openalex.org/W4220696681,,Jie Yan|Yuxiang Xie|Xidao Luan|Yanming Guo|Quanzhi Gong|Suru Feng,National University of Defense Technology|National University of Defense Technology|Changsha University|National University of Defense Technology|National University of Defense Technology|National University of Defense Technology,12,Caption TLSTMs: combining transformer with LSTMs for image captioning,https://doi.org/10.1007/s13735-022-00228-7,1.48550161,FALSE,en,FALSE,closed,International Journal of Multimedia Information Retrieval,journal,2022-03-23,article
https://openalex.org/W4226463491,,Kang Zhou|Chi Guo|Huyin Zhang,Wuhan University|Wuhan University|Wuhan University,8,Improving indoor visual navigation generalization with scene priors and Markov relational reasoning,https://doi.org/10.1007/s10489-022-03317-6,0.99033441,FALSE,en,FALSE,closed,Applied Intelligence,journal,2022-04-04,article
https://openalex.org/W4223459566,,Peng Tian|Hongwei Mo|Laihao Jiang,Harbin Engineering University|Harbin Engineering University|Harbin Engineering University,1,Exploring correlation of relationship reasoning for scene graph generation,https://doi.org/10.1007/s13042-022-01538-2,0.1237918,FALSE,en,FALSE,closed,International Journal of Machine Learning and Cybernetics,journal,2022-04-12,article
https://openalex.org/W4224251870,,Shanshan Zhao|Lixiang Li|Haipeng Peng,Beijing University of Posts and Telecommunications|Beijing University of Posts and Telecommunications|Beijing University of Posts and Telecommunications,25,Aligned visual semantic scene graph for image captioning,https://doi.org/10.1016/j.displa.2022.102210,3.09479503,FALSE,en,FALSE,closed,Displays,journal,2022-04-20,article
https://openalex.org/W4226094317,,Tianwen Qian|Jingjing Chen|Shaoxiang Chen|Bo Wu|Yu–Gang Jiang,Fudan University|Fudan University|Fudan University|IBM (United States)|Fudan University,57,Scene Graph Refinement Network for Visual Question Answering,https://doi.org/10.1109/tmm.2022.3169065,6.93234086,FALSE,en,FALSE,closed,IEEE Transactions on Multimedia,journal,2022-04-22,article
https://openalex.org/W4225145471,,Yixue Kong|Yong Feng|Mingliang Zhou|Xiancai Xiong|Yongheng Wang|Baohua Qiang,Chongqing University|Chongqing University|Chongqing University|Ministry of Natural Resources|Chongqing Municipal Health Commission|Zhejiang Lab|Guilin University of Electronic Technology,1,Clustering-Based Semi-Supervised Cross-Modal Retrieval Using Scene Graph,https://doi.org/10.1142/s0218126622502139,0.1237918,FALSE,en,FALSE,closed,Journal of Circuits Systems and Computers,journal,2022-04-27,article
https://openalex.org/W4224931701,,Feicheng Huang|Zhixin Li,Guangxi Normal University|Guangxi Normal University,5,Improve Image Captioning Via Relation Modeling,https://doi.org/10.1109/icassp43922.2022.9747820,0.34522128,FALSE,en,FALSE,closed,"ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",conference,2022-04-27,article
https://openalex.org/W4224931256,,Xuezhi Tong|Rui Wang|Chuan Wang|Sanyi Zhang|Xiaochun Cao,Tianjin University|Zhejiang Lab|Institute of Information Engineering|Chinese Academy of Sciences|Institute of Information Engineering|Chinese Academy of Sciences|Chinese Academy of Sciences|Institute of Information Engineering|Chinese Academy of Sciences|Institute of Information Engineering,2,PMP-NET: Rethinking Visual Context for Scene Graph Generation,https://doi.org/10.1109/icassp43922.2022.9747415,0.13808851,FALSE,en,FALSE,closed,"ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",conference,2022-04-27,article
https://openalex.org/W4288045581,"Visual understanding involves detecting objects in a scene and investigating rich semantic relationships between the objects, which is required for downstream visual reasoning tasks. The scene graph is widely used for structured scene representation, however, the performance of the scene graph generation for visual reasoning is limited due to challenges posed by imbalanced datasets and insufficient attention toward common sense knowledge infusion. Most of the existing approaches use statistical or language priors for knowledge infusion. Common Sense knowledge infusion using heterogeneous knowledge graphs can help in improving the accuracy, robustness, and generalizability of the scene graph generation and enable explainable higher level reasoning by providing rich and diverse background and factual knowledge about the concepts in visual scenes. In this article, we present the background and applications of the scene graph generation and the initial approaches and key challenges in common sense knowledge infusion using heterogeneous knowledge graphs for visual understanding and reasoning.",Muhammad Jaleed Khan|John G. Breslin|Edward Curry,Ollscoil na Gaillimhe – University of Galway|Ollscoil na Gaillimhe – University of Galway|Ollscoil na Gaillimhe – University of Galway,11,"Common Sense Knowledge Infusion for Visual Understanding and Reasoning: Approaches, Challenges, and Applications",https://doi.org/10.1109/mic.2022.3176500,1.36170981,FALSE,en,TRUE,hybrid,IEEE Internet Computing,journal,2022-05-20,article
https://openalex.org/W4281846538,"Surgical scene understanding is a key barrier for situation-aware robotic surgeries and the associated surgical training. With the presence of domain shifts and the inclusion of new instruments and tissues, learning domain generalization (DG) plays a pivotal role in expanding instrument–tissue interaction detection to new domains in robotic surgery. Mimicking the ability of humans to incrementally learn new skills without forgetting their old skills in a similar domain, we employ incremental DG on scene graphs to predict instrument–tissue interaction during robot-assisted surgery. To achieve incremental DG, incorporate incremental learning (IL) to accommodate new instruments and knowledge-distillation-based student–teacher learning to tackle domain shifts in the new domain. Additionally, we designed an enhanced curriculum by smoothing (E-CBS) based on Laplacian of Gaussian (LoG) and Gaussian kernels, and integrated it with the feature extraction network (FEN) and graph network to improve the instrument–tissue interaction performance. Furthermore, the FEN’s and graph network’s logits are normalized by temperature normalization (T-Norm), and its effect in model calibration was studied. Quantitative and qualitative analysis proved that our incrementally-domain generalized interaction detection model was able to adapt to the target domain (transoral robotic surgery) while retaining its performance in the source domain (nephrectomy surgery). Additionally, the graph model enhanced by E-CBS and T-Norm outperformed other state-of-the-art models, and the incremental DG technique performed better than the naive domain adaption and DG technique.",Lalithkumar Seenivasan|Mobarakol Islam|Chi‐Fai Ng|Chwee Ming Lim|Hongliang Ren,National University of Singapore|Imperial College London|Chinese University of Hong Kong|Prince of Wales Hospital|Singapore General Hospital|Chinese University of Hong Kong|National University of Singapore,5,Biomimetic Incremental Domain Generalization with a Graph Network for Surgical Scene Understanding,https://doi.org/10.3390/biomimetics7020068,0.9789941,FALSE,en,TRUE,gold,Biomimetics,journal,2022-05-28,article
https://openalex.org/W4312561757,,Yiming Li|Xiaoshan Yang|Changsheng Xu,Zhengzhou University|Peng Cheng Laboratory|University of Chinese Academy of Sciences|Chinese Academy of Sciences|University of Chinese Academy of Sciences|Peng Cheng Laboratory|Chinese Academy of Sciences,33,Dynamic Scene Graph Generation via Anticipatory Pre-training,https://doi.org/10.1109/cvpr52688.2022.01350,2.27846042,FALSE,en,FALSE,closed,2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),conference,2022-06-01,article
https://openalex.org/W4312286390,,Yangjun Ou|Mi Li|Zhenzhong Chen,Wuhan University|Wuhan University|Wuhan University,25,Object-Relation Reasoning Graph for Action Recognition,https://doi.org/10.1109/cvpr52688.2022.01950,1.72610638,FALSE,en,FALSE,closed,2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),conference,2022-06-01,article
https://openalex.org/W4313161463,,Wei Li|Haiwei Zhang|Qijie Bai|Guoqing Zhao|Ning Jiang|Xiaojie Yuan,Data Assurance and Communication Security|Nankai University|Data Assurance and Communication Security|Data Assurance and Communication Security|||Nankai University|Data Assurance and Communication Security,61,PPDL: Predicate Probability Distribution based Loss for Unbiased Scene Graph Generation,https://doi.org/10.1109/cvpr52688.2022.01884,4.21169957,FALSE,en,FALSE,closed,2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),conference,2022-06-01,article
https://openalex.org/W4282961302,,Haiyan Gao|Dibo Shi|Tianling Jiang|Xin Li|Zefan Zhang|Yi Ji|Ying Li|Chunping Liu,Soochow University|Soochow University|Soochow University|Soochow University|Soochow University|Soochow University|Soochow University|Soochow University,1,Scene graph generation with award-punishment strategy,https://doi.org/10.1016/j.knosys.2022.109239,0.1237918,FALSE,en,FALSE,closed,Knowledge-Based Systems,journal,2022-06-15,article
https://openalex.org/W4283267713,,Junzhong Ji|Mingzhan Wang|Xiaodan Zhang|Minglong Lei|Liangqiong Qu,Beijing University of Technology|Beijing University of Technology|Beijing University of Technology|Beijing University of Technology|Stanford University,19,Relation constraint self-attention for image captioning,https://doi.org/10.1016/j.neucom.2022.06.062,2.35204422,FALSE,en,FALSE,closed,Neurocomputing,journal,2022-06-21,article
https://openalex.org/W4283375995,,Minghao Geng|Qingjie Zhao,Beijing Institute of Technology|Beijing Institute of Technology,4,Improve Image Captioning by Modeling Dynamic Scene Graph Extension,https://doi.org/10.1145/3512527.3531401,0.4951672,FALSE,en,FALSE,closed,,,2022-06-23,article
https://openalex.org/W4283399970,,Wenliang Tang|Zhenzhen Hu|Zijie Song|Richang Hong,Hefei University of Technology|Hefei University of Technology|Hefei University of Technology|Hefei University of Technology,6,OCR-oriented Master Object for Text Image Captioning,https://doi.org/10.1145/3512527.3531431,0.61895901,FALSE,en,FALSE,closed,,,2022-06-23,article
https://openalex.org/W4293055734,"Visual relationship detection (VRD) aims to locate objects and recognize their pairwise relationships for parsing scene graphs. To enable a higher understanding of the visual scene, we propose a symmetric fusion learning model for visual relationship detection and scene graph parsing. We integrate objects and relationship features at visual and semantic levels for better relations feature mapping. First, we apply a feature fusion for the construction of the visual module and introduce a semantic representation learning module combined with large-scale external knowledge. We minimize the loss by matching the visual and semantic embeddings using our designed symmetric learning module. The symmetric learning module based on reverse cross-entropy can boost cross-entropy symmetrically and perform reverse supervision for inaccurate annotations. Our model is compared with other state-of-the-art methods in two public data sets. Experiments show that our proposed model achieves encouraging performance in various metrics for the two data sets investigated. The further detailed analysis demonstrates that the proposed method performs better by partially alleviating the impact of inaccurate annotations.",Xuan Liu|Xiaochuan Jing|Zhong Zheng|Wanru Du|Xingxing Ding|Quan Zhu,China Aerospace Science and Technology Corporation|China Aerospace Science and Technology Corporation|China Aerospace Science and Technology Corporation|China Aerospace Science and Technology Corporation||,1,A Symmetric Fusion Learning Model for Detecting Visual Relations and Scene Parsing,https://doi.org/10.1155/2022/5985392,0.1237918,FALSE,en,TRUE,hybrid,Scientific Programming,journal,2022-06-27,article
https://openalex.org/W4283658195,"Abstract Aiming to help improve quality of life of the visually impaired people, this paper presents a novel wearable aid in the shape of a helmet for helping them find objects in indoor scenes. An object‐goal navigation system based on a wearable device is developed, which consists of four modules: object relation prior knowledge (ORPK), perception, decision and feedback. To make the aid also work well in unfamiliar environment, ORPK is used for sub‐goal inference to help the user find the target goal. And a method that learns the ORPK from unlabelled images by utilising a scene graph and knowledge graph is proposed. The effectiveness of the aid is demonstrated in real world experiments.",Xuan Hou|Huailin Zhao|Chenxu Wang|Huaping Liu,Shanghai Institute of Technology|Shanghai Institute of Technology|Tsinghua University|Tsinghua University,10,Knowledge driven indoor object‐goal navigation aid for visually impaired people,https://doi.org/10.1049/ccs2.12061,1.23791801,FALSE,en,TRUE,gold,Cognitive Computation and Systems,journal,2022-06-27,article
https://openalex.org/W4283792031,"Scene graph in a video conveys a wealth of information about objects and their relationships in the scene, thus benefiting many downstream tasks such as video captioning and visual question answering. Existing methods of scene graph generation require large-scale training videos annotated with objects and relationships in each frame to learn a powerful model. However, such comprehensive annotation is time-consuming and labor-intensive. On the other hand, it is much easier and less cost to annotate images with scene graphs, so we investigate leveraging annotated images to facilitate training a scene graph generation model for unannotated videos, namely image-to-video scene graph generation. This task presents two challenges: 1) infer unseen dynamic relationships in videos from static relationships in images due to the absence of motion information in images; 2) adapt objects and static relationships from images to video frames due to the domain shift between them. To address the first challenge, we exploit external commonsense knowledge to infer the unseen dynamic relationship from the temporal evolution of static relationships. We tackle the second challenge by hierarchical adversarial learning to reduce the data distribution discrepancy between images and video frames. Extensive experiment results on two benchmark video datasets demonstrate the effectiveness of our method.",Jin Chen|Xiaofeng Ji|Xinxiao Wu,Beijing Institute of Technology|Beijing Institute of Technology|Beijing Institute of Technology,3,Adaptive Image-to-Video Scene Graph Generation via Knowledge Reasoning and Adversarial Learning,https://doi.org/10.1609/aaai.v36i1.19903,0.20713277,FALSE,en,TRUE,diamond,Proceedings of the AAAI Conference on Artificial Intelligence,conference,2022-06-28,article
https://openalex.org/W4283799386,"To synthesize images with preferred objects and interactions, a controllable way is to generate the image from a scene graph and a large pool of object crops, where the spatial arrangements of the objects in the image are defined by the scene graph while their appearances are determined by the retrieved crops from the pool. In this paper, we propose a novel framework with such a semi-parametric generation strategy. First, to encourage the retrieval of mutually compatible crops, we design a sequential selection strategy where the crop selection for each object is determined by the contents and locations of all object crops that have been chosen previously. Such process is implemented via a transformer trained with contrastive losses. Second, to generate the final image, our hierarchical generation strategy leverages hierarchical gated convolutions which are employed to synthesize areas not covered by any image crops, and a patch guided spatially adaptive normalization module which is proposed to guarantee the final generated images complying with the crop appearance and the scene graph. Evaluated on the challenging Visual Genome and COCO-Stuff dataset, our experimental results demonstrate the superiority of our proposed method over existing state-of-the-art methods.",Xiaogang Xu|Ning Xu,Chinese University of Hong Kong|Adobe Systems (United States),9,Hierarchical Image Generation via Transformer-Based Sequential Patch Selection,https://doi.org/10.1609/aaai.v36i3.20199,0.6213983,FALSE,en,TRUE,diamond,Proceedings of the AAAI Conference on Artificial Intelligence,conference,2022-06-28,article
https://openalex.org/W4283816285,"Existing scene graph generation methods suffer the limitations when the image lacks of sufficient visual contexts. To address this limitation, we propose a knowledge-enhanced scene graph generation model with multimodal relation alignment, which supplements the missing visual contexts by well-aligned textual knowledge. First, we represent the textual information into contextualized knowledge which is guided by the visual objects to enhance the contexts. Furthermore, we align the multimodal relation triplets by co-attention module for better semantics fusion. The experimental results show the effectiveness of our method.",Ze Fu|Junhao Feng|Changmeng Zheng|Yi Cai,South China University of Technology|South China University of Technology|Hong Kong Polytechnic University|South China University of Technology,2,Knowledge-Enhanced Scene Graph Generation with Multimodal Relation Alignment (Student Abstract),https://doi.org/10.1609/aaai.v36i11.21610,0.13808851,FALSE,en,TRUE,diamond,Proceedings of the AAAI Conference on Artificial Intelligence,conference,2022-06-28,article
https://openalex.org/W4283811591,"Deep vision models have provided new capability across a spectrum of applications in transportation, manufacturing, agriculture, commerce, and security. However, recent studies have demonstrated that these models are vulnerable to adversarial attack, exposing a risk-of-use in critical applications where untrusted parties have access to the data environment or even directly to the sensor inputs. Existing adversarial defense methods are either limited to specific types of attacks or are too complex to be applied to practical vision models. More importantly, these methods rely on techniques that are not interpretable to humans. In this work, we argue that an effective defense should produce an explanation as to why the system is attacked, and by using a representation that is easily readable by a human user, e.g. a logic formalism. To this end, we propose logic adversarial defense (LogicDef), a defense framework that utilizes the scene graph of the image to provide a contextual structure for detecting and explaining object classification. Our framework first mines inductive logic rules from the extracted scene graph, and then uses these rules to construct a defense model that alerts the user when the vision model violates the consistency rules. The defense model is interpretable and its robustness is further enhanced by incorporating existing relational commonsense knowledge from projects such as ConceptNet. In order to handle the hierarchical nature of such relational reasoning, we use a curriculum learning approach based on object taxonomy, yielding additional improvements to training and performance.",Yuan Yang|James C Kerce|Faramarz Fekri,Georgia Institute of Technology|Georgia Institute of Technology|Georgia Institute of Technology,5,LOGICDEF: An Interpretable Defense Framework against Adversarial Examples via Inductive Scene Graph Reasoning,https://doi.org/10.1609/aaai.v36i8.20865,0.5877752,FALSE,en,TRUE,diamond,Proceedings of the AAAI Conference on Artificial Intelligence,conference,2022-06-28,article
https://openalex.org/W4283689365,"To semantically understand remote sensing images, it is not only necessary to detect the objects in them but also to recognize the semantic relationships between the instances. Scene graph generation aims to represent the image as a semantic structural graph, where objects and relationships between them are described as nodes and edges, respectively. Some existing methods rely only on visual features to sequentially predict the relationships between objects, ignoring contextual information and making it difficult to generate high-quality scene graphs, especially for remote sensing images. Therefore, we propose a novel model for remote sensing image scene graph generation by fusing contextual information and statistical knowledge, namely RSSGG_CS. To integrate contextual information and calculate attention among all objects, the RSSGG_CS model adopts a filter module (FiM) that is based on adjusted transformer architecture. Moreover, to reduce the blindness of the model when searching semantic space, statistical knowledge of relational predicates between objects from the training dataset and the cleaned Wikipedia text is used as supervision when training the model. Experiments show that fusing contextual information and statistical knowledge allows the model to generate more complete scene graphs of remote sensing images and facilitates the semantic understanding of remote sensing images.",Zhiyuan Lin|Feng Zhu|Qun Wang|Yanzi Kong|Jianyu Wang|Liang Huang|Yingming Hao,Shenyang Institute of Automation|Chinese Academy of Sciences|University of Chinese Academy of Sciences|Chinese Academy of Sciences|Shenyang Institute of Automation|University of Chinese Academy of Sciences|Chinese Academy of Sciences|Shenyang Institute of Automation|Shenyang Institute of Automation|Chinese Academy of Sciences|University of Chinese Academy of Sciences|Northeastern University|Chinese Academy of Sciences|Shenyang Institute of Automation|Shenyang Institute of Automation|Chinese Academy of Sciences|University of Chinese Academy of Sciences|Chinese Academy of Sciences|Shenyang Institute of Automation,11,RSSGG_CS: Remote Sensing Image Scene Graph Generation by Fusing Contextual Information and Statistical Knowledge,https://doi.org/10.3390/rs14133118,1.36170981,FALSE,en,TRUE,gold,Remote Sensing,journal,2022-06-29,article
https://openalex.org/W4225428129,"Learning graph structure for graph neural networks (GNNs) is crucial to facilitate the GNN-based downstream learning tasks. It is challenging due to the non-differentiable discrete graph structure and lack of ground-truth. In this paper, we address these problems and propose a novel graph structure learning framework for GNNs. Firstly, we directly model the continuous graph structure with dual-normalization, which implicitly imposes sparse constraint and reduces the influence of noisy edges. Secondly, we formulate the whole training process as a bilevel programming problem, where the inner objective is to optimize the GNNs given learned graphs, while the outer objective is to optimize the graph structure to minimize the generalization error of downstream task. Moreover, for bilevel optimization, we propose an improved Neumann-IFT algorithm to obtain an approximate solution, which is more stable and accurate than existing optimization methods. Besides, it makes the bilevel optimization process memory-efficient and scalable to large graphs. Experiments on node classification and scene graph generation show that our method can outperform related methods, especially with noisy graphs.",Minyang Hu|Hong Chang|Bingpeng Ma|Shiguang Shan,Chinese Academy of Sciences|Institute of Computing Technology|University of Chinese Academy of Sciences|University of Chinese Academy of Sciences|Institute of Computing Technology|Chinese Academy of Sciences|University of Chinese Academy of Sciences|Chinese Academy of Sciences|University of Chinese Academy of Sciences|Institute of Computing Technology,3,Learning Continuous Graph Structure with Bilevel Programming for Graph Neural Networks,https://doi.org/10.24963/ijcai.2022/424,0.35266512,FALSE,en,TRUE,bronze,Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence,conference,2022-07-01,article
https://openalex.org/W4285287023,,Xianjing Han|Xuemeng Song|Xingning Dong|Yinwei Wei|Meng Liu|Liqiang Nie,Shandong University of Science and Technology|Shandong University of Science and Technology|Shandong University of Science and Technology|National University of Singapore|Shandong Jianzhu University|Shandong University of Science and Technology,18,DBiased-P: Dual-Biased Predicate Predictor for Unbiased Scene Graph Generation,https://doi.org/10.1109/tmm.2022.3190135,2.22825242,FALSE,en,FALSE,closed,IEEE Transactions on Multimedia,journal,2022-07-13,article
https://openalex.org/W4293518011,,Jiale Lu|Lianggangxu Chen|Yiqing Cai|Haoyue Guan|Changhong Lü|Changbo Wang|Gaoqi He,"East China Normal University|East China Normal University|East China Normal University|University of California, Davis|East China Normal University|East China Normal University|East China Normal University",2,DH-GCN: Saliency-Aware Complex Scene Graph Generation Using Dual-Hierarchy Graph Convolutional Network,https://doi.org/10.1109/icme52920.2022.9859919,0.13808851,FALSE,en,FALSE,closed,2022 IEEE International Conference on Multimedia and Expo (ICME),conference,2022-07-18,article
https://openalex.org/W4312855689,,Vivek B.S.|Jayavardhana Gubbi|Rajan M.A.|P. Balamuralidhar|Arpan Pal,Tata Consultancy Services (India)|Tata Consultancy Services (India)|Tata Consultancy Services (India)|Tata Consultancy Services (India)|Tata Consultancy Services (India),2,EdgeNet for efficient scene graph classification,https://doi.org/10.1109/ijcnn55064.2022.9892703,0.13808851,FALSE,en,FALSE,closed,2022 International Joint Conference on Neural Networks (IJCNN),conference,2022-07-18,article
https://openalex.org/W4293518023,,Xin Zhao|Lei Wu|Xu Chen|Bin Gong,Shandong University|Shandong University|Shandong University|Shandong University,5,High-Quality Image Generation from Scene Graphs with Transformer,https://doi.org/10.1109/icme52920.2022.9859841,0.34522128,FALSE,en,FALSE,closed,2022 IEEE International Conference on Multimedia and Expo (ICME),conference,2022-07-18,article
https://openalex.org/W4293518226,,Min Chen|Xinyu Lyu|Yuyu Guo|Jingwei Liu|Lianli Gao|Jingkuan Song,University of Electronic Science and Technology of China|University of Electronic Science and Technology of China|University of Electronic Science and Technology of China|University of Electronic Science and Technology of China|University of Electronic Science and Technology of China|University of Electronic Science and Technology of China,5,Multi-Scale Graph Attention Network for Scene Graph Generation,https://doi.org/10.1109/icme52920.2022.9859970,0.34522128,FALSE,en,FALSE,closed,2022 IEEE International Conference on Multimedia and Expo (ICME),conference,2022-07-18,article
https://openalex.org/W4293733630,,Xiang Yu|Ruoxin Chen|Jie Li|Jiawei Sun|Shijing Yuan|Huxiao Ji|Xinyu Lu|Chentao Wu,Shanghai Jiao Tong University|Shanghai Jiao Tong University|Shanghai Jiao Tong University|Shanghai Jiao Tong University|Shanghai Jiao Tong University|Shanghai Jiao Tong University|Shanghai Jiao Tong University|Shanghai Jiao Tong University,5,Zero-Shot Scene Graph Generation with Knowledge Graph Completion,https://doi.org/10.1109/icme52920.2022.9859944,0.34522128,FALSE,en,FALSE,closed,2022 IEEE International Conference on Multimedia and Expo (ICME),conference,2022-07-18,article
https://openalex.org/W4285813463,,文彬 王|瑞平 王|熙霖 陈,,0,Balanced scene graph generation assisted by an additional biased predictor,https://doi.org/10.1360/ssi-2022-0105,0.0,FALSE,en,FALSE,closed,Scientia Sinica Informationis,journal,2022-07-19,article
https://openalex.org/W4286505178,"Scene graph generation is the basis of various computer vision applications, including image retrieval, visual question answering, and image captioning. Previous studies have relied on visual features or incorporated auxiliary information to predict object relationships. However, the rich semantics of external knowledge have not yet been fully utilized, and the combination of visual and auxiliary information can lead to visual dependencies, which impacts relationship prediction among objects. Therefore, we propose a novel knowledge-based model with adjustable visual contextual dependency. Our model has three key components. The first module extracts the visual features and bounding boxes in the input image. The second module uses two encoders to fully integrate visual information and external knowledge. Finally, visual context loss and visual relationship loss are introduced to adjust the visual dependency of the model. The difference between the initial prediction results and the visual dependency results is calculated to generate the dependency-corrected results. The proposed model can obtain better global and contextual information for predicting object relationships, and the visual dependencies can be adjusted through the two loss functions. The results of extensive experiments show that our model outperforms most existing methods.",Lizong Zhang|Haojun Yin|Bei Hui|Sijuan Liu|Wei Zhang,University of Electronic Science and Technology of China|University of Electronic Science and Technology of China|University of Electronic Science and Technology of China|Southwestern University of Finance and Economics|University of Electronic Science and Technology of China,9,Knowledge-Based Scene Graph Generation with Visual Contextual Dependency,https://doi.org/10.3390/math10142525,1.11412621,FALSE,en,TRUE,gold,Mathematics,journal,2022-07-20,article
https://openalex.org/W4286484858,,Andreas Weidman,,0,SOLARIS SPEED RUN,https://doi.org/10.1145/3532725.3545311,0.0,FALSE,en,FALSE,closed,,,2022-07-21,article
https://openalex.org/W4288062562,,Xianjing Han|Xingning Dong|Xuemeng Song|Tian Gan|Yibing Zhan|Yan Yan|Liqiang Nie,Shandong University of Science and Technology|Shandong University of Science and Technology|Shandong University of Science and Technology|Shandong University of Science and Technology|Jingdong (China)|Illinois Institute of Technology|Shenzhen Institute of Information Technology|Harbin Institute of Technology,29,Divide-and-Conquer Predictor for Unbiased Scene Graph Generation,https://doi.org/10.1109/tcsvt.2022.3193857,3.58996223,FALSE,en,FALSE,closed,IEEE Transactions on Circuits and Systems for Video Technology,journal,2022-07-26,article
https://openalex.org/W4292448832,,Yini Wang|Yongbin Gao|Wenjun Yu|Ruyan Guo|Weibing Wan|Shuqun Yang|Bo Huang,Shanghai University of Engineering Science|Shanghai University of Engineering Science|Shanghai University of Engineering Science|Shanghai University of Engineering Science|Shanghai University of Engineering Science|Shanghai University of Engineering Science|Shanghai University of Engineering Science,2,Transformer networks with adaptive inference for scene graph generation,https://doi.org/10.1007/s10489-022-04022-0,0.2475836,FALSE,en,FALSE,closed,Applied Intelligence,journal,2022-08-10,article
https://openalex.org/W4312663267,,Yizhou Zhang|Zhaoheng Zheng|Ram Nevatia|Yan Liu,University of Southern California|University of Southern California|University of Southern California|University of Southern California,0,Improving Weakly Supervised Scene Graph Parsing through Object Grounding,https://doi.org/10.1109/icpr56361.2022.9956641,0.0,FALSE,en,FALSE,closed,2022 26th International Conference on Pattern Recognition (ICPR),conference,2022-08-21,article
https://openalex.org/W4312402300,,Takuma Yamamoto|Yuya Obinata|Osafumi Nakayama,Fujitsu (Japan)|Fujitsu (Japan)|Fujitsu (Japan),0,Transformer-based Scene Graph Generation Network With Relational Attention Module,https://doi.org/10.1109/icpr56361.2022.9956599,0.0,FALSE,en,FALSE,closed,2022 26th International Conference on Pattern Recognition (ICPR),conference,2022-08-21,article
https://openalex.org/W4312862807,,Xiang Yu|Jie Li|Shijing Yuan|Chao Wang|Chentao Wu,Shanghai Jiao Tong University|Shanghai Jiao Tong University|Shanghai Jiao Tong University|Shanghai Jiao Tong University|Shanghai Jiao Tong University,5,Zero-shot Scene Graph Generation with Relational Graph Neural Networks,https://doi.org/10.1109/icpr56361.2022.9956712,0.34522128,FALSE,en,FALSE,closed,2022 26th International Conference on Pattern Recognition (ICPR),conference,2022-08-21,article
https://openalex.org/W4292603841,,Fangbo Zhou|Huaping Liu|Huailin Zhao|Lanjun Liang,Shanghai Institute of Technology|Tsinghua University|Shanghai Institute of Technology|Shanghai Institute of Technology,16,Long-term object search using incremental scene graph updating,https://doi.org/10.1017/s0263574722001205,5.07271142,FALSE,en,FALSE,closed,Robotica,journal,2022-08-22,article
https://openalex.org/W4293541940,,Lite Zhang|Junjie Wang|Yanbo Wang|Hai Sun|Xuebing Zhao,Ocean University of China|Ocean University of China|Tongji University|Ocean University of China|Ocean University of China,74,Automatic construction site hazard identification integrating construction scene graphs with BERT based domain knowledge,https://doi.org/10.1016/j.autcon.2022.104535,15.29611864,FALSE,en,FALSE,closed,Automation in Construction,journal,2022-08-26,article
https://openalex.org/W4293371121,,Shichao Wu|Lei Zhou|Zhengxi Hu|Jingtai Liu,Nankai University|Nankai University|Nankai University|Nankai University,18,Hierarchical Context-Based Emotion Recognition With Scene Graphs,https://doi.org/10.1109/tnnls.2022.3196831,4.18189383,FALSE,en,FALSE,closed,IEEE Transactions on Neural Networks and Learning Systems,journal,2022-08-26,article
https://openalex.org/W4293340765,,Jing Zhang|Yingshuai Xie|Kangkang Li|Zhe Wang|Wen Du,East China University of Science and Technology|East China University of Science and Technology|East China University of Science and Technology|East China University of Science and Technology|,9,Hierarchical decoding with latent context for image captioning,https://doi.org/10.1007/s00521-022-07726-z,0.99033441,FALSE,en,FALSE,closed,Neural Computing and Applications,journal,2022-08-27,article
https://openalex.org/W4317418773,,Chenxing Li|Yiping Duan|Qiyuan Du|Chengkang Pan|Guangyi Liu|Xiaoming Tao,Tsinghua University|National Engineering Research Center for Information Technology in Agriculture|Tsinghua University|National Engineering Research Center for Information Technology in Agriculture|Tsinghua University|National Engineering Research Center for Information Technology in Agriculture|China Mobile (China)|China Mobile (China)|Tsinghua University|National Engineering Research Center for Information Technology in Agriculture,5,Image Generation from Scene Graph with Object Edges,https://doi.org/10.1109/vtc2022-fall57202.2022.10012878,0.34522128,FALSE,en,FALSE,closed,2022 IEEE 96th Vehicular Technology Conference (VTC2022-Fall),conference,2022-09-01,article
https://openalex.org/W4312261724,,Tianyu Zhang|Xusheng Du|Chia-Ming Chang|Xi Yang|Haoran Xie,Japan Advanced Institute of Science and Technology|Japan Advanced Institute of Science and Technology|The University of Tokyo|Jilin University|Jilin Medical University|Japan Advanced Institute of Science and Technology,2,Interactive Drawing Interface for Editing Scene Graph,https://doi.org/10.1109/cw55638.2022.00041,0.2475836,FALSE,en,FALSE,closed,,,2022-09-01,article
https://openalex.org/W4295679074,,Jiaming Pei|Kaiyang Zhong|Zhi Yu|Lukun Wang|Kuruva Lakshmanna,University of Sydney|Southwestern University of Finance and Economics|Chongqing University|Shandong University of Science and Technology|Vellore Institute of Technology University,30,Scene Graph Semantic Inference for Image and Text Matching,https://doi.org/10.1145/3563390,3.58996223,FALSE,en,FALSE,closed,ACM Transactions on Asian and Low-Resource Language Information Processing,journal,2022-09-14,article
https://openalex.org/W4295903970,,Xuewei Li|Peihan Miao|Songyuan Li|Xi Li,Zhejiang University of Science and Technology|Zhejiang University|Ningbo University|Zhejiang University of Science and Technology|Zhejiang University of Science and Technology|Shanghai Artificial Intelligence Laboratory|Shanghai Advanced Research Institute,10,MLMG-SGG: Multilabel Scene Graph Generation With Multigrained Features,https://doi.org/10.1109/tip.2022.3199089,1.23791801,FALSE,en,FALSE,closed,IEEE Transactions on Image Processing,journal,2022-09-15,article
https://openalex.org/W4296079526,,Lei Zhao|Junlin Li|Lianli Gao|Yunbo Rao|Jingkuan Song|Heng Tao Shen,University of Electronic Science and Technology of China|University of Electronic Science and Technology of China|University of Electronic Science and Technology of China|University of Electronic Science and Technology of China|University of Electronic Science and Technology of China|University of Electronic Science and Technology of China,20,Heterogeneous Knowledge Network for Visual Dialog,https://doi.org/10.1109/tcsvt.2022.3207228,2.47583602,FALSE,en,FALSE,closed,IEEE Transactions on Circuits and Systems for Video Technology,journal,2022-09-16,article
https://openalex.org/W4296754720,,Donggu Kang|Jiyeon Kim|Jong-Jin Jung,,0,A Framework of Automatic Ontology Construction based on Scene Graph Generation Model for Analysis of Story Video Contents,https://doi.org/10.5370/kiee.2022.71.9.1286,0.0,FALSE,en,FALSE,closed,The Transactions of The Korean Institute of Electrical Engineers,journal,2022-09-22,article
https://openalex.org/W4308088215,,Muhammad Osaid|Zulfiqar Ali Memon,National University of Computer and Emerging Sciences|National University of Computer and Emerging Sciences,2,A Survey On Image Captioning,https://doi.org/10.1109/icetst55735.2022.9922935,0.1237918,FALSE,en,FALSE,closed,,,2022-09-23,article
https://openalex.org/W4297330273,,Karen Stephen|Rishabh Sheoran|Satoshi Yamazaki,NEC (Japan)|National University of Singapore|NEC (Japan),2,Narrative Dataset: Towards Goal-Driven Narrative Generation,https://doi.org/10.1145/3552463.3557021,0.2475836,FALSE,en,FALSE,closed,,,2022-09-27,article
https://openalex.org/W4298145766,"In deep neural network model training and prediction, due to the limitation of GPU memory and computing resources, massive image data must be cropped into limited-sized samples. Moreover, in order to improve the generalization ability of the model, the samples need to be randomly distributed in the experimental area. Thus, the background information is often incomplete or even missing. On this condition, a knowledge graph must be applied to the semantic segmentation of remote sensing. However, although a single sample contains only a limited number of geographic categories, the combinations of geographic objects are diverse and complex in different samples. Additionally, the involved categories of geographic objects often span different classification system branches. Therefore, existing studies often directly regard all the categories involved in the knowledge graph as candidates for specific sample segmentation, which leads to high computation cost and low efficiency. To address the above problems, a parallel walking algorithm based on cross modality information is proposed for the scene graph—knowledge graph matching (PWGM). The algorithm uses a graph neural network to map the visual features of the scene graph into the semantic space of the knowledge graph through anchors and designs a parallel walking algorithm of the knowledge graph that takes into account the visual features of complex scenes. Based on the algorithm, we propose a semantic segmentation model for remote sensing. The experiments demonstrate that our model improves the overall accuracy by 3.7% compared with KGGAT (which is a semantic segmentation model using a knowledge graph and graph attention network (GAT)), by 5.1% compared with GAT and by 13.3% compared with U-Net. Our study not only effectively improves the recognition accuracy and efficiency of remote sensing objects, but also offers useful exploration for the development of deep learning from a data-driven to a data-knowledge dual drive.",Wei Cui|Yuanjie Hao|Xing Xu|Zhanyun Feng|Huilin Zhao|Cong Xia|Jin Wang,Wuhan University of Technology|Wuhan University of Technology|Wuhan University of Technology|Wuhan University of Technology|Wuhan University of Technology|Wuhan University of Technology|Wuhan University of Technology,12,Remote Sensing Scene Graph and Knowledge Graph Matching with Parallel Walking Algorithm,https://doi.org/10.3390/rs14194872,2.15378702,FALSE,en,TRUE,gold,Remote Sensing,journal,2022-09-29,article
https://openalex.org/W4328029385,,Lianggangxu Chen|Jiale Lu|Yiqing Cai|Changbo Wang|Gaoqi He,East China Normal University|East China Normal University|East China Normal University|East China Normal University|East China Normal University,4,Exploring Contextual Relationships in 3D Cloud Points by Semantic Knowledge Mining,https://doi.org/10.1111/cgf.14658,1.01442357,FALSE,en,FALSE,closed,Computer Graphics Forum,journal,2022-10-01,article
https://openalex.org/W4328029478,,Yuping Zhao|Zhi Jin|Huimin Zhao|Fan Zhang|Zongxiao Tao|Chengfeng Dou|Xiaohang Xu|Daocheng Liu,Ministry of Education|Academy of Military Medical Sciences|Peking University|PLA Academy of Military Science|Ministry of Education|Peking University|Peking University|Ministry of Education|PLA Academy of Military Science|Academy of Military Medical Sciences|Peking University|Academy of Military Medical Sciences|Ministry of Education|PLA Academy of Military Science|Ministry of Education|Peking University|Academy of Military Medical Sciences|PLA Academy of Military Science|Academy of Military Medical Sciences|PLA Academy of Military Science,0,Fine‐Grained Scene Graph Generation with Overlap Region and Geometrical Center,https://doi.org/10.1111/cgf.14683,0.0,FALSE,en,FALSE,closed,Computer Graphics Forum,journal,2022-10-01,article
https://openalex.org/W4312598948,,Winnie Pang|Mobarakol Islam|Sai Mitheran|Lalithkumar Seenivasan|Mengya Xu|Hongliang Ren,National University of Singapore|NIHR Imperial Biomedical Research Centre|Imperial College London|National Institute of Technology Tiruchirappalli|National University of Singapore|National University of Singapore|Chinese University of Hong Kong|National University of Singapore,7,Rethinking Feature Extraction: Gradient-Based Localized Feature Extraction for End-To-End Surgical Downstream Tasks,https://doi.org/10.1109/lra.2022.3221310,0.86654261,FALSE,en,FALSE,closed,IEEE Robotics and Automation Letters,journal,2022-10-01,article
https://openalex.org/W4309675213,,Zongming Yang|Liang Yang|Li Ren Kong|Ailin Wei|Jesse Leaman|Johnell O. Brooks|Bing Li,Clemson University|City College of New York|Clemson University|Clemson University|Clemson University|Clemson University|Clemson University,9,SeeWay: Vision-Language Assistive Navigation for the Visually Impaired,https://doi.org/10.1109/smc53654.2022.9945087,2.07749382,FALSE,en,FALSE,closed,"2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",conference,2022-10-09,article
https://openalex.org/W4313563617,"Scene graph generation takes an image and derives a graph representation of key objects in the image and their relations. This core computer vision task is often used in autonomous driving, where traditional software and machine learning (ML) components are used in tandem. However, in such a safety-critical context, valid scene graphs can be further restricted by consistency constraints captured by domain or safety experts. Existing ML approaches for scene graph generation focus exclusively on relation-level accuracy but provide little to no guarantee that consistency constraints are satisfied in the generated scene graphs. In this paper, we aim to complement existing ML-based approaches by a post-processing step using constraint optimization over probabilistic scene graphs that can (1) guarantee that no consistency constraints are violated and (2) improve the overall accuracy of scene graph generation by fixing constraint violations. We evaluate the effectiveness of our approach using well-known, and novel metrics in the context of two popular ML datasets augmented with consistency constraints and two ML-based scene graph generation approaches as baselines.",Boqi Chen|Kristóf Marussy|Sebastian Pilarski|Oszkár Semeráth|Dániel Varró,McGill University|Budapest University of Technology and Economics|McGill University|Budapest University of Technology and Economics|,5,Consistent Scene Graph Generation by Constraint Optimization,https://doi.org/10.1145/3551349.3560433,0.61895901,FALSE,en,TRUE,gold,,,2022-10-10,article
https://openalex.org/W4304080820,,Shuang Wang|Lianli Gao|Xinyu Lyu|Yuyu Guo|Pengpeng Zeng|Jingkuan Song,University of Electronic Science and Technology of China|University of Electronic Science and Technology of China|University of Electronic Science and Technology of China|University of Electronic Science and Technology of China|University of Electronic Science and Technology of China|University of Electronic Science and Technology of China|Peng Cheng Laboratory,19,Dynamic Scene Graph Generation via Temporal Prior Inference,https://doi.org/10.1145/3503161.3548324,1.31184085,FALSE,en,FALSE,closed,Proceedings of the 30th ACM International Conference on Multimedia,conference,2022-10-10,article
https://openalex.org/W4304080485,,Yunqing He|Tongwei Ren|Jinhui Tang|Gangshan Wu,Nanjing University|Nanjing University|Nanjing University of Science and Technology|Nanjing University,3,Heterogeneous Learning for Scene Graph Generation,https://doi.org/10.1145/3503161.3548356,0.20713277,FALSE,en,FALSE,closed,Proceedings of the 30th ACM International Conference on Multimedia,conference,2022-10-10,article
https://openalex.org/W4304080600,"The task of Visual Question Generation (VQG) aims to generate natural language questions for images. Many methods regard it as a reverse Visual Question Answering (VQA) task. They trained a data-driven generator on VQA datasets, which is hard to obtain questions that can challenge robots and humans. Other methods rely heavily on elaborate but expensive artificial preprocessing to generate. To overcome these limitations, we propose a method to generate inferential questions from the image with noisy captions. Our method first introduces a core scene graph generation module, which can align text features and salient visual features to the initial scene graph. It constructs a special core scene graph with expanded linkage outwards from the high-confidence nodes hop by hop. Next, a question generation module uses the core scene graph as a basis to instantiate the function templates, resulting in questions with varying inferential paths. Experiments show that the visual questions generated by our method are controllable in both content and difficulty, and demonstrate clear inferential properties. In addition, since the salient region, captions, and function templates can be replaced by human-customized ones, our method has strong scalability and potential for more interactive applications. Finally, we use our method to automatically build a new dataset, InVQA, containing about 120k images and 480k question-answer pairs, to facilitate the development of more versatile VQA models.",Chao Bi|Shuhui Wang|Zhe Xue|Shengbo Chen|Qingming Huang,University of Chinese Academy of Sciences|Institute of Computing Technology|Chinese Academy of Sciences|Peng Cheng Laboratory|Institute of Computing Technology|Chinese Academy of Sciences|Beijing University of Posts and Telecommunications|Henan University|Chinese Academy of Sciences|Institute of Computing Technology|University of Chinese Academy of Sciences,7,Inferential Visual Question Generation,https://doi.org/10.1145/3503161.3548055,0.48330979,FALSE,en,TRUE,bronze,Proceedings of the 30th ACM International Conference on Multimedia,conference,2022-10-10,article
https://openalex.org/W4306317683,,Chunhui Zhang|Chao Huang|Youhuan Li|Xiangliang Zhang|Yanfang Ye|Chuxu Zhang,Brandeis University|University of Hong Kong|Hunan University|University of Notre Dame|University of Notre Dame|Brandeis University,4,Look Twice as Much as You Say,https://doi.org/10.1145/3511808.3557382,0.27617702,FALSE,en,FALSE,closed,Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management,conference,2022-10-16,article
https://openalex.org/W4310009518,,Jiyoun Lim|NamKyung Lee,Electronics and Telecommunications Research Institute|Electronics and Telecommunications Research Institute,0,Graph Feature Generation based on Scene Graph Benchmark Application on Video,https://doi.org/10.1109/ictc55196.2022.9952683,0.0,FALSE,en,FALSE,closed,2022 13th International Conference on Information and Communication Technology Convergence (ICTC),conference,2022-10-19,article
https://openalex.org/W4312311771,,Xingchen Song|Miao Kang|Sanping Zhou|Jianji Wang|Yishu Mao|Nanning Zheng,Xi'an Jiaotong University|Xi'an Jiaotong University|Xi'an Jiaotong University|Xi'an Jiaotong University|Xi'an Jiaotong University|Xi'an Jiaotong University,11,Pedestrian Intention Prediction Based on Traffic-Aware Scene Graph Model,https://doi.org/10.1109/iros47612.2022.9981690,2.78915563,FALSE,en,FALSE,closed,2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),conference,2022-10-23,article
https://openalex.org/W4320507205,,Takahiro Kawamura|Shusaku Egami|Kyoumoto Matsushita|Takanori Ugai|Ken Fukuda|Kouji Kozaki,National Agriculture and Food Research Organization|National Institute of Advanced Industrial Science and Technology|Fujitsu (Japan)|Fujitsu (Japan)|National Institute of Advanced Industrial Science and Technology|Osaka Electro-Communication University,3,Contextualized Scene Knowledge Graphs for XAI Benchmarking,https://doi.org/10.1145/3579051.3579061,0.58739646,FALSE,en,FALSE,closed,,,2022-10-27,article
https://openalex.org/W4312941295,,Nannan Hu|Yue Ming|Chunxiao Fan|Fan Feng|Boyang Lyu,Beijing University of Posts and Telecommunications|Beijing University of Posts and Telecommunications|Beijing University of Posts and Telecommunications|Beijing University of Posts and Telecommunications|Beijing University of Posts and Telecommunications,11,TSFNet: Triple-Steam Image Captioning,https://doi.org/10.1109/tmm.2022.3215861,1.23791801,FALSE,en,FALSE,closed,IEEE Transactions on Multimedia,journal,2022-11-03,article
https://openalex.org/W4311167584,,Jie Wang|Yan Yang|Keyu Liu|Zhiping Zhu|Xiaorong Liu,Southwest Jiaotong University|Southwest Jiaotong University|Southwest Jiaotong University|Southwest Jiaotong University|Southwest Jiaotong University,45,M3S: Scene Graph Driven Multi-Granularity Multi-Task Learning for Multi-Modal NER,https://doi.org/10.1109/taslp.2022.3221017,8.61514809,FALSE,en,FALSE,closed,IEEE/ACM Transactions on Audio Speech and Language Processing,journal,2022-11-09,article
https://openalex.org/W4308974728,,Xuyang Lu|Yang Gao,Beijing Institute of Technology|Beijing Institute of Technology|Beijing Computing Center,5,Guide and interact: scene-graph based generation and control of video captions,https://doi.org/10.1007/s00530-022-01012-7,0.61895901,FALSE,en,FALSE,closed,Multimedia Systems,journal,2022-11-14,article
https://openalex.org/W4313306953,,Weifeng Liu|Nan Zhang|Yaning Wang|Di Wu,Shaanxi University of Science and Technology|Shaanxi University of Science and Technology|Hangzhou Dianzi University|Shaanxi University of Chinese Medicine,1,An Image Caption Model Based on the Scene Graph and Semantic Prior Network,https://doi.org/10.1109/iccais56082.2022.9990458,0.06904426,FALSE,en,FALSE,closed,"2022 11th International Conference on Control, Automation and Information Sciences (ICCAIS)",conference,2022-11-21,article
https://openalex.org/W4309994913,,Hyeonbyeong Lee|Sangho Song|Dojin Choi|Jongtae Lim|Kyoungsoo Bok|Jae-Soo Yoo,Chungbuk National University|Chungbuk National University|Changwon National University|Chungbuk National University|Wonkwang University|Chungbuk National University,0,High-Dimensional Indexing Scheme for Scene Graph Retrieval,https://doi.org/10.1109/avss56176.2022.9959346,0.0,FALSE,en,FALSE,closed,,,2022-11-24,article
https://openalex.org/W4310068879,,Zhixin Li|Jiahui Wei|Feicheng Huang|Huifang Ma,Guangxi Normal University|Guangxi Normal University|Guangxi Normal University|Northwest Normal University,19,Modeling graph-structured contexts for image captioning,https://doi.org/10.1016/j.imavis.2022.104591,2.35204422,FALSE,en,FALSE,closed,Image and Vision Computing,journal,2022-11-24,article
https://openalex.org/W4317515984,,Liwen Ma|Weifeng Liu|Yaning Wang,Shaanxi University of Science and Technology|Shaanxi University of Science and Technology|Hangzhou Dianzi University,2,A Multimodal Fusion Scene Graph Generation Method Based on Semantic Description,https://doi.org/10.1109/ccis57298.2022.10016416,0.13808851,FALSE,en,FALSE,closed,2022 IEEE 8th International Conference on Cloud Computing and Intelligent Systems (CCIS),conference,2022-11-26,article
https://openalex.org/W4311057659,"Humans can easily infer the motivations behind human actions from only visual data by comprehensively analyzing the complex context information and utilizing abundant life experiences. Inspired by humans’ reasoning ability, existing motivation prediction methods have improved image-based deep classification models using the commonsense knowledge learned by pre-trained language models. However, the knowledge learned from public text corpora is probably incompatible with the task-specific data of the motivation prediction, which may impact the model performance. To address this problem, this paper proposes a dual scene graph convolutional network (dual-SGCN) to comprehensively explore the complex visual information and semantic context prior from the image data for motivation prediction. The proposed dual-SGCN has a visual branch and a semantic branch. For the visual branch, we build a visual graph based on scene graph where object nodes and relation edges are represented by visual features. For the semantic branch, we build a semantic graph where nodes and edges are directly represented by the word embeddings of the object and relation labels. In each branch, node-oriented and edge-oriented message passing is adopted to propagate interaction information between different nodes and edges. Besides, a multi-modal interactive attention mechanism is adopted to cooperatively attend and fuse the visual and semantic information. The proposed dual-SGCN is learned in an end-to-end form by a multi-task co-training scheme. In the inference stage, Total Direct Effect is adopted to alleviate the bias caused by the semantic context prior. Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance.",Yuyang Wanyan|Xiaoshan Yang|Xuan Ma|Changsheng Xu,Institute of Automation|University of Chinese Academy of Sciences|Chinese Academy of Sciences|Peng Cheng Laboratory|Institute of Automation|University of Chinese Academy of Sciences|University of Chinese Academy of Sciences|Institute of Automation|Institute of Automation|University of Chinese Academy of Sciences|Peng Cheng Laboratory|Chinese Academy of Sciences,5,Dual Scene Graph Convolutional Network for Motivation Prediction,https://doi.org/10.1145/3572914,0.61895901,FALSE,en,TRUE,bronze,ACM Transactions on Multimedia Computing Communications and Applications,journal,2022-12-01,article
https://openalex.org/W4312922040,,Jiahao Yin|Xinyu Zhou|Huahui Xiao|Zhili Liu|Wei Li|Xue Li|Shengyin Fan,Beihang University|University of International Business and Economics|Beihang University||Beihang University||,2,HAPOR: Hierarchical-Features Aligned Projection Optimization for Relocalization,https://doi.org/10.1109/lra.2022.3226069,0.67636152,FALSE,en,FALSE,closed,IEEE Robotics and Automation Letters,journal,2022-12-01,article
https://openalex.org/W4319309602,,Afsana Airin|Rezab Ud Dawla|Ahmed Shabab Noor|Muhib Al Hasan|Ahmed Rafi Hasan|Akib Zaman|Dewan Md. Farid,United International University|United International University|United International University|United International University|United International University|United International University|United International University,6,Attention-Based Scene Graph Generation: A Review,https://doi.org/10.1109/skima57145.2022.10029570,0.61895901,FALSE,en,FALSE,closed,,,2022-12-02,review
https://openalex.org/W4310642348,,Tao He|Lianli Gao|Jingkuan Song|Yuan-Fang Li,Monash University|University of Electronic Science and Technology of China|University of Electronic Science and Technology of China|Monash University,16,State-Aware Compositional Learning Toward Unbiased Training for Scene Graph Generation,https://doi.org/10.1109/tip.2022.3224872,1.98066882,FALSE,en,FALSE,closed,IEEE Transactions on Image Processing,journal,2022-12-02,article
https://openalex.org/W4317383099,,Wenjie Geng|Zhiqiang Cao|Yingbo Tang|Shuo Wang|Fengshui Jing,Chinese Academy of Sciences|Beijing Academy of Artificial Intelligence|University of Chinese Academy of Sciences|Shandong Institute of Automation|Chinese Academy of Sciences|Shandong Institute of Automation|University of Chinese Academy of Sciences|Beijing Academy of Artificial Intelligence|Shandong Institute of Automation|Chinese Academy of Sciences|Beijing Academy of Artificial Intelligence|University of Chinese Academy of Sciences|Shandong Institute of Automation|Chinese Academy of Sciences|Beijing Academy of Artificial Intelligence|University of Chinese Academy of Sciences|Beijing Academy of Artificial Intelligence|Chinese Academy of Sciences|Shandong Institute of Automation|University of Chinese Academy of Sciences,0,Visual Grasping with Spectral Clustering and Heuristic Searching for Robot in Cluttered Environments,https://doi.org/10.1109/robio55434.2022.10011962,0.0,FALSE,en,FALSE,closed,2022 IEEE International Conference on Robotics and Biomimetics (ROBIO),conference,2022-12-05,article
https://openalex.org/W4312926411,,Zheng Wang|Xing Xu|Yin Zhang⋆|Yang Yang|Heng Tao Shen,University of Electronic Science and Technology of China|University of Electronic Science and Technology of China|University of Electronic Science and Technology of China|University of Electronic Science and Technology of China|University of Electronic Science and Technology of China,10,Complex Relation Embedding for Scene Graph Generation,https://doi.org/10.1109/tnnls.2022.3226871,1.23791801,FALSE,en,FALSE,closed,IEEE Transactions on Neural Networks and Learning Systems,journal,2022-12-15,article
https://openalex.org/W4318148218,,Bing Chen|Bin Bai|Xintong Mao|Bin Liu|Zhenye Gu,"Shanghai Electric (China)|Shanghai Electric (China)|Shanghai Electric (China)|Jiangsu Frontier Electric Technology Co., Ltd. (China)|Nanjing Institute of Railway Technology|Nanjing Institute of Railway Technology|Jiangsu Frontier Electric Technology Co., Ltd. (China)",0,Research and application of key technologies for full-loop fast imaging based on cable-laying information,https://doi.org/10.1109/imcec55388.2022.10019886,0.0,FALSE,en,FALSE,closed,"2022 IEEE 5th Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)",conference,2022-12-16,article
https://openalex.org/W4313448292,,Charulata Patil|Aditya Abhyankar,Savitribai Phule Pune University|Savitribai Phule Pune University,2,Generating comprehensive scene graphs with integrated multiple attribute detection,https://doi.org/10.1007/s00138-022-01361-3,0.2475836,FALSE,en,FALSE,closed,Machine Vision and Applications,journal,2022-12-20,article
https://openalex.org/W4312377805,,Ze Fu|Changmeng Zheng|Junhao Feng|Yi Cai|Xiao-Yong Wei|Yaowei Wang|Qing Li,South China University of Technology|Hong Kong Polytechnic University|South China University of Technology|Peng Cheng Laboratory|South China University of Technology|Chengdu University|Peng Cheng Laboratory|Sichuan University|Peng Cheng Laboratory|Hong Kong Polytechnic University,14,DRAKE: Deep Pair-Wise Relation Alignment for Knowledge-Enhanced Multimodal Scene Graph Generation in Social Media Posts,https://doi.org/10.1109/tcsvt.2022.3231437,1.73308522,FALSE,en,FALSE,closed,IEEE Transactions on Circuits and Systems for Video Technology,journal,2022-12-21,article
https://openalex.org/W4312058667,,Xuewei Li|Tao Wu|Guangcong Zheng|Yunlong Yu|Xi Li,Zhejiang University of Science and Technology|Zhejiang University of Science and Technology|Zhejiang University of Science and Technology|Zhejiang University|Zhejiang University of Science and Technology|Shanghai Advanced Research Institute|Shanghai Artificial Intelligence Laboratory,6,Uncertainty-Aware Scene Graph Generation,https://doi.org/10.1016/j.patrec.2022.12.011,0.74275081,FALSE,en,FALSE,closed,Pattern Recognition Letters,journal,2022-12-21,article
https://openalex.org/W4313219784,"As a structured abstraction method for objects and their interactions in visual scene, scene graph captures entities in the scene and the relationships between the entity pairs, and helps in understanding the visual scene better. Currently, scene graphs in most research works are generated by modeling using context information among targets, focusing only on the inference process but ignoring the integrity of the input target information and the impact of the global information of the targets on relationship inference. Therefore, a new scene graph generation method based on global embedding and contextual fusion (GECF) is proposed in this paper. In this method, richer entity information is obtained by embedding global information into the entity features, while more robust inference of entity interaction information and more reasonable relationship fusion are acquired by combining the attention weighting module and the context inference module as a joint inference module, and merging the obtained entity features according to their discrepancy. The experiment on Visual Genome dataset shows that GECF method performs better than the existing methods in scene graph visual relationship detection.",zhiyong zhao|Ronglin Hu|hongtai ma|Xinxin Zhang,Huaiyin Institute of Technology|Huaiyin Institute of Technology|Huaiyin Institute of Technology|Huaiyin Institute of Technology,1,Scene graph generation based on global embedding and contextual fusion,https://doi.org/10.1117/12.2661798,0.06904426,FALSE,en,TRUE,bronze,Third International Conference on Computer Science and Communication Technology (ICCSCT 2022),conference,2022-12-29,article
https://openalex.org/W4313306352,,Yuanyan Xie|Yu Guo|Zhenqiang Mi|Xiaokun Wang|Yang Yang|Mohammad S. Obaidat,University of Science and Technology Beijing|University of Science and Technology Beijing|University of Science and Technology Beijing|University of Science and Technology Beijing|University of Science and Technology Beijing|The University of Texas of the Permian Basin|Amity University|University of Science and Technology Beijing|University of Jordan,3,Indoor Visual Re-Localization for Long-Term Autonomous Robots Based on Object-Level Features and Semantic Relationships,https://doi.org/10.1109/lra.2022.3233235,1.01454228,FALSE,en,FALSE,closed,IEEE Robotics and Automation Letters,journal,2022-12-30,article
https://openalex.org/W4313346374,,An-An Liu|Hongwei Du|Ning Xu|Quan Zhang|Shenyuan Zhang|Yejun Tang|Xuanya Li,Tianjin University|Tianjin University|Tianjin University|Peking University||Kuaishou (China)|Baidu (China),2,Exploring visual relationship for social media popularity prediction,https://doi.org/10.1016/j.jvcir.2022.103738,1.02562264,FALSE,en,FALSE,closed,Journal of Visual Communication and Image Representation,journal,2022-12-31,article
https://openalex.org/W4388622909,"Summarization is a challenging task that aims to generate a summary by grasping common information of a given set of information. Text summarization is a popular task of determining the topic or generating a textual summary of documents. In contrast, image summarization aims to find a representative summary of a collection of images. However, current methods are still restricted to generating a visual scene graph, tags, and noun phrases, but cannot generate a fitting textual description of an image collection. Thus, we introduce a novel framework for generating a summarized caption of an image collection. Since scene graph generation shows advancement in describing objects and their relationships on a single image, we use it in the proposed method to generate a scene graph for each image in an image collection. Then, we find common objects and their relationships from all scene graphs and represent them as a summarized scene graph. For this, we merge all scene graphs and select part of it by estimating the most common objects and relationships. Finally, the summarized scene graph is input into a captioning model. In addition, we introduce a technique to generalize specific words in the final caption into common concept words incorporating external knowledge. To evaluate the proposed method, we construct a dataset for this task by extending the annotation of the MS-COCO dataset using an image retrieval method. The evaluation of the proposed method on this dataset showed promising performance compared to text summarization-based methods.",Itthisak Phueaksri|Marc A. Kastner|Yasutomo Kawanishi|Takahiro Komamizu|Ichiro Ide,Nagoya University|Kyoto University|Kyoto College of Graduate Studies for Informatics|Nagoya University|Nagoya University|Nagoya University,15,An Approach to Generate a Caption for an Image Collection Using Scene Graph Generation,https://doi.org/10.1109/access.2023.3332098,2.7295251,FALSE,en,TRUE,gold,IEEE Access,journal,2023-01-01,article
https://openalex.org/W4323244288,,Mitsuki Yoshida|Ryogo Yamamoto|Wakayama Kazuki|Hiroki Tomoe|Tanaka Kanji,University of Fukui|University of Fukui|University of Fukui|University of Fukui|University of Fukui,0,Classification and Embedding of Semantic Scene Graphs for Active Cross-Domain Self-Localization,https://doi.org/10.5220/0011621200003417,0.0,FALSE,en,TRUE,gold,,,2023-01-01,article
https://openalex.org/W4319301038,"Scene graph generation (SGG) methods extract relationships between objects. While most methods focus on improving top-down approaches, which build a scene graph based on detected objects from an off-the-shelf object detector, there is a limited amount of work on bottom-up approaches, which jointly detect objects and their relationships in a single stage. In this work, we present a novel bottom-up SGG approach by representing relationships using Composite Relationship Fields (CoRF). CoRF turns relationship detection into a dense regression and classification task, where each cell of the output feature map identifies surrounding objects and their relationships. Furthermore, we propose a refinement head that leverages Transformers for global scene reasoning, resulting in more meaningful relationship predictions. By combining both contributions, our method outperforms previous bottom-up methods on the Visual Genome dataset by 26% while preserving real-time performance.",George Adaimi|David Mizrahi|Alexandre Alahi,Instituto Vita|Instituto Vita|Instituto Vita,3,Composite Relationship Fields with Transformers for Scene Graph Generation,https://doi.org/10.1109/wacv56688.2023.00014,0.2414103,FALSE,en,TRUE,green,2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),conference,2023-01-01,article
https://openalex.org/W4318614816,,Mohammad Javad Parseh|Mohammad Rahmanimanesh|Parviz Keshavarzi|Zohreh Azimifar,Semnan University|Semnan University|Semnan University|Shiraz University,0,G-Cnn: A Two-Branch Neural Network Model for Representing Visual Scenes,https://doi.org/10.2139/ssrn.4333691,,FALSE,en,TRUE,green,SSRN Electronic Journal,repository,2023-01-01,article
https://openalex.org/W4323240317,"Generating realistic images is one of the important problems in the field of computer vision.In image generation tasks, generating images consistent with an input given by the user is called conditional image generation.Due to the recent advances in generating high-quality images with Generative Adversarial Networks, many conditional image generation models have been proposed, such as text-to-image, scene-graph-to-image, and layout-to-image models.Among them, scene-graph-to-image models have the advantage of generating an image for a complex situation according to the structure of a scene graph.However, existing scene-graph-toimage models have difficulty in capturing positional relations among three or more objects since a scene graph can only represent relations between two objects.In this paper, we propose a novel image generation model which addresses this shortcoming by generating images from a hyper scene graph with trinomial edges.We also use a layout-to-image model supplementally to generate higher resolution images.Experimental validations on COCO-Stuff and Visual Genome datasets show that the proposed model generates more natural and faithful images to user's inputs than a cutting-edge scene-graph-to-image model.",Ryosuke Miyake|Tetsu Matsukawa|Einoshin Suzuki,Kyushu University|Kyushu University|Kyushu University,1,Image Generation from a Hyper Scene Graph with Trinomial Hyperedges,https://doi.org/10.5220/0011699300003417,0.18196834,FALSE,en,TRUE,gold,,,2023-01-01,article
https://openalex.org/W4319299846,,So Hasegawa|Masayuki Hiromoto|Akira Nakagawa|Yuhei Umeda,Fujitsu (Japan)|Fujitsu (Japan)|Fujitsu (Japan)|Fujitsu (Japan),2,Improving Predicate Representation in Scene Graph Generation by Self-Supervised Learning,https://doi.org/10.1109/wacv56688.2023.00276,0.1609402,FALSE,en,FALSE,closed,2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),conference,2023-01-01,article
https://openalex.org/W4319300178,"Visual understanding concerns to what extent a cognitive system can reason about the visual surroundings before it reacts accordingly. While visual understanding is considered crucial, what go beyond are the capabilities of multi-modal reasoning which involve also other modalities. That is, a cognitive system is often faced with a daunting process – how to capitalize on the inputs, usually from one or more modalities – to adapt itself to the world of multiple modalities. More importantly, different machine learning paradigms may be exploited to learn both uni-modal and multi-modal reasoning tasks. This defines the main research question initiating the research endeavour presented in this thesis. In response to the dissertation's core research question, the work provides a number of methods empowered by different machine learning paradigms for both uni-modal and multi-modal contexts. More concretely, it is shown that one can estimate visual saliency, which is one of the most crucial fundamentals of visual understanding, with visual cues learned in an unsupervised fashion. Semi-supervised learning principle is found to be effective in combating class-imbalance issues in scene graph generation, which aims at discovering relationships among visual objects in an image. Moreover, to overcome the primary drawback in vision-language (VL) pre-training and other VL applications, which conventionally necessitate annotated image-text pairs, a novel weakly supervised approach is introduced. Besides, several enhancements have been made to supervised learning applications: Firstly, an improved dense image captioning model is proposed to better exploit different types of relationships between visual objects in an image. Secondly, an enhanced video captioning model is proposed to alleviate the impact brought by the modality gap, which can be commonly found in the widely adopted Transformer models. Lastly, an uncertainty-aware classification model is proposed to learn more robustly under noisy supervision when accounting for data and model uncertainties. These results suggest the usefulness and wide applicability of different learning paradigms. In terms of models' robustness, several breakthroughs have been made and elaborated for both uni-modal and multi-modal applications. The research outcomes encompass numerous findings related to computer vision techniques and their bridges to natural language. The thesis concludes with a discussion on the limitations of each published work and potential future endeavours in both uni-modal and multi-modal research.",Tzu-Jui Julius Wang|Jorma Laaksonen|Tomas Langer|Heikki Arponen|Tom E. Bishop,Aalto University|Aalto University|||,2,Learning by Hallucinating: Vision-Language Pre-training with Weak Supervision,https://doi.org/10.1109/wacv56688.2023.00113,0.1609402,FALSE,en,TRUE,green,2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),conference,2023-01-01,article
https://openalex.org/W4319300107,,Zhanwen Chen|Saed Rezayi|Sheng Li,University of Virginia|University of Georgia|University of Virginia,14,"More Knowledge, Less Bias: Unbiasing Scene Graph Generation with Explicit Ontological Adjustment",https://doi.org/10.1109/wacv56688.2023.00401,1.1265814,FALSE,en,FALSE,closed,2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),conference,2023-01-01,article
https://openalex.org/W4386825446,,Xingwu Ji|Peilin Liu|Haochen Niu|Xiang Chen|Rendong Ying|Fei Wen,Shanghai Center for Brain Science and Brain-Inspired Technology|Shanghai Jiao Tong University|Shanghai Jiao Tong University|Shanghai Center for Brain Science and Brain-Inspired Technology|Shanghai Center for Brain Science and Brain-Inspired Technology|Shanghai Jiao Tong University|Shanghai Jiao Tong University|Shanghai Center for Brain Science and Brain-Inspired Technology|Shanghai Jiao Tong University|Shanghai Center for Brain Science and Brain-Inspired Technology|Shanghai Jiao Tong University|Shanghai Center for Brain Science and Brain-Inspired Technology,7,Object SLAM Based on Spatial Layout and Semantic Consistency,https://doi.org/10.1109/tim.2023.3316258,3.63965188,FALSE,en,FALSE,closed,IEEE Transactions on Instrumentation and Measurement,journal,2023-01-01,article
https://openalex.org/W4317796337,,Zheng Wang|Xing Xu|Guoqing Wang|Yang Yang|Heng Tao Shen,University of Electronic Science and Technology of China|University of Electronic Science and Technology of China|University of Electronic Science and Technology of China|University of Electronic Science and Technology of China|University of Electronic Science and Technology of China|Peng Cheng Laboratory,25,Quaternion Relation Embedding for Scene Graph Generation,https://doi.org/10.1109/tmm.2023.3239229,4.5492085,FALSE,en,FALSE,closed,IEEE Transactions on Multimedia,journal,2023-01-01,article
https://openalex.org/W4386320506,"Scene graph generation aims to detect objects and their relations in images, providing structured representations for scene understanding. Currently, mainstream approaches first detect the objects and then solve a classification task to determine the relation between each object pair, ignoring the other combinations of the subject-predicate-object triplet. In this work we propose a triplet learning paradigm for scene graph generation, where given any two entities of the triplet we learn to predict the third. The multi-task learning scheme is adopted to equip a scene graph generation model with the triplet learning task, in which the prediction heads for the subject, object and predicate share the same backbone and are jointly trained. The proposed method does not require any additional annotation and is easy to embed in existing networks. It benefits scene graph generation models in gaining more generalizability and thus can be applied to both biased and unbiased methods. Moreover, we introduce a new Graph Structure-Aware Transformer (GSAT) model that incorporates the structural information of the scene graph via a modified self-attention mechanism. Extensive experiments show that the proposed triplet learning consistently improves the performance of several state-of-the-art models on the Visual Genome dataset.",Xuecheng Sun|Zhe‐Ming Lu|Zewei He|Ziqian Lu|Hao Luo,Zhejiang University|Zhejiang University|Zhejiang University|Zhejiang University|Zhejiang University,0,Reasoning in Different Directions: Triplet Learning for Scene Graph Generation,https://doi.org/10.1109/access.2023.3310544,0.0,FALSE,en,TRUE,gold,IEEE Access,journal,2023-01-01,article
https://openalex.org/W4360596135,"Automated identification of the relationships between traffic actors and surrounding objects, in order to describe their behavior and predict their intentions, has become the focus of increasing attention in the field of autonomous driving. Therefore, in this work, we propose a Road Scene Graphs-Graph Convolutional Network (RSG-GCN) as a novel, graph-based model for predicting the topological graph structure of a given traffic scene. The status of the actors and HD map information are integrated as prior knowledge, allowing the edges linking the actor nodes to capture potential semantic relationships, such as &#x201C;vehicle approaching pedestrian&#x201D; and &#x201C;pedestrian waiting at intersection&#x201D;. To train this model, we created our own RSG dataset, as well as a relational dataset and benchmark derived from nuScenes. Our extensive range of experiments demonstrate that our model can more accurately predict semantic relationships and behavior in a given traffic scene than other popular traffic scene prediction models. In particular, regarding the use of HD map prior knowledge, we found that the resulting increase in accuracy significantly outweighs performance loss caused by the increase in graph size. The downstream applications of RSG include traffic scene retrieval and synthetic traffic scene generation, which are briefly described.",Yafu Tian|Alexander Carballo|Ruifeng Li|Kazuya Takeda,Harbin Institute of Technology|Nagoya University|Gifu University|Nagoya University|Harbin Institute of Technology|Nagoya University,11,RSG-GCN: Predicting Semantic Relationships in Urban Traffic Scene With Map Geometric Prior,https://doi.org/10.1109/ojits.2023.3260624,1.79917318,FALSE,en,TRUE,gold,IEEE Open Journal of Intelligent Transportation Systems,journal,2023-01-01,article
https://openalex.org/W4328007230,,Hongrui Sang|Rong Jiang|Zhipeng Wang|Yanmin Zhou|Ping Lu|Bin He,Tongji University|Tongji University|Tongji University|Beijing Institute of Technology|Tongji University|Tongji University|Tongji University,3,Scene Augmentation Methods for Interactive Embodied AI Tasks,https://doi.org/10.1109/tim.2023.3259033,0.54590502,FALSE,en,FALSE,closed,IEEE Transactions on Instrumentation and Measurement,journal,2023-01-01,article
https://openalex.org/W4389518639,"Referring Expression Comprehension (ReC) is a task that involves localizing objects in images based on natural language expressions. Most ReC methods typically approach the task as a supervised learning problem. However, the need for costly annotations, such as clear image-text pairs or region-text pairs, hinders the scalability of existing approaches. In this work, we propose a novel scene graph-based framework that automatically generates high-quality pseudo region-query pairs. Our method harnesses scene graphs to capture the relationships between objects in images and generate expressions enriched with relation information. To ensure accurate mapping between visual regions and text, we introduce an external module that employs a calibration algorithm to filter out ambiguous queries. Additionally, we employ a rewriter module to enhance the diversity of our generated pseudo queries through rewriting. Extensive experiments demonstrate that our method outperforms previous pseudo-labeling methods by about 10%, 12%, and 11% on RefCOCO, RefCOCO+, and RefCOCOg, respectively. Furthermore, it surpasses the state-of-the-art unsupervised approach by more than 15% on the RefCOCO dataset.",Cantao Wu|Yi Cai|Liuwu Li|Jiexin Wang,South China University of Technology|South China University of Technology|South China University of Technology|South China University of Technology,0,Scene Graph Enhanced Pseudo-Labeling for Referring Expression Comprehension,https://doi.org/10.18653/v1/2023.findings-emnlp.802,0.0,FALSE,en,TRUE,gold,,,2023-01-01,article
https://openalex.org/W4388579765,,Tao He|Lianli Gao|Jingkuan Song|Yuan-Fang Li,University of Electronic Science and Technology of China|Monash University|University of Electronic Science and Technology of China|University of Electronic Science and Technology of China|Monash University,13,Toward a Unified Transformer-Based Framework for Scene Graph Generation and Human-Object Interaction Detection,https://doi.org/10.1109/tip.2023.3330304,2.36558842,FALSE,en,FALSE,closed,IEEE Transactions on Image Processing,journal,2023-01-01,article
https://openalex.org/W7128281867,,Oliver Schulz|Jeroen Werbrouck|Jakob Beetz,,0,Towards Scene Graph Descriptions for Spatial Representations in the Built Environment,,0.0,FALSE,en,FALSE,closed,RWTH Publications (RWTH Aachen),repository,2023-01-01,article
https://openalex.org/W4319301030,,Yue Qiu|Yoshiki Nagasaki|Kensho Hara|Hirokatsu Kataoka|Ryota Suzuki|Kenji Iwata|Yutaka Satoh,National Institute of Advanced Industrial Science and Technology|National Institute of Advanced Industrial Science and Technology|National Institute of Advanced Industrial Science and Technology|National Institute of Advanced Industrial Science and Technology|National Institute of Advanced Industrial Science and Technology|National Institute of Advanced Industrial Science and Technology|National Institute of Advanced Industrial Science and Technology,4,VirtualHome Action Genome: A Simulated Spatio-Temporal Scene Graph Dataset with Consistent Relationship Labels,https://doi.org/10.1109/wacv56688.2023.00335,0.3218804,FALSE,en,FALSE,closed,2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),conference,2023-01-01,article
https://openalex.org/W4316813765,,Jinghui Peng|Zhen Wang|Shizhe Wang,||Naval University of Engineering,2,Similarity calculation method for images based on the scene graph,https://doi.org/10.1007/s11760-022-02456-0,0.36393668,FALSE,en,FALSE,closed,Signal Image and Video Processing,journal,2023-01-17,article
https://openalex.org/W4317886239,"Purpose Autonomous robots must be able to understand long-term manipulation tasks described by humans and perform task analysis and planning based on the current environment in a variety of scenes, such as daily manipulation and industrial assembly. However, both classical task and motion planning algorithms and single data-driven learning planning methods have limitations in practicability, generalization and interpretability. The purpose of this work is to overcome the limitations of the above methods and achieve generalized and explicable long-term robot manipulation task planning. Design/methodology/approach The authors propose a planning method for long-term manipulation tasks that combines the advantages of existing methods and the prior cognition brought by the knowledge graph. This method integrates visual semantic understanding based on scene graph generation, regression planning based on deep learning and multi-level representation and updating based on a knowledge base. Findings The authors evaluated the capability of this method in a kitchen cooking task and tabletop arrangement task in simulation and real-world environments. Experimental results show that the proposed method has a significantly improved success rate compared with the baselines and has excellent generalization performance for new tasks. Originality/value The authors demonstrate that their method is scalable to long-term manipulation tasks with varying complexity and visibility. This advantage allows their method to perform better in new manipulation tasks. The planning method proposed in this work is meaningful for the present robot manipulation task and can be intuitive for similar high-level robot planning.",Runqing Miao|Qingxuan Jia|Fuchun Sun,Beijing University of Posts and Telecommunications|Beijing University of Posts and Telecommunications|Tsinghua University,21,Long-term robot manipulation task planning with scene graph and semantic knowledge,https://doi.org/10.1108/ria-09-2022-0226,3.82133514,FALSE,en,TRUE,bronze,Robotic Intelligence and Automation,journal,2023-01-24,article
https://openalex.org/W4318588135,"Construction hazards occur at any time in outfield test sites and frequently result from improper interactions between objects. The majority of casualties might be avoided by following on-site regulations. However, workers may be unable to comply with the safety regulations fully because of stress, fatigue, or negligence. The development of deep-learning-based computer vision and on-site video surveillance facilitates safety inspections, but automatic hazard identification is often limited due to the semantic gap. This paper proposes an automatic hazard identification method that integrates on-site scene graph generation and domain-specific knowledge extraction. A BERT-based information extraction model is presented to automatically extract the key regulatory information from outfield work safety requirements. Subsequently, an on-site scene parsing model is introduced for detecting interaction between objects in images. An automatic safety checking approach is also established to perform PPE compliance checks by integrating detected textual and visual relational information. Experimental results show that our proposed method achieves strong performance in various metrics on self-built and widely used public datasets. The proposed method can precisely extract relational information from visual and text modalities to facilitate on-site hazard identification.",Xuan Liu|Xiaochuan Jing|Quan Zhu|Wanru Du|Xiaoyin Wang,China Aerospace Science and Technology Corporation|China Aerospace Science and Technology Corporation|China Aerospace Science and Technology Corporation|China Aerospace Science and Technology Corporation|China Aerospace Science and Technology Corporation,11,Automatic Construction Hazard Identification Integrating On-Site Scene Graphs with Information Extraction in Outfield Test,https://doi.org/10.3390/buildings13020377,3.88311865,FALSE,en,TRUE,gold,Buildings,journal,2023-01-29,article
https://openalex.org/W4319303303,"Visual localization is a challenging task due to the presence of illumination changes, occlusion, and perception from novel viewpoints. Re-localizing the camera pose in long-term setups raises difficulties caused by changes in scene appearance and geometry introduced by human or natural deterioration. Many existing methods use static scene assumptions and fail in dynamic indoor scenes. Only a few works handle scene changes by introducing outlier awareness with pure learning methods. Other recent approaches use semantics to robustify camera localization in changing setups. However, to the best of our knowledge, no method has yet used scene graphs in feature-based approaches to introduce change awareness. In this work, we propose a novel feature-based camera re-localization method that leverages scene graphs within retrieval and feature detection and matching. Semantic scene graphs are used to estimate scene changes by matching instances and relationship triplets. The knowledge of scene changes is then used for our change-aware image retrieval and feature correspondence verification. We show the potential of integrating higher-level knowledge about the scene within a retrieval-based localization pipeline. Our method is evaluated on the <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">RIO10</i> benchmark with comprehensive evaluations on different levels of scene changes.",Julia Kabalar|Shun-Cheng Wu|Johanna Wald|Keisuke Tateno|Nassir Navab|Federico Tombari,Technical University of Munich|Technical University of Munich|Technical University of Munich|Technical University of Munich|Johns Hopkins University|Technical University of Munich|Technical University of Munich,11,Towards Long-Term Retrieval-Based Visual Localization in Indoor Environments With Changes,https://doi.org/10.1109/lra.2023.3242872,2.00165174,FALSE,en,TRUE,green,IEEE Robotics and Automation Letters,journal,2023-02-06,article
https://openalex.org/W4319970255,"Conceptual representations of images involving descriptions of entities and their relations are often represented using scene graphs. Such scene graphs can express relational concepts by using sets of triplets ⟨subject—predicate—object⟩. Instead of building dedicated models for scene graph generation, our model tends to extract the latent relational information implicitly encoded in image captioning models. We explored dependency parsing to build grammatically sound parse trees from captions. We used detection algorithms for the region propositions to generate dense region-based concept graphs. These were optimally combined using the approximate sub-graph isomorphism to create holistic concept graphs for images. The major advantages of this approach are threefold. Firstly, the proposed graph generation module is completely rule-based and, hence, adheres to the principles of explainable artificial intelligence. Secondly, graph generation can be used as plug-and-play along with any region proposition and caption generation framework. Finally, our results showed that we could generate rich concept graphs without explicit graph-based supervision.",Swarnendu Ghosh|Teresa Gonçalves|Nibaran Das,University of Engineering & Management|Jadavpur University|University of Évora|Jadavpur University,1,Im2Graph: A Weakly Supervised Approach for Generating Holistic Scene Graphs from Regional Dependencies,https://doi.org/10.3390/fi15020070,0.18196834,FALSE,en,TRUE,gold,Future Internet,journal,2023-02-10,article
https://openalex.org/W4321459108,,Hui Liang|Xiaohang Dong|Junjun Pan|Xiangyu Zheng,Zhengzhou University of Light Industry|Zhengzhou University of Light Industry|Beihang University|Zhengzhou University of Light Industry,6,Virtual scene generation promotes shadow puppet art conservation,https://doi.org/10.1002/cav.2148,1.4931186,FALSE,en,FALSE,closed,Computer Animation and Virtual Worlds,journal,2023-02-20,article
https://openalex.org/W4321605134,"Abstract Dense video captioning (DVC) aims at generating description for each scene in a video. Despite attractive progress for this task, previous works usually only concentrate on exploiting visual features while neglecting audio information in the video, resulting in inaccurate scene event location. In this article, we propose a novel DVC model named CMCR, which is mainly composed of a cross-modal processing (CM) module and a commonsense reasoning (CR) module. CM utilizes a cross-modal attention mechanism to encode data in different modalities. An event refactoring algorithm is proposed to deal with inaccurate event localization caused by overlapping events. Besides, a shared encoder is utilized to reduce model redundancy. CR optimizes the logic of generated captions with both heterogeneous prior knowledge and entities’ association reasoning achieved by building a knowledge-enhanced unbiased scene graph. Extensive experiments are conducted on ActivityNet Captions dataset, the results demonstrate that our model achieves better performance than state-of-the-art methods. To better understand the performance achieved by CMCR, we also apply ablation experiments to analyze the contributions of different modules.",Shixing Han|Jin Liu|Jinyingming Zhang|Peizhu Gong|Xiliang Zhang|Huihua He,Shanghai Maritime University|Shanghai Maritime University|Shanghai Maritime University|Shanghai Maritime University|Shanghai Maritime University|Shanghai Normal University,27,Lightweight dense video captioning with cross-modal attention and knowledge-enhanced unbiased scene graph,https://doi.org/10.1007/s40747-023-00998-5,4.91314518,FALSE,en,TRUE,gold,Complex & Intelligent Systems,journal,2023-02-24,article
https://openalex.org/W4323359280,"Abstract Visual Question Answering (VQA) aims to appropriately answer a text question by understanding the image content. Attention‐based VQA models mine the implicit relationships between objects according to the feature similarity, which neglects the explicit relationships between objects, for example, the relative position. Most Visual Scene Graph‐based VQA models exploit the relative positions or visual relationships between objects to construct the visual scene graph, while they suffer from the semantic insufficiency of visual edge relations. Besides, the scene graph of text modality is often ignored in these works. In this article, a novel Dual Scene Graph Enhancement Module (DSGEM) is proposed that exploits the relevant external knowledge to simultaneously construct two interpretable scene graph structures of image and text modalities, which makes the reasoning process more logical and precise. Specifically, the authors respectively build the visual and textual scene graphs with the help of commonsense knowledge and syntactic structure, which explicitly endows the specific semantics to each edge relation. Then, two scene graph enhancement modules are proposed to propagate the involved external and structural knowledge to explicitly guide the feature interaction between objects (nodes). Finally, the authors embed such two scene graph enhancement modules to existing VQA models to introduce the explicit relation reasoning ability. Experimental results on both VQA V2 and OK‐VQA datasets show that the proposed DSGEM is effective and compatible to various VQA architectures.",Boyue Wang|Yujian Ma|Xiaoyan Li|Heng Liu|Yongli Hu|Baocai Yin,Beijing University of Technology|Beijing University of Technology|Beijing University of Technology|Beijing University of Technology|Beijing University of Technology|Beijing University of Technology,0,DSGEM: Dual scene graph enhancement module‐based visual question answering,https://doi.org/10.1049/cvi2.12186,0.0,FALSE,en,TRUE,gold,IET Computer Vision,journal,2023-03-07,article
https://openalex.org/W4380896660,,Xiaomeng Wang|Tong Xu|Shiwei Wu,University of Science and Technology of China|University of Science and Technology of China|University of Science and Technology of China,2,SGAT: Scene Graph Attention Network for Video Recommendation,https://doi.org/10.1145/3591156.3591173,1.23697525,FALSE,en,FALSE,closed,,,2023-03-24,article
https://openalex.org/W4367597810,,Alina T. Latipova|Paul P. Kumar,South Ural State University|South Ural State University,1,Overview of Scene Graph Generation Approaches in Computer Vision,https://doi.org/10.1109/smartindustrycon57312.2023.10110820,0.18196834,FALSE,en,FALSE,closed,,,2023-03-27,article
https://openalex.org/W4361011058,,Lianggangxu Chen|Yiqing Cai|Changhong Lü|Changbo Wang|Gaoqi He,Chongqing Normal University|East China Normal University|East China Normal University|Chongqing Normal University|East China Normal University|East China Normal University|Chongqing Normal University|East China Normal University|Chongqing Normal University,2,Video-based spatio-temporal scene graph generation with efficient self-supervision tasks,https://doi.org/10.1007/s11042-023-14640-6,0.36393668,FALSE,en,FALSE,closed,Multimedia Tools and Applications,journal,2023-03-27,article
https://openalex.org/W4361282814,,Tomoya Ohta|Tanaka Kanji|Ryogo Yamamoto,University of Fukui|University of Fukui|University of Fukui,7,Scene graph descriptors for visual place classification from noisy scene data,https://doi.org/10.1016/j.icte.2022.11.003,1.27377838,FALSE,en,TRUE,gold,ICT Express,journal,2023-03-29,article
https://openalex.org/W4362587934,,Anqi Zheng|Shiqi Zheng|Cong Bai|Deng Chen,Zhejiang University of Technology|Zhejiang University of Technology|Zhejiang University of Technology|Zhejiang Institute of Science and Technology Information|Zhejiang Lab,3,Triple-level relationship enhanced transformer for image captioning,https://doi.org/10.1007/s00530-023-01073-2,0.54590502,FALSE,en,FALSE,closed,Multimedia Systems,journal,2023-04-02,article
https://openalex.org/W4362676904,,Muhammad Umair Hassan|Saleh Alaliyat|Ibrahim A. Hameed,Norwegian University of Science and Technology|Norwegian University of Science and Technology|Norwegian University of Science and Technology,16,Image generation models from scene graphs and layouts: A comparative analysis,https://doi.org/10.1016/j.jksuci.2023.03.021,2.91149344,FALSE,en,TRUE,hybrid,Journal of King Saud University - Computer and Information Sciences,journal,2023-04-07,article
https://openalex.org/W4366815235,,Jinmeng Wu|Fulin Ge|Hanyu Hong|Yu Shi|Yanbin Hao|Лей Ма,Wuhan Institute of Technology|Wuhan Institute of Technology|Wuhan Institute of Technology|Wuhan Institute of Technology|University of Science and Technology of China|Wuhan Institute of Technology,8,Question-aware dynamic scene graph of local semantic representation learning for visual question answering,https://doi.org/10.1016/j.patrec.2023.04.014,1.45574672,FALSE,en,FALSE,closed,Pattern Recognition Letters,journal,2023-04-24,article
https://openalex.org/W4381745577,,Zhengbo Wu|Zhuo Li,Heilongjiang University|Heilongjiang University,0,Unity3D Based on the Impact of Landscape Planning and Design Virtual and Realistic,https://doi.org/10.1109/icdcece57866.2023.10151199,0.0,FALSE,en,FALSE,closed,,,2023-04-29,article
https://openalex.org/W4368617621,"The Panoptic Scene Graph Generation (PSG) challenge evaluates computer vision models to identify relations in images beyond object classification and localization, enabling a deeper understanding of scenes for real-world AI applications.",Jingkang Yang|Zheng Ma|Qixun Wang|Xiaofeng Guo|Haofan Wang|Ziwei Liu|Wei Zhang|Xing Xu|Hai Zhang,"Group Sense (China)|Nanyang Technological University|Group Sense (China)||Institute of Optics and Electronics, Chinese Academy of Sciences|Chinese Academy of Sciences||Nanyang Technological University|Group Sense (China)|University of Electronic Science and Technology of China|Northwest University",3,The PSG challenge: towards comprehensive scene understanding,https://doi.org/10.1093/nsr/nwad126,0.54590502,FALSE,en,TRUE,gold,National Science Review,journal,2023-05-04,article
https://openalex.org/W4372341181,"As a core task of computer vision perception, 3D scene understanding has received widespread attention. However, the current research mainly focuses on the semantic understanding task at the level of entity objects and often neglects the semantic relationships between objects in the scene. This paper proposes a 3D scene graph prediction model based on deep learning methods for scanned point cloud data of indoor scenes to predict the semantic graph about the class of entity objects and their relationships. The model uses a multi-scale pyramidal feature extraction network, MP-DGCNN, to fuse features with the learned category-related unbiased meta-embedding vectors, and the relationship inference of the scene graph uses an ENA-GNN network incorporating node and edge cross-attention; in addition, considering the long-tail distribution effect, a category grouping re-weighting scheme is used in the embedded prior knowledge and loss function. For the 3D scene graph prediction task, experiments on the indoor point cloud 3DSSG dataset show that the model proposed in this paper performs well compared with the latest baseline model, and the prediction effectiveness and accuracy are substantially improved.",Chaolin Han|Hongwei Li|Jian Xu|Bing Dong|Yalin Wang|Xiaowen Zhou|Shan Zhao,Zhengzhou University|Zhengzhou University|Zhengzhou University|Zhengzhou University|Zhengzhou University|Zhengzhou University|Zhengzhou University,6,Unbiased 3D Semantic Scene Graph Prediction in Point Cloud Using Deep Learning,https://doi.org/10.3390/app13095657,2.01603893,FALSE,en,TRUE,gold,Applied Sciences,journal,2023-05-04,article
https://openalex.org/W4372270232,,Wenxi Ma|Tianxiang Hou|Qianji Di|Zhongang Qi|Ying Shan|Hanzi Wang,Xiamen University|Xiamen University|Xiamen University|Tencent (China)|Tencent (China)|Xiamen University,1,ERBNet: An Effective Representation Based Network for Unbiased Scene Graph Generation,https://doi.org/10.1109/icassp49357.2023.10094727,0.18196834,FALSE,en,FALSE,closed,,,2023-05-05,article
https://openalex.org/W4375857446,,Hailong Gao|Tao He,Lanzhou Jiaotong University|Lanzhou Jiaotong University,0,Segmentation of railway region of interest based on image recognition,https://doi.org/10.1117/12.2678976,0.0,FALSE,en,FALSE,closed,,,2023-05-08,article
https://openalex.org/W4384158955,,Xiaoyang Liu|Lei Wang|Yangji Fan|Shuo Liang,Hainan University|Hainan University|Hainan University|Harbin Engineering University|Harbin University,4,Research on Map partitioning and Preprocessing Algorithms for Global Path Planning,https://doi.org/10.1109/icecai58670.2023.10176863,0.72787336,FALSE,en,FALSE,closed,,,2023-05-12,article
https://openalex.org/W4377235251,,Mingtao Feng|Haoran Hou|Liang Zhang|Yulan Guo|Hongshan Yu|Yaonan Wang|Ajmal Mian,Xidian University|Xidian University|Xidian University|Sun Yat-sen University|Hunan University|Hunan University|University of Western Australia,45,Exploring Hierarchical Spatial Layout Cues for 3D Point Cloud Based Scene Graph Prediction,https://doi.org/10.1109/tmm.2023.3277736,15.120292,FALSE,en,FALSE,closed,IEEE Transactions on Multimedia,journal,2023-05-22,article
https://openalex.org/W4379114483,,Karthik Pandit|Aditya Mogare|Achal Shah|Prachi Thete|Megharani Patil,,3,Building a Virtual Reality‐Based Framework for the Education of Autistic Kids,https://doi.org/10.1002/9781119905172.ch5,,FALSE,en,FALSE,closed,,,2023-05-29,other
https://openalex.org/W4383108766,,Taiki Nakamura|Seiya Kawano|Akishige Yuguchi|Yasutomo Kawanishi|Koichiro Yoshino,The University of Tokyo||||,0,Operative Action Captioning for Estimating System Actions,https://doi.org/10.1109/icra48891.2023.10161545,0.0,FALSE,en,FALSE,closed,,,2023-05-29,article
https://openalex.org/W4383108736,,Hogun Kee|Minjae Kang|Dohyeong Kim|Jaegoo Choy|Songhwai Oh,Seoul National University|Seoul National University|Seoul National University|Seoul National University|Seoul National University,1,SDF-Based Graph Convolutional Q-Networks for Rearrangement of Multiple Objects,https://doi.org/10.1109/icra48891.2023.10161394,0.2488531,FALSE,en,FALSE,closed,,,2023-05-29,article
https://openalex.org/W4383109139,,Dong Yang|Xiao Xu|Mengchen Xiong|Edwin Babaians|Eckehard Steinbach,Technical University of Munich|Technical University of Munich|Technical University of Munich|Technical University of Munich|Technical University of Munich,4,SRI-Graph: A Novel Scene-Robot Interaction Graph for Robust Scene Understanding,https://doi.org/10.1109/icra48891.2023.10161085,0.72787336,FALSE,en,FALSE,closed,,,2023-05-29,article
https://openalex.org/W4381299118,"Action quality assessment (AQA) automatically evaluates how well humans perform actions in a given video, a technique widely used in fields such as rehabilitation medicine, athletic competitions, and specific skills assessment. However, existing works that uniformly divide the video sequence into small clips of equal length suffer from intra-clip confusion and inter-clip incoherence, hindering the further development of AQA. To address this issue, we propose a hierarchical graph convolutional network (GCN). First, semantic information confusion is corrected through clip refinement, generating the 'shot' as the basic action unit. We then construct a scene graph by combining several consecutive shots into meaningful scenes to capture local dynamics. These scenes can be viewed as different procedures of a given action, providing valuable assessment cues. The video-level representation is finally extracted via sequential action aggregation among scenes to regress the predicted score distribution, enhancing discriminative features and improving assessment performance. Experiments on the AQA-7, MTL-AQA, and JIGSAWS datasets demonstrate the superiority of the proposed hierarchical GCN over state-of-the-art methods.",Kanglei Zhou|Yue Ma|Hubert P. H. Shum|Xiaohui Liang,Beihang University|Beihang University|Durham University|Beihang University,58,Hierarchical Graph Convolutional Networks for Action Quality Assessment,https://doi.org/10.1109/tcsvt.2023.3281413,10.55416373,FALSE,en,TRUE,gold,IEEE Transactions on Circuits and Systems for Video Technology,journal,2023-05-30,article
https://openalex.org/W7124281835,,Kang Zhou|Chi Guo|Huyin Zhang|Wenfei Guo,Wuhan University|Wuhan University of Technology|Wuhan University|Wuhan University|Wuhan University,0,RTransNav:Relation-wise Transformer Network for More Successful Object Goal Navigation,https://doi.org/10.65109/rxvn9247,0.0,FALSE,,FALSE,closed,,,2023-05-30,article
https://openalex.org/W4386076041,,Mingtao Feng|Haoran Hou|Liang Zhang|Ziiie Wu|Yulan Guo|Ajmal Mian,Xidian University|Xidian University|Xidian University|Hunan University||University of Western Australia,14,3D Spatial Multimodal Knowledge Accumulation for Scene Graph Prediction in Point Cloud,https://doi.org/10.1109/cvpr52729.2023.00886,4.70409085,FALSE,en,FALSE,closed,,,2023-06-01,article
https://openalex.org/W4386075933,,Tianlei Jin|Fangtai Guo|Qiwei Meng|Shiqiang Zhu|Xiangming Xi|Wen Wang|Zonghao Mu|Wei Song,Zhejiang Lab|Zhejiang Lab|Zhejiang Lab|Zhejiang Lab|Zhejiang Lab|Zhejiang Lab|Zhejiang Lab|Zhejiang Lab,17,Fast Contextual Scene Graph Generation with Unbiased Context Augmentation,https://doi.org/10.1109/cvpr52729.2023.00610,3.09346178,FALSE,en,FALSE,closed,,,2023-06-01,article
https://openalex.org/W4386072171,,Sanjoy Kundu|Sathyanarayanan N. Aakur,Oklahoma State University Oklahoma City|Oklahoma State University|Oklahoma State University|Oklahoma State University Oklahoma City,21,IS-GGT: Iterative Scene Graph Generation with Generative Transformers,https://doi.org/10.1109/cvpr52729.2023.00609,3.82133514,FALSE,en,FALSE,closed,,,2023-06-01,article
https://openalex.org/W4386071767,,Yong Zhang|Yingwei Pan|Ting Yao|Rui Huang|Tao Mei|Chang‐Wen Chen,"Chinese University of Hong Kong, Shenzhen|||Chinese University of Hong Kong, Shenzhen||Hong Kong Polytechnic University",28,Learning to Generate Language-Supervised and Open-Vocabulary Scene Graph Using Pre-Trained Visual-Semantic Space,https://doi.org/10.1109/cvpr52729.2023.00285,5.09511352,FALSE,en,FALSE,closed,,,2023-06-01,article
https://openalex.org/W4386249615,,David Abou Chacra|John Zelek,University of Waterloo|University of Waterloo,0,Naive Scene Graphs: How Visual is Modern Visual Relationship Detection?,https://doi.org/10.1109/crv60082.2023.00034,0.0,FALSE,en,FALSE,closed,,,2023-06-01,article
https://openalex.org/W4385801675,,Tripti Shukla|Paridhi Maheshwari|Rajhans Singh|Ankita Shukla|Kuldeep Kulkarni|Pavan Turaga,|Stanford University|Arizona State University|Arizona State University||Arizona State University,5,Scene Graph Driven Text-Prompt Generation for Image Inpainting,https://doi.org/10.1109/cvprw59228.2023.00083,0.9098417,FALSE,en,FALSE,closed,,,2023-06-01,article
https://openalex.org/W4385801526,,Yoshio Rubio|Marco A. Contreras-Cruz,Samsung (United States)|Samsung (United States),1,Wildlife Image Generation from Scene Graphs,https://doi.org/10.1109/cvprw59228.2023.00036,0.18196834,FALSE,en,FALSE,closed,,,2023-06-01,article
https://openalex.org/W4385300828,,Yafu Tian|Alexander Carballo|Ruifeng Li|Kazuya Takeda,Harbin Institute of Technology|Nagoya University|Nagoya University|Gifu University|Harbin Institute of Technology|Nagoya University,4,RSG-Search: Semantic Traffic Scene Retrieval Using Graph-Based Scene Representation,https://doi.org/10.1109/iv55152.2023.10186641,0.72787336,FALSE,en,FALSE,closed,,,2023-06-04,article
https://openalex.org/W4379619455,,Yong Zhang|Yingwei Pan|Ting Yao|Rui Huang|Tao Mei|Chang Wen Chen,"Chinese University of Hong Kong, Shenzhen|||Chinese University of Hong Kong, Shenzhen||Hong Kong Polytechnic University",9,End-to-End Video Scene Graph Generation With Temporal Propagation Transformer,https://doi.org/10.1109/tmm.2023.3283879,1.63771506,FALSE,en,FALSE,closed,IEEE Transactions on Multimedia,journal,2023-06-07,article
https://openalex.org/W4379931325,,Junhua Jia|Xiangqian Ding|Shunpeng Pang|Xiaoyan Gao|Xiaowei Xin|Ruotong Hu|Jie Nie,Ocean University of China|Ocean University of China|Weifang University|Ocean University of China|Ocean University of China|Ocean University of China|Ocean University of China,15,Image captioning based on scene graphs: A survey,https://doi.org/10.1016/j.eswa.2023.120698,2.7295251,FALSE,en,FALSE,closed,Expert Systems with Applications,journal,2023-06-08,article
https://openalex.org/W4380085067,,Wenbin Wang|Ruiping Wang|Shiguang Shan|Xilin Chen,Chinese Academy of Sciences|Institute of Computing Technology|University of Chinese Academy of Sciences|Chinese Academy of Sciences|Institute of Computing Technology|University of Chinese Academy of Sciences|Chinese Academy of Sciences|Institute of Computing Technology|University of Chinese Academy of Sciences|Chinese Academy of Sciences|Institute of Computing Technology|University of Chinese Academy of Sciences,5,Importance First: Generating Scene Graph of Human Interest,https://doi.org/10.1007/s11263-023-01817-7,0.9098417,FALSE,en,FALSE,closed,International Journal of Computer Vision,journal,2023-06-09,article
https://openalex.org/W4380077379,"The driving process involves many layers of planning and navigation, in order to enable tractable solutions for the otherwise highly complex problem of autonomous driving. One such layer involves an inherent discrete layer of decision-making corresponding to tactical maneuvers. Inspired by this, the focus of this work is predicting high-level maneuvers for the ego-vehicle. As maneuver prediction is fundamentally feedback-structured, it requires modeling techniques that take into consideration the interaction awareness of the traffic agents involved. This work addresses this challenge by modeling the traffic scenario as an interaction graph and proposing three deep learning architectures for interaction-aware tactical maneuver prediction of the ego-vehicle. These architectures are based on graph neural networks (GNNs) for extracting spatial features among traffic agents and recurrent neural networks (RNNs) for extracting dynamic motion patterns of surrounding agents. These proposed architectures have been trained and evaluated using BLVD dataset. Moreover, this dataset is expanded using data augmentation, data oversampling and data undersampling approaches, to strengthen model’s resilience and enhance the learning process. Lastly, we compare proposed learning architectures for ego-vehicle maneuver prediction in various driving circumstances with various numbers of surrounding traffic agents in order to effectively verify the proposed architectures.",Petrit Rama|Naim Bajçinca,Daimler (Germany)|Rheinland-Pfälzische Technische Universität Kaiserslautern-Landau|University of Kaiserslautern|Rheinland-Pfälzische Technische Universität Kaiserslautern-Landau|University of Kaiserslautern|Daimler (Germany),0,Maneuver Prediction Using Traffic Scene Graphs via Graph Neural Networks and Recurrent Neural Networks,https://doi.org/10.1142/s1793351x23620040,0.0,FALSE,en,TRUE,hybrid,International Journal of Semantic Computing,journal,2023-06-10,article
https://openalex.org/W4380995416,"In scene parsing, the model is required to be able to process complex multi-modal data such as images and contexts in real scenes, and discover their implicit connections from objects existing in the scene. As a storage method that contains entity information and the relationship between entities, a knowledge graph can well express objects and the semantic relationship between objects in the scene. In this paper, a new multi-phase process was proposed to solve scene parsing tasks; first, a knowledge graph was used to align the multi-modal information and then the graph-based model generates results. We also designed an experiment of feature engineering’s validation for a deep-learning model to preliminarily verify the effectiveness of this method. Hence, we proposed a knowledge representation method named Entity Descriptor Encoder of Transformer (EDET), which uses both the entity itself and its internal attributes for knowledge representation. This method can be embedded into the transformer structure to solve multi-modal scene parsing tasks. EDET can aggregate the multi-modal attributes of entities, and the results in the scene graph generation and image captioning tasks prove that EDET has excellent performance in multi-modal fields. Finally, the proposed method was applied to the industrial scene, which confirmed the viability of our method.",Sai Ma|Weibing Wan|Zedong Yu|Yuming Zhao,Shanghai University of Engineering Science|Shanghai University of Engineering Science|Shanghai University of Engineering Science|Shanghai Jiao Tong University,1,EDET: Entity Descriptor Encoder of Transformer for Multi-Modal Knowledge Graph in Scene Parsing,https://doi.org/10.3390/app13127115,0.18196834,FALSE,en,TRUE,gold,Applied Sciences,journal,2023-06-14,article
https://openalex.org/W4385482776,"The modern deep learning-based architectures have performed well for pixel-wise segmentation tasks. The consideration of context is of vital importance for generation of accurate semantic information. In this research, a deep learning-based image parsing framework is proposed that utilizes novel relation-aware context learning technique. The proposed technique explores the graph constructs from the training data to learn the co-occurring context associations of object category labels using the graph edge connections. The proposed graph-based context learning technique defines the scene specific relation-awareness among semantic object categories, e.g., the probability of sky, road and building to co-exist in a scene is high. The proposed image parsing architecture (including the novel graph-based context learning technique) is evaluated on the benchmark datasets. In addition, a comprehensive comparison with existing image parsing techniques is presented to establish the efficacy of the scene-graph generation. The in-depth investigation of graph generation is presented to demonstrate the improvement in pixel-wise labeling.",Basim Azam|Brijesh Verma,Griffith University|Griffith University,0,A Graph-based Context Learning Technique for Image Parsing,https://doi.org/10.1109/ijcnn54540.2023.10191800,0.0,FALSE,en,TRUE,green,,,2023-06-18,article
https://openalex.org/W4385482837,,Chengkai Ren|Xiuhua Liu|Mengyuan Cao|Jian Zhang|Hongwei Wang,Zhejiang University-University of Edinburgh Institute|China Aerospace Science and Industry Corporation (China)|China Aerospace Science and Industry Corporation (China)|Zhejiang University-University of Edinburgh Institute|Zhejiang University-University of Edinburgh Institute,2,A Novel End-to-End Transformer for Scene Graph Generation,https://doi.org/10.1109/ijcnn54540.2023.10191798,0.36393668,FALSE,en,FALSE,closed,,,2023-06-18,article
https://openalex.org/W4385482811,,Hongbo Xu|Lichun Wang|Kai Xu|Fangyu Fu|Baocai Yin|Qingming Huang,Beijing University of Technology|Beijing University of Technology|Beijing University of Technology|Beijing University of Technology|Beijing University of Technology|Chinese Academy of Sciences,0,Augmented Spatial Context Fusion Network for Scene Graph Generation,https://doi.org/10.1109/ijcnn54540.2023.10191284,0.0,FALSE,en,FALSE,closed,,,2023-06-18,article
https://openalex.org/W4381611727,,Bo Sun|Zhuo Hao|Lejun Yu|Jun He,Beijing Normal University|Beijing Normal University|Beijing Normal University|Beijing Normal University,2,Unbiased scene graph generation using the self-distillation method,https://doi.org/10.1007/s00371-023-02924-9,0.36393668,FALSE,en,FALSE,closed,The Visual Computer,journal,2023-06-22,article
https://openalex.org/W4381853731,,Gang Lv|Yining Sun|Fudong Nian|Maofei Zhu|Wenliang Tang|Zhenzhen Hu,University of Science and Technology of China|Chinese Academy of Sciences|Hefei Institutes of Physical Science|Chizhou University|University of Science and Technology of China|Chinese Academy of Sciences|Hefei Institutes of Physical Science|||Hefei University of Technology|Hefei University of Technology,18,COME: Clip-OCR and Master ObjEct for text image captioning,https://doi.org/10.1016/j.imavis.2023.104751,3.27543012,FALSE,en,FALSE,closed,Image and Vision Computing,journal,2023-06-24,article
https://openalex.org/W4382466020,"Scene Graph Generation (SGG) aims to capture the semantic information in an image and build a structured representation, which facilitates downstream tasks. The current challenge in SGG is to tackle the biased predictions caused by the long-tailed distribution of predicates. Since multiple predicates in SGG are coupled in an image, existing data re-balancing methods cannot completely balance the head and tail predicates. In this work, a decoupled learning framework is proposed for unbiased scene graph generation by using attribute-guided predicate features to construct a balanced training set. Specifically, the predicate recognition is decoupled into Predicate Feature Representation Learning (PFRL) and predicate classifier training with a class-balanced predicate feature set, which is constructed by our proposed Attribute-guided Predicate Feature Generation (A-PFG) model. In the A-PFG model, we first define the class labels of and corresponding visual feature as attributes to describe a predicate. Then the predicate feature and the attribute embedding are mapped into a shared hidden space by a dual Variational Auto-encoder (VAE), and finally the synthetic predicate features are forced to learn the contextual information in the attributes via cross reconstruction and distribution alignment. To demonstrate the effectiveness of our proposed method, our decoupled learning framework and A-PFG model are applied to various SGG models. The empirical results show that our method is substantially improved on all benchmarks and achieves new state-of-the-art performance for unbiased scene graph generation. Our code is available at https://github.com/wanglei0618/A-PFG.",Lei Wang|Zejian Yuan|Badong Chen,Xi'an Jiaotong University|Xi'an Jiaotong University|Xi'an Jiaotong University,7,Learning to Generate an Unbiased Scene Graph by Using Attribute-Guided Predicate Features,https://doi.org/10.1609/aaai.v37i2.25356,0.5632907,FALSE,en,TRUE,diamond,Proceedings of the AAAI Conference on Artificial Intelligence,conference,2023-06-26,article
https://openalex.org/W4382317990,"Multimodal relation extraction is an essential task for knowledge graph construction. In this paper, we take an in-depth empirical analysis that indicates the inaccurate information in the visual scene graph leads to poor modal alignment weights, further degrading performance. Moreover, the visual shuffle experiments illustrate that the current approaches may not take full advantage of visual information. Based on the above observation, we further propose a strong baseline with an implicit fine-grained multimodal alignment based on Transformer for multimodal relation extraction. Experimental results demonstrate the better performance of our method. Codes are available at https://github.com/zjunlp/DeepKE/tree/main/example/re/multimodal.",Lei Li|Xiang Chen|Shuofei Qiao|Feiyu Xiong|Huajun Chen|Ningyu Zhang,Zhejiang University|Zhejiang University|Zhejiang University|Alibaba Group (United States)|Zhejiang University|Zhejiang University,14,On Analyzing the Role of Image for Visual-Enhanced Relation Extraction (Student Abstract),https://doi.org/10.1609/aaai.v37i13.26987,1.1265814,FALSE,en,TRUE,diamond,Proceedings of the AAAI Conference on Artificial Intelligence,conference,2023-06-26,article
https://openalex.org/W4382239130,"Several techniques have recently aimed to improve the performance of deep learning models for Scene Graph Generation (SGG) by incorporating background knowledge. State-of-the-art techniques can be divided into two families: one where the background knowledge is incorporated into the model in a subsymbolic fashion, and another in which the background knowledge is maintained in symbolic form. Despite promising results, both families of techniques face several shortcomings: the first one requires ad-hoc, more complex neural architectures increasing the training or inference cost; the second one suffers from limited scalability w.r.t. the size of the background knowledge. Our work introduces a regularization technique for injecting symbolic background knowledge into neural SGG models that overcomes the limitations of prior art. Our technique is model-agnostic, does not incur any cost at inference time, and scales to previously unmanageable background knowledge sizes. We demonstrate that our technique can improve the accuracy of state-of-the-art SGG models, by up to 33%.",Davide Buffelli|Efthymia Tsamoura,University of Padua|Samsung (United States),3,Scalable Theory-Driven Regularization of Scene Graph Generation Models,https://doi.org/10.1609/aaai.v37i6.25839,0.2414103,FALSE,en,TRUE,diamond,Proceedings of the AAAI Conference on Artificial Intelligence,conference,2023-06-26,article
https://openalex.org/W4382240073,"In this paper, we study graph-to-image generation conditioned exclusively on scene graphs, in which we seek to disentangle the veiled semantics between knowledge graphs and images. While most existing research resorts to laborious auxiliary information such as object layouts or segmentation masks, it is also of interest to unveil the generality of the model with limited supervision, moreover, avoiding extra cross-modal alignments. To tackle this challenge, we delve into the causality of the adversarial generation process, and reason out a new principle to realize a simultaneous semantic disentanglement with an alignment on target and model distributions. This principle is named knowledge consensus, which explicitly describes a triangle causal dependency among observed images, graph semantics and hidden visual representations. The consensus also determines a new graph-to-image generation framework, carried on several adversarial optimization objectives. Extensive experimental results demonstrate that, even conditioned only on scene graphs, our model surprisingly achieves superior performance on semantics-aware image generation, without losing the competence on manipulating the generation through knowledge graphs.",Yang Wu|Pengxu Wei|Liang Lin,Sun Yat-sen University|Sun Yat-sen University|Sun Yat-sen University,5,Scene Graph to Image Synthesis via Knowledge Consensus,https://doi.org/10.1609/aaai.v37i3.25387,0.4023505,FALSE,en,TRUE,diamond,Proceedings of the AAAI Conference on Artificial Intelligence,conference,2023-06-26,article
https://openalex.org/W4382725382,"In this letter, we present an evolved version of Situational Graphs, which jointly models in a single optimizable factor graph (1) a pose graph, as a set of robot keyframes comprising associated measurements and robot poses, and (2) a 3D scene graph, as a high-level representation of the environment that encodes its different geometric elements with semantic attributes and the relational information between them. Specifically, our S-Graphs+ is a novel four-layered factor graph that includes: (1) A keyframes layer with robot pose estimates, (2) a walls layer representing wall surfaces, (3) a rooms layer encompassing sets of wall planes, and (4) a floors layer gathering the rooms within a given floor level. The above graph is optimized in real-time to obtain a robust and accurate estimate of the robot's pose and its map, simultaneously constructing and leveraging high-level information of the environment. To extract this high-level information, we present novel room and floor segmentation algorithms utilizing the mapped wall planes and free-space clusters. We tested S-Graphs+ on multiple datasets, including simulated and real data of indoor environments from varying construction sites, and on a real public dataset of several indoor office areas. On average over our datasets, S-Graphs+ outperforms the accuracy of the second-best method by a margin of 10.67%, while extending the robot situational awareness by a richer scene model. Moreover, we make the software available as a docker file.",Hriday Bavle|José Luis Sánchez-López|Muhammad Shaheer|Javier Civera|Holger Voos,University of Luxembourg|University of Luxembourg|University of Luxembourg|Universidad de Zaragoza|University of Luxembourg,29,<i>S-Graphs+:</i> Real-Time Localization and Mapping Leveraging Hierarchical Representations,https://doi.org/10.1109/lra.2023.3290512,15.07855778,FALSE,en,TRUE,hybrid,IEEE Robotics and Automation Letters,journal,2023-06-29,article
https://openalex.org/W4386160459,,Guikun Chen|Lin Li|Yawei Luo|Jun Xiao,Zhejiang University|Ningbo University|Zhejiang University of Science and Technology|Zhejiang University|Ningbo University|Zhejiang University of Science and Technology,6,Addressing Predicate Overlap in Scene Graph Generation with Semantic Granularity Controller,https://doi.org/10.1109/icme55011.2023.00022,1.09181004,FALSE,en,FALSE,closed,,,2023-07-01,article
https://openalex.org/W4386066652,,Ravindi de Silva|Arkady Zaslavsky|Seng W. Loke|Prem Prakash Jayaraman,Deakin University|Deakin University|Deakin University|Swinburne University of Technology,0,Context Query Generation using Scene Graph approach,https://doi.org/10.1109/mdm58254.2023.00060,0.0,FALSE,en,FALSE,closed,,,2023-07-01,article
https://openalex.org/W4386160445,,Bowen Zhao|Weidong Chen|Bo Hu|Hongtao Xie|Zhendong Mao,University of Science and Technology of China|University of Science and Technology of China|University of Science and Technology of China|University of Science and Technology of China|University of Science and Technology of China,0,Difference-Aware Iterative Reasoning Network for Key Relation Detection,https://doi.org/10.1109/icme55011.2023.00055,0.0,FALSE,en,FALSE,closed,,,2023-07-01,article
https://openalex.org/W4386158716,,Lianggangxu Chen|Jiale Lu|Changbo Wang|Gaoqi He,East China Normal University|East China Normal University|East China Normal University|Chongqing Normal University|East China Normal University,0,Scene Graph Generation using Depth-based Multimodal Network,https://doi.org/10.1109/icme55011.2023.00199,0.0,FALSE,en,FALSE,closed,,,2023-07-01,article
https://openalex.org/W4386160366,,Hongshuo Tian|Ning Xu|Yanhui Wang|Chenggang Yan|Bolun Zheng|Xuanya Li|An-An Liu,Tianjin University|Tianjin University|Tianjin University|Hangzhou Dianzi University|Hangzhou Dianzi University|Baidu (China)|Tianjin University,3,Towards Confidence-Aware Commonsense Knowledge Integration for Scene Graph Generation,https://doi.org/10.1109/icme55011.2023.00385,0.54590502,FALSE,en,FALSE,closed,,,2023-07-01,article
https://openalex.org/W4385310277,,Kyohoon Jin|Kyungsu Kang|Byeong-Kwon Shin|Junehyoung Kwon|Soojin Jang|Youngbin Kim|Han-Guk Ryu,Chung-Ang University|Sahmyook University||Chung-Ang University|Chung-Ang University|Chung-Ang University|Sahmyook University,6,Development of robust detector using the weather deep generative model for outdoor monitoring system,https://doi.org/10.1016/j.eswa.2023.120984,1.22704191,FALSE,en,FALSE,closed,Expert Systems with Applications,journal,2023-07-27,article
https://openalex.org/W4387444724,,Bingrui Chen|Yingchi Mao|Jinlei Jiang|Jiahong Sui|Rongzhi Qi|Zicheng Wang,Hohai University|Hohai University||Hohai University|Hohai University|PowerChina (China),0,Image Caption Method with Verb-Specific Scene Graph Decomposition,https://doi.org/10.1109/icivc58118.2023.10270444,0.0,FALSE,en,FALSE,closed,,,2023-07-27,article
https://openalex.org/W4385767947,"Dynamic scene graph generation aims to identify visual relationships (subject-predicate-object) in frames based on spatio-temporal contextual information in the video. Previous work implicitly models the spatio-temporal interaction simultaneously, which leads to entanglement of spatio-temporal contextual information. To this end, we propose a Grafting-Then-Reassembling framework (GTR), which explicitly extracts intra-frame spatial information and inter-frame temporal information in two separate stages to decouple spatio-temporal contextual information. Specifically, we first graft a static scene graph generation model to generate static visual relationships within frames. Then we propose the temporal dependency model to extract the temporal dependencies across frames, and explicitly reassemble static visual relationships into dynamic scene graphs. Experimental results show that GTR achieves the state-of-the-art performance on Action Genome dataset. Further analyses reveal that the reassembling stage is crucial to the success of our framework.",Jiafeng Liang|Yuxin Wang|Zekun Wang|Ming Liu|Ruiji Fu|Zhongyuan Wang|Bing Qin,Harbin Institute of Technology|Harbin Institute of Technology|Kuaishou (China)|Harbin Institute of Technology|Harbin Institute of Technology|Peng Cheng Laboratory|Kuaishou (China)|Harbin Institute of Technology|Harbin Institute of Technology|Kuaishou (China)|Peng Cheng Laboratory|Harbin Institute of Technology,1,GTR: A Grafting-Then-Reassembling Framework for Dynamic Scene Graph Generation,https://doi.org/10.24963/ijcai.2023/131,0.18196834,FALSE,en,TRUE,gold,,,2023-08-01,article
https://openalex.org/W4385767743,"This paper is concerned with synthesizing images conditioned on a scene graph (SG), a set of object nodes and their edges of interactive relations. We divide existing works into image-oriented and code-oriented methods. In our analysis, the image-oriented methods do not consider object interaction in spatial hidden feature. On the other hand, in empirical study, the code-oriented methods lose object consistency as their generated images miss certain objects in the input scene graph. To alleviate these two issues, we propose Learning Object Consistency and Interaction (LOCI). To preserve object consistency, we design a consistency module with a weighted augmentation strategy for objects easy to be ignored and a matching loss between scene graphs and image codes. To learn object interaction, we design an interaction module consisting of three kinds of message propagation between the input scene graph and the learned image code. Experiments on COCO-stuff and Visual Genome datasets show our proposed method alleviates the ignorance of objects and outperforms the state-of-the-art on visual fidelity of generated images and objects.",Yangkang Zhang|Chenye Meng|Zejian Li|Pei Chen|Guang Yang|Changyuan Yang|Lingyun Sun,Zhejiang University|Zhejiang University of Science and Technology|Zhejiang University|Zhejiang University|Zhejiang University|Zhejiang University of Science and Technology|Alibaba Group (United States)|Alibaba Group (United States)|Zhejiang University of Science and Technology|Zhejiang University|Alibaba Group (United States),5,Learning Object Consistency and Interaction in Image Generation from Scene Graphs,https://doi.org/10.24963/ijcai.2023/192,0.9098417,FALSE,en,TRUE,gold,,,2023-08-01,article
https://openalex.org/W4385828216,"Scene graph generation (SGG) has been developed to detect objects and their relationships from the visual data and has attracted increasing attention in recent years. Existing works have focused on extracting object context for SGG. However, very few works have attempted to exploit implicit contextual correlations among relationships of the objects. Furthermore, most existing SGG schemes rely on high-level features to predict the predicates while overlooking the potential inherent association of low-level features with the object relationships. We present in this article a novel scheme to capture enhanced contextual information for both objects and relationships. We design a Dual-branch Context Analysis Transformer (DCAT) architecture to extract both object context and relationship context from the visual data with dual transformer branches and then effectively fuse both high-level and low-level features by an adaptive approach to facilitate relationship prediction. Specifically, we first conduct feature representation learning to enrich relation representations by the visual, spatial, and linguistic feature extractors. Next, two transformer branches are designed to leverage the modeling of global associative interaction and mine the hidden association among objects and relationships. Then, we devise a novel feature disentangling method to decouple contextualized high-level features with guidance from the visual semantics. Finally, we develop a refined attention module to perform low-level feature recalibration for the refinement of the final predicate prediction. Experiments on Visual Genome and Action Genome datasets demonstrate the effectiveness of DCAT for both image and video SGG settings. Moreover, we also test the quality of the generated image scene graphs to verify the generalizability on downstream tasks like sentence-to-graph retrieval and image retrieval.",Shiqi Sun|Danlan Huang|Xiaoming Tao|Chengkang Pan|Guangyi Liu|Chang Wen Chen,Tsinghua University|Beijing University of Posts and Telecommunications|Tsinghua University|China Mobile (China)|China Mobile (China)|Hong Kong Polytechnic University,3,Boosting Scene Graph Generation with Contextual Information,https://doi.org/10.1145/3615868,0.54590502,FALSE,en,TRUE,hybrid,ACM Transactions on Multimedia Computing Communications and Applications,journal,2023-08-15,article
https://openalex.org/W4386071201,,Zhiming Wang|Yuxiao Li|Danlan Huang|Juan Wang|Ning Ge|Jianhua Lu,National Engineering Research Center for Information Technology in Agriculture|Tsinghua University|Tsinghua University|National Engineering Research Center for Information Technology in Agriculture|Tsinghua University|Institute of Automation|Chinese Academy of Sciences|Tsinghua University|National Engineering Research Center for Information Technology in Agriculture|Tsinghua University|National Engineering Research Center for Information Technology in Agriculture,3,DeformSg2im: Scene graph based multi-instance image generation with a deformable geometric layout,https://doi.org/10.1016/j.neucom.2023.126684,0.54590502,FALSE,en,FALSE,closed,Neurocomputing,journal,2023-08-22,article
https://openalex.org/W4386090265,,Jianhao Lv|Rong Zhang|Xinyu Li|Shimin Liu|Tianyuan Liu|Qi Zhang|Jinsong Bao,Donghua University|Donghua University|Donghua University|Hong Kong Polytechnic University|Hong Kong Polytechnic University|Donghua University|Donghua University,9,A Multimodality Scene Graph Generation Approach for Robust Human–Robot Collaborative Assembly Visual Relationship Representation,https://doi.org/10.1109/tii.2023.3303964,1.63771506,FALSE,en,FALSE,closed,IEEE Transactions on Industrial Informatics,journal,2023-08-23,article
https://openalex.org/W4388137869,"Modern man today is very dependent on his life with computers. Because with the help of computer technology can ease their task. A computer can create a situation similar to the original. This technology is commonly called simulation. Serious simulations depict a system that is difficult to perform in the real world and is at high risk. These serious simulations are usually much more developed in the making. This development technology is called Virtual Reality (VR). Virtual Reality (VR) is a development of stereo display techniques. Where users can see an object with dimensional depth that will give a 3D effect on the human brain. Based on the background above, the author tries to implement stereo displays in firefighting simulation (FFS) applications. The simulation engine used is Open Scene Graph with Microsoft Visual Studio 2008 compiler, which uses the C++ programming language. The GUI display design uses Qt 4.7.3. Users will try the stereo display effect in the FFS application and interact between the fire particle system and the water particle system to compare factors that cause fire particle system outages. Of all the implementations and trials carried out, the author succeeded in implementing stereoscopic fire suppression simulation applications. Because all objects in this application already have a red shadow for the left side of the thing and a blue shadow for the right side. The interaction between the user and the fire suppression simulation application was also successfully displayed in this Fire Fighting Simulation (FFS) application. The fire particle system will be extinguished if the user performs fire suppression interactions.",Windarko Windarko,Universitas Atma Jaya Makassar,1,Fire Suppression Simulation Application Development Stereo Graphics Based (Virtual Reality),https://doi.org/10.58812/wsist.v1i01.260,0.61848763,FALSE,en,TRUE,diamond,West Science Information System and Technology,journal,2023-08-28,article
https://openalex.org/W4392412489,,Zhirui Wang|Xinlong Jiang|Chenlong Gao|Yiqiang Chen,Chinese Academy of Sciences|University of Chinese Academy of Sciences|Institute of Computing Technology|University of Chinese Academy of Sciences|Institute of Computing Technology|Chinese Academy of Sciences|Institute of Computing Technology|Chinese Academy of Sciences|University of Chinese Academy of Sciences|Institute of Computing Technology|University of Chinese Academy of Sciences|Chinese Academy of Sciences,0,Interactive Scene-driven Multi-stream Graph Neural Network for ADHD Diagnosis,https://doi.org/10.1109/swc57546.2023.10448928,0.0,FALSE,en,FALSE,closed,,,2023-08-28,article
https://openalex.org/W4386231006,,Xinxin Liu|Yuchen Zhou|Chao Gou,Sun Yat-sen University|Sun Yat-sen University|Sun Yat-sen University,27,Learning From Interaction-Enhanced Scene Graph for Pedestrian Collision Risk Assessment,https://doi.org/10.1109/tiv.2023.3309274,7.44756427,FALSE,en,FALSE,closed,IEEE Transactions on Intelligent Vehicles,journal,2023-08-28,article
https://openalex.org/W4386936529,"In this paper, we propose a novel 3D scene graph generation model, L3DSG, which can make use of rich prior knowledge obtained from large language model (LLM) by prompt engineering. The proposed model is built upon our previous 3D scene graph generation model, C3DSG, that adopts Point Transformer as 3D geometric feature extractor and uses the NE-GAT graph neural network as context reasoner. The new proposed model addresses the inability of C3DSG to utilize prior knowledge on indoor physical environments. It focuses on issues of how to obtain prior knowledge from LLM and how to make use of it for predicting objects and their relations effectively. The proposed model is extended from C3DSG by adding several elaborate modules to prompt, encode, and fuse prior knowledge from LLM. Through various experiments using the benchmark dataset 3DSSG, we show the superiority of the proposed model.",Ho-Jun Baek|Incheol Kim,,0,3D Scene Graph Generation Using Prior Knowledge from Large Language Model (LLM),https://doi.org/10.9717/kmms.2023.26.8.859,0.0,FALSE,en,TRUE,diamond,Journal of Korea Multimedia Society,journal,2023-08-31,article
https://openalex.org/W4386307233,"A rapidly expanding multimedia environment in recent years has led to an explosive increase in demand for multimodality that can communicate with humans in various ways. Even though the convergence of vision and language intelligence has shed light on the remarkable success over the last few years, there is still a caveat: it is unknown whether they truly understand the semantics of the image. More specifically, how they correctly capture relationships between objects represented within the image is still regarded as a black box. In order to testify whether such relationships are well understood, this work mainly focuses on the Graph-structured visual Question Answering (GQA) task which evaluates the understanding of an image by reasoning a scene graph describing the structural characteristics of an image in the form of natural language together with the image. Unlike the existing approaches that have been accompanied by an additional encoder for scene graphs, we propose a simple yet effective framework using pre-trained multimodal transformers for scene graph reasoning. Inspired by the fact that a scene graph can be regarded as a set of sentences describing two related objects with a relationship, we fuse them into the framework separately from the question. In addition, we propose a multi-task learning method that utilizes evaluating the grammatical validity of questions as an auxiliary task to better understand a question with complex structures. This utilizes the semantic role labels of the question to randomly shuffle the sentence structure of the question. We have conducted extensive experiments to evaluate the effectiveness in terms of task capabilities, ablation studies, and generalization.",Yoonseok Heo|Sangwoo Kang,Sogang University|Gachon University,1,A Simple Framework for Scene Graph Reasoning with Semantic Understanding of Complex Sentence Structure,https://doi.org/10.3390/math11173751,0.18196834,FALSE,en,TRUE,gold,Mathematics,journal,2023-08-31,article
https://openalex.org/W4387546385,,Chenxing Li|Yiping Duan|Qiyuan Du|Shiqi Sun|Xin Deng|Xiaoming Tao,Tsinghua University|Tsinghua University|Tsinghua University|Tsinghua University|Beihang University|Tsinghua University,4,VR+HD: Video Semantic Reconstruction From Spatio-Temporal Scene Graphs,https://doi.org/10.1109/jstsp.2023.3323654,0.72787336,FALSE,en,FALSE,closed,IEEE Journal of Selected Topics in Signal Processing,journal,2023-09-01,article
https://openalex.org/W4386590339,,Shiqi Sun|Danlan Huang|Zhijin Qin|Xiaoming Tao|Chengkang Pan|Guangyi Liu,National Engineering Research Center for Information Technology in Agriculture|Tsinghua University|Beijing University of Posts and Telecommunications|Tsinghua University|Tsinghua University|China Mobile (China)|China Mobile (China),0,USGG: Union Message Based Scene Graph Generation,https://doi.org/10.1109/icip49359.2023.10222408,0.0,FALSE,en,FALSE,closed,,,2023-09-11,article
https://openalex.org/W4386869667,,Zi Qin Liew|Minrui Xu|Wei Yang Bryan Lim|Zehui Xiong|Dusit Niyato|Dong In Kim,|Nanyang Technological University|Nanyang Technological University|Singapore University of Technology and Design|Nanyang Technological University|Sungkyunkwan University,15,Mechanism Design for Semantic Communication in UAV-Assisted Metaverse: A Combinatorial Auction Approach,https://doi.org/10.1109/tvt.2023.3317069,6.59328438,FALSE,en,FALSE,closed,IEEE Transactions on Vehicular Technology,journal,2023-09-19,article
https://openalex.org/W4386961215,"The Simultaneous Localization and Mapping (SLAM) method is widely used in the positioning and mapping of robots. In the medical field, SLAM is used in auxiliary medical robots and surgical robots. In endoscopic surgery, SLAM performs endoscopic positioning and scene graph construction for the surgical environment based on the information collected by the endoscope. For research on endoscopic SLAM, this article will first introduce the application of SLAM in endoscopic surgery in recent years. This paper summarizes the innovations and future work of relevant literature in recent years and identifies existing problems in SLAM in endoscopic surgery. Next, this article will introduce the combination of deep learning and SLAM in endoscopic surgery and list some specific applications. Finally, this paper will give a prospect for the future application of SLAM in endoscopic surgery. The research in this paper will be of great value to applying SLAM in endoscopic surgery and conducive to the development of future endoscopic SLAM.",Zheng Zhang,Harbin Institute of Technology,1,Research on endoscopic surgery based on SLAM,https://doi.org/10.54254/2755-2721/12/20230296,0.51995027,FALSE,en,TRUE,hybrid,Applied and Computational Engineering,journal,2023-09-22,article
https://openalex.org/W4387827795,"Scene-graph-based Visual Question Answering (VQA) has emerged as a burgeoning field in Deep Learning research, with a growing demand for robust and interpretable VQA systems. In this paper, we present a novel visual analysis approach that addresses two critical objectives in VQA: identifying and correcting prediction issues and providing insights into model decision-making processes through visualizing internal information. Our approach builds on the GraphVQA framework, which uses graph neural networks to process scene graphs representing images and which was trained on the widely-used GQA dataset. Our analysis tool aims at users familiar with the basics of graph-based VQA. By leveraging query-based scene analysis and visualization of crucial internal states, we are able to detect and pinpoint reasons for inaccurate predictions, facilitating model refinement and dataset curation. Identifying expressive internal states is a challenge. Through rigorous computer-based evaluations and presentation of a use case, we demonstrate the effectiveness of our analysis tool and model state visualization.",Noel Schäfer|Sebastian Künzel|Tanja Munz|Pascal Tilli|Sandeep Vidyapu|Ngoc Thang Vu|Daniel Weiskopf,University of Stuttgart|University of Stuttgart|University of Stuttgart|University of Stuttgart|University of Stuttgart|University of Stuttgart|University of Stuttgart,1,Visual Analysis of Scene-Graph-Based Visual Question Answering,https://doi.org/10.1145/3615522.3615547,0.18196834,FALSE,en,TRUE,gold,,,2023-09-22,article
https://openalex.org/W4391768674,,Yongwei Li|Tao Song|Xinkai Wu,Beihang University|Beihang University|Beihang University,4,Robust Construction of Spatial-Temporal Scene Graph Considering Perception Failures for Autonomous Driving,https://doi.org/10.1109/itsc57777.2023.10422096,0.65424479,FALSE,en,FALSE,closed,,,2023-09-24,article
https://openalex.org/W4387029683,,Ho-Jun Baek|Incheol Kim,,0,C3DSG: A 3D Scene Graph Generation Model Using Point Clouds of Indoor Environment,https://doi.org/10.5626/jok.2023.50.9.758,0.0,FALSE,en,FALSE,closed,Journal of KIISE,journal,2023-09-25,article
https://openalex.org/W4387092463,,Zhenghao Wang|Jing Lian|Linhui Li|Jian Zhao,Dalian University of Technology|Dalian University of Technology|Dalian University of Technology|Dalian University of Technology,4,A Novel Framework for Scene Graph Generation via Prior Knowledge,https://doi.org/10.1109/tcsvt.2023.3319633,0.72787336,FALSE,en,FALSE,closed,IEEE Transactions on Circuits and Systems for Video Technology,journal,2023-09-27,article
https://openalex.org/W4387127047,"In modeling vision, there has been a remarkable progress in recognizing a range of scene components, but the problem of analyzing full scenes, an ultimate goal of visual perception, is still largely open. To deal with complete scenes, recent work focused on the training of models for extracting the full graph-like structure of a scene. In contrast with scene graphs, humans’ scene perception focuses on selected structures in the scene, starting with a limited interpretation and evolving sequentially in a goal-directed manner [G. L. Malcolm, I. I. A. Groen, C. I. Baker, Trends. Cogn. Sci. 20 , 843–856 (2016)]. Guidance is crucial throughout scene interpretation since the extraction of full scene representation is often infeasible. Here, we present a model that performs human-like guided scene interpretation, using an iterative bottom–up, top–down processing, in a “counterstream” structure motivated by cortical circuitry. The process proceeds by the sequential application of top–down instructions that guide the interpretation process. The results show how scene structures of interest to the viewer are extracted by an automatically selected sequence of top–down instructions. The model shows two further benefits. One is an inherent capability to deal well with the problem of combinatorial generalization—generalizing broadly to unseen scene configurations, which is limited in current network models [B. Lake, M. Baroni, 35th International Conference on Machine Learning, ICML 2018 (2018)]. The second is the ability to combine visual with nonvisual information at each cycle of the interpretation process, which is a key aspect for modeling human perception as well as advancing AI vision systems.",Shimon Ullman|Liav Assif|Alona Strugatski|Ben-Zion Vatashsky|Hila Levi|Aviv Netanyahu|Adam Yaari,Weizmann Institute of Science|Weizmann Institute of Science|Weizmann Institute of Science|Weizmann Institute of Science|Weizmann Institute of Science|Massachusetts Institute of Technology|Massachusetts Institute of Technology,3,Human-like scene interpretation by a guided counterstream processing,https://doi.org/10.1073/pnas.2211179120,0.54590502,FALSE,en,TRUE,hybrid,Proceedings of the National Academy of Sciences,journal,2023-09-28,article
https://openalex.org/W4390873597,,Kunal Pratap Singh|Jordi Salvador|Luca Weihs|Aniruddha Kembhavi,Allen Institute|Allen Institute|Allen Institute|Allen Institute,14,Scene Graph Contrastive Learning for Embodied Navigation,https://doi.org/10.1109/iccv51070.2023.00999,2.54755676,FALSE,en,FALSE,closed,,,2023-10-01,article
https://openalex.org/W4390190087,"We propose a new task, non localized scene graph verification, whose objective is to provide a justified expression of inconsistencies between the visual content of the image and its non-localized scene graph in order to diagnose errors or anticipate corrections. We introduce a sequential algorithm capable of detecting and proposing plausible corrections, taking into account the information already present in the scene graph and exploiting knowledge priors. Instead of relying on object detection that requires bounding box annotations, we use a simple visual question answering (VQA) as a proxy for visual content analysis. We show on the VG150 dataset that our strategy is efficient compared to a baseline adapted from a caption editing approach. We also show that our algorithm is able to efficiently correct corrupted scene graphs.",Dao Thauvin|Stéphane Herbin,Université Paris-Saclay|Office National d'Études et de Recherches Aérospatiales|Université Paris-Saclay|Office National d'Études et de Recherches Aérospatiales,2,Knowledge Informed Sequential Scene Graph Verification Using VQA,https://doi.org/10.1109/iccvw60793.2023.00009,0.36393668,FALSE,en,TRUE,green,,,2023-10-02,article
https://openalex.org/W4387500623,,Huihui Li|Baorong Liu|Dongqing Wu|Hang Liu|Lei Guo,Northwestern Polytechnical University|Northwestern Polytechnical University|Northwestern Polytechnical University|Northwestern Polytechnical University|Northwestern Polytechnical University,0,Bel: Batch Equalization Loss for scene graph generation,https://doi.org/10.1007/s10044-023-01199-z,0.0,FALSE,en,FALSE,closed,Pattern Analysis and Applications,journal,2023-10-10,article
https://openalex.org/W4387564020,,Ege Özsoy|Tobias Czempiel|Evin Pınar Örnek|Ulrich Eck|Federico Tombari|Nassir Navab,Technical University of Munich|Technical University of Munich|Technical University of Munich|Technical University of Munich|Google (Switzerland)|Technical University of Munich|Technical University of Munich,14,Holistic OR domain modeling: a semantic scene graph approach,https://doi.org/10.1007/s11548-023-03022-w,7.05229843,FALSE,en,TRUE,hybrid,International Journal of Computer Assisted Radiology and Surgery,journal,2023-10-12,article
https://openalex.org/W4388855749,,Shan Yang|Ruofan Wang|Lijin Fang|Chule Yang|Yi Yang|Yufeng Yue,Beijing Institute of Technology|China Academy of Launch Vehicle Technology|Beijing Institute of Technology|Academy of Military Medical Sciences|Beijing Institute of Technology|Beijing Institute of Technology,0,Bidirectional Edge-Based 3D Scene Graph Generation from Point Clouds,https://doi.org/10.1109/icus58632.2023.10318268,0.0,FALSE,en,FALSE,closed,,,2023-10-13,article
https://openalex.org/W4388757893,,Jessica D’Souza|P. K. Aleema|Shayanashree. N Dhanyashree|Clita Fernandes|K. Kavitha|Chandra Naik,,2,Knowledge-Based Scene Graph Generation in Medical Field,https://doi.org/10.1109/discover58830.2023.10316715,0.36393668,FALSE,en,FALSE,closed,,,2023-10-13,article
https://openalex.org/W4387698267,"Scene graph generation (SGG) methods have suffered from a severe training bias towards frequent (head) predicate classes. Recent works owe it to the long-tailed distribution of predicates and alleviate the long-tailed problem to conduct de-biasing. However, the ""unbiased'' models are in turn biased to tail predicate classes, resulting in a significant performance loss on head predicate classes. The main cause of such a trade-off between head and tail predicates is the fact that multiple predicates from the head or tail ones can be labeled as the ground-truth. To this end, we propose a multi-expert de-biasing method (MED) for SGG that can produce unbiased scene graphs with minor influence on recognizing head predicates. We avoid the dilemma of balancing between head and tail predicates by adaptively classifying the predicates with multiple complementary models. Experiments on the Visual Genome dataset show that MED provides significant gains on mRecall@K without harming the performance on Recall@K, and achieves a state-of-the-art on the mean of Recall@K and mRecall@K.",Xuezhi Tong|Rui Wang|Lihua Jing,Tianjin University|Chinese Academy of Sciences|Institute of Information Engineering|Chinese Academy of Sciences|Institute of Information Engineering,0,Alleviating Training Bias with Less Cost via Multi-expert De-biasing Method in Scene Graph Generation,https://doi.org/10.1145/3607541.3616816,0.0,FALSE,en,TRUE,gold,,,2023-10-17,article
https://openalex.org/W4387698214,"Recent works on Scene Graph Generation (SGG) have been concentrating on solving the problem of long-tailed distribution. While these methods are making significant improvements on the tail predicate categories, they sacrifice the performance of the head ones severely. The major issue lies in the semantic ambiguity problem, which is the contradiction between the commonly used criterion and the nature of relationships in the SGG datasets. The models are evaluated with graph constraint, which allows merely one relationship between a pair of objects. However, the relationships are much more complex and can always be described from different views. For example, when a man is in front of a computer, we can also say he is watching it. Both options are plausible, describing the different aspects of the relationship. Which of them is determined to be the ground-truth is highly subjective. In this paper, we claim that the relationships should be considered from multiple views to avoid the semantic ambiguity. In other words, the model should provide all the possibilities, rather than being biased to any one of the options. To this end, we propose the Multi-View Predicate Recognition (MVPR), which separates the label set into multiple views and enables the model to represent and predict in a ""multi-view'' style. Specifically, MVPR can be divided into three parts: Adaptive Bounding Box for Predicate is proposed to help the model attend to the crucial areas for the predicate categories in different views; Multi-View Predicate Feature Learning is designed to separate the feature space of different views of predicate categories; Multi-View Predicate Prediction and Multi-View Graph Constraint are used to allow the model to provide multi-view predictions to accurately estimate ambiguous relationships. Experimental results on the Visual Genome dataset show that our MVPR can significantly improve the model performance on the SGG task, and achieves a new state-of-the-art.",Xuezhi Tong|Lihua Jing|C.L. Zou|Rui Wang,Tianjin University|Institute of Information Engineering|Chinese Academy of Sciences|Chinese Academy of Sciences|Institute of Information Engineering|Chinese Academy of Sciences|Institute of Information Engineering,0,Multi-View Predicate Recognition for Solving Semantic Ambiguity Problem in Scene Graph Generation,https://doi.org/10.1145/3607541.3616817,0.0,FALSE,en,TRUE,gold,,,2023-10-17,article
https://openalex.org/W4387885774,,Jian Cheng|Kaifang Long|Shuang Zhang|Tian Zhang|Lianbo Ma|Shi Cheng|Yinan Guo,China Coal Research Institute (China)|Northeastern University|Northeastern University|Northeastern University|Northeastern University|Shaanxi Normal University|China University of Mining and Technology,7,Text-Image Scene Graph Fusion for Multimodal Named Entity Recognition,https://doi.org/10.1109/tai.2023.3326416,1.27377838,FALSE,en,FALSE,closed,IEEE Transactions on Artificial Intelligence,journal,2023-10-23,article
https://openalex.org/W4387934928,,Guorong Li|Hanhua Ye|Yuankai Qi|Shuhui Wang|Laiyun Qing|Qingming Huang|Ming–Hsuan Yang,"University of Chinese Academy of Sciences|University of Chinese Academy of Sciences|University of Adelaide|Australian Centre for Robotic Vision|Institute of Computing Technology|Chinese Academy of Sciences|University of Chinese Academy of Sciences|University of Chinese Academy of Sciences|Google (United States)|Yonsei University|University of California, Merced",25,Learning Hierarchical Modular Networks for Video Captioning,https://doi.org/10.1109/tpami.2023.3327677,4.5492085,FALSE,en,FALSE,closed,IEEE Transactions on Pattern Analysis and Machine Intelligence,journal,2023-10-25,article
https://openalex.org/W4387969215,,Lianggangxu Chen|Jiale Lu|Youqi Song|Changbo Wang|Gaoqi He,East China Normal University|East China Normal University|East China Normal University|East China Normal University|East China Normal University|Chongqing Normal University,5,Beware of Overcorrection: Scene-induced Commonsense Graph for Scene Graph Generation,https://doi.org/10.1145/3581783.3612210,0.9098417,FALSE,en,FALSE,closed,,,2023-10-26,article
https://openalex.org/W4387969367,"Panoptic Scene Graph Generation (PSG) translates visual scenes to structured linguistic descriptions, i.e., mapping visual instances to subjects/objects, and their relationships to predicates. However, the annotators' preferences and semantic overlaps between predicates inevitably lead to the semantic mappings of multiple predicates to one relationship, i.e., biased-predicate annotations. As a result, with the contradictory mapping between visual and linguistics, PSG models are struggled to construct clear decision planes among predicates, so as to cause existing poor performances. Obviously, it is essential for the PSG task to tackle this multi-modal contradiction. Therefore, we propose a novel method that utilizes unbiased visual predicate representations for Biased-Annotation Identification (BAI) as a fundamental step for PSG/SGG tasks. Our BAI includes three main steps: predicate representation extraction, predicate representation debiasing, and biased-annotation identification. With flexible biased annotation processing methods, our BAI can act as a fundamental step of dataset debiasing. Experimental results demonstrate that our proposed BAI has achieved state-of-the-art performance, which promotes the performance of benchmark models to various degrees with ingenious biased annotation processing methods. Furthermore, our BAI shows great generalization and effectiveness on multiple datasets. Our codes are released at https://github.com/lili0415/BAI.",Li Li|Chenwei Wang|You Qin|Wei Ji|Renjie Liang,National University of Singapore|University of Electronic Science and Technology of China|National University of Singapore|National University of Singapore|Nanyang Technological University,9,Biased-Predicate Annotation Identification via Unbiased Visual Predicate Representation,https://doi.org/10.1145/3581783.3611847,1.63771506,FALSE,en,TRUE,gold,,,2023-10-26,article
https://openalex.org/W4387968684,,Zhiqing Chen|Yawei Luo|Jian Shao|Yi Yang|Chunping Wang|Lei Chen|Jun Xiao,Zhejiang University|Zhejiang University|Zhejiang University|Zhejiang University|||Zhejiang University,7,Dark Knowledge Balance Learning for Unbiased Scene Graph Generation,https://doi.org/10.1145/3581783.3612031,1.27377838,FALSE,en,FALSE,closed,,,2023-10-26,article
https://openalex.org/W4387968400,"Panoptic Scene Graph Generation (PSG) presents pixel-wise instance detection and localization, leading to comprehensive and precise scene graphs. Current methods employ conventional Scene Graph Generation (SGG) frameworks to solve the PSG problem, neglecting the fundamental differences between bounding boxes and masks, i.e., bounding boxes are allowed overlap but masks are not. Since segmentation from the panoptic head has deviations, non-overlapping masks may not afford complete instance information. Subsequently, in the training phase, incomplete segmented instances may not be well-aligned to annotated ones, causing mismatched relations and insufficient training. During the inference phase, incomplete segmentation leads to incomplete scene graph prediction. To alleviate these problems, we construct a novel two-stage framework for the PSG problem. In the training phase, we design a proposal matching strategy, which replaces deterministic segmentation results with proposals extracted from the off-the-shelf panoptic head for label alignment, thereby ensuring the all-matching of training samples. In the inference phase, we present an innovative concept of employing relation predictions to constrain segmentation and design a relation-constrained segmentation algorithm. By reconstructing the process of generating segmentation results from proposals using predicted relation results, the algorithm recovers more valid instances and predicts more complete scene graphs. The experimental results show overall superiority, effectiveness, and robustness against adversarial attacks.",Jiarui Yang|Chuan Wang|Zeming Liu|Jiahong Wu|Dongsheng Wang|Liang Yang|Xiaochun Cao,Institute of Information Engineering|Institute of Information Engineering|Beihang University||China University of Political Science and Law|Hebei University of Technology|Sun Yat-sen University,7,Focusing on Flexible Masks: A Novel Framework for Panoptic Scene Graph Generation with Relation Constraints,https://doi.org/10.1145/3581783.3612544,1.27377838,FALSE,en,TRUE,gold,,,2023-10-26,article
https://openalex.org/W4387969090,,Chenyang Lyu|Wenxi Li|Tianbo Ji|Longyue Wang|Liting Zhou|Cathal Gurrin|Linyi Yang|Yi Yu|Yvette Graham|Jennifer Foster,Dublin City University|Shanghai Jiao Tong University|Nantong University|Tencent (China)|Dublin City University|Dublin City University|Westlake University|National Institute of Informatics|Trinity College Dublin|Dublin City University,6,Graph-Based Video-Language Learning with Multi-Grained Audio-Visual Alignment,https://doi.org/10.1145/3581783.3612132,1.09181004,FALSE,en,FALSE,closed,,,2023-10-26,article
https://openalex.org/W4388190284,,Yulu Wang|Pengwen Dai|Xiaojun Jia|Zhitao Zeng|Rui Li|Xiaochun Cao,University of Chinese Academy of Sciences|Sun Yat-sen University|University of Chinese Academy of Sciences|University of Chinese Academy of Sciences|University of Chinese Academy of Sciences|Sun Yat-sen University,4,Hi-SIGIR: Hierachical Semantic-Guided Image-to-image Retrieval via Scene Graph,https://doi.org/10.1145/3581783.3612283,0.72787336,FALSE,en,FALSE,closed,,,2023-10-26,article
https://openalex.org/W4387968658,,Yiming Li|Xiaoshan Yang|Changsheng Xu,Zhengzhou University|University of Chinese Academy of Sciences|Institute of Automation|University of Chinese Academy of Sciences|Institute of Automation,1,Iterative Learning with Extra and Inner Knowledge for Long-tail Dynamic Scene Graph Generation,https://doi.org/10.1145/3581783.3612430,0.18196834,FALSE,en,FALSE,closed,,,2023-10-26,article
https://openalex.org/W4387968391,,Jiale Lu|Lianggangxu Chen|Youqi Song|Shaohui Lin|Changbo Wang|Gaoqi He,East China Normal University|East China Normal University|East China Normal University|East China Normal University|East China Normal University|East China Normal University,6,Prior Knowledge-driven Dynamic Scene Graph Generation with Causal Inference,https://doi.org/10.1145/3581783.3612249,1.09181004,FALSE,en,FALSE,closed,,,2023-10-26,article
https://openalex.org/W4388192171,,Tao Pu,Sun Yat-sen University,1,Video Scene Graph Generation with Spatial-Temporal Knowledge,https://doi.org/10.1145/3581783.3613433,0.18196834,FALSE,en,FALSE,closed,,,2023-10-26,article
https://openalex.org/W4388441133,"Scene graphs are structured representations that can clearly convey objects and the relationships between them, but are often heavily biased due to the highly skewed, long-tailed relational labeling in the dataset.Indeed, the visual world itself and its descriptions are biased.Therefore, Unbiased Scene Graph Generation (USGG) prefers to train models to eliminate long-tail effects as much as possible, rather than altering the dataset directly.To this end, we propose Geometric and Semantic Improvement (GSI) for USGG to mitigate this issue.First, to fully exploit the feature information in the images, geometric dimension and semantic dimension enhancement modules are designed.The geometric module is designed from the perspective that the position information between neighboring object pairs will affect each other, which can improve the recall rate of the overall relationship in the dataset.The semantic module further processes the embedded word vector, which can enhance the acquisition of semantic information.Then, to improve the recall rate of the tail data, the Class Balanced Seesaw Loss (CBSLoss) is designed for the tail data.The recall rate of the prediction is improved by penalizing the body or tail relations that are judged incorrectly in the dataset.The experimental findings demonstrate that the GSI method performs better than mainstream models in terms of the mean Recall@K (mR@K) metric in three tasks.The long-tailed imbalance in the Visual Genome 150 (VG150) dataset is addressed better using the GSI method than by most of the existing methods.",Ruhui Zhang|Pengcheng Xu|Kang Kang|You Yang,Chongqing Normal University|Chongqing University of Posts and Telecommunications|Chongqing Normal University|Chongqing Normal University,0,Geometric and Semantic Improvement for Unbiased Scene Graph Generation,https://doi.org/10.3837/tiis.2023.10.003,0.0,FALSE,en,TRUE,diamond,KSII Transactions on Internet and Information Systems,journal,2023-10-31,article
https://openalex.org/W4389342065,"Intelligent transport systems (ITS) have revolutionized the transportation industry by integrating cutting-edge technologies to enhance road safety, reduce traffic congestion and optimize the transportation network. Scene understanding is a critical component of ITS that enables real-time decision-making by interpreting the environment's contextual information. However, achieving accurate scene understanding requires vast amounts of labeled data, which can be costly and time-consuming. It is quite challenging to Understand traffic scene captured from vehicle mounted cameras. In recent times, the combination of road scene-graph representations and graph learning techniques has demonstrated superior performance compared to cutting-edge deep learning methods across various tasks such as action classification, risk assessment, and collision prediction. It's a grueling problem due to large variations under different illumination conditions. Transfer learning is a promising approach to address this challenge. Transfer learning involves leveraging pre-trained deep learning models on large-scale datasets to develop efficient models for new tasks with limited data. In the context of ITS, transfer learning can enable accurate scene understanding with less data by reusing learned features from other domains.&#x0D; This paper presents a comprehensive overview of the application of transfer learning for scene understanding in cross domain. It highlights the benefits of transfer learning for ITS and presents various transfer learning techniques used for scene understanding. This survey paper provides systematic review on cross domain outdoor scene understanding and transfer learning approaches from different perspective, presents information on current state of art and significant methods in choosing the right transfer learning model for specific scene understanding applications.",Et al. Deepa Mane,,1,A Review on Cross Weather Traffic Scene Understanding Using Transfer Learning for Intelligent Transport System,https://doi.org/10.17762/ijritcc.v11i10.8886,0.21420827,FALSE,en,TRUE,diamond,International Journal on Recent and Innovation Trends in Computing and Communication,journal,2023-11-07,review
https://openalex.org/W4388543957,,Wenwen Wei|Ping Wei|Jialu Qin|Zhimin Liao|Shuaijie Wang|Xiang Cheng|Meiqin Liu|Nanning Zheng,Xi'an Jiaotong University|Xi'an Jiaotong University|Xi'an Jiaotong University|Xi'an Jiaotong University|Xi'an Jiaotong University|Peking University|Xi'an Jiaotong University|Xi'an Jiaotong University,4,3D Scene Graph Generation From Point Clouds,https://doi.org/10.1109/tmm.2023.3331583,1.34402596,FALSE,en,FALSE,closed,IEEE Transactions on Multimedia,journal,2023-11-09,article
https://openalex.org/W4394995618,,Frederik Bark|Kevin Daun|Oskar von Stryk,Technical University of Darmstadt|Technical University of Darmstadt|Technical University of Darmstadt,3,Affordance-based Actionable Semantic Mapping and Planning for Mobile Rescue Robots,https://doi.org/10.1109/ssrr59696.2023.10499938,0.54590502,FALSE,en,FALSE,closed,,,2023-11-13,article
https://openalex.org/W4388642339,,Asim Unmesh|Rahul Jain|Jingyu Shi|V. K. Chaithanya Manam|Hyung‐gun Chi|Subramanian Chidambaram|Alexander J. Quinn|Karthik Ramani,Purdue University West Lafayette|Purdue University West Lafayette|Purdue University West Lafayette|Purdue University West Lafayette|Purdue University West Lafayette|Purdue University West Lafayette|Purdue University West Lafayette|Purdue University West Lafayette,8,Interacting Objects: A Dataset of Object-Object Interactions for Richer Dynamic Scene Representations,https://doi.org/10.1109/lra.2023.3332554,1.45574672,FALSE,en,FALSE,closed,IEEE Robotics and Automation Letters,journal,2023-11-13,article
https://openalex.org/W4398234303,,Lei Wang|Tao Zuo,Wuhan University of Science and Technology|Wuhan University of Science and Technology,1,A Keyword-Guided GATv2-LSTM Graph Question Answering Network,https://doi.org/10.1145/3652628.3652749,0.18196834,FALSE,en,FALSE,closed,,,2023-11-17,article
https://openalex.org/W4392940403,,Xiling He|Zhenfeng Xue|Xiongtao Zhang|Yong Liu,Zhejiang University|Huzhou University|Huzhou University|Zhejiang University|Huzhou University|Huzhou University|Zhejiang University,0,Learning to Simulate Labelled Datasets with an Image-Level Content Consistent Graph Constraint,https://doi.org/10.1109/cac59555.2023.10451580,0.0,FALSE,en,FALSE,closed,,,2023-11-17,article
https://openalex.org/W4388796305,"3D scene synthesis facilitates and benefits many real-world applications. Most scene generators focus on making indoor scenes plausible via learning from training data and leveraging extra constraints such as adjacency and symmetry. Although the generated 3D scenes are mostly plausible with visually realistic layouts, they can be functionally unsuitable for human users to navigate and interact with furniture. Our key observation is that human activity plays a critical role and sufficient free space is essential for human-scene interactions. This is exactly where many existing synthesized scenes fail—the seemingly correct layouts are often not fit for living. To tackle this, we present a human-aware optimization framework Haisor for 3D indoor scene arrangement via reinforcement learning, which aims to find an action sequence to optimize the indoor scene layout automatically. Based on the hierarchical scene graph representation, an optimal action sequence is predicted and performed via Deep Q-Learning with Monte Carlo Tree Search (MCTS), where MCTS is our key feature to search for the optimal solution in long-term sequences and large action space. Multiple human-aware rewards are designed as our core criteria of human-scene interaction, aiming to identify the next smart action by leveraging powerful reinforcement learning. Our framework is optimized end-to-end by giving the indoor scenes with part-level furniture layout including part mobility information. Furthermore, our methodology is extensible and allows utilizing different reward designs to achieve personalized indoor scene synthesis. Extensive experiments demonstrate that our approach optimizes the layout of 3D indoor scenes in a human-aware manner, which is more realistic and plausible than original state-of-the-art generator results, and our approach produces superior smart actions, outperforming alternative baselines.",Jia-Mu Sun|Jie Yang|Kaichun Mo|Yu‐Kun Lai|Leonidas Guibas|Lin Gao,Institute of Computing Technology|University of Chinese Academy of Sciences|Institute of Computing Technology|Stanford University|Nvidia (United States)|Cardiff University|Stanford University|Institute of Computing Technology|University of Chinese Academy of Sciences,8,<scp>Haisor</scp> : Human-aware Indoor Scene Optimization via Deep Reinforcement Learning,https://doi.org/10.1145/3632947,4.15960215,FALSE,en,TRUE,hybrid,ACM Transactions on Graphics,journal,2023-11-18,article
https://openalex.org/W4391489071,,Bo Lu|Tao Yang|Huanle Lu|Zunwen Chen|Xiaodong Duan,Dalian Minzu University|Minzu University of China|Minzu University of China|Dalian Minzu University|Minzu University of China|Dalian Minzu University|Dalian Minzu University|Minzu University of China|Dalian Minzu University|Minzu University of China,0,Transformer-based Deep Embedding Network for Scene Graph Generation,https://doi.org/10.1145/3603273.3635669,0.0,FALSE,en,FALSE,closed,,,2023-11-18,article
https://openalex.org/W4389113238,,Qipeng Li|Yuan Zhuang|Jianzhu Huai|Yiwen Chen|Alper Yılmaz,"State Key Laboratory of Information Engineering in Surveying Mapping and Remote Sensing|Wuhan University|City University of Hong Kong, Shenzhen Research Institute|State Key Laboratory of Information Engineering in Surveying Mapping and Remote Sensing|Wuhan University|State Key Laboratory of Information Engineering in Surveying Mapping and Remote Sensing|Wuhan University|Wuhan University|State Key Laboratory of Information Engineering in Surveying Mapping and Remote Sensing|The Ohio State University",10,An efficient point cloud place recognition approach based on transformer in dynamic environment,https://doi.org/10.1016/j.isprsjprs.2023.11.013,5.19950268,FALSE,en,FALSE,closed,ISPRS Journal of Photogrammetry and Remote Sensing,journal,2023-11-28,article
https://openalex.org/W4389104767,,Chen Lin|Zhenfeng Zhu|Yawei Zhao|Ying Zhang|Kunlun He|Yao Zhao,Beijing Jiaotong University|Beijing Jiaotong University|Chinese PLA General Hospital|Beijing Jiaotong University|Nankai University|Chinese PLA General Hospital|Chinese PLA General Hospital|Beijing Jiaotong University,12,SGT++: Improved Scene Graph-Guided Transformer for Surgical Report Generation,https://doi.org/10.1109/tmi.2023.3335909,2.18362008,FALSE,en,FALSE,closed,IEEE Transactions on Medical Imaging,journal,2023-11-28,article
https://openalex.org/W4389243038,,Mohammad Javad Parseh|Mohammad Rahmanimanesh|Parviz Keshavarzi|Zohreh Azimifar,Jahrom University|Semnan University|Semnan University|Shiraz University,2,Scene representation using a new two-branch neural network model,https://doi.org/10.1007/s00371-023-03162-9,0.36393668,FALSE,en,FALSE,closed,The Visual Computer,journal,2023-12-01,article
https://openalex.org/W4391286985,,Mathieu Riand|Patrick Le Callet|Laurent Dollé,École Centrale de Nantes|École Centrale de Nantes|Commissariat à l'Énergie Atomique et aux Énergies Alternatives|Direction de la Recherche Technologique,0,Rethinking Scene Graphs for Action Recognition,https://doi.org/10.1109/vcip59821.2023.10402749,0.0,FALSE,en,FALSE,closed,,,2023-12-04,article
https://openalex.org/W4392152400,,Shiqi Sun|Zhijin Qin|Huiqiang Xie|Xiaoming Tao,Tsinghua University|Tsinghua University|Queen Mary University of London|Tsinghua University,3,Task-Oriented Explainable Semantic Communications Based on Structured Scene Graphs,https://doi.org/10.1109/globecom54140.2023.10436793,0.76632866,FALSE,en,FALSE,closed,,,2023-12-04,article
https://openalex.org/W4389339279,,Zejia Su|Qingnan Fan|Xuelin Chen|Oliver van Kaick|Hui Huang|Ruizhen Hu,Shenzhen University||Tencent (China)|Carleton University|Shenzhen University|Shenzhen University,2,Scene-Aware Activity Program Generation with Language Guidance,https://doi.org/10.1145/3618338,0.51088578,FALSE,en,FALSE,closed,ACM Transactions on Graphics,journal,2023-12-05,article
https://openalex.org/W4390479072,"The Panoptic Scene Graph generation (PSG) task aims to extract the triplets composed of subject, object, and relation based on panoptic segmentation. For one-stage methods, PSGTR predicts the subject, object, and relation by one query. However, the integrated query is too implicit to simultaneously ascertain pairs of instances and relations. In PSGFormer, it learns instances and relation queries separately and establishes matches between subject-relation and object-relation pairs by employing the relation as an index. Nevertheless, this method could potentially impede the accurate determination of the optimal match. To address the aforementioned issues, we propose a new one-stage method, Contextual Associated Triplet Queries (CATQ), which employs three branches to decode subject, object, and relation features separately. Additionally, we leverage instance information to guide the relation decoding process. Furthermore, we introduce the triplet context fusion block to enable the extraction of more comprehensive instance pairs and triplet relations. Our proposed method achieves 34.8 Recall@20 and 20.9 mRecall@20 respectively and surpasses the state-of-the-art baseline method by 22.5% and 26.0% with half of the training session.",Jingbin Xu|Junwen Chen|‪Keiji Yanai‬,University of Electro-Communications|University of Electro-Communications|University of Electro-Communications,2,Contextual Associated Triplet Queries for Panoptic Scene Graph Generation,https://doi.org/10.1145/3595916.3626745,0.36393668,FALSE,en,TRUE,gold,,,2023-12-06,article
https://openalex.org/W4389513728,,Jiale Lu|Lianggangxu Chen|Haoyue Guan|Shaohui Lin|Chunhua Gu|Changbo Wang|Gaoqi He,Chongqing Normal University|East China Normal University|East China Normal University|Johns Hopkins University|East China Normal University|University of Shanghai for Science and Technology|East China Normal University|Chongqing Normal University|East China Normal University,6,Improving rare relation inferring for scene graph generation using bipartite graph network,https://doi.org/10.1016/j.cviu.2023.103901,1.09181004,FALSE,en,FALSE,closed,Computer Vision and Image Understanding,journal,2023-12-09,article
https://openalex.org/W4392980461,,Mingyuan Wu|Yuhan Lu|Shiv Trivedi|Bo Chen|Qian Zhou|Lingdong Wang|Simran Singh|Michael Zink|Ramesh K. Sitaraman|Jacob Chakareski|Klara Nahrstedt,University of Illinois Urbana-Champaign|University of Illinois Urbana-Champaign|University of Illinois Urbana-Champaign|University of Illinois Urbana-Champaign|University of Illinois Urbana-Champaign|Amherst College|University of Massachusetts Amherst|New Jersey Institute of Technology|University of Massachusetts Amherst|Amherst College|University of Massachusetts Amherst|Amherst College|New Jersey Institute of Technology|University of Illinois Urbana-Champaign,2,Interactive Scene Graph Analysis for Future Intelligent Teleconferencing Systems,https://doi.org/10.1109/ism59092.2023.00048,0.36393668,FALSE,en,FALSE,closed,,,2023-12-11,article
https://openalex.org/W4389692483,,Zhongmou Ying|Xianfeng Yuan|Baojiang Yang|Yong Song|Qingyang Xu|Fengyu Zhou|Weihua Sheng,Shandong University|Shandong University|Shandong University|Shandong University|Shandong University|Shandong University|Oklahoma State University,2,RP-SG: Relation Prediction in 3D Scene Graphs for Unobserved Objects Localization,https://doi.org/10.1109/lra.2023.3342666,0.36393668,FALSE,en,FALSE,closed,IEEE Robotics and Automation Letters,journal,2023-12-13,article
https://openalex.org/W4389782456,"Exploring the potential of neuro-symbolic hybrid approaches offers promising avenues for seamless high-level understanding and reasoning about visual scenes. Scene Graph Generation (SGG) is a symbolic image representation approach based on deep neural networks (DNN) that involves predicting objects, their attributes, and pairwise visual relationships in images to create scene graphs, which are utilized in downstream visual reasoning. The crowdsourced training datasets used in SGG are highly imbalanced, which results in biased SGG results. The vast number of possible triplets makes it challenging to collect sufficient training samples for every visual concept or relationship. To address these challenges, we propose augmenting the typical data-driven SGG approach with common sense knowledge to enhance the expressiveness and autonomy of visual understanding and reasoning. We present a loosely-coupled neuro-symbolic visual understanding and reasoning framework that employs a DNN-based pipeline for object detection and multi-modal pairwise relationship prediction for scene graph generation and leverages common sense knowledge in heterogenous knowledge graphs to enrich scene graphs for improved downstream reasoning. A comprehensive evaluation is performed on multiple standard datasets, including Visual Genome and Microsoft COCO, in which the proposed approach outperformed the state-of-the-art SGG methods in terms of relationship recall scores, i.e. Recall@K and mean Recall@K, as well as the state-of-the-art scene graph-based image captioning methods in terms of SPICE and CIDEr scores with comparable BLEU, ROGUE and METEOR scores. As a result of enrichment, the qualitative results showed improved expressiveness of scene graphs, resulting in more intuitive and meaningful caption generation using scene graphs. Our results validate the effectiveness of enriching scene graphs with common sense knowledge using heterogeneous knowledge graphs. This work provides a baseline for future research in knowledge-enhanced visual understanding and reasoning. The source code is available at https://github.com/jaleedkhan/neusire.",Muhammad Jaleed Khan|John G. Breslin|Edward Curry,Ollscoil na Gaillimhe – University of Galway|Ollscoil na Gaillimhe – University of Galway|Ollscoil na Gaillimhe – University of Galway,6,NeuSyRE: Neuro-symbolic visual understanding and reasoning framework based on scene graph enrichment,https://doi.org/10.3233/sw-233510,1.09181004,FALSE,en,TRUE,hybrid,Semantic Web,journal,2023-12-15,article
https://openalex.org/W4392942756,,Sen Jia|Homa Fashandi,,0,Revisiting Knowledge Graph Embedding: An Alternative Solution to Biased Visual Scene Graphs,https://doi.org/10.1109/icmla58977.2023.00293,0.0,FALSE,en,FALSE,closed,,,2023-12-15,article
https://openalex.org/W4389891300,,Hongbo Xu|Lichun Wang|Kai Xu|Fangyu Fu|Baocai Yin|Qingming Huang,Beijing University of Technology|Beijing University of Technology|Beijing University of Technology|Beijing University of Technology|Beijing University of Technology|University of Chinese Academy of Sciences,2,A New Training Data Organization Form and Training Mode for Unbiased Scene Graph Generation,https://doi.org/10.1109/tcsvt.2023.3344569,0.36393668,FALSE,en,FALSE,closed,IEEE Transactions on Circuits and Systems for Video Technology,journal,2023-12-18,article
https://openalex.org/W4390143879,,Jianxin Li|Guannan Si|Pengxin Tian|Zhaoliang An|Fengyu Zhou,Shandong Jiaotong University|Shandong Jiaotong University|Shandong Jiaotong University|Shandong Jiaotong University|Shandong University,5,Overview of indoor scene recognition and representation methods based on multimodal knowledge graphs,https://doi.org/10.1007/s10489-023-05235-7,0.9098417,FALSE,en,FALSE,closed,Applied Intelligence,journal,2023-12-23,article
https://openalex.org/W4390195594,,Jingyu Li|Zhendong Mao|Hao Li|Weidong Chen|Yongdong Zhang,University of Science and Technology of China|University of Science and Technology of China|University of Science and Technology of China|University of Science and Technology of China|University of Science and Technology of China,16,Exploring Visual Relationships via Transformer-based Graphs for Enhanced Image Captioning,https://doi.org/10.1145/3638558,2.91149344,FALSE,en,FALSE,closed,ACM Transactions on Multimedia Computing Communications and Applications,journal,2023-12-25,article
https://openalex.org/W4390277776,,Mengnan Zhao|Yuqiu Kong|Lihe Zhang|Baocai Yin,Dalian University of Technology|Dalian University of Technology|Dalian University of Technology|Dalian University of Technology,9,Class correlation correction for unbiased scene graph generation,https://doi.org/10.1016/j.patcog.2023.110221,1.63771506,FALSE,en,FALSE,closed,Pattern Recognition,journal,2023-12-27,article
https://openalex.org/W4390372556,,Yaodong Wang|Zhong Ji|Di Wang|Yanwei Pang|Xuelong Li,Tianjin University|Tianjin University|Tianjin University|Tianjin University|Northwestern Polytechnical University,6,Towards Unsupervised Referring Expression Comprehension with Visual Semantic Parsing,https://doi.org/10.1016/j.knosys.2023.111318,1.09181004,FALSE,en,FALSE,closed,Knowledge-Based Systems,journal,2023-12-28,article
https://openalex.org/W4390599795,"Scene graphs serve as semantic abstractions of images and play a crucial role in enhancing visual comprehension and reasoning.However, the performance of Scene Graph Generation is often compromised when working with biased data in real-world situations.While many existing systems focus on a single stage of learning for both feature extraction and classification, some employ Class-Balancing strategies, such as Re-weighting, Data Resampling, and Transfer Learning from head to tail.In this paper, we propose a novel approach that decouples the feature extraction and classification phases of the scene graph generation process.For feature extraction, we leverage a transformer-based architecture and design an adaptive calibration function specifically for predicate classification.This function enables us to dynamically adjust the classification scores for each predicate category.Additionally, we introduce a Distribution Alignment technique that effectively balances the class distribution after the feature extraction phase reaches a stable state, thereby facilitating the retraining of the classification head.Importantly, our Distribution Alignment strategy is model-independent and does not require additional supervision, making it applicable to a wide range of SGG models.Using the scene graph diagnostic toolkit on Visual Genome and several popular models, we achieved significant improvements over the previous state-of-the-art methods with our model.Compared to the TDE model, our model improved mR@100 by 70.5% for PredCls, by 84.0% for SGCls, and by 97.6% for SGDet tasks.",Jia Dongdong|Meili Zhou|Wei Wei|Dong Wang|Zongwen Bai,Yanan University Affiliated Hospital|Yanan University Affiliated Hospital||Yanan University Affiliated Hospital|Yanan University Affiliated Hospital,0,A Novel Two-Stage Training Method for Unbiased Scene Graph Generation via Distribution Alignment,https://doi.org/10.3837/tiis.2023.12.009,0.0,FALSE,en,TRUE,diamond,KSII Transactions on Internet and Information Systems,journal,2023-12-31,article
https://openalex.org/W4390667056,,Woo‐Suk Choi|Yu‐Jung Heo|Byoung‐Tak Zhang,,0,Scene Graph Generation Framework using Image Region Description,https://doi.org/10.5626/ktcp.2023.29.12.583,0.0,FALSE,en,FALSE,closed,KIISE Transactions on Computing Practices,journal,2023-12-31,article
https://openalex.org/W4396576559,"This article tackles the visual localization of unmanned aerial vehicles (UAVs) in the presence of multisource and cross-view images are involved. We present a lightweight end-to-end scene graph encoding and matching network that finds the best matches for the airborne camera views from the reference image maps. The scene graph addresses the challenges of encoding the semantic scene by aggregating the image convolutional features into global and structured semiglobal descriptors. The principal contributions of this article are as follows: First, we develop a new network architecture that embeds a nonlocal block and a modified vector of locally aggregated descriptors network (NetVLAD) into a backbone convolutional neural network. The main component of the modified NetVLAD is a cluster similarity masking graph (CSMG) encoder, which is proposed to replace the feature-cluster residuals computing in NetVLAD with cluster consensus feature aggregation and structure-aware scene graph extraction. In addition, a global descriptor is extracted by a nonlocal block to label each image with a discriminative global feature descriptor. Second, we develop a new triplet loss for the network training procedure to learn the features at different semantic levels. The proposed global descriptor and CSMG encoder are trained together according to a weighted sum of cosine triplet losses. Third, the global descriptor from the nonlocal block and semiglobal descriptor from the CSMG encoder work hierarchically for coarse-to-fine image retrieval and can achieve real-time efficiency and favorable accuracy of image searching and matching from the reference image map. We train and test the model on two challenging benchmark datasets. We also test the pretrained model on a dataset collected by a fixed-wing UAV to further evaluate the model&#x0027;s generalizability. The benchmark evaluations and ablation experiments show that the developed method outperforms state-of-the-art methods and achieves superior performance in the real-time matching of UAV images and reference image maps for UAV visual localization. Open-source code is available on GitHub.",Ran Duan|Long Chen|Zhaojin Li|Zeyu Chen|Bo Wu,Hong Kong Polytechnic University|Hong Kong Polytechnic University|Hong Kong Polytechnic University|Hong Kong Polytechnic University|Hong Kong Polytechnic University,5,A Scene Graph Encoding and Matching Network for UAV Visual Localization,https://doi.org/10.1109/jstars.2024.3396168,2.6507878,FALSE,en,TRUE,gold,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,journal,2024-01-01,article
https://openalex.org/W4392930798,,Jiarui Yang|Chuan Wang|Liang Yang|Yuchen Jiang|Angelina Cao,Institute of Information Engineering|University of Chinese Academy of Sciences|Chinese Academy of Sciences|Chinese Academy of Sciences|Institute of Information Engineering|Hebei University of Technology|Sun Yat-sen University|Montgomery High School,8,Adaptive Feature Learning for Unbiased Scene Graph Generation,https://doi.org/10.1109/tip.2024.3374644,4.24126048,FALSE,en,FALSE,closed,IEEE Transactions on Image Processing,journal,2024-01-01,article
https://openalex.org/W4390488700,,Minchao Ye|Junbin Chen|Fengchao Xiong|Yuntao Qian,China Jiliang University|China Jiliang University|Nanjing University of Science and Technology|Zhejiang University,20,Adaptive Graph Modeling With Self-Training for Heterogeneous Cross-Scene Hyperspectral Image Classification,https://doi.org/10.1109/tgrs.2023.3348953,12.29870003,FALSE,en,FALSE,closed,IEEE Transactions on Geoscience and Remote Sensing,journal,2024-01-01,article
https://openalex.org/W4391406903,,Mengnan Zhao|Lihe Zhang|Wei Wang|Yuqiu Kong|Baocai Yin,Dalian University of Technology|Dalian University of Technology|Chinese Academy of Sciences|Institute of Automation|Dalian University of Technology|Dalian University of Technology,2,Adversarial Attacks on Scene Graph Generation,https://doi.org/10.1109/tifs.2024.3360880,1.06031512,FALSE,en,FALSE,closed,IEEE Transactions on Information Forensics and Security,journal,2024-01-01,article
https://openalex.org/W4403885841,,Zhuyu Shen,Xidian University,0,Exploring Predicate Distributions in Scene Graph Generation,https://doi.org/10.2139/ssrn.5004879,,FALSE,en,TRUE,green,SSRN Electronic Journal,repository,2024-01-01,preprint
https://openalex.org/W4401387126,"High-speed path planning is a real-time task that requires vehicles to operate at the edge of their performance limits and holds significant practical importance. However, the significant differences between the simulated training environments and real-world environments pose a challenge, leading to performance discrepancies between the simulator and real-world applications, thus impacting effective high-speed path planning. To address this issue, we propose a novel reinforcement learning network that identifies the environment by extracting scene graphs. We utilize a forward dynamics environment simulation network to mitigate the discrepancies between the simulator and reality and enhance the reward function to expedite training convergence. Ultimately, our algorithm demonstrates excellent performance in both physical simulations and real-world scenarios, achieving high-speed path planning. Compared to previous reinforcement learning path planning algorithms, we have increased the race completion success rate by more than 20.83%, proving the effectiveness of our algorithm.",Jingjing Shi|Ruiqin Li|Daguo Yu,North University of China|North University of China|North University of China,1,High-Speed Racing Reinforcement Learning Network: Learning the Environment Using Scene Graphs,https://doi.org/10.1109/access.2024.3440183,0.39929135,FALSE,en,TRUE,gold,IEEE Access,journal,2024-01-01,article
https://openalex.org/W4394595158,,Yuchen Zhou|Xinxin Liu|Zipeng Guo|Ming Cai|Chao Gou,Sun Yat-sen University|Shenzhen University|Sun Yat-sen University|Shenzhen University|Shenzhen University|Sun Yat-sen University|Sun Yat-sen University|Shenzhen University|Shenzhen University|Sun Yat-sen University,16,HKTSG: A Hierarchical Knowledge-guided Traffic Scene Graph Representation Learning Framework for Intelligent Vehicles,https://doi.org/10.1109/tiv.2024.3384989,10.22045679,FALSE,en,FALSE,closed,IEEE Transactions on Intelligent Vehicles,journal,2024-01-01,article
https://openalex.org/W4392402364,,Ryosuke Miyake|Tetsu Matsukawa|Einoshin Suzuki,Kyushu University|Kyushu University|Kyushu University,1,Image Generation from Hyper Scene Graphs with Trinomial Hyperedges Using Object Attention,https://doi.org/10.5220/0012472500003660,0.53015756,FALSE,en,TRUE,gold,,,2024-01-01,article
https://openalex.org/W4391341947,"Summarization tasks aim to summarize multiple pieces of information into a short description or representative information. A text summarization task summarizes textual information into a short description, whereas an image collection summarization task summarizes an image collection into images or textual representation in which the challenge is to understand the relationship between images. In recent years, scene-graph generation has shown the advantage of describing the visual contexts of a single-image, and incorporating external knowledge into the scene-graph generation model has also given effective directions for unseen single-image scene-graph generation. While external knowledge has been implemented in related work, it is still challenging to use this information efficiently for relationship estimation during the summarization. Following this trend, in this paper, we propose a novel scene-graph-based image-collection summarization model that aims to generate a summarized scene-graph of an image collection. The key idea of the proposed method is to enhance the relation predictor toward relationships between images in an image collection incorporating knowledge graphs as external knowledge for training a model. With this approach, we build an end-to-end framework that can generate a summarized scene graph of an image collection. To evaluate the proposed method, we also build an extended annotated MS-COCO dataset for this task and introduce an evaluation process that focuses on estimating the similarity between a summarized scene graph and ground-truth scene graphs. Traditional evaluation focuses on calculating precision and recall scores, which involve true positive predictions without balancing precision and recall. Meanwhile, the proposed evaluation process focuses on calculating the F-score of the similarity between a summarized scene graph and ground-truth scene graphs, which aims to balance both false positives and false negatives. Experimental results show that using external knowledge to enhance the relation predictor achieves better results than existing methods.",Itthisak Phueaksri|Marc A. Kastner|Yasutomo Kawanishi|Takahiro Komamizu|Ichiro Ide,Nagoya University|Kyoto College of Graduate Studies for Informatics|Kyoto University|Nagoya University|Nagoya University|Nagoya University,4,Image-Collection Summarization Using Scene-Graph Generation With External Knowledge,https://doi.org/10.1109/access.2024.3360113,2.12063024,FALSE,en,TRUE,gold,IEEE Access,journal,2024-01-01,article
https://openalex.org/W7104043326,"Unbiased Scene Graph Generation SGG is a major development direction of SGG. Recent years, a number of great approaches have emerged in this field but many of them tend to overlook a fundamental factor – study on loss function. Because there is a serious conflict between datasets, loss function and pursuit metrics. In most relevant datasets, status of different predicates usually varies greatly, as the common predicate 'on' appears 400 times more frequently than the rare predicate 'walking in'. But each predicate has the same weight in the loss function which does not properly reflect their status gap in datasets. And when we evaluate results, we also treat predicates in a uniform way. Now we can sum up this conflict with an interesting statement: sometimes fairness means a kind of unfairness. In response to this challenge, we introduce Meta Weighted Loss MWL , a approach based on meta-learning. MWL leverages meta-learning principles to construct a meta-neural network during model training. This network establishes a rational relationship between various predicates and their respective weight in the loss function so that the conflict above can be solved. We verify the effectiveness and generalization of this approach on multiple datasets. Comprehensive experiments demonstrate superior performance of MWL in SGG.",Yisen Wang,,0,Meta Weighted Loss: Balanced Scene Graph Generation with Meta-Learning,https://doi.org/10.65286/icic.v20i2.30589,0.0,FALSE,,TRUE,gold,,,2024-01-01,article
https://openalex.org/W4393170774,"With the development of intelligent collection technology and popularization of intelligent terminals, multi-source heterogeneous data are growing rapidly. The effective utilization of rich semantic information contained in massive amounts of multi-source heterogeneous data to provide users with high-quality cross-modal information retrieval services has become an urgent problem to be solved in the current field of information retrieval. In this paper, we propose a novel cross-modal retrieval method, named MGSGH, which deeply explores the internal correlation between data of different granularities by integrating coarse-grained global semantic information and fine-grained scene graph information to model global semantic concepts and local semantic relationship graphs within a modality respectively. By enforcing cross-modal consistency constraints and intra-modal similarity preservation, we effectively integrate the visual features of image data and semantic information of text data to overcome the heterogeneity between the two types of data. Furthermore, we propose a new method for learning hash codes directly, thereby reducing the impact of quantization loss. Our comprehensive experimental evaluation demonstrated the effectiveness and superiority of the proposed model in achieving accurate and efficient cross-modal retrieval.",Zhichao Han|Azreen Azman|Fatimah Khalid|Mas Rina Mustaffa,Universiti Putra Malaysia|Universiti Putra Malaysia|Universiti Putra Malaysia|Universiti Putra Malaysia,6,Multi-Granularity Semantic Information Integration Graph for Cross-Modal Hash Retrieval,https://doi.org/10.1109/access.2024.3380019,3.18094536,FALSE,en,TRUE,gold,IEEE Access,journal,2024-01-01,article
https://openalex.org/W4395471088,,Yuqi Zhang|Xiucheng Li|Hao Xie|Weijun Zhuang|Shihui Guo|Zhijun Li,Harbin Institute of Technology|Harbin Institute of Technology|Harbin Institute of Technology|Harbin Institute of Technology|Xiamen University|Harbin Institute of Technology,3,Multi-Label Action Anticipation for Real-World Videos With Scene Understanding,https://doi.org/10.1109/tip.2024.3391692,1.59047268,FALSE,en,FALSE,closed,IEEE Transactions on Image Processing,journal,2024-01-01,article
https://openalex.org/W4403972106,International audience,Pierre Lefebvre|Steven Le Moal|Ahmed Azough|Nicolas Travers,Pôle Léonard de Vinci|Vinci (France)|Ecole Supérieure d'Ingénieurs Léonard de Vinci|Pôle Léonard de Vinci|Pôle Léonard de Vinci|Centre d'Etudes et De Recherche en Informatique et Communications,0,NeoSGG: A Scene Graph Generation Framework for Video-Surveillance Tasks,https://doi.org/10.48786/edbt.2024.83,,FALSE,en,TRUE,green,SPIRE - Sciences Po Institutional REpository,repository,2024-01-01,article
https://openalex.org/W4394711680,"Scene recognition is a fundamental task in 3-D scene understanding. It answers the question, 'What is this place?' In an indoor environment, the answer can be an office, kitchen, lobby, and so on. As the number of point clouds increases, using embedded point information in scene recognition becomes computationally heavy to process. To achieve computational efficiency and accurate classification, our idea is to use an indoor scene graph that represents the 3-D spatial structures via object instances. The proposed method comprises two parts, namely: 1) construction of indoor scene graphs leveraging object instances and their spatial relationships and 2) classification of these graphs using a deep learning network. Specifically, each indoor scene is represented by a graph, where each node represents either a structural element (like a ceiling, a wall, or a floor) or a piece of furniture (like a chair or a table), and each edge encodes the spatial relationship between these elements. Then, these graphs are used as input for our proposed graph classification network to learn different scene representations. The public indoor dataset, ScanNet v2, with 625.53 million points, is selected to test our method. Experiments yield good results with up to 88.00% accuracy and 82.30% F1 score in the fixed validation dataset and 90.46% accuracy and 81.45% F1 score in the ten-fold cross-validation method; moreover, if some indoor objects cannot be successfully identified, the scene classification accuracy depends sublinearly on the rate of missing objects in the scene.",Han Yue|Ville Lehtola|Hangbin Wu|George Vosselman|Jincheng Li|Chun Liu,Tongji University|University of Twente|Tongji University|University of Twente|Capital Normal University|Tongji University,8,Recognition of Indoor Scenes Using 3-D Scene Graphs,https://doi.org/10.1109/tgrs.2024.3387556,4.24126048,FALSE,en,TRUE,gold,IEEE Transactions on Geoscience and Remote Sensing,journal,2024-01-01,article
https://openalex.org/W4401943198,"Scene graph generation (SGG) aims to detect the relationships of objects in an image. Recently, it has been extended to open-set SGG, which also considers unknown objects unseen in a training phase and thereby enables various applications in complex real-world scenes. However, previous research on open-set SGG addressed unknown object detection simply by thresholding confidence scores from object classification trained only for known objects. In reality, these scores become low for both unknown objects and failure detections of the background since they look different from known objects. Therefore, the current state of the art of open-set SGG cannot distinguish unknown objects from backgrounds, thereby overlooking their relationships. In this paper, we propose a novel relationship-aware unknown detection technique. Our main idea is to exploit the fact that only foreground regions containing objects can have relationships with other regions. To this end, we define a Bayesian model on objects and relationships and derive an algorithm of variational inference, which propagates foregroundness between regions and region pairs to assign foreground regions that have more related objects and relationships. As the results of extensive experiments using a public benchmark for open-set SGG, the proposed technique outperformed previous methods, including the state-of-the-art thresholding technique, in the standard OSGDet metrics regardless of the SGG models with which the proposed technique was combined (e.g., +0.61 improvement in OSGDet@100 with the VCTree model).",Motoharu Sonogashira|Masaaki Iiyama|Yasutomo Kawanishi,|Shiga University|,1,Relationship-Aware Unknown Object Detection for Open-Set Scene Graph Generation,https://doi.org/10.1109/access.2024.3450908,0.53015756,FALSE,en,TRUE,gold,IEEE Access,journal,2024-01-01,article
https://openalex.org/W4398757510,,Fei Wang|Xianzhang Zhu|Xiaojian Liu|Yongjun Zhang|Yansheng Li,Jiangsu Changjiang Electronics Technology (China)|Hubei Water Resources Research Institute|Wuhan University|Hunan City University|Tianjin Research Institute of Water Transport Engineering|Wuhan University|Hubei Zhongshan Hospital|Wuhan University,7,Scene Graph-Aware Hierarchical Fusion Network for Remote Sensing Image Retrieval With Text Feedback,https://doi.org/10.1109/tgrs.2024.3404605,3.71110292,FALSE,en,FALSE,closed,IEEE Transactions on Geoscience and Remote Sensing,journal,2024-01-01,article
https://openalex.org/W4405146223,,Motoharu Sonogashira|Masaaki Iiyama|Yasutomo Kawanishi,,0,Self-Supervised Learning for Open-Set Scene Graph Generation by Unknown-Object and Relationship Pseudo-Labels,https://doi.org/10.2139/ssrn.5047895,,FALSE,en,TRUE,green,SSRN Electronic Journal,repository,2024-01-01,preprint
https://openalex.org/W4403291788,,Eunbin Hong|June-Seong Yi,Ewha Womans University Medical Center|Ewha Womans University Medical Center,0,Sequential Image Layout Prediction from Scene Graphs Via Domain-Specific Autonomous Named Entity Recognition for Post-Accident Simulation in Construction,https://doi.org/10.2139/ssrn.4983052,,FALSE,en,TRUE,green,SSRN Electronic Journal,repository,2024-01-01,preprint
https://openalex.org/W4393082953,,Dina Khattab|Mohamed F. Tolba|Howida A. Shedeed|Mohammad Essam,,0,SwinRelTR: An Efficient Single-Stage Scene Graph Generation Model for Low-Resolution Images,https://doi.org/10.1504/ijiei.2024.10063131,0.0,FALSE,en,FALSE,closed,International Journal of Intelligent Engineering Informatics,journal,2024-01-01,article
https://openalex.org/W4402592239,,Ke Su|Xingxing Zhang|Siyang Zhang|Jun Zhu|Bo Zhang,Tsinghua University|Tsinghua University|Nankai University|Tsinghua University|Tsinghua University,5,To Boost Zero-Shot Generalization for Embodied Reasoning With Vision-Language Pre-Training,https://doi.org/10.1109/tip.2024.3459800,2.6507878,FALSE,en,FALSE,closed,IEEE Transactions on Image Processing,journal,2024-01-01,article
https://openalex.org/W4401703229,,Zuoxu Wang|Zhijie Yan|Shufei Li|Jihong Liu,Oldham Council|Oldham Council|Oldham Council|Oldham Council,0,Vlm-Based Scene Graph Generation for Industrial Spatial Intelligence,https://doi.org/10.2139/ssrn.4926945,,FALSE,en,TRUE,green,SSRN Electronic Journal,repository,2024-01-01,preprint
https://openalex.org/W4394597032,,Yujie Zang|Yaochen Li|Yuan Gao|Yimou Guo|Wenneng Tang|Yanxue Li|Meklit Atlaw,Xi'an Jiaotong University|Xi'an Jiaotong University|Xi'an Jiaotong University|Xi'an Jiaotong University|Xi'an Jiaotong University|Xi'an Jiaotong University|Xi'an Jiaotong University,4,Refine and Redistribute: Multi-Domain Fusion and Dynamic Label Assignment for Unbiased Scene Graph Generation,https://doi.org/10.1109/wacv57701.2024.00135,2.12063024,FALSE,en,FALSE,closed,,,2024-01-03,article
https://openalex.org/W4390577554,"Citizen engagement and technology usage are two emerging trends driven by smart city initiatives.Typically, citizens report issues, such as broken roads, garbage dumps, etc. through web portals and mobile apps, in order for the government authorities to take appropriate actions.Several mediums -text, image, audio, video -are used to report these issues.Through a user study with 13 citizens and 3 authorities, we found that image is the most preferred medium to report civic issues.We aim Through this platform, we aim to achieve the following: Enable citizens to easily submit complaints, particularly via image-based submissions, Automate the complaint handling process, from Geo-fencing tagging to department allocation, Utilize AI, ML, and Data Science to enhance efficiency and accuracy in complaint resolution, Foster a sense of community engagement and participation in local governance.Method which implemented are Image Processing, Computer Vision and Adversarial Scene Graph Model: To capture and interpret complaint images, Geo-fencing: To precisely determine the location of each issue, AI and ML: To automatically categorize complaints and allocate them to the relevant departments, Data Science: To analyze patterns and trends, aiding in efficient resource allocation, User-Centric Design: Ensuring an intuitive and accessible interface for citizens.Citizens can report issues promptly through images, reducing reporting barriers.The Geo-fencing and automated department allocation result in faster response times.AI and ML technologies ensure accurate tagging and categorization of complaints.By harnessing the power of automation and technology, we enable citizens to actively participate in improving their local communities.This application not only streamlines the process of addressing complaints but also promotes transparency and civic engagement.",Sandeep Shukla|Tejas Bhandare|Rohit Bava|Ishan Ahirrao|Dnyanesh Walwadkar|Saurav Yadav|Pengfei Xu|Xiaojun Chang|Ling Guo|Poyao Huang|Xiaojiang Chen|G Alexander|Shanu Kumar|Anjali Singh|K Vishesh|Kandhari|D Keertika|Kumar Prasad|Ritesh Patil|Sagar Beldare|Prof|Soman Kp|Sowmya Vishvanathan,Guru Gobind Singh Medical College and Hospital|Guru Gobind Singh Medical College and Hospital|Guru Gobind Singh Medical College and Hospital|Guru Gobind Singh Medical College and Hospital|||||||||||||||||||,0,Civic Complaints Registration Application for Citizens of RuralUrban Areas,https://doi.org/10.56726/irjmets48054,,FALSE,en,TRUE,hybrid,International Research Journal of Modernization in Engineering Technology and Science,journal,2024-01-04,article
https://openalex.org/W4390813334,,Xianglu Zhu|Zhang Zhang|Wei Wang|Zilei Wang,University of Science and Technology of China|Institute of Automation|Chinese Academy of Sciences|Chinese Academy of Sciences|Institute of Automation|University of Chinese Academy of Sciences|Chinese Academy of Sciences|Institute of Automation|University of Science and Technology of China,2,Comprehensive Relation Modelling for Image Paragraph Generation,https://doi.org/10.1007/s11633-022-1408-2,1.06031512,FALSE,en,FALSE,closed,Machine Intelligence Research,journal,2024-01-12,article
https://openalex.org/W4390810680,,Han Yue|Hangbin Wu|Ville Lehtola|Junyi Wei|Chun Liu,Tongji University|Tongji University|University of Twente|Tongji University|Tongji University,5,Indoor functional subspace division from point clouds based on graph neural network,https://doi.org/10.1016/j.jag.2024.103656,2.69987214,FALSE,en,TRUE,gold,International Journal of Applied Earth Observation and Geoinformation,journal,2024-01-12,article
https://openalex.org/W4395015903,,Bing Li|Guangheng Jia|Xiyan Gao|Can Ma,Institute of Information Engineering|University of Chinese Academy of Sciences|Chinese Academy of Sciences|Chinese Academy of Sciences|Institute of Information Engineering|Institute of Information Engineering|Chinese Academy of Sciences|Institute of Information Engineering|Chinese Academy of Sciences,1,Multidimensional Semantic Augmented Visual Storytelling,https://doi.org/10.1109/nnice61279.2024.10498935,0.53015756,FALSE,en,FALSE,closed,,,2024-01-19,article
https://openalex.org/W4392151910,,Songqing Cai|Xiaojun Chang|Shengsheng Ren,Southeast University|Southeast University|Southeast University|Southeast University|Southeast University,0,Pair with prior queries for end-to-end scene graph generation,https://doi.org/10.1049/icp.2024.0068,0.0,FALSE,en,FALSE,closed,IET conference proceedings.,journal,2024-01-26,article
https://openalex.org/W4391483642,"The Internet of Vehicles (IoV) has transfigured transportation with connected vehicles, smart infrastructure, and self-driving cars. Road collisions and accidents are still a problem for road safety. This review of the literature discusses the prediction of IoV accidents and collisions as well as the detection of hazards using data mining, deep learning, and machine learning techniques. It describes the most recent developments to these methods and how they enhanced IoV safety. The article starts off by going over data collection, data quality, and the ever-changing nature of IoV traffic scenarios. What follows is a detailed breakdown of the ML, DL, and DM methods used in IoV safety applications. Convolutional neural networks, artificial neural networks, recurrent neural networks, support vector machines, and decision trees. As examples of real-world applications and case studies, intelligent accident prediction models, driver attention forecasting, traffic congestion forecasting, spatiotemporal analysis in autonomous vehicles, scene-graph embedding, and V2P collision risk alerts are discussed. The goal of this review is to give readers a comprehensive overview of the cutting-edge methods enhancing IoV accident prediction, collision avoidance, and hazard detection.",Ajay Kumar Manchala|V. Vijaya Kishore,,4,Advancements in Machine Learning and Data Mining Techniques for Collision Prediction and Hazard Detection in Internet of Vehicles,https://doi.org/10.24271/psr.2023.403821.1342,2.15989771,FALSE,en,TRUE,diamond,passer,journal,2024-01-30,article
https://openalex.org/W4391341382,"Abstract An image is an important form of information transmission and contains a lot of effective information. With the explosive growth of multi-modal data represented by pictures, the multi-modal knowledge graph (MMKG) has become an effective means to manage and apply. It is necessary to obtain comprehensive and effective image data to construct a high-quality MMKG. This research focuses on the construction of MMKG, mainly from the analysis of graph structure and characteristics. Firstly, the structural characteristics and elements composition of the MMKG are described. Then, introduced the existing forms of image entity recognition, multi-features capture, scene graphs generation respectively, and description text generation is in the graph and summarized the main mining methods. Finally, analyzed several applications of image data in a commodity multi-modal knowledge graph.",Jinghui Peng|Xinyu Hu|Jian Yang|Yi Li,Anhui Polytechnic University|Sun Yat-sen University|Xidian University|Anhui Polytechnic University,0,Research on Image Information Mining Needed for Multi-modal Knowledge Graph Construction and Application,https://doi.org/10.21203/rs.3.rs-3892969/v1,,FALSE,en,TRUE,green,Research Square (Research Square),repository,2024-01-30,preprint
https://openalex.org/W4391751628,"Simulation is a system to imitate the operation of various kinds of real-world facilities or processes for training, behavior study or entertainment purposes. Some implementation of fighter aircraft simulation is done sequentially using FORTRAN and MATLAB / Simulink. On the other hand, a multicore (parallel) computer system has been in the personal computer environment, so the multicore computing paradigm should be considered. Intel Threading Building Blocks (TBB) library need to be utilized in multicore programming. This paper discusses the design and implementation of real-time nonlinear dynamic simulation of fighter aircraft on multicore processor. The aircraft model used in this study is F16 data. Implementation of the simulation code is written in C++ with Intel TBB, OpenGL, Open Scene Graph, and Open Dynamic Engine library. The testing results of non-linear dynamics simulation of fighter aircraft show that the speedup on an Intel Core 2 Duo processor is 1.1776 x for elevator doublet input (short period), while speedup on an Intel i7 is 1.5405 x for rudder doublet input (dutch roll). Key Words: multicore computing, real-time simulation, nonlinear dynamic, fighter aircraft model",Muhammad Faris Fathoni|Sutiyo Sutiyo,Telkom University|Telkom University,1,DESIGN AND IMPLEMENTATION OF REAL-TIME NON-LINEAR DYNAMIC SIMULATION OF FIGHTER AIRCRAFT ON MULTICORE PROCESSOR,https://doi.org/10.25124/tektrika.v8i2.6902,0.63620868,FALSE,en,TRUE,diamond,TEKTRIKA - Jurnal Penelitian dan Pengembangan Telekomunikasi Kendali Komputer Elektrik dan Elektronika,journal,2024-02-02,article
https://openalex.org/W4392007279,,Minglang Qiao|Mai Xu|Lai Jiang|Peng Lei|Shi-Jie Wen|Yunjin Chen|Leonid Sigal,Beihang University|Beihang University|Beihang University|Beihang University|Beihang University|Alibaba Group (China)|University of British Columbia,42,HyperSOR: Context-Aware Graph Hypernetwork for Salient Object Ranking,https://doi.org/10.1109/tpami.2024.3368158,22.2666175,FALSE,en,FALSE,closed,IEEE Transactions on Pattern Analysis and Machine Intelligence,journal,2024-02-21,article
https://openalex.org/W4402594620,,Chikwendu Ijeoma Amuche|Xiaoling Zhang|Chiagoziem C. Ukwuoma|Isaac Adjei-Mensah|Assila Abdallah Abdou|Chikwendu Chinyere Onyedikachi,University of Electronic Science and Technology of China|University of Electronic Science and Technology of China|Chengdu University of Technology|University of Electronic Science and Technology of China|University of Electronic Science and Technology of China|University of Port Harcourt,1,Rethinking Image Generation From Scene Graphs With Attention Mechanism,https://doi.org/10.1109/cccis63483.2024.00021,0.53015756,FALSE,en,FALSE,closed,,,2024-02-27,article
https://openalex.org/W4399119664,,Jia Liu|Renjie Zhang|Taishi Sawabe|Yuichiro Fujimoto|Masayuki Kanbara|Hirokazu Kato,Nara Institute of Science and Technology|Nara Institute of Science and Technology|Nara Institute of Science and Technology|Nara Institute of Science and Technology|Nara Institute of Science and Technology|Nara Institute of Science and Technology,0,Integrating Spatial Design with Reality: An AR Game Scene Generation Approach,https://doi.org/10.1109/vrw62533.2024.00154,0.0,FALSE,en,FALSE,closed,,,2024-03-16,article
https://openalex.org/W4392904155,,Nanhao Liang|Yong Liu|Wenfang Sun|Yingwei Xia|Fan Wang,University of Science and Technology of China|Chinese Academy of Sciences|Hefei Institutes of Physical Science|Chinese Academy of Sciences|Hefei Institutes of Physical Science|University of Science and Technology of China|Chinese Academy of Sciences|Hefei Institutes of Physical Science|Chinese Academy of Sciences|Hefei Institutes of Physical Science|Chinese Academy of Sciences|Hefei Institutes of Physical Science,4,CKT-RCM: Clip-Based Knowledge Transfer and Relational Context Mining for Unbiased Panoptic Scene Graph Generation,https://doi.org/10.1109/icassp48485.2024.10446810,2.12063024,FALSE,en,FALSE,closed,,,2024-03-18,article
https://openalex.org/W4392903152,,Aakansha Mishra|Miriyala Srinivas Soumitri|Vikram N Rajendiran,Samsung (India)|Samsung (India)|Samsung (India),0,Learning Representations from Explainable and Connectionist Approaches for Visual Question Answering,https://doi.org/10.1109/icassp48485.2024.10447493,0.0,FALSE,en,FALSE,closed,,,2024-03-18,article
https://openalex.org/W4392902969,,Jiapeng Liu|Chengyang Fang|Liang Li|Bing Li|Dayong Hu|Can Ma,University of Chinese Academy of Sciences|Institute of Information Engineering|Chinese Academy of Sciences|Chinese Academy of Sciences|Institute of Information Engineering|University of Chinese Academy of Sciences|Institute of Information Engineering|Chinese Academy of Sciences|Institute of Information Engineering|Chinese Academy of Sciences||Institute of Information Engineering|Chinese Academy of Sciences,2,Prompting Large Language Models with Fine-Grained Visual Relations from Scene Graph for Visual Question Answering,https://doi.org/10.1109/icassp48485.2024.10448321,1.06031512,FALSE,en,FALSE,closed,,,2024-03-18,article
https://openalex.org/W4392902645,,Minxi Yang|Dahua Gao|Feng Xie|Jiaxuan Li|Xiaodan Song|Guangming Shi,Xidian University|Xidian University|Xidian University|Xidian University|Xidian University|Peng Cheng Laboratory|Xidian University,13,SG2SC: A Generative Semantic Communication Framework for Scene Understanding-Oriented Image Transmission,https://doi.org/10.1109/icassp48485.2024.10446594,8.30412114,FALSE,en,FALSE,closed,,,2024-03-18,article
https://openalex.org/W4392909069,,Yujie Zang|Yaochen Li|Luguang Cao|Ruitao Lu,Xi'an Jiaotong University|Xi'an Jiaotong University||PLA Rocket Force University of Engineering,0,Template-Guided Data Augmentation for Unbiased Scene Graph Generation,https://doi.org/10.1109/icassp48485.2024.10448033,0.0,FALSE,en,FALSE,closed,,,2024-03-18,article
https://openalex.org/W4393117268,"Abstract Image captioning is an important task for understanding images. Recently, many studies have used tags to build alignments between image information and language information. However, existing methods ignore the problem that simple semantic tags have difficulty expressing the detailed semantics for different image contents. Therefore, the authors propose a tag‐inferring and tag‐guided Transformer for image captioning to generate fine‐grained captions. First, a tag‐inferring encoder is proposed, which uses the tags extracted by the scene graph model to infer tags with deeper semantic information. Then, with the obtained deep tag information, a tag‐guided decoder that includes short‐term attention to improve the features of words in the sentence and gated cross‐modal attention to combine image features, tag features and language features to produce informative semantic features is proposed. Finally, the word probability distribution of all positions in the sequence is calculated to generate descriptions for the image. The experiments demonstrate that the authors’ method can combine tags to obtain precise captions and that it achieves competitive performance with a 40.6% BLEU‐4 score and 135.3% CIDEr score on the MSCOCO data set.",Yaohua Yi|Yinkai Liang|Dezhu Kong|Ziwei Tang|Jibing Peng,State Key Laboratory of Information Engineering in Surveying Mapping and Remote Sensing|Wuhan University|State Key Laboratory of Information Engineering in Surveying Mapping and Remote Sensing|Wuhan University|Zhuhai Institute of Advanced Technology|Wuhan University|State Key Laboratory of Information Engineering in Surveying Mapping and Remote Sensing|Wuhan University|Zhuhai Institute of Advanced Technology,1,Tag‐inferring and tag‐guided Transformer for image captioning,https://doi.org/10.1049/cvi2.12280,0.53015756,FALSE,en,TRUE,gold,IET Computer Vision,journal,2024-03-22,article
https://openalex.org/W4393158786,"Graph neural networks (GNNs) has demonstrated its capabilities in the field of scene graph generation (SGG) by updating node representations from neighboring nodes. Actually it can be viewed as a form of low-pass filter in the spatial domain, which smooths node feature representation and retains commonalities among nodes. However, spatial GNNs does not work well in the case of heterophilic SGG in which fine-grained predicates are always connected to a large number of coarse-grained predicates. Blind smoothing undermines the discriminative information of the fine-grained predicates, resulting in failure to predict them accurately. To address the heterophily, our key idea is to design tailored filters by wavelet transform from the spectral domain. First, we prove rigorously that when the heterophily on the scene graph increases, the spectral energy gradually shifts towards the high-frequency part. Inspired by this observation, we subsequently propose the Kumaraswamy Wavelet Graph Neural Network (KWGNN). KWGNN leverages complementary multi-group Kumaraswamy wavelets to cover all frequency bands. Finally, KWGNN adaptively generates band-pass filters and then integrates the filtering results to better accommodate varying levels of smoothness on the graph. Comprehensive experiments on the Visual Genome and Open Images datasets show that our method achieves state-of-the-art performance.",Lianggangxu Chen|Youqi Song|Shaohui Lin|Changbo Wang|Gaoqi He,East China Normal University|East China Normal University|East China Normal University|East China Normal University|East China Normal University,3,Kumaraswamy Wavelet for Heterophilic Scene Graph Generation,https://doi.org/10.1609/aaai.v38i2.27875,0.58187213,FALSE,en,TRUE,diamond,Proceedings of the AAAI Conference on Artificial Intelligence,conference,2024-03-24,article
https://openalex.org/W4393158871,"In the domain of scene graph generation, modeling commonsense as a single-prototype representation has been typically employed to facilitate the recognition of infrequent predicates. However, a fundamental challenge lies in the large intra-class variations of the visual appearance of predicates, resulting in subclasses within a predicate class. Such a challenge typically leads to the problem of misclassifying diverse predicates due to the rough predicate space clustering. In this paper, inspired by cognitive science, we maintain multi-prototype representations for each predicate class, which can accurately find the multiple class centers of the predicate space. Technically, we propose a novel multi-prototype learning framework consisting of three main steps: prototype-predicate matching, prototype updating, and prototype space optimization. We first design a triple-level optimal transport to match each predicate feature within the same class to a specific prototype. In addition, the prototypes are updated using momentum updating to find the class centers according to the matching results. Finally, we enhance the inter-class separability of the prototype space through iterations of the inter-class separability loss and intra-class compactness loss. Extensive evaluations demonstrate that our approach significantly outperforms state-of-the-art methods on the Visual Genome dataset.",Lianggangxu Chen|Youqi Song|Yiqing Cai|Jiale Lu|Yang Li|Yuan Xie|Changbo Wang|Gaoqi He,East China Normal University|East China Normal University|East China Normal University|East China Normal University|East China Normal University|East China Normal University|East China Normal University|East China Normal University,9,Multi-Prototype Space Learning for Commonsense-Based Scene Graph Generation,https://doi.org/10.1609/aaai.v38i2.27874,5.47297299,FALSE,en,TRUE,diamond,Proceedings of the AAAI Conference on Artificial Intelligence,conference,2024-03-24,article
https://openalex.org/W4393159695,"Multimodal event causality reasoning aims to recognize the causal relations based on the given events and accompanying image pairs, requiring the model to have a comprehensive grasp of visual and textual information. However, existing studies fail to effectively model the relations of the objects within the image and capture the object interactions across the image pair, resulting in an insufficient understanding of visual information by the model. To address these issues, we propose a Scene Graph Enhanced Interaction Network (SEIN) in this paper, which can leverage the interactions of the generated scene graph for multimodal event causality reasoning. Specifically, the proposed method adopts a graph convolutional network to model the objects and their relations derived from the scene graph structure, empowering the model to exploit the rich structural and semantic information in the image adequately. To capture the object interactions between the two images, we design an optimal transport-based alignment strategy to match the objects across the images, which could help the model recognize changes in visual information and facilitate causality reasoning. In addition, we introduce a cross-modal fusion module to combine textual and visual features for causality prediction. Experimental results indicate that the proposed SEIN outperforms state-of-the-art methods on the Vis-Causal dataset.",Jin‐Tao Liu|Kaiwen Wei|Chenglong Liu,University of Chinese Academy of Sciences|University of Chinese Academy of Sciences|University of Chinese Academy of Sciences,1,Multimodal Event Causality Reasoning with Scene Graph Enhanced Interaction Network,https://doi.org/10.1609/aaai.v38i8.28724,0.24251649,FALSE,en,TRUE,diamond,Proceedings of the AAAI Conference on Artificial Intelligence,conference,2024-03-24,article
https://openalex.org/W4393148852,"Image generation tasks have achieved remarkable performance using large-scale diffusion models. However, these models are limited to capturing the abstract relations (viz., interactions excluding positional relations) among multiple entities of complex scene graphs. Two main problems exist: 1) fail to depict more concise and accurate interactions via abstract relations; 2) fail to generate complete entities. To address that, we propose a novel Relation-aware Compositional Contrastive Control Diffusion method, dubbed as R3CD, that leverages large-scale diffusion models to learn abstract interactions from scene graphs. Herein, a scene graph transformer based on node and edge encoding is first designed to perceive both local and global information from input scene graphs, whose embeddings are initialized by a T5 model. Then a joint contrastive loss based on attention maps and denoising steps is developed to control the diffusion model to understand and further generate images, whose spatial structures and interaction features are consistent with a priori relation. Extensive experiments are conducted on two datasets: Visual Genome and COCO-Stuff, and demonstrate that the proposal outperforms existing models both in quantitative and qualitative metrics to generate more realistic and diverse images according to different scene graph specifications.",Jinxiu Liu|Qi Liu,South China University of Technology|South China University of Technology,12,R3CD: Scene Graph to Image Generation with Relation-Aware Compositional Contrastive Control Diffusion,https://doi.org/10.1609/aaai.v38i4.28155,2.32748853,FALSE,en,TRUE,diamond,Proceedings of the AAAI Conference on Artificial Intelligence,conference,2024-03-24,article
https://openalex.org/W4393159162,"Referring 3D instance segmentation is a challenging task aimed at accurately segmenting a target instance within a 3D scene based on a given referring expression. However, previous methods have overlooked the distinct roles played by different words in referring expressions. Additionally, they have failed to incorporate the positional relationship within referring expressions with the spatial correlations in 3D scenes. To alleviate these issues, we present a novel model called X-RefSeg3D, which constructs a cross-modal graph for the input 3D scene and unites textual and spatial relationships for reasoning via graph neural networks. Our approach begins by capturing object-specific text features, which are then fused with the instance features to construct a comprehensive cross-modal scene graph. Subsequently, we integrate the obtained cross-modal features into graph neural networks, leveraging the K-nearest algorithm to derive explicit instructions from expressions and factual relationships in scenes. This enables the effective capture of higher-order relationships among instances, thereby enhancing feature fusion and facilitating reasoning. Finally, the refined feature undergoes a matching module to compute the ultimate matching score. Experimental results on ScanRefer demonstrate the effectiveness of our method, surpassing previous approaches by a substantial margin of +3.67% in terms of mIOU.",Zhipeng Qian|Yiwei Ma|Jiayi Ji|Xiaoshuai Sun,Xiamen University|Xiamen University|Xiamen University|Xiamen University,12,X-RefSeg3D: Enhancing Referring 3D Instance Segmentation via Structured Cross-Modal Graph Neural Networks,https://doi.org/10.1609/aaai.v38i5.28254,2.32748853,FALSE,en,TRUE,diamond,Proceedings of the AAAI Conference on Artificial Intelligence,conference,2024-03-24,article
https://openalex.org/W4394564148,,Xuewei Li|Guangcong Zheng|Yunlong Yu|Naye Ji|Xi Li,Zhejiang University of Science and Technology|Zhejiang University of Science and Technology|Zhejiang University|Communication University of Zhejiang|Zhejiang University of Science and Technology,8,Relationship-Incremental Scene Graph Generation by a Divide-and-Conquer Pipeline With Feature Adapter,https://doi.org/10.1109/tip.2024.3384096,4.24126048,FALSE,en,FALSE,closed,IEEE Transactions on Image Processing,journal,2024-04-08,article
https://openalex.org/W4394769520,"Autonomous vehicles (AVs) must be able to operate in a wide range of scenarios including those in the long tail distribution that include rare but safety-critical events. The collection of sensor input and expected output datasets from such scenarios is crucial for the development and testing of such systems. Yet, approaches to quantify the extent to which a dataset covers test specifications that capture critical scenarios remain limited in their ability to discriminate between inputs that lead to distinct behaviors, and to render interpretations that are relevant to AV domain experts. To address this challenge, we introduce S3C, a framework that abstracts sensor inputs to coverage domains that account for the spatial semantics of a scene. The approach leverages scene graphs to produce a sensor-independent abstraction of the AV environment that is interpretable and discriminating. We provide an implementation of the approach and a study for camera-based autonomous vehicles operating in simulation. The findings show that S3C outperforms existing techniques in discriminating among classes of inputs that cause failures, and offers spatial interpretations that can explain to what extent a dataset covers a test specification. Further exploration of S3C with open datasets complements the study findings, revealing the potential and shortcomings of deploying the approach in the wild.",Trey Woodlief|Felipe Toledo|Sebastian Elbaum|Matthew B. Dwyer,University of Virginia|University of Virginia|University of Virginia|University of Virginia,7,S3C: Spatial Semantic Scene Coverage for Autonomous Vehicles,https://doi.org/10.1145/3597503.3639178,4.47144984,FALSE,en,TRUE,gold,,,2024-04-12,article
https://openalex.org/W4394841770,,Ruonan Zhang|Gaoyun An|Yiqing Hao|Dapeng Wu,Beijing Jiaotong University|Beijing Jiaotong University|Beijing Jiaotong University|City University of Hong Kong,5,Bridging Visual and Textual Semantics: Towards Consistency for Unbiased Scene Graph Generation,https://doi.org/10.1109/tpami.2024.3389030,2.6507878,FALSE,en,FALSE,closed,IEEE Transactions on Pattern Analysis and Machine Intelligence,journal,2024-04-16,article
https://openalex.org/W4399307640,,Ting Liu|Dong Sun|Chongke Bi|Yi Sun|Siming Chen,Fudan University|Nitto (United States)|Tianjin University|Fudan University|Fudan University,1,Dynamic-Scene-Graph-Supported Visual Understanding of Autonomous Driving Scenarios,https://doi.org/10.1109/pacificvis60374.2024.00018,0.53015756,FALSE,en,FALSE,closed,,,2024-04-23,article
https://openalex.org/W4395465371,"Abstract The task of object goal navigation is to drive an embodied agent to find the location of a given target only using visual observation. The mapping from visual perception of observation determines the navigation actions. Heterogeneous relationships in the observation are the essential part of the scene graph, which can guide the agent to find the target more easily. In this work, we propose a novel Heterogeneous Zone Graph Visual Transformer formulation for graph representation and visual perception. It consists of two key ideas: (1) Heterogeneous Zone Graph (HZG) that explores the heterogeneous target-related zones graph and spatial information. It allows the agent to navigate efficiently. (2) Relation-wise Transformer Network (RTNet) that transforms the relationship between previously observed objects and navigation actions. RTNet extracts rich nodes and edges features as pays more attention to the target-related zone. We model self-attention on the node-to-node encoder and cross-attention on the edge-to-node decoder. We evaluate our methods on the AI2THOR dataset and show superior navigation performance. Code and datasets can be found in https://github.com/zhoukang12321/RTNet_VN_2023 .",Yu He|Kang Zhou,Huanghuai University|Henan Provincial Center for Disease Control and Prevention|City University of Hong Kong|Wuhan University,3,Relation-wise transformer network and reinforcement learning for visual navigation,https://doi.org/10.1007/s00521-024-09693-z,1.59047268,FALSE,en,TRUE,hybrid,Neural Computing and Applications,journal,2024-04-25,article
https://openalex.org/W4396641353,,Yinjia Guo|Yanyan Chen|Xin Gu|Jifu Guo|Shuyan Zheng|Yuntong Zhou,Beijing University of Technology|Beijing University of Technology|Beijing University of Technology|Beijing Transportation Research Center|Beijing University of Technology|Beijing University of Technology,7,Dynamic traffic graph based risk assessment of multivehicle lane change interaction scenarios,https://doi.org/10.1016/j.physa.2024.129791,2.79503942,FALSE,en,FALSE,closed,Physica A Statistical Mechanics and its Applications,journal,2024-05-03,article
https://openalex.org/W4396623002,"This paper addresses the significant challenges in 3D Semantic Scene Graph (3DSSG) prediction, essential for understanding complex 3D environments. Traditional approaches, primarily using PointNet and Graph Convolutional Networks, struggle with effectively extracting multi-grained features from intricate 3D scenes, largely due to a focus on global scene processing and single-scale feature extraction. To overcome these limitations, we introduce Granular3D, a novel approach that shifts the focus toward multi-granularity analysis by predicting relation triplets from specific sub-scenes. One key is the Adaptive Instance Enveloping Method (AIEM), which establishes an approximate envelope structure around irregular instances, providing shape-adaptive local point cloud sampling, thereby comprehensively covering the contextual environments of instances. Moreover, Granular3D incorporates a Hierarchical Dual-Stage Network (HDSN), which differentiates and processes features of instances and their pairs at varying scales, leading to a targeted prediction of instance categories and their relationships. To advance the perception of sub-scene in HDSN, we design a Gather Point Transformer structure (GaPT) that enables the combinatorial interaction of local information from multiple point cloud sets, achieving a more comprehensive local contextual feature extraction. Extensive evaluations on the challenging 3DSSG benchmark demonstrate that our methods provide substantial improvements, establishing a new state-of-the-art in 3DSSG prediction, boosting the top-50 triplet accuracy by +2.8%.",Kaixiang Huang|Jingru Yang|Jin Wang|Shengfeng He|Zhan Wang|Haiyan He|Qifeng Zhang|Guodong Lu,Zhejiang University|Zhejiang University|Zhejiang University|Singapore Management University|||Zhejiang University|Zhejiang University,1,Granular3D: Delving into multi-granularity 3D scene graph prediction,https://doi.org/10.1016/j.patcog.2024.110562,0.72079647,FALSE,en,TRUE,hybrid,Pattern Recognition,journal,2024-05-03,article
https://openalex.org/W4396744838,"<title>Abstract</title> As the demand for artificial intelligence robots and cognitive agents increases, it becomes essential for these agents to comprehend previous encounters and respond to inquiries based on their past experiences. In essence, they need to maintain their memories in an episodic manner. This paper presents a novel approach to address this demand by leveraging the real-life experiences of robots to enrich their knowledge base. To achieve this goal, we employ diverse artificial intelligence techniques, including computer vision, multimodal cross embeddings, speech processing, and generative AI. These methods are utilised to establish a knowledge base that functions as memories for an agent, enabling it to maintain a memory akin to that of a human.To ensure comprehensive memory retention, an agent encounters diverse scenarios such as interacting with individuals, observing conversations, and visiting various locations. To maintain a robust visual and linguistic knowledge base encompassing these experiences, we employ techniques like scene graphs, along with the aforementioned AI methodologies. Existing approaches that involve understanding language and vision used in problem statements such as video question answering, dialogue understanding, or world understanding often overlook the temporal order in which events are observed by the agent or may be restricted to the set of characters or the world in which it has been trained. They struggle to effectively retrieve memories and generate meaningful answers based on this chronological context or in cases where we may rely on a number of past experiences which may reason an event that happened in the future. So, in our study, we've worked on building a solid knowledge base and a way for an agent to remember and link events, just like people do.In conclusion, our work aims to make AI more like humans by helping agents remember and understand events better. This could lead to smarter AI systems that adapt well to different situations in the real world.",Shweta Singh|Srinivasan Seshadri,"International Institute of Information Technology, Hyderabad|Amrita Vishwa Vidyapeetham",2,Episodic Question Answering for Cognitive Agents,https://doi.org/10.21203/rs.3.rs-4351479/v1,,FALSE,en,TRUE,green,Research Square (Research Square),repository,2024-05-08,preprint
https://openalex.org/W4396829552,"Abstract. A 3D scene graph is a compact and explicit representation in scene analysis. In today’s 3D scene graph prediction methods, the feature encoding method of nodes and edges is relatively simple, which essentially hinders the network from fully learning 3D point cloud features. In this paper, we propose a 3D scene graph task framework that fully expresses node and edge features, trying to meet the requirements of fully utilizing point cloud features to achieve high-precision prediction. Experimental results show that with the help of the new representation method, the prediction performance of 3D scene graphs has been significantly improved.",Han Du|Benhe Cai|Xiaoming Li|Weixi Wang|Shengjun Tang,Guangdong-Hongkong-Macau Joint Laboratory of Collaborative Innovation for Environmental Quality|Shenzhen University|Guangdong-Hongkong-Macau Joint Laboratory of Collaborative Innovation for Environmental Quality|Shenzhen University|Guangdong-Hongkong-Macau Joint Laboratory of Collaborative Innovation for Environmental Quality|Shenzhen University|Shenzhen University|Guangdong-Hongkong-Macau Joint Laboratory of Collaborative Innovation for Environmental Quality|Shenzhen University|Guangdong-Hongkong-Macau Joint Laboratory of Collaborative Innovation for Environmental Quality,0,Method for Generating Indoor 3D Scene Graphs Based on Instance Features and Relationship Encoding,https://doi.org/10.5194/isprs-archives-xlviii-1-2024-135-2024,0.0,FALSE,en,TRUE,diamond,"The international archives of the photogrammetry, remote sensing and spatial information sciences/International archives of the photogrammetry, remote sensing and spatial information sciences",journal,2024-05-10,article
https://openalex.org/W4396827020,"Image-text retrieval, a fundamental cross-modal task, performs similarity reasoning for images and texts. The primary challenge for image-text retrieval is cross-modal semantic heterogeneity, where the semantic features of visual and textual modalities are rich but distinct. Scene graph is an effective representation for images and texts as it explicitly models objects and their relations. Existing scene graph based methods have not fully taken the features regarding various granularities implicit in scene graph into consideration (e.g., triplets), the inadequate feature matching incurs the absence of non-trivial semantic information (e.g., inner relations among triplets). Therefore, we propose a S emantic-Consistency E nhanced M ulti-Level Scene Graph Matching (SEMScene) network, which exploits the semantic relevance between visual and textual scene graphs from fine-grained to coarse-grained. Firstly, under the scene graph representation, we perform feature matching including low-level node matching, mid-level semantic triplet matching, and high-level holistic scene graph matching. Secondly, to enhance the semantic-consistency for object-fused triplets carrying key correlation information, we propose a dual-step constraint mechanism in mid-level matching. Thirdly, to guide the model to learn the semantic-consistency of matched image-text pairs, we devise effective loss functions for each stage of the dual-step constraint. Comprehensive experiments on Flickr30K and MS-COCO datasets demonstrate that SEMScene achieves state-of-the-art performances with significant improvements.",Yuankun Liu|Xiang Yuan|Haochen Li|Zhijie Tan|J.-B. Huang|J. Xiao|Weiping Li|Tong Mo,North University of China|Peking University|Peking University|Peking University|Peking University|Peking University|Peking University|Peking University,5,SEMScene: Semantic-Consistency Enhanced Multi-Level Scene Graph Matching for Image-Text Retrieval,https://doi.org/10.1145/3664816,2.6507878,FALSE,en,TRUE,bronze,ACM Transactions on Multimedia Computing Communications and Applications,journal,2024-05-11,article
https://openalex.org/W4401414469,,Chao Wei|Zhidong Deng,Tsinghua University|Tsinghua University,0,Incorporating Scene Graphs into Pre-trained Vision-Language Models for Multimodal Open-vocabulary Action Recognition,https://doi.org/10.1109/icra57147.2024.10611454,0.0,FALSE,en,FALSE,closed,,,2024-05-13,article
https://openalex.org/W4401416728,,Felipe Toledo|Trey Woodlief|Sebastian Elbaum|Matthew B. Dwyer,University of Virginia|University of Virginia|University of Virginia|University of Virginia,7,Specifying and Monitoring Safe Driving Properties with Scene Graphs,https://doi.org/10.1109/icra57147.2024.10610973,2.79503942,FALSE,en,FALSE,closed,,,2024-05-13,article
https://openalex.org/W4396889120,"Combining deep learning and common sense knowledge via neurosymbolic integration is essential for semantically rich scene representation and intuitive visual reasoning. This survey paper delves into data- and knowledge-driven scene representation and visual reasoning approaches based on deep learning, common sense knowledge and neurosymbolic integration. It explores how scene graph generation, a process that detects and analyses objects, visual relationships and attributes in scenes, serves as a symbolic scene representation. This representation forms the basis for higher-level visual reasoning tasks such as visual question answering, image captioning, image retrieval, image generation, and multimodal event processing. Infusing common sense knowledge, particularly through the use of heterogeneous knowledge graphs, improves the accuracy, expressiveness and reasoning ability of the representation and allows for intuitive downstream reasoning. Neurosymbolic integration in these approaches ranges from loose to tight coupling of neural and symbolic components. The paper reviews and categorises the state-of-the-art knowledge-based neurosymbolic approaches for scene representation based on the types of deep learning architecture, common sense knowledge source and neurosymbolic integration used. The paper also discusses the visual reasoning tasks, datasets, evaluation metrics, key challenges and future directions, providing a comprehensive review of this research area and motivating further research into knowledge-enhanced and data-driven neurosymbolic scene representation and visual reasoning.",Muhammad Jaleed Khan|Filip Ilievski|John G. Breslin|Edward Curry,Ollscoil na Gaillimhe – University of Galway|University of Southern California|Southern California University for Professional Studies|Southern States University|Ollscoil na Gaillimhe – University of Galway|Ollscoil na Gaillimhe – University of Galway,5,A survey of neurosymbolic visual reasoning with scene graphs and common sense knowledge,https://doi.org/10.3233/nai-240719,2.6507878,FALSE,en,TRUE,hybrid,Neurosymbolic Artificial Intelligence,journal,2024-05-14,article
https://openalex.org/W4399435792,"multi-modal topic models strive to integrate semantic information from multi-modal data to generate more precise topics. Topic modeling methods encounter challenges in terms of topic diversity and effectiveness. To address this issue, the majority of current approaches focus on modeling the correlation among numerous multi-modal sources. Nevertheless, little emphasis has been placed on fine-grained feature representation and structured knowledge. In this regard, we propose a fine-grained Prompt representation method. Specifically, we adopt a dual-stream structure where a pre-trained language model and an image model are parallelly combined to construct a multi-modal model. We then enhance the structured representation by integrating fine-grained scene graph knowledge through a Knowledge-Enhanced Encoder, which is constructed based on the scene graph. To validate the effectiveness of the proposed framework, we significantly improve topic quality (such as coherence and diversity) using the aforementioned approach. On publicly available datasets, our approach outperforms state-of-the-art multi-modal topic models respectively.",Hongzhang Mu|S. F. Zhang|Hongbo Xu,Institute of Information Engineering|University of Chinese Academy of Sciences|Institute of Information Engineering|University of Chinese Academy of Sciences|Institute of Information Engineering|Chinese Academy of Sciences,0,A Knowledge-Driven Approach to Enhance Topic Modeling with Multi-Modal Representation Learning,https://doi.org/10.1145/3652583.3658069,0.0,FALSE,en,TRUE,gold,,,2024-05-30,article
https://openalex.org/W4400645513,,Hayato Suzuki|Kota Shimomura|Tsubasa Hirakawa|Takayoshi Yamashita|Hironobu Fujiyoshi|Shota Okubo|Takuya Nanri|Wang Siyuan,Chubu University|Chubu University|Chubu University|Chubu University|Chubu University|Nissan (Japan)|Nissan (Japan)|Nissan (Japan),0,Human-like Guidance by Generating Navigation Using Spatial-Temporal Scene Graph,https://doi.org/10.1109/iv55156.2024.10588867,0.0,FALSE,en,FALSE,closed,,,2024-06-02,article
https://openalex.org/W4400645415,,Yafu Tian|Alexander Carballo|Ruifeng Li|Simon Thompson|Kazuya Takeda,,3,RSG-Search Plus:An Advanced Traffic Scene Retrieval Methods based on Road Scene Graph,https://doi.org/10.1109/iv55156.2024.10588840,2.1375128,FALSE,en,FALSE,closed,,,2024-06-02,article
https://openalex.org/W4399437878,,Ryosuke Miyake|Tetsu Matsukawa|Einoshin Suzuki,Kyushu University|Kyushu University|Kyushu University,3,Image Generation from Hyper Scene Graph with Multiple Types of Trinomial Hyperedges,https://doi.org/10.1007/s42979-024-02791-8,1.59047268,FALSE,en,FALSE,closed,SN Computer Science,journal,2024-06-07,article
https://openalex.org/W4399424243,,David A. Handelman|Corban G. Rivera|William Paul|Andrew R. Badger|Emma Holmes|Martha Cervantes|Bethany Kemp|Erin C. Butler,Johns Hopkins University Applied Physics Laboratory|Johns Hopkins University Applied Physics Laboratory|Johns Hopkins University Applied Physics Laboratory|Johns Hopkins University Applied Physics Laboratory|Johns Hopkins University Applied Physics Laboratory|Johns Hopkins University Applied Physics Laboratory|Johns Hopkins University Applied Physics Laboratory|Johns Hopkins University Applied Physics Laboratory,1,Leveraging foundation models for scene understanding in human-robot teaming,https://doi.org/10.1117/12.3013906,1.31910614,FALSE,en,FALSE,closed,,,2024-06-07,article
https://openalex.org/W4399565495,,Ru Peng|Chao Zhao|Xingyu Chen|Ziru Wang|Yaxin Liu|Yulong Liu|Xuguang Lan,Xi'an Jiaotong University|Xi'an Jiaotong University|Xi'an Jiaotong University|Xi'an Jiaotong University|Xi'an Jiaotong University|Xi'an Jiaotong University|Xi'an Jiaotong University,6,A causality guided loss for imbalanced learning in scene graph generation,https://doi.org/10.1016/j.neucom.2024.128042,3.18094536,FALSE,en,FALSE,closed,Neurocomputing,journal,2024-06-12,article
https://openalex.org/W4399576782,,Zhihan Yao|Yuhang Chen|Jiahao Cui|Shoulong Zhang|Shuai Li|Aimin Hao,Beihang University|Beihang University|Beihang University|Beihang University|Beihang University|Beihang University,7,Conditional room layout generation based on graph neural networks,https://doi.org/10.1016/j.cag.2024.103971,4.45346079,FALSE,en,FALSE,closed,Computers & Graphics,journal,2024-06-12,article
https://openalex.org/W4401880555,,Chun‐Wei Chang|I-Ting Lin|Chia‐Hsiang Yang,National Taiwan University|National Taiwan University|National Taiwan University,0,"A 101mW, 280fps Scene Graph Generation Processor for Visual Context Understanding on Mobile Devices",https://doi.org/10.1109/vlsitechnologyandcir46783.2024.10631476,0.0,FALSE,en,FALSE,closed,,,2024-06-16,article
https://openalex.org/W4402704531,,Lianggangxu Chen|Xuejiao Wang|Jiale Lu|Shaohui Lin|Changbo Wang|Gaoqi He,East China Normal University|East China Normal University|East China Normal University|East China Normal University|East China Normal University|East China Normal University,13,CLIP-Driven Open-Vocabulary 3D Scene Graph Generation via Cross-Modality Contrastive Learning,https://doi.org/10.1109/cvpr52733.2024.02632,6.89204827,FALSE,en,FALSE,closed,,,2024-06-16,article
https://openalex.org/W4402915938,,Chaoyi Zhang|Xitong Yang|Ji Hou|Kris Kitani|Weidong Cai|Fu-Jen Chu,University of Sydney||||University of Sydney|,6,EgoSG: Learning 3D Scene Graphs from Egocentric RGB-D Sequences,https://doi.org/10.1109/cvprw63382.2024.00260,4.32477881,FALSE,en,FALSE,closed,,,2024-06-17,article
https://openalex.org/W4400034895,"Fine-grained scene graph generation aims to parse the objects and their fine-grained relationships within scenes. Despite the significant progress in recent years, their performance is still limited by two major issues: (1) ambiguous perception under a global view; (2) the lack of reliable, fine-grained annotations. We argue that understanding the local context is important in addressing the two issues. However, previous works often overlook it, which limits their effectiveness in fine-grained scene graph generation. To tackle this challenge, we introduce a Local-context Attention Learning method that concentrates on local context and can generate high-reliability, fine-grained annotations. It comprises two components: (1) The Fine-grained Location Attention Network (FLAN), a multi-branch network that encompasses global and local branches, can attend to local informative context and perceive granularity levels in different regions, thereby adaptively enhancing the learning of fine-grained locations. (2) The Fine-grained Location Label Transfer (FLLT) method identifies coarse-grained labels inconsistent with the local context and determines which labels should be transferred through the global confidence thresholding strategy, finally transferring them to reliable local context-consistent fine-grained ones. Experiments conducted on the Visual Genome, OpenImage, and GQA-200 datasets show that the proposed methods achieve significant improvements on the fine-grained scene graph generation task. By addressing the challenge mentioned above, our method also achieves state-of-the-art performances on the three datasets.",Xuhan Zhu|Ruiping Wang|Xiangyuan Lan|Yaowei Wang,University of Chinese Academy of Sciences|Peng Cheng Laboratory|Chinese Academy of Sciences|Institute of Computing Technology|University of Chinese Academy of Sciences|Peng Cheng Laboratory|Peng Cheng Laboratory,6,Local context attention learning for fine-grained scene graph generation,https://doi.org/10.1016/j.patcog.2024.110708,3.18094536,FALSE,en,TRUE,hybrid,Pattern Recognition,journal,2024-06-26,article
https://openalex.org/W4402353014,,Nanhao Liang|Yong Liu|Fan Wang|Yingwei Xia,Chinese Academy of Sciences|Hefei Institutes of Physical Science|Chinese Academy of Sciences|Hefei Institutes of Physical Science|Chinese Academy of Sciences|Hefei Institutes of Physical Science|Hefei Institutes of Physical Science|Chinese Academy of Sciences,0,Addressing Predicate Overlap in Scene Graph Generation with Semantics-prototype Learning,https://doi.org/10.1109/ijcnn60899.2024.10650600,0.0,FALSE,en,FALSE,closed,,,2024-06-30,article
https://openalex.org/W4401329442,,Lucie Kunitomo-Jacquin|Ken Fukuda,National Institute of Advanced Industrial Science and Technology|National Institute of Advanced Industrial Science and Technology,2,Calibration of Evidential Mass Function for Trustworthy Information Processing in Scene Graph Generation,https://doi.org/10.1109/fuzz-ieee60900.2024.10612064,1.06031512,FALSE,en,FALSE,closed,,,2024-06-30,article
https://openalex.org/W4400280618,"Scene graphs can enhance the understanding capability of intelligent ships in navigation scenes. However, the complex entity relationships and the presence of significant noise in contextual information within navigation scenes pose challenges for navigation scene graph generation (NSGG). To address these issues, this paper proposes a novel NSGG network named SGK-Net. This network comprises three innovative modules. The Semantic-Guided Multimodal Fusion (SGMF) module utilizes prior information on relationship semantics to fuse multimodal information and construct relationship features, thereby elucidating the relationships between entities and reducing semantic ambiguity caused by complex relationships. The Graph Structure Learning-based Structure Evolution (GSLSE) module, based on graph structure learning, reduces redundancy in relationship features and optimizes the computational complexity in subsequent contextual message passing. The Key Entity Message Passing (KEMP) module takes full advantage of contextual information to refine relationship features, thereby reducing noise interference from non-key nodes. Furthermore, this paper constructs the first Ship Navigation Scene Graph Simulation dataset, named SNSG-Sim, which provides a foundational dataset for the research on ship navigation SGG. Experimental results on the SNSG-sim dataset demonstrate that our method achieves an improvement of 8.31% (R@50) in the PredCls task and 7.94% (R@50) in the SGCls task compared to the baseline method, validating the effectiveness of our method in navigation scene graph generation.",Wenbin Yang|Hao Qiu|Xiangfeng Luo|Shaorong Xie,Shanghai University of Engineering Science|Shanghai University of Engineering Science|Shanghai University of Engineering Science|Shanghai University of Engineering Science,2,SGK-Net: A Novel Navigation Scene Graph Generation Network,https://doi.org/10.3390/s24134329,1.06031512,FALSE,en,TRUE,gold,Sensors,journal,2024-07-03,article
https://openalex.org/W4400411348,,Ke Zhang|Yan Yang|Jun Yu|Jianping Fan|Hanliang Jiang|Qingming Huang|Weidong Han,Hangzhou Dianzi University|Hangzhou Dianzi University|Harbin Institute of Technology|Hangzhou Dianzi University|Lenovo (China)|Sir Run Run Shaw Hospital|Zhejiang University|University of Chinese Academy of Sciences|Zhejiang Cancer Hospital,11,Attribute Prototype-Guided Iterative Scene Graph for Explainable Radiology Report Generation,https://doi.org/10.1109/tmi.2024.3424505,7.02656404,FALSE,en,FALSE,closed,IEEE Transactions on Medical Imaging,journal,2024-07-08,article
https://openalex.org/W4400453703,,Jiahui Wei|Zhixin Li|Canlong Zhang|Huifang Ma,,9,Mining core information by evaluating semantic importance for unpaired image captioning,https://doi.org/10.1016/j.neunet.2024.106519,4.77141804,FALSE,en,FALSE,closed,Neural Networks,journal,2024-07-09,article
https://openalex.org/W4402980606,,Shuang Liang|Long Zhang|Chi Xie|Lili Chen,Tongji University|Tongji University|Tongji University|Tongji University,2,Causal Intervention for Panoptic Scene Graph Generation,https://doi.org/10.1109/icme57554.2024.10687509,1.2775571,FALSE,en,FALSE,closed,,,2024-07-15,article
https://openalex.org/W4402981778,,Sisi You|Bing-Kun Bao,Nanjing University of Posts and Telecommunications|Nanjing University of Posts and Telecommunications,0,Dynamic Scene Graph Generation with Unified Temporal Modeling,https://doi.org/10.1109/icme57554.2024.10687612,0.0,FALSE,en,FALSE,closed,,,2024-07-15,article
https://openalex.org/W4403864903,,Christos Chronis|Iraklis Varlamis|Dimitrios Michail|Konstantinos Tserpes|George Dimitrakopoulos,Harokopio University of Athens|Harokopio University of Athens|Harokopio University of Athens|Harokopio University of Athens|Harokopio University of Athens,3,From Perception to Action: Leveraging LLMs and Scene Graphs for Intuitive Robotic Task Execution,https://doi.org/10.1109/bigdataservice62917.2024.00009,1.91633565,FALSE,en,FALSE,closed,,,2024-07-15,article
https://openalex.org/W4402980031,,Bowen Zhao|Licheng Zhang|Lei Zhang|Zhendong Mao,University of Science and Technology of China|University of Science and Technology of China|University of Science and Technology of China|University of Science and Technology of China,0,Neighborhood-Adaptive Context Enhancement Learning For Scene Graph Generation,https://doi.org/10.1109/icme57554.2024.10688027,0.0,FALSE,en,FALSE,closed,,,2024-07-15,article
https://openalex.org/W4400925567,"Abstract Social relationships, such as parent-offspring and friends, are crucial and stable connections between individuals, especially at the person level, and are essential for accurately describing the semantics of videos. In this paper, we analogize such a task to scene graph generation, which we call video social relationship graph generation (VSRGG). It involves generating a social relationship graph for each video based on person-level relationships. We propose a context-aware graph neural network (CAGNet) for VSRGG, which effectively generates social relationship graphs through message passing, capturing the context of the video. Specifically, CAGNet detects persons in the video, generates an initial graph via relationship proposal, and extracts facial and body features to describe the detected individuals, as well as temporal features to describe their interactions. Then, CAGNet predicts pairwise relationships between individuals using graph message passing. Additionally, we construct a new dataset, VidSoR, to evaluate VSRGG, which contains 72 h of video with 6276 person instances and 5313 relationship instances of eight relationship types. Extensive experiments show that CAGNet can make accurate predictions with a comparatively high mean recall (mRecall) when using only visual features.",Fan Yu|Yaqun Fang|Zhixiang Zhao|Jia Bei|Tongwei Ren|Gangshan Wu,Nanjing University|Nanjing University|Nanjing University|Nanjing University|Nanjing University|Nanjing University,5,CAGNet: a context-aware graph neural network for detecting social relationships in videos,https://doi.org/10.1007/s44267-024-00056-9,33.63775049,FALSE,en,TRUE,diamond,Visual Intelligence,journal,2024-07-23,article
https://openalex.org/W4402254924,,Bochuan He|Xue Zhang|Xuan Xie,,0,Cross-modal Image Retrieval Based on Scene Graphs,https://doi.org/10.1145/3686424.3686471,0.0,FALSE,en,FALSE,closed,,,2024-07-26,article
https://openalex.org/W4405521852,,Yu Gu|Guohui Tian|Zhengsong Jiang,Shandong University|Shandong University|Shandong University,0,Hierarchical Generation of Action Sequence for Service Rots Based on Scene Graph via Large Language Models,https://doi.org/10.1109/icosr63848.2024.00049,0.0,FALSE,en,FALSE,closed,,,2024-07-26,article
https://openalex.org/W4401077909,,Shiquan Lin|Zhengye Xiao|Li-Xin Wang|Xiuan Wan|Lan Ni|Yuchun Fang,Shanghai University of Engineering Science|Shanghai University of Engineering Science|Shanghai University of Engineering Science|Shanghai University of Engineering Science|Shanghai University|Shanghai University of Engineering Science,4,Structure-aware sign language recognition with spatial–temporal scene graph,https://doi.org/10.1016/j.ipm.2024.103850,3.10988015,FALSE,en,FALSE,closed,Information Processing & Management,journal,2024-07-29,article
https://openalex.org/W4403421017,,Debaleen Das Spandan|Razib Iqbal,Missouri State University|Missouri State University,0,ProxeGraph: Scene Graph Generation Utilizing Proxemics for Smart Homes,https://doi.org/10.1109/mipr62202.2024.00024,0.0,FALSE,en,FALSE,closed,,,2024-08-07,article
https://openalex.org/W4401538477,,S Monesh|N. Senthilkumar,,0,Review on scene graph generation methods,https://doi.org/10.3233/mgs-230132,0.0,FALSE,en,FALSE,closed,Multiagent and Grid Systems,journal,2024-08-12,article
https://openalex.org/W4401567860,,Hongshuo Tian|Ning Xu|Mohan Kankanhalli|An-An Liu,Tianjin University|Tianjin University|National University of Singapore|Tianjin University,7,Gaussian Distribution-Aware Commonsense Knowledge Learning for Scene Graph Generation,https://doi.org/10.1109/tcsvt.2024.3443417,4.98752986,FALSE,en,FALSE,closed,IEEE Transactions on Circuits and Systems for Video Technology,journal,2024-08-14,article
https://openalex.org/W4401896769,,Zhihong Liu|Jianji Wang|Hui Chen|Yongqiang Ma|Nanning Zheng,National Intelligence University|Xi'an Jiaotong University|National Intelligence University|Xi'an Jiaotong University|National Intelligence University|Xi'an Jiaotong University|National Intelligence University|Xi'an Jiaotong University|National Intelligence University|Xi'an Jiaotong University,7,Relation-Specific Feature Augmentation for unbiased scene graph generation,https://doi.org/10.1016/j.patcog.2024.110936,3.71110292,FALSE,en,FALSE,closed,Pattern Recognition,journal,2024-08-26,article
https://openalex.org/W4403678335,,Zhijie Yan|Zuoxu Wang|Shufei Li|Mingrui Li|Xinxin Liang|Jihong Liu,Beihang University|Beihang University|Hong Kong Polytechnic University|Beihang University|Beihang University|Beihang University,2,ManufVisSGG: A Vision-Language-Model Approach for Cognitive Scene Graph Generation in Manufacturing Systems,https://doi.org/10.1109/case59546.2024.10711649,1.06031512,FALSE,en,FALSE,closed,,,2024-08-28,article
https://openalex.org/W4401990649,,Dominic Maggio|Yun Chang|Nathan Hughes|Matthew Trang|Dan Griffith|Carlyn Dougherty|Eric Cristofalo|Lukas Schmid|Luca Carlone,Decision Systems (United States)|Massachusetts Institute of Technology|Decision Systems (United States)|Massachusetts Institute of Technology|Massachusetts Institute of Technology|Decision Systems (United States)|MIT Lincoln Laboratory|MIT Lincoln Laboratory|MIT Lincoln Laboratory|MIT Lincoln Laboratory|Massachusetts Institute of Technology|Decision Systems (United States)|Massachusetts Institute of Technology|Decision Systems (United States),28,<i>Clio:</i> Real-Time Task-Driven Open-Set 3D Scene Graphs,https://doi.org/10.1109/lra.2024.3451395,36.93497193,FALSE,en,TRUE,green,IEEE Robotics and Automation Letters,journal,2024-08-29,article
https://openalex.org/W4402217218,,Shiqi Sun|Zhijin Qin|Huiqiang Xie|Xiaoming Tao,Tsinghua University|Tsinghua University|Jinan University|Tsinghua University,8,Task-Oriented Scene Graph-Based Semantic Communications With Adaptive Channel Coding,https://doi.org/10.1109/twc.2024.3450697,5.11022839,FALSE,en,FALSE,closed,IEEE Transactions on Wireless Communications,journal,2024-09-04,article
https://openalex.org/W4402823728,"With the rapid development of remote sensing image data, the efficient retrieval of target images of interest has become an important issue in various applications including computer vision and remote sensing. This research addressed the low-accuracy problem in traditional content-based image retrieval algorithms, which largely rely on comparing entire image features without capturing sufficient semantic information. We proposed a scene graph similarity-based remote sensing image retrieval algorithm. Firstly, a one-shot object detection algorithm was designed for remote sensing images based on Siamese networks and tailored to the objects of an unknown class in the query image. Secondly, a scene graph construction algorithm was developed, based on the objects and their attributes and spatial relationships. Several construction strategies were designed based on different relationships, including full connections, random connections, nearest connections, star connections, or ring connections. Thirdly, by making full use of edge features for scene graph feature extraction, a graph feature extraction network was established based on edge features. Fourthly, a neural tensor network-based similarity calculation algorithm was designed for graph feature vectors to obtain image retrieval results. Fifthly, a dataset named remote sensing images with scene graphs (RSSG) was built for testing, which contained 929 remote sensing images with their corresponding scene graphs generated by the developed construction strategies. Finally, through performance comparison experiments with remote sensing image retrieval algorithms AMFMN, MiLaN, and AHCL, in precision rates, Precision@1 improved by 10%, 7.2%, and 5.2%, Precision@5 improved by 3%, 5%, and 1.7%; and Precision@10 improved by 1.7%, 3%, and 0.6%. In recall rates, Recall@1 improved by 2.5%, 4.3%, and 1.3%; Recall@5 improved by 3.7%, 6.2%, and 2.1%; and Recall@10 improved by 4.4%, 7.7% and 1.6%.",Yougui Ren|Zhibin Zhao|Junjian Jiang|Yuning Jiao|Yining Yang|Dawei Liu|Kefu Chen|Ge Yu,Northeastern University|Educational Department of Liaoning Province|Northeastern University|Northeastern University|Northeastern University|Educational Department of Liaoning Province|Northeastern University|Northeastern University|Northeastern University,3,A Scene Graph Similarity-Based Remote Sensing Image Retrieval Algorithm,https://doi.org/10.3390/app14188535,1.59047268,FALSE,en,TRUE,gold,Applied Sciences,journal,2024-09-22,article
https://openalex.org/W4402737612,,Lei Wang|Zejian Yuan|Yao Lu|Badong Chen,Xi'an Jiaotong University|Xi'an Jiaotong University|State Key Laboratory of Remote Sensing Science|Xi'an Jiaotong University,1,Unbiased scene graph generation via head-tail cooperative network with self-supervised learning,https://doi.org/10.1016/j.imavis.2024.105283,0.53015756,FALSE,en,FALSE,closed,Image and Vision Computing,journal,2024-09-23,article
https://openalex.org/W4402758293,,Ruonan Zhang|Gaoyun An|Yigang Cen|Qiuqi Ruan,Beijing Jiaotong University|Computer Network Information Center|Beijing Jiaotong University|Computer Network Information Center|Beijing Jiaotong University|Computer Network Information Center|Beijing Jiaotong University|Computer Network Information Center,2,Attention redirection transformer with semantic oriented learning for unbiased scene graph generation,https://doi.org/10.1016/j.patcog.2024.111039,1.06031512,FALSE,en,FALSE,closed,Pattern Recognition,journal,2024-09-24,article
https://openalex.org/W4403653984,,Mohammad Essam|Dina Khattab|Howida A. Shedeed|Mohamed F. Tolba,Ain Shams University|Ain Shams University||Ain Shams University,0,Transformer-Based Backbones for Scene Graph Generation A Comparative Analysis,https://doi.org/10.21608/ijicis.2024.301597.1342,0.0,FALSE,en,TRUE,diamond,International journal of intelligent computing and information sciences/International Journal of Intelligent Computing and Information Sciences,journal,2024-09-30,article
https://openalex.org/W4405317243,,Frank W. Gallagher|Louis Gallagher|Marco Cognetti|John McDonald,"National University of Ireland, Maynooth|National University of Ireland, Maynooth|Université de Toulouse|Centre National de la Recherche Scientifique|National University of Ireland, Maynooth",0,A dataset of domestic environment room adjacency scene graphs,https://doi.org/10.1049/icp.2024.3327,0.0,FALSE,en,TRUE,hybrid,IET conference proceedings.,journal,2024-10-01,article
https://openalex.org/W4403658227,"ABSTRACT The advent of deep neural networks and improved computational power have brought a revolutionary transformation in the fields of computer vision and image processing. Within the realm of computer vision, there has been a significant interest in the area of synthetic image generation, which is a creative side of AI. Many researchers have introduced innovative methods to identify deep neural network‐based architectures involved in image generation via different modes of input, like text, scene graph layouts and so forth to generate synthetic images. Computer‐generated images have been found to contribute a lot to the training of different machine and deep‐learning models. Nonetheless, we have observed an immediate need for a comprehensive and systematic literature review that encompasses a summary and critical evaluation of current primary studies' approaches toward image generation. To address this, we carried out a systematic literature review on synthetic image generation approaches published from 2018 to February 2023. Moreover, we have conducted a systematic review of various datasets, approaches to image generation, performance metrics for existing methods, and a brief experimental comparison of DCGAN (deep convolutional generative adversarial network) and cGAN (conditional generative adversarial network) in the context of image generation. Additionally, we have identified applications related to image generation models with critical evaluation of the primary studies on the subject matter. Finally, we present some future research directions to further contribute to the field of image generation using deep neural networks.",Aisha Zulfiqar|Sher Muhammad Daudpota|Ali Shariq Imran|Zenun Kastrati|Mohib Ullah|Suraksha Sadhwani,Sukkur IBA University|Sukkur IBA University|Norwegian University of Science and Technology|Linnaeus University|Norwegian University of Science and Technology|Sukkur IBA University,5,Synthetic Image Generation Using Deep Learning: A Systematic Literature Review,https://doi.org/10.1111/coin.70002,2.6507878,FALSE,en,TRUE,hybrid,Computational Intelligence,journal,2024-10-01,article
https://openalex.org/W4403095669,,Ting He|Wei Chen|Jun Zhang|Shuo Li|Shengze Hu,National University of Defense Technology|National University of Defense Technology|National University of Defense Technology|National University of Defense Technology|National University of Defense Technology,0,Multifeature fusion embedding network for unbiased scene graph generation,https://doi.org/10.1117/12.3048382,0.0,FALSE,en,FALSE,closed,,,2024-10-03,article
https://openalex.org/W4403182292,,Xinxin Liu|Yuchen Zhou|Y. Ye|Chao Gou,Sun Yat-sen University|Sun Yat-sen University|Sun Yat-sen University|Sun Yat-sen University,1,Edge Feature-Enhanced Network for Collision Risk Assessment Using Traffic Scene Graphs,https://doi.org/10.1109/mits.2024.3458393,0.39929135,FALSE,en,FALSE,closed,IEEE Intelligent Transportation Systems Magazine,journal,2024-10-07,article
https://openalex.org/W4403919954,,Irani Hoeronis|Bambang Riyanto Trilaksono|Nugraha Priya Utama,Informatics Institute of Technology|Bandung Institute of Technology|Bandung Institute of Technology|Informatics Institute of Technology|Informatics Institute of Technology|Bandung Institute of Technology,0,Fine Tuning Panoptic Scene Graph Generation,https://doi.org/10.1109/ic3ina64086.2024.10732695,0.0,FALSE,en,FALSE,closed,,,2024-10-09,article
https://openalex.org/W4405361524,,Ruiqi Hu|Bin Jiang|Chao Yang|Yiming Fan|Zheng Zhou|Chenglong Lei,Hunan University|Hunan University|Hunan University|Hunan University|Hunan University|Hunan University,0,Unbiased Scene Graph Generation Based on Relationship Label Division,https://doi.org/10.1109/aiotsys63104.2024.10780618,0.0,FALSE,en,FALSE,closed,,,2024-10-17,article
https://openalex.org/W4403601779,"With the development of artificial intelligence and deep learning technologies, image captioning has become an important research direction at the intersection of computer vision and natural language processing. The purpose of image captioning is to generate corresponding natural language descriptions by understanding the content of images. This technology has broad application prospects in fields such as image retrieval, autonomous driving, and visual question answering. Currently, many researchers have proposed region-based image captioning methods. These methods generate captions by extracting features from different regions of an image. However, they often rely on local features of the image and overlook the understanding of the overall scene, leading to captions that lack coherence and accuracy when dealing with complex scenes. Additionally, image captioning methods are unable to extract complete semantic information from visual data, which may lead to captions with biases and deficiencies. Due to these reasons, existing methods struggle to generate comprehensive and accurate captions. To fill this gap, we propose the Semantic Scenes Encoder (SSE) for image captioning. It first extracts a scene graph from the image and integrates it into the encoding of the image information. Then, it extracts a semantic graph from the captions and preserves semantic information through a learnable attention mechanism, which we refer to as the dictionary. During the generation of captions, it combines the encoded information of the image and the learned semantic information to generate complete and accurate captions. To verify the effectiveness of the SSE, we tested the model on the MSCOCO dataset. The experimental results show that the SSE improves the overall quality of the captions. The improvement in scores across multiple evaluation metrics further demonstrates that the SSE possesses significant advantages when processing identical images.",Fengzhi Zhao|Zhezhou Yu|Tao Wang|LV Yi,Jilin University|Guangdong Peizheng College|Jilin University|Jilin University|Jilin University,1,Image Captioning Based on Semantic Scenes,https://doi.org/10.3390/e26100876,0.53015756,FALSE,en,TRUE,gold,Entropy,journal,2024-10-18,article
https://openalex.org/W4403791922,,Zizhao Wu|Huangxia Li|Guangsheng Chen|Yu Zhou|Xiaoling Gu|Yigang Wang,Hangzhou Dianzi University|Hangzhou Dianzi University|Hangzhou Dianzi University|Hangzhou Dianzi University|Hangzhou Dianzi University|Hangzhou Dianzi University,4,3D Question Answering with Scene Graph Reasoning,https://doi.org/10.1145/3664647.3681517,2.12063024,FALSE,en,FALSE,closed,,,2024-10-26,article
https://openalex.org/W4403791539,,Xuhan Zhu|Yifei Xing|Ruiping Wang|Yaowei Wang|Xiangyuan Lan,University of Chinese Academy of Sciences|University of Chinese Academy of Sciences|Institute of Computing Technology|University of Chinese Academy of Sciences|Peng Cheng Laboratory|Peng Cheng Laboratory,3,Calibration for Long-tailed Scene Graph Generation,https://doi.org/10.1145/3664647.3680818,1.59047268,FALSE,en,FALSE,closed,,,2024-10-26,article
https://openalex.org/W4403791241,,Ziyue Wu|Junyu Gao|Changsheng Xu,Tianjin University of Technology|Institute of Automation|Institute of Automation|Peng Cheng Laboratory,5,Open-Vocabulary Video Scene Graph Generation via Union-aware Semantic Alignment,https://doi.org/10.1145/3664647.3681061,2.6507878,FALSE,en,FALSE,closed,,,2024-10-26,article
https://openalex.org/W4403791458,,Mingyuan Wu|Ruifan Ji|Haozhen Zheng|Jiaxi Li|Beitong Tian|Bo Chen|Rui-Xiao Zhang|Jacob Chakareski|Michael Zink|Ramesh K. Sitaraman|Klara Nahrstedt,University of Illinois Urbana-Champaign|University of Illinois Urbana-Champaign|University of Illinois Urbana-Champaign|University of Illinois Urbana-Champaign|University of Illinois Urbana-Champaign|University of Illinois Urbana-Champaign|University of Illinois Urbana-Champaign|New Jersey Institute of Technology|||University of Illinois Urbana-Champaign,2,Scene Graph Driven Hybrid Interactive VR Teleconferencing,https://doi.org/10.1145/3664647.3684996,1.55494008,FALSE,en,FALSE,closed,,,2024-10-26,article
https://openalex.org/W4406755126,,Luchen Zhang|Gaoyun An|Shan Cao,Beijing Jiaotong University|Beijing Jiaotong University|Capital Normal University,1,Incorporating Transformer-based Scene Graphs Progressively for Image Captioning,https://doi.org/10.1109/icsp62129.2024.10846478,0.53015756,FALSE,en,FALSE,closed,,,2024-10-28,article
https://openalex.org/W4406754660,,Chenshu Wang|Jiying Wu|Cong Du|Gaoyun An,Beijing Jiaotong University|||Beijing Jiaotong University,0,Vision-Language Collaboration Enhancement for Unbiased Scene Graph Generation,https://doi.org/10.1109/icsp62129.2024.10846357,0.0,FALSE,en,FALSE,closed,,,2024-10-28,article
https://openalex.org/W4403921466,"Abstract Visual navigation needs the agent locate the given target with visual perception. To enable robots to effectively execute tasks, combining large language models (LLMs) with multi-modal inputs in navigation is necessary. While LLMs offer rich semantic knowledge, they lack specific real-world information and real-time interaction capabilities. This paper introduces a Multi-modal Scene Graph (MMSG) navigation framework that aligns LLMs with visual perception models to predict next steps. Firstly, a multi-modal scene dataset is constructed, containing triplets of object-relations-target words. We provide target words and lists of existing objects in the scene to generate a large number of instructions and corresponding action plans for GPT $$-$$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:mo>-</mml:mo> </mml:math> 3.5. The generated data is then utilized for pre-train LLM for path planning. During inference, we discover objects in the scene by extending the DETR visual object detector to multi-view RGB image collected from different reachable positions. Experimental results show that path planning generated from MMSG outperforms state-of-the-art methods, indicating its feasibility in complex environments. We evaluate our methods on the ProTHOR dataset and show superior navigation performance.",Yu He|Kang Zhou|Tao Tian,Henan Provincial Center for Disease Control and Prevention|Huanghuai University|Wuhan University|City University of Hong Kong|Henan Provincial Center for Disease Control and Prevention|Huanghuai University,4,Multi-modal scene graph inspired policy for visual navigation,https://doi.org/10.1007/s11227-024-06541-8,2.12063024,FALSE,en,TRUE,hybrid,The Journal of Supercomputing,journal,2024-10-30,article
https://openalex.org/W4404104597,,Liqin Wang|Pengcheng Yang|Xu Wang|Zhihong Xu|Yongfeng Dong,Intelligent Health (United Kingdom)|Hebei University of Technology|Hebei University of Technology|Hebei University of Technology|Intelligent Health (United Kingdom)|Hebei University of Technology|Intelligent Health (United Kingdom)|Hebei University of Technology|Intelligent Health (United Kingdom),4,Scene graph fusion and negative sample generation strategy for image-text matching,https://doi.org/10.1007/s11227-024-06652-2,2.12063024,FALSE,en,FALSE,closed,The Journal of Supercomputing,journal,2024-11-06,article
https://openalex.org/W4404133916,"Abstract. Using street-view imagery for interpreting diverse street-scale elements and their relationships within historical districts offers high efficiency and low cost for preservation and management. Scene graphs provide a structured representation of objects and their relationships within a scene. However, applying existing scene graph generation techniques directly to street-view imagery presents challenges due to the complexity of elements and narrow street spaces. This paper introduces HSSGG (Historical Street-view Scene Graph Generation), a predictive model that effectively identifies elements and their relationships. By incorporating an end-to-end Relation Transformer with the parameter-free attention and coordinate attention modules, HSSGG improves relationship prediction accuracy, even with limited samples, and enhances the precision of scene graph generation in complex environments. Test on 200 panoramic images from historical districts in Beijing shows that HSSGG outperforms existing single-stage relation prediction models (such as RelTR and FCSGG) in accuracy and stability. These results provide valuable insights for the preservation and management of historical districts.",Guo Xian|Xianglei Liu|Jie Jiang,Beijing University of Civil Engineering and Architecture|Beijing University of Civil Engineering and Architecture|Beijing University of Civil Engineering and Architecture,0,"A Scene Graph Generation Method for Historical District Street-view Imagery: A Case Study in Beijing, China",https://doi.org/10.5194/isprs-archives-xlviii-3-2024-209-2024,0.0,FALSE,en,TRUE,diamond,"The international archives of the photogrammetry, remote sensing and spatial information sciences/International archives of the photogrammetry, remote sensing and spatial information sciences",journal,2024-11-07,article
https://openalex.org/W4405907708,,Erdal Akin|Kayode S. Adewole|Héctor Caltenco|Reza Malekian|Jan Persson,||Ericsson (Sweden)||,0,Privacy-aware Hydra (PA-Hydra) for 3D Scene Graph Construction,https://doi.org/10.1109/wf-iot62078.2024.10811446,0.0,FALSE,en,FALSE,closed,,,2024-11-10,article
https://openalex.org/W4405440795,,Mohammad Essam|Howida A. Shedeed|Dina Khattab,Ain Shams University|Ain Shams University|Ain Shams University,0,MiT-RelTR: An Advanced Scene Graph-Based Cross-Modal Retrieval Model for Real-Time Capabilities,https://doi.org/10.1109/miucc62295.2024.10783495,0.0,FALSE,en,FALSE,closed,,,2024-11-13,article
https://openalex.org/W4404999448,"This paper advances contextual image understanding within perspective-aware Ai (PAi), an emerging paradigm in human–computer interaction that enables users to perceive and interact through each other’s perspectives. While PAi relies on multimodal data—such as text, audio, and images—challenges in data collection, alignment, and privacy have led us to focus on enabling the contextual understanding of images. To achieve this, we developed perspective-aware scene graph generation with LLM post-processing (PASGG-LM). This framework extends traditional scene graph generation (SGG) by incorporating large language models (LLMs) to enhance contextual understanding. PASGG-LM integrates classical scene graph outputs with LLM post-processing to infer richer contextual information, such as emotions, activities, and social contexts. To test PASGG-LM, we introduce the context-aware scene graph generation task, where the goal is to generate a context-aware situation graph describing the input image. We evaluated PASGG-LM pipelines using state-of-the-art SGG models, including Motifs, Motifs-TDE, and RelTR, and showed that fine-tuning LLMs, particularly GPT-4o-mini and Llama-3.1-8B, improves performance in terms of R@K, mR@K, and mAP. Our method is capable of generating scene graphs that capture complex contextual aspects, advancing human–machine interaction by enhancing the representation of diverse perspectives. Future directions include refining contextual scene graph models and expanding multi-modal data integration for PAi applications in domains such as healthcare, education, and social robotics.",Daniel Platnick|Marjan Alirezaie|Hossein Rahnama,Toronto Metropolitan University|Toronto Metropolitan University|Human Media,2,Enabling Perspective-Aware Ai with Contextual Scene Graph Generation,https://doi.org/10.3390/info15120766,1.06031512,FALSE,en,TRUE,gold,Information,journal,2024-12-02,article
https://openalex.org/W4404918889,,Trey Woodlief|Felipe Toledo|Sebastian Elbaum|Matthew B. Dwyer,University of Virginia|University of Virginia|University of Virginia|University of Virginia,3,The SGSM framework: Enabling the specification and monitor synthesis of safe driving properties through scene graphs,https://doi.org/10.1016/j.scico.2024.103252,1.19787404,FALSE,en,FALSE,closed,Science of Computer Programming,journal,2024-12-02,article
https://openalex.org/W4407214533,,Anfel Amirat|Nadia Baha|Mohamed Bentarzi|Zoheir Bessai,University of Algiers Benyoucef Benkhedda|University of Sciences and Technology Houari Boumediene|University of Algiers Benyoucef Benkhedda|University of Sciences and Technology Houari Boumediene|University of Algiers Benyoucef Benkhedda|University of Sciences and Technology Houari Boumediene|University of Algiers Benyoucef Benkhedda|University of Sciences and Technology Houari Boumediene,0,Efficient Multi-Head Attention for Human-Object Interaction Recognition and Video Scene Graph Generation,https://doi.org/10.1109/ic3it63743.2024.10869430,0.0,FALSE,en,FALSE,closed,,,2024-12-03,article
https://openalex.org/W4405868447,,Hao Zhang|Xingning Dong|J. Gao|Hao Liang|Pei Shen|Tian Gan,Shandong University|Antea Group (France)|Shandong University|||Shandong University,0,MBC-ATA: Maximum Binary Classification and Anchor-based Triplet Augmentation for Unbiased Scene Graph Generation,https://doi.org/10.1145/3696409.3700292,0.0,FALSE,en,FALSE,closed,,,2024-12-03,article
https://openalex.org/W4405310279,"Recognizing human intentions is a key challenge in human-robot interaction research. Much of the current work in this area centers on identifying human intentions within specific activities, often relying on a limited set of features. In contrast, this paper introduces a more versatile framework for intention recognition and introduces a novel model: the Spatial-Temporal Graph Attention Informer Neural Network (STGAIN). To recognize intentions, this model leverages spatial relationships between humans and objects in different scenes, along with their temporal evolution. In addition, to address an existing research gap, this research developed a new dataset called Dynamic Scene Graph (DSG) with representative dynamic relationships, derived from 471 videos covering 20 categories of human intentions. This dataset represents people and objects in different scenes, and the relationships between them. The model was tested rigorously at different points in the videos to track how the scenes evolved and to assess prediction accuracy, comparing the results to a range of advanced algorithms. Our findings clearly demonstrate that STGAIN outperforms these models, showcasing its potential for advanced human intention recognition applications. This model represents a significant advance toward creating more human-centered robots, capable of understanding and adapting to human intentions in real-world situations.",Tong Tong|Rossitza Setchi|Yulia Hicks,Cardiff University|Cardiff University|Cardiff University,5,Human intention recognition using context relationships in complex scenes,https://doi.org/10.1016/j.eswa.2024.126147,2.6507878,FALSE,en,TRUE,hybrid,Expert Systems with Applications,journal,2024-12-12,article
https://openalex.org/W4409103017,,J. Y. Liao|Linbo Qing|Lindong Li|Pingyu Wang|Honggang Chen|Wei Zhao,Chengdu University of Information Technology|Chengdu University of Information Technology|Chengdu University of Information Technology|Chengdu University of Information Technology|Chengdu University of Information Technology|Chengdu University of Information Technology,0,Scene Graph Generation Based on Depth Information and Feature Enhancement,https://doi.org/10.1109/iccc62609.2024.10941810,0.0,FALSE,en,FALSE,closed,,,2024-12-13,article
https://openalex.org/W4405428002,,Jiayuan Xie|Jiang Yu Zheng|Wenhao Fang|Yi Cai|Qing Li,Hong Kong Polytechnic University|Chinese Academy of Sciences|Institute of Software|South China University of Technology|South China University of Technology|Hong Kong Polytechnic University,4,Explicitly diverse visual question generation,https://doi.org/10.1016/j.neunet.2024.107002,2.12063024,FALSE,en,FALSE,closed,Neural Networks,journal,2024-12-16,article
https://openalex.org/W4405541992,,Kai Xu|Lichun Wang|Shuang Li|Tong Gao|Baocai Yin,Beijing University of Technology|Beijing University of Technology|Beijing Information Science & Technology University|Beijing University of Technology|Beijing University of Technology,0,Scene Adaptive Context Modeling and Balanced Relation Prediction for Scene Graph Generation,https://doi.org/10.1145/3708350,0.0,FALSE,en,FALSE,closed,ACM Transactions on Multimedia Computing Communications and Applications,journal,2024-12-18,article
https://openalex.org/W4408017926,,Jie Liu|Shaorong Xie,Shanghai University of Engineering Science|Shanghai University of Engineering Science,1,Multi-Source Information Fusion Navigation Based on Dynamic Attribute Scene Graphs,https://doi.org/10.1109/aiac63745.2024.10899632,3.30982661,FALSE,en,FALSE,closed,,,2024-12-20,article
https://openalex.org/W4405896160,,Xiaying Kuang|Yuxin Che|Huiyan Han|Yimin Liu,Northeastern University|North University of China|North University of China|Shanxi University,3,Semantic-enhanced panoptic scene graph generation through hybrid and axial attentions,https://doi.org/10.1007/s40747-024-01746-z,1.59047268,FALSE,en,TRUE,gold,Complex & Intelligent Systems,journal,2024-12-30,article
https://openalex.org/W4408941304,"Scene Graph Generation (SGG) plays a vital role in determining the graph structure of an image by classifying objects based on their pairwise visual relationships. In the SGG, visually grouped graphs are generated by considering edges as visual relationships between objects and nodes as object classes. Various schemes have been developed to generate scene graphs; however, these techniques require significant computational resources and time for the SGG. In this study, a deep learning-based optimization model, VisionNet_Taylor Hiking Optimization Algorithm (VisionNet_THOA), was introduced to generate high-quality scene graphs from noisy samples. Here, objects were detected by performing semantic segmentation using dynamic routing. The attention areas and actions were determined using the BiFormer method. The nodes in the graph are signified as detected objects and the detected action is represented by edges. Prediction head classification was performed to measure the accuracy of predicting object labels and relationships using VisionNet. The superiority of VisionNet is increased by training the hyperparameters using the Taylor hiking optimization algorithm (THOA). Furthermore, extensive experimental results were obtained using VisionNet_THOA, where VisionNet_THOA attained an accuracy of 94.867%, a True Negative Rate (TNR) of 93.877%, and a True Positive Rate (TPR) of 96.654%, a Precision of 91.765%, and F-Measure of 94.146%.",S Monesh|Manohar Srinivasan,Vellore Institute of Technology University|Vellore Institute of Technology University,0,BiFormer for Scene Graph Generation Based on VisionNet With Taylor Hiking Optimization Algorithm,https://doi.org/10.1109/access.2025.3555230,0.0,FALSE,en,TRUE,gold,IEEE Access,journal,2025-01-01,article
https://openalex.org/W4414404597,,Zhaodi Wang|Yangyan Zeng|Biao Leng|Xiaokang Zhou,Beihang University|Shanghai Zhangjiang Laboratory|Beihang University|Kansai University,0,CDC: Enhancing Scene Graph Generation for IoST-Driven Social Behavioral Modeling With Cooperative Dual Classifier,https://doi.org/10.1109/tcss.2025.3600391,0.0,FALSE,en,FALSE,closed,IEEE Transactions on Computational Social Systems,journal,2025-01-01,article
https://openalex.org/W4416058094,"Image-text retrieval is a crucial task, which targets at finding the counterparts from the opposing modalities. Scene graph based image-text retrieval methods leverage the object and predicate features to reason the cross-modal similarity, therefore increasing the retrieval accuracy. However, existing scene graph based image-text retrieval methods simply fuse the similarity calculations for features at each granularity in a single network, which only brings a slight improvement in the retrieval performance. The features of the scene graph fail to be effectively utilized. Therefore, this paper proposes a Coarse-to-Fine Scene Graph Similarity Reasoning CFSGR method to conduct coarse-grained and fine-grained cross-modal similarity reasoning, separately. CFSGR includes two networks: coarse-grained similarity reasoning network for graphs, fine-grained similarity reasoning network for objects and predicates. Moreover, CFSGR conducts local and global alignments for each feature, ensuring that the similarities at each granularity of visual and textual scene graphs are fully exploited. The evaluation and ablation study on Flickr30K demonstrates the superiority of CFSGR among the SOTA State-Of-The-Art image-text retrieval methods, and CFSGR achieves competitive results with Rsum as 506. The source code is available at https: github.com okeike CFSGR.",Yuankun Liu,,0,Coarse-to-Fine Scene Graph Similarity Reasoning for Image-text Retrieval,https://doi.org/10.65286/icic.v21i3.19398,,FALSE,,TRUE,gold,,,2025-01-01,article
https://openalex.org/W7115190674,"Résumé. Notre contribution s’inscrit dans le cadre des FacLabs, espaces académiques inspirés des FabLabs, dédiés à la co-création et à la fabrication numérique en milieu universitaire. Ces espaces se caractérisent par une hybridité multidimensionnelle, où les étudiants évoluent entre des environnements physiques et numériques, produisent des artefacts tangibles et intangibles, et travaillent en groupe ou individuellement, en présentiel ou à distance, de manière synchrone ou asynchrone. L’étude explore la continuité phygitale,un concept qui vise à intégrer de manière fluide les espaces physiques et numériques en surmontant les discontinuités propres à l’hybridité. La métaphore de l’écotone, empruntée à l’écologie, est utilisée pour décrire les transitions complexes entre les dimensions de cette hybridité et analyser les interactions qu’elles génèrent. Ce cadre conceptuel a conduit à l’identification de technologies-passerelles capables de réduire les discontinuités, inspirées des métaverses mais éloignées de leur logique hyperréaliste.Ces technologies incluent la réalité augmentée, les interfaces tangibles, le retour haptique et l’Internet des objets, toutes coordonnées par un Scene Graph Phygital. Cette structure de données hiérarchique centralise et gère les traces numériques issues des différentes dimensions d’hybridité. Abstract. Our contribution is situated in the context of FacLabs, academic spaces inspired by FabLabs and dedicated to co-creation anddigital fabrication in university environments. These spaces are characterized by a multidimensional hybridity that spans theinterplay between physical and digital environments, tangible and intangible artifacts, and various forms of collaboration—whether in groups or individually, in person or remotely, synchronously or asynchronously. The study examines the concept of phygital continuity, aiming to create a seamless integration of physical and digital spaces by addressing the discontinuities inherent in hybridity. The ecotone metaphor, drawn from ecology, is employed to illustrate and analyze the complex transitions between thesedimensions and the interactions they generate. This conceptual framework identifies bridging technologies, inspired by metaverses but distinct from their hyper-realistic approach, including augmented reality, tangible interfaces, haptic feedback, and the Internet ofThings. These technologies are coordinated by a phygital scene graph, a hierarchical data structure designed to centralize and manage digital traces across hybrid dimensions.","Borgognon, Nathalie|Morcozzet, Laurent|Molinari, Gaëlle",,0,Continuité phygitale des activités collaboratives hybrides de fabrication dans les FacLabs,https://doi.org/10.23709/sticef.32.1.4,0.0,FALSE,,TRUE,green,Association des Technologies et l'Information pour l'Education et la Formation,repository,2025-01-01,article
https://openalex.org/W7106560785,"Scene graph generation has emerged as a powerful tool for AI-driven visual understandingof images by not only detecting objects in an image but also predicting the relationshipsbetween them, such as car–stops at–traffic light or pedestrian–crosses–street. This capabilityis particularly important for autonomous driving, where relational context between roadusers and infrastructure plays a critical role. However, the application of scene graphgeneration in this domain is hindered by the scarcity of annotated datasets. Drivingsimulators such as CARLA provide a scalable alternative, enabling efficient data generationcompared to manual annotation. Yet models trained exclusively on simulated data oftenfail to generalize to real-world data due to the substantial domain gap between the two.This thesis addresses this challenge by proposing a novel data fusion framework thatcombines simulated and real datasets to construct autonomous driving–specific relationshipannotations and subsequently bridge the domain gap for real-world prediction. The workpresents the complete pipeline, including dataset generation in simulation, adaptationof publicly available resources, and augmentation strategies. The Relation Transformermodel is analyzed in depth, and particular attention is given to interpreting its internalmechanisms by visualizing the learned attention maps as heatmaps. This analysis providesinsights into whether the model focuses on semantically meaningful regions when predictingrelationships. Building on this understanding, two new approaches are introduced to enableinference on real data while transferring relational knowledge acquired in simulation. Anablation study further quantifies the impact of the domain gap on model performance andhighlights the strengths and limitations of the proposed methods. Results demonstratethat one of the developed approaches effectively mitigates the simulation-to-reality gapand concrete suggestions for advancing this technique toward further uses for AI-drivenvisual understanding of images in the automotive context are provided.","Kromm, Edward",Forschungszentrum Jülich,0,Data Fusion for Scene Graph Generation: Bridging Simulated and Real-World Datasets,https://doi.org/10.34734/fzj-2025-03665,0.0,FALSE,en,TRUE,green,JuSER Publikationsportal,repository,2025-01-01,article
https://openalex.org/W4412607740,,Shengnan Ke|Shibin Li|Jun Gong|Lingxiang Liu|Jianjun Luo|Bing Wang|Shengjun Tang,Jiangxi Normal University|Jiangxi Normal University|Jiangxi Normal University|Shenzhen University|Jiangxi Normal University||Shenzhen University,0,Dependency-Aware Indoor 3d Scene Graph Prediction Via Multimodal Feature Learning,https://doi.org/10.2139/ssrn.5364013,,FALSE,en,TRUE,green,SSRN Electronic Journal,repository,2025-01-01,preprint
https://openalex.org/W7125951488,"Robotic companions have the potential to revolutionize daily life by helping users with ev-eryday tasks, improving safety, and enhancing quality of life in home environments. However, advancing these robotic companions requires addressing three distinct technical challenges. First, representative household simulation environments lack realistic human agents that move and in- teract naturally with scenes, which limits the development and testing of human-robot interaction algorithms. Second, robots must be able to interpret and respond to user commands given in nat- ural language, a task complicated by the inherent ambiguity in human communication. Finally, to meaningfully improve home safety, robots must be able to detect and respond to emergency situations. This dissertation addresses each of these challenges through novel methods spanning simulation, communication, and emergency response. In simulation, we present PAAK and PACE, novel methods for creating realistic virtual human agents in 3D environments. PAAK places motion-captured human animations into 3D scenes, maintaining human-scene interactions by using “keyframes”—the most important frames for modeling those interactions. PACE builds upon this by modifying motion sequences to adapt to dense, cluttered environments, adjusting the high-DOF pose at each frame to account for unique geometric constraints, and received Best Paper Honorable Mention at IEEE VR 2023. For user communication, we present LGX and LBAP, which enable robots to understand natural language commands and handle ambiguity. LGX leverages Large Language Models (LLMs) for language-guided zero-shot object navigation, achieving a 27% improvement in zero-shot success rate over prior methods on RoboTHOR, with successful real-world validation on a TurtleBot 2 platform. LBAP uses Bayesian inference for uncertainty alignment in LLM planners, mitigat- ing hallucinations and better aligning confidence measures with probability of success, reducing human intervention by over 33% at 70% success rate in real-world experiments on a ClearPath TurtleBot. For household emergencies, we present SafetyDetect and HomeEmergency, enabling robots to detect and respond to dangerous situations. SafetyDetect introduces a dataset of 1000 anoma- lous home scenarios and an LLM-based approach using scene graphs to identify unsafe or unsan- itary conditions, achieving a 96% anomaly detection rate in simulation and 89% on a ClearPath TurtleBot in real-world scenarios. HomeEmergency addresses actively occurring emergencies like falls and fires using audio localization through a novel probabilistic dynamic scene graph, where Bayesian inference enables efficient agent localization, with validation across 30 real- world experiments in apartment environments. While PAAK is primarily a simulation tool, our other methods include real-world validation: LGX, LBAP, SafetyDetect, and HomeEmergency all demonstrate effectiveness on physical robotic platforms. Together, these contributions ad- vance robotic companions across research tools, communication capabilities, and safety features, moving toward more helpful and trustworthy robotic companions.",James Mullen,,0,"ENABLING SAFE ROBOTIC COMPANIONS THROUGH HUMAN-AWARE SIMULATION, NATURAL COMMUNICATION, AND EMERGENCY RESPONSE",https://doi.org/10.13016/lhn0-g6g5,,FALSE,en,TRUE,green,Digital Repository at the University of Maryland (University of Maryland College Park),repository,2025-01-01,other
https://openalex.org/W4412888293,,Bowen Yan|Zhengsong Zhang|Liqiang Jing|Eftekhar Hossain|Xinya Du,,0,FIHA: Automated Fine-grained Hallucinations Evaluations in Large Vision Language Models with Davidson Scene Graphs,https://doi.org/10.18653/v1/2025.findings-acl.622,0.0,FALSE,en,TRUE,gold,,,2025-01-01,article
https://openalex.org/W4411071448,,Jiho Bae|Bong-Jong Choi|Sang–Ho Yeon|Suwon Lee,,1,Fusion Prototypical Network for 3D Scene Graph Prediction,https://doi.org/10.32604/cmes.2025.064789,4.0917585,FALSE,en,TRUE,diamond,Computer Modeling in Engineering & Sciences,journal,2025-01-01,article
https://openalex.org/W7118397357,"Perception systems in autonomous and advanced driver assistance vehicles increasingly rely on large, data driven neural architectures that achieve strong accuracy but remain fundamentally opaque. Their internal reasoning is difficult to interpret, verify, or trace, which poses challenges for the safety certification, debugging, and regulatory transparency. Existing attempts at interpretable perception such as symbolic reasoning, attention visualization, or post hoc saliency rarely provide structured, causally meaningful explanations that planners, auditors, or human operators can reliably trust. This paper introduces a generative perception framework that produces a fully interpretable scene graph representation as the primary output rather than as an optional diagnostic layer. The scene graph encodes objects, semantic attributes, relations, interactions, and driving relevant affordances in a structured form compatible with downstream decision making and formal analysis. The proposed approach employs a generative model that operates in graph latent space to enforce global physical and semantic consistency. Instead of passively extracting relations, the model actively predicts missing, uncertain, or occluded components while maintaining adherence to vehicle dynamics constraints, traffic rules, and common sense priors learned from data. This generative mechanism allows the perception system to expose uncertainty at the node, relation, and affordance levels, enabling explicit traceability of potential failure modes. The resulting graph structure serves as both an interpretable explanation of system’s perception and a robust intermediate representation for planning. Experiments conducted on multi sensor autonomous driving datasets demonstrate that generative scene graphs substantially improve explanation quality and relational correctness, especially under occlusions and degraded sensing. At the same time, detection performance remains competitive with state of the art black box methods. By unifying generative modeling, relational reasoning, and structured explainability, this work positions generative scene graphs as a practical step toward transparent, auditable, and regulator aligned perception pipelines in autonomous vehicles.",Gaurav Pokharkar,Film Independent,0,Generative Scene Graphs for Explainable Perception in Autonomous Vehicles,https://doi.org/10.63282/3050-9262.ijaidsml-v6i4p125,0.0,FALSE,,TRUE,hybrid,International Journal of Artificial Intelligence Data Science and Machine Learning,journal,2025-01-01,article
https://openalex.org/W4409580820,"Deformable Multi-Linear Objects (DMLOs), or Branched Deformable Linear Objects (BDLOs), are flexible objects that possess a linear structure similar to DLOs but also feature branching or bifurcation points where the object’s path diverges into multiple sections. The representation of complex DMLOs, such as wiring harnesses, poses significant challenges in various applications, including robotic systems’ perception and manipulation planning. This paper proposes an approach to address the robust and efficient estimation of a topological representation for DMLOs leveraging a graph-based description of the scene obtained via graph neural networks. Starting from a binary mask of the scene, graph nodes are sampled along the objects’ estimated centerlines. Then, a data-driven pipeline is employed to learn the assignment of graph edges between nodes and to characterize the node’s type based on their local topology and orientation. Finally, by utilizing the learned information, a solver combines the predictions and generates a coherent representation of the objects in the scene. The approach is experimentally evaluated using a test set of complex real-world DMLOs. Within an offline evaluation, the proposed approach achieves a Dice score exceeding 90% in predicting graph edges. Similarly, the identification accuracy of branch and intersection points in the graph topology is above 90%. Additionally, the method demonstrates efficient performance, achieving a runtime of over 20 FPS. In an online assessment employing a dual-arm robotic setup, the approach is successfully applied to disentangle three automotive wiring harnesses, demonstrating the effectiveness of the proposed approach in a real-world scenario.",Alessio Caporali|Kevin Galassi|Riccardo Zanella|Gianluca Palli,University of Bologna|University of Bologna|University of Twente|University of Bologna,0,GNN Topology Representation Learning for Deformable Multi-Linear Objects Dual-Arm Robotic Manipulation,https://doi.org/10.1109/tase.2025.3562231,0.0,FALSE,en,TRUE,hybrid,IEEE Transactions on Automation Science and Engineering,journal,2025-01-01,article
https://openalex.org/W4407948571,,Fan Yu|Beibei Zhang|Tongwei Ren|Jiale Liu|Gangshan Wu|Jinhui Tang,Nanjing University|Nanjing University|Nanjing University|Nanjing University|Nanjing University|Nanjing University of Science and Technology,0,Group Visual Relation Detection,https://doi.org/10.1109/tip.2025.3543114,0.0,FALSE,en,FALSE,closed,IEEE Transactions on Image Processing,journal,2025-01-01,article
https://openalex.org/W7128098921,,Nyffeler Jon|Tombari Federico|Dániel Baráth,,0,Hierarchical 3D Scene Graphs Construction Outdoors,,0.0,FALSE,en,FALSE,closed,SZTAKI Publication Repository (Hungarian Academy of Sciences),repository,2025-01-01,article
https://openalex.org/W4415883255,,Linlin Wang|Tingzhu Wang|Junwei Luo|Yansheng Li,Wuhan University|Wuhan University|Wuhan University|Wuhan University,0,Hierarchical Prototype Learning via Aggregation-Decomposition for Fine-Grained Geospatial Scene Graph Generation,https://doi.org/10.1109/tgrs.2025.3629400,,FALSE,,FALSE,closed,IEEE Transactions on Geoscience and Remote Sensing,journal,2025-01-01,article
https://openalex.org/W7125807346,"In recent years, computer vision has undergone a paradigm shift from static, model-centric analysis to dynamic, interactive systems. These systems empower users to guide and refine model behavior with visual and textual prompts, transforming them from passive observers into active collaborators. This evolution significantly enhances model effectiveness, resolves ambiguity, and broadens the accessibility of AI-driven applications. This thesis presents a cohesive journey through the landscape of interactive prompting, charting a course from simple pixel-level guidance to complex semantic reasoning. The journey begins with interactive visual prompting, where minimal user input is leveraged to solve complex segmentation tasks. The first chapter introduces SimpSON, a novel framework that dramatically simplifies photo cleanup by segmenting multiple distracting objects with just a single click. The subsequent chapter advances this theme with MaGGIe, which tackles the inherent ambiguity of instance matting in multi-person scenes. By using coarse mask prompts, MaGGIe achieves precise alpha mattes, demonstrating how minimal but targeted guidance can ensure robust and accurate outcomes. These works underscore how simple visual interactions can profoundly enhance segmentation accuracy while minimizing user effort. The thesis then transitions to interactive textual prompting, exploring how natural language unlocks greater expressivity in vision systems. The third part introduces CoLLM, a retrieval framework that aligns nuanced textual descriptions with precise visual content without relying on explicitly annotated triplet datasets. Building upon this, the fourth part expands into the omni-modal domain with OmniRet, a unified model that seamlessly integrates image, video, audio, and text. OmniRet learns a joint embedding space to interpret complex, cross-modal user queries, establishing a new foundation model for flexible and efficient information retrieval systems. The final part of this thesis confronts the core challenge of unstructured prompts by introducing structured semantic representations. This section establishes the scene graph as a critical intermediate representation for bridging the gap between language and vision. We demonstrate its power across three domains: enhancing the relational alignment in image-text matching, guiding the compositional generation of content in text-to-image synthesis, and enabling verifiable, grounded action sequences in robotic task planning. These applications reveal how structured semantics are key to unlocking more robust, accurate, and interpretable AI. Collectively, this thesis charts a clear path from simple interactions to sophisticated reasoning, providing a comprehensive exploration of prompting paradigms that are making computer vision systems more responsive, precise, and intuitive. These advancements not only redefine user-model collaboration but also mark significant progress toward more powerful and accessible artificial intelligence.",Chuong Minh Huynh,,0,INTERACTIVE VISION SYSTEMS WITH VISUAL AND TEXTUAL PROMPTS,https://doi.org/10.13016/mkmn-bjzj,,FALSE,en,TRUE,green,Digital Repository at the University of Maryland (University of Maryland College Park),repository,2025-01-01,other
https://openalex.org/W7106019790,"Recent years have seen increasing interest in developing robots that are capable of lifelong and reliable operations around humans in home and factory environments. Despite impressive recent progress towards long-horizon tasks such as laundry fold ing, current efforts are predominantly focused on quasi-static tasks in very structured settings. General-purpose robots that assist humans in daily tasks should be capable of performing a wider range of dynamic and dexterous tasks in unstructured environments, safely interact with humans and their environment, and effectively learn from and adapt to new experiences. One promising approach to building robots capable of assistive lifelong opera tions is to endow them with three core capabilities: a metric, semantic, and temporal (memory) understanding of the world, the ability to perform long-horizon reasoning and planning, and the capacity to execute real-time, closed-loop policies that are dex terous, reactive, and safe in dynamic environments. Such an approach necessitates bridging a fundamental gap in the field. On one hand, traditional approaches based on classical control can provide strong guarantees for safety and optimality for real-time reactive control of dynamic tasks; however, they often rely on low-dimensional, fully observable states and often lack semantic awareness. In contrast, recent advances in vision-language-action models capture rich world semantics and generalize to novel settings, but are typically limited to quasi-static tasks and do not explicitly handle safety or adapt to dynamic changes in real time. The goal of this thesis work is to bridge this gap by exploring methods at the intersection of modern control theory, robot learning, and multimodal foundation models, with the goal of learning generalizable and safety-aware robot policies capable of performing complex, dynamic, and interactive tasks in unstructured environments. Towards this goal, this thesis explores various approaches for leveraging strong structural, algorithmic and semantic priors that can enable both generalization as well as real-time reactive control of dynamic tasks. Our works (1; 2) focus on learn ing to perform dynamic pickup tasks using dynamic-Graph Neural Networks (GNNs) as structural priors and differentiable control algorithms (iLQR) as algorithmic priors for learning compositional skills that operate over multiple dynamic modes as well as generalize to complex environments and novel goals. (1) introduces a framework for learning differentiable optimal skills (LQR) for switching linear dynamical systems from expert demonstrations. The resulting control scheme predicts and accounts for discontinuities due to contact, reacts to unanticipated contact events and generalizes to novel goal conditions and skill compositions. (2) extends this framework to non-linear multibody interactive systems by learning stable locally linear dynamics models using dynamic-GNNs, which enables generalization to novel number of objects and interactions unseen during training. MResT (3) combines the seman tic reasoning capabilities of pre-trained frozen vision-language models (used at low frequency), with the adaptiveness of fine-tuned smaller networks (used at high fre quency), to enable zero-shot generalization to semantic scene variations as well as real time closed-loop control of precise and dynamic tasks. This work focuses on learning generalizable multi-task policies using multi-resolution sensing in table-top settings for short-horizon tasks. GraphEQA(4), considers the long-horizon task of embodied question answering in large unseen indoor environments. GraphEQA, in real-time, constructs a multi-modal memory comprising a real-time 3D metric-semantic scene graph and task-relevant images, for grounding a VLM-based hierarchical planner to perform situated and semantically-informed long-horizon exploration and planning. Finally, VLTSafe (5) combines the semantic safety reasoning capabilities of large pre trained VLMs with safety guarantees of low-level reachability-based RL policies to enable safe control of dynamic tasks in cluttered environments. This approach uses a VLM for identifying relevant task- and safety constraints which inform pretrained parameterized reachability-based RL policies during execution. This thesis takes a step towards the development of generalist embodied agents that integrate the semantic understanding and generalizability of multimodal foundation models with robust, closed-loop control policies that ensure efficiency, dexterity and safety and opens new avenues for future research in scalable, safe, and generalizable robot learning","Saxena, Saumya",Carnegie Mellon University,0,Learning Generalizable Robot Skills for Dynamic and Interactive Tasks,https://doi.org/10.1184/r1/30533204,0.0,FALSE,,TRUE,green,KiltHub Repository,repository,2025-01-01,article
https://openalex.org/W4406365662,,Tao He|Xin Hu|Tongtong Wu|Dongyang Zhang|Xiaogang Li|Yuan-Fang Li|F. Richard Yu,Oldham Council|Oldham Council|Australian Regenerative Medicine Institute|Monash University|University of Electronic Science and Technology of China|Oldham Council|Australian Regenerative Medicine Institute|Monash University|Carleton University,0,Lifelong Scene Graph Generation,https://doi.org/10.2139/ssrn.5097096,,FALSE,en,TRUE,green,SSRN Electronic Journal,repository,2025-01-01,preprint
https://openalex.org/W4415820634,,Zhipeng Wang|Qiao Pan|Yanmin Zhou|Rong Jiang|Jiajun Ma|Bin He|Xin Li,Intelligent Systems Research (United States)|Intelligent Systems Research (United States)|Intelligent Systems Research (United States)|Intelligent Systems Research (United States)|Intelligent Systems Research (United States)|Intelligent Systems Research (United States)|Shanghai Maritime University,1,Long-Sequence Task Planning for Flexible Assembly With Spatio-Temporal Scene Graph,https://doi.org/10.1109/tase.2025.3628487,,FALSE,,FALSE,closed,IEEE Transactions on Automation Science and Engineering,journal,2025-01-01,article
https://openalex.org/W4408131094,,Xiaoguang Chang|Damien Teney|Teng Wang|Lele Xu|Changyin Sun,,0,Masked Vision-and-Language Pre-Training for Scene Graph Generation,https://doi.org/10.2139/ssrn.5165690,,FALSE,en,TRUE,green,SSRN Electronic Journal,repository,2025-01-01,preprint
https://openalex.org/W4412856867,,Xuejiao Wang|Ziheng Huang|Weiliang Meng|Changbo Wang|Gaoqi He,,0,Medp: Multimodal-Enhanced Dynamic Prototype Learning for Few-Shot Dynamic Scene Graph Generation,https://doi.org/10.2139/ssrn.5376753,,FALSE,en,TRUE,green,SSRN Electronic Journal,repository,2025-01-01,preprint
https://openalex.org/W4413238099,,Xiaotian Lv|Yue Zhao|Hanlong Yin|Yifei Chen|Jianxing Liu,Harbin Institute of Technology|Harbin Institute of Technology|Harbin Institute of Technology|Harbin Institute of Technology|Harbin Institute of Technology,0,Msg-Clip: Enhancing Clip's Ability to Learn Fine-Grained Structural Associations Through Multi-Modal Scene Graph Alignment,https://doi.org/10.2139/ssrn.5385250,,FALSE,en,TRUE,green,SSRN Electronic Journal,repository,2025-01-01,preprint
https://openalex.org/W4408100020,,Xinyu Lyu|Lianli Gao|Junlin Xie|Pengpeng Zeng|Yulu Tian|Jie Shao|Hengtao Shen,Southwestern University of Finance and Economics||University of Electronic Science and Technology of China|||University of Electronic Science and Technology of China|University of Electronic Science and Technology of China,1,Multi-Concept Learning for Scene Graph Generation,https://doi.org/10.1109/tip.2025.3540296,19.30239969,FALSE,en,FALSE,closed,IEEE Transactions on Image Processing,journal,2025-01-01,article
https://openalex.org/W4408441582,"Neurosymbolic Scene Graph Generation (SGG) is a promising approach that jointly leverages the perception capabilities of deep neural networks and the reasoning capabilities of symbolic techniques for scene understanding and visual reasoning. SGG systematically captures semantic components, including objects and their relationships, in images, enabling structured representations of visual data. However, existing SGG methods exhibit constrained accuracy and limited expressiveness, particularly in long-tail relationship prediction. To address these limitations, we present MuRelSGG, a novel neurosymbolic SGG framework that integrates a Transformer-based multimodal relationship prediction pipeline with common sense knowledge enrichment. This synergistic combination encapsulates global context, long-range dependencies, and complex object interactions to enhance relationship prediction in SGG. The proposed neurosymbolic architecture begins with object detection via Faster R-CNN, followed by a cascade of Multi-Head Attention Transformers (M-HAT) and Vision Transformers (ViT) for relationship prediction. Subsequently, CSKG enrichment refines and augments visual relationships, improving both accuracy and expressiveness. We conduct extensive evaluations on both the Visual Genome (VG) and GQA datasets to assess performance and generalizability. MuRelSGG achieves substantial gains in recall rates (VG: R@ 100=43.2 , mR@ 100=14.9 ; GQA: R@ 100=42.1 ), outperforming state-of-the-art SGG techniques. Ablation studies confirm the critical contributions of M-HAT, ViT, linguistic features, CSKG enrichment and embedding similarity thresholds, demonstrating the effectiveness of structured knowledge integration for long-tail relationship prediction. These findings underscore the potential of combining deep learning architectures with structured knowledge bases to advance visual scene representation and reasoning.",Muhammad Junaid Khan|Adil Masood Siddiqui|Hamid Saeed Khan|Faisal Akram|Muhammad Jaleed Khan,National University of Sciences and Technology|Ollscoil na Gaillimhe – University of Galway|National University of Sciences and Technology|Medical Diagnostic Laboratories (United States)|National University of Sciences and Technology|National University of Sciences and Technology|Ollscoil na Gaillimhe – University of Galway,2,MuRelSGG: Multimodal Relationship Prediction for Neurosymbolic Scene Graph Generation,https://doi.org/10.1109/access.2025.3551267,9.63949029,FALSE,en,TRUE,gold,IEEE Access,journal,2025-01-01,article
https://openalex.org/W7128203524,,Mohammad Rafeeq|Narendra Bijarniya|Chandramani Chaudhary,"National Institute of Technology Calicut|Birla Institute of Technology and Science, Pilani|National Institute of Technology Calicut",0,Next-Event Prediction in Cybercrime Complaint Narratives Using Temporal Event Scene Graphs,https://doi.org/10.5220/0013647000003967,0.0,FALSE,,FALSE,closed,,,2025-01-01,article
https://openalex.org/W4417002434,,Liguang Zhou|Junjie Hu|Yuhongze Zhou|Tin Lun Lam|Yangsheng Xu,"Chinese University of Hong Kong, Shenzhen|Chinese University of Hong Kong, Shenzhen|McGill University|Chinese University of Hong Kong, Shenzhen|Chinese University of Hong Kong, Shenzhen",0,Peer Learning Approach to Unbiased Scene Graph Generation for Traffic Scene Understanding,https://doi.org/10.1109/tits.2025.3635279,,FALSE,,FALSE,closed,IEEE Transactions on Intelligent Transportation Systems,journal,2025-01-01,article
https://openalex.org/W7126756877,,Nathalie S. Borgognon|Laurent Morcozzet|Gaëlle Molinari,,0,Phygital Continuity of Hybrid Collaborative Fabrication Activities in FacLabs,,,FALSE,,FALSE,closed,Open MIND,repository,2025-01-01,other
https://openalex.org/W7115094698,,"Leong, Jing Yun",,0,Revolutionising surgical training through a chain-of-thought approach to visual-question answering,,,FALSE,en,FALSE,closed,DR-NTU (Nanyang Technological University),repository,2025-01-01,other
https://openalex.org/W4410491790,"With the rapid development in the field of artificial intelligence, isolated object-level perception tasks have achieved tremendous success. However, in the field of object group understanding, existing remote sensing scene graph generation (SGG) methods, a structured description of objects and their inter relationships, tend to focus on comprehensive scenes with numerous elements and complex relationships, which may obscure the characterization of high-value relationships. Consequently, in specific scenes, the concise and efficient description of spatial-temporal relationships between high-value objects (such as vehicles and ships) presents a challenge. Simultaneously, the complexity of spatial relationships within the remote sensing object group necessitates the use of global context to aid in predicting these relationships, while fine-grained local context is essential for accurate relationship prediction. To address this, we propose a ship group relationship description (SGRD) method based on remote sensing SGG with a global and local context fusion network, called GLFN. The proposed network integrates global feature fusion through a transformer-based self-attention mechanism and enhances local feature fusion using a graph convolutional network focused on object-specific graph structures. In addition, we introduce a new dataset featuring ship group scene graphs, named SG2, derived and refined from the HRSC2016 dataset. Experimental results demonstrate that GLFN achieves competitive performance on SG2 in specific scenes compared to classical and recent SGG methods, offering potential for relationship-guided change detection and situational awareness.",Qing Rui|Yanan You|Jingyi Cao|Kaiwen Zhu|Yuanyuan Qiao,Beijing University of Posts and Telecommunications|Beijing University of Posts and Telecommunications|Beijing University of Posts and Telecommunications|Beijing University of Posts and Telecommunications|Beijing University of Posts and Telecommunications,0,SGRD: A Ship Group Relationship Description Method Based on Scene Graph Generation With a Global-Local Context Fusion Network,https://doi.org/10.1109/jstars.2025.3571939,0.0,FALSE,en,TRUE,gold,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,journal,2025-01-01,article
https://openalex.org/W4409129235,,Weijun Zhuang|Bowen Dong|Zhilin Zhu|Zhijun Li|Jie Liu|Yaowei Wang|Xiaopeng Hong|Xin Li|Wangmeng Zuo,Harbin Institute of Technology|Shenzhen Institute of Information Technology|Harbin Institute of Technology|Harbin Institute of Technology|Shenzhen Institute of Information Technology|Harbin Institute of Technology|Harbin Institute of Technology|Peng Cheng Laboratory|Harbin Institute of Technology|Peng Cheng Laboratory|Harbin Institute of Technology,1,Spatial-Temporal Saliency Guided Unbiased Contrastive Learning for Video Scene Graph Generation,https://doi.org/10.1109/tmm.2025.3557688,4.77340731,FALSE,en,FALSE,closed,IEEE Transactions on Multimedia,journal,2025-01-01,article
https://openalex.org/W4408111717,,Kenta Tsukahara|Ryogo Yamamoto|Tanaka Kanji|Hiroki Tomoe,University of Fukui|University of Fukui|University of Fukui|University of Fukui,0,SSGA: Synthetic Scene Graph Augmentation via Multiple Pipeline Variants,https://doi.org/10.5220/0013098100003912,0.0,FALSE,en,TRUE,gold,,,2025-01-01,article
https://openalex.org/W4413612951,,S Monesh|Senthilkumar NC,Vellore Institute of Technology University|Vellore Institute of Technology University,0,Visformer-Based Features and Mixed Attention-Based Transformer Architectures (Matan) for Scene Graph Generation,https://doi.org/10.2139/ssrn.5405539,,FALSE,en,TRUE,green,SSRN Electronic Journal,repository,2025-01-01,preprint
https://openalex.org/W4406099822,"With the increasing demand for accurate multimodal data analysis in complex scenarios, existing models often struggle to effectively capture and fuse information across diverse modalities, especially when data include varying scales and levels of detail. To address these challenges, this study presents an enhanced Swin Transformer V2-based model designed for robust multimodal data processing. The method analyzes urban economic activities and spatial layout using satellite and street view images, with applications in traffic flow and business activity intensity, highlighting its practical significance. The model incorporates a multi-scale feature extraction module into the window attention mechanism, combining local and global window attention with adaptive pooling to achieve comprehensive multi-scale feature fusion and representation. This approach enables the model to effectively capture information at different scales, enhancing its expressiveness in complex scenes. Additionally, a cross-attention-based multimodal feature fusion mechanism integrates spatial structure information from scene graphs with Swin Transformer’s image classification outputs. By calculating similarities and correlations between scene graph embeddings and image classifications, this mechanism dynamically adjusts each modality’s contribution to the fused representation, leveraging complementary information for a more coherent multimodal understanding. Compared with the baseline method, the proposed bimodal model performs superiorly and the accuracy is improved by 3%, reaching 91.5%, which proves its effectiveness in processing and fusing multimodal information. These results highlight the advantages of combining multi-scale feature extraction and cross-modal alignment to improve performance on complex multimodal tasks.",Chun Zhong|Shihong Zeng|Hongqiu Zhu,Hunan University of Science and Technology|Hunan University of Science and Technology|Central South University,0,Adaptive Multimodal Fusion with Cross-Attention for Robust Scene Segmentation and Urban Economic Analysis,https://doi.org/10.3390/app15010438,0.0,FALSE,en,TRUE,gold,Applied Sciences,journal,2025-01-06,article
https://openalex.org/W4406195336,,Chenghang Lai|Shoumeng Qiu,Fudan University|Fudan University,3,MKER: multi-modal knowledge extraction and reasoning for future event prediction,https://doi.org/10.1007/s40747-024-01741-4,14.45923544,FALSE,en,TRUE,gold,Complex & Intelligent Systems,journal,2025-01-09,article
https://openalex.org/W4406278238,,Zuoxu Wang|Zhijie Yan|Shufei Li|Jihong Liu,Beihang University|Beihang University|Huazhong University of Science and Technology|Nanyang Technological University|Beihang University,33,IndVisSGG: VLM-based scene graph generation for industrial spatial intelligence,https://doi.org/10.1016/j.aei.2024.103107,157.52244122,FALSE,en,FALSE,closed,Advanced Engineering Informatics,journal,2025-01-11,article
https://openalex.org/W4406321963,,S. L. Wang|Fei Zhou|Ming Yang|Lei Shi|Chaohong Tan,Zhejiang Meteorological Bureau|Guangxi Zhuang Autonomous Region Health and Family Planning|Zhejiang Meteorological Bureau|Communication University of China|Guangxi Zhuang Autonomous Region Health and Family Planning|Zhejiang Meteorological Bureau,2,SGG-MVAR: Cross-Modal Retrieval With Scene Graph Generation and Multiview Attribute Relationship Guidance,https://doi.org/10.1109/tcss.2024.3524297,9.54681462,FALSE,en,FALSE,closed,IEEE Transactions on Computational Social Systems,journal,2025-01-13,article
https://openalex.org/W4406523182,,Chi Xie|Shuang Liang|Jie Li|Zhao Zhang|Feng Zhu|Rui Zhao|Yichen Wei,Tongji University|Tongji University|Sensimetrics Corporation|Sensimetrics Corporation|Sensimetrics Corporation|Sensimetrics Corporation|TU Wien,4,<i>RelationLMM</i>: Large Multimodal Model as Open and Versatile Visual Relationship Generalist,https://doi.org/10.1109/tpami.2025.3531452,19.09362924,FALSE,en,FALSE,closed,IEEE Transactions on Pattern Analysis and Machine Intelligence,journal,2025-01-17,article
https://openalex.org/W4407692086,,Suhyeon Lee|Jinwoo Jeong|Taehyun Jeong|Sungjei Kim,Korea Electronics Technology Institute|Korea Electronics Technology Institute|Korea Electronics Technology Institute|Korea University of Technology and Education,0,Language-Guided Semantic Alignment for Point Cloud-Based 3D Scene Graph Generation,https://doi.org/10.1109/iceic64972.2025.10879649,0.0,FALSE,en,FALSE,closed,,,2025-01-19,article
https://openalex.org/W4407691079,,Jaehoon Kim|Byoung Chul Ko,Keimyung University|Keimyung University,1,Semantic Image Retrieval Using Scene Graphs and Natural Language Queries,https://doi.org/10.1109/iceic64972.2025.10879722,4.77340731,FALSE,en,FALSE,closed,,,2025-01-19,article
https://openalex.org/W4406682654,,Maëlic Neau|Paulo E. Santos|Anne-Gwenn Bosser|Alistair Macvicar|Cédric Buche,Centre National de la Recherche Scientifique|Université de Bretagne Occidentale|Flinders University|École nationale d'ingénieurs de Brest|Laboratoire des Sciences et Techniques de l’Information de la Communication et de la Connaissance|Institut Universitaire Européen de la Mer|Flinders University|Department of Primary Industries and Regions South Australia|Centre National de la Recherche Scientifique|Université de Bretagne Occidentale|École nationale d'ingénieurs de Brest|Laboratoire des Sciences et Techniques de l’Information de la Communication et de la Connaissance|Institut Universitaire Européen de la Mer|Flinders University|École nationale d'ingénieurs de Brest|Department of Primary Industries and Regions South Australia,0,Mining informativeness in scene graphs: Prioritizing informative relations in Scene Graph Generation for enhanced performance in applications,https://doi.org/10.1016/j.patrec.2025.01.008,0.0,FALSE,en,FALSE,closed,Pattern Recognition Letters,journal,2025-01-21,article
https://openalex.org/W4406761700,"Image-Text Matching (ITM) is a fundamental problem in computer vision. The key issue lies in jointly learning the visual and textual representation to estimate their similarity accurately. Most existing methods focus on feature enhancement within modality or feature interaction across modalities, which, however, neglects the contextual information of the object representation based on the inter-object relationships that match the corresponding sentences with rich contextual semantics. In this article, we propose a Hybrid-modal Interaction with multiple Relational Enhancements (termed Hire ) for ITM, which correlates the intra- and inter-modal semantics between objects and words with implicit and explicit relationship modeling. In particular, the explicit intra-modal spatial-semantic graph-based reasoning network is designed to improve the contextual representation of visual objects with salient spatial and semantic relational connectivities, guided by the explicit relationships of the objects’ spatial positions and their scene graph. We use implicit relationship modeling for potential relationship interactions before explicit modeling to improve the fault tolerance of explicit relationship detection. Then the visual and textual semantic representations are refined jointly via inter-modal interactive attention and cross-modal alignment. To correlate the context of objects with the textual context, we further refine the visual semantic representation via cross-level object-sentence and word-image-based interactive attention. Extensive experiments validate that the proposed hybrid-modal interaction with implicit and explicit modeling is more beneficial for ITM. And the proposed Hire obtains new state-of-the-art results on MS-COCO and Flickr30K benchmarks.",Xuri Ge|Fuhai Chen|Songpei Xu|Fuxiang Tao|Jie Wang|Joemon M. Jose,Shandong University|University of Glasgow|Fuzhou University|University of Glasgow|University of Sheffield|University of Glasgow|University of Glasgow,2,<i>Hire</i> : Hybrid-Modal Interaction with Multiple Relational Enhancements for Image-Text Matching,https://doi.org/10.1145/3714431,9.54681462,FALSE,en,TRUE,green,ACM Transactions on Intelligent Systems and Technology,journal,2025-01-23,article
https://openalex.org/W4406772623,"Scene graph generation (SGG) aims to identify and extract objects from images and elucidate their interrelations. This task faces two primary challenges. Firstly, the long-tail distribution of relation categories causes SGG models to favor high-frequency relations, such as “ on” and “ in” . Secondly, some subject-object pairs may have multiple reasonable relations, which often possess a certain degree of semantic similarity. However, the use of one-hot ground-truth relation labels does not effectively represent the semantic similarities and distinctions among relations. In response to these challenges, we propose a model-agnostic method named Mixup and Balanced Relation Learning (MBRL). This method assigns soft labels to samples exhibiting semantic ambiguities and optimizes model training by adjusting the loss weights for fine-grained and low-frequency relation samples. Its model-agnostic design facilitates seamless integration with diverse SGG models, enhancing their performance across various relation categories. Our approach is evaluated on widely-used datasets, including Visual Genome and Generalized Question Answering, both with over 100,000 images, providing rich visual contexts for scene graph model evaluation. Experimental results show that our method outperforms state-of-the-art approaches on multiple scene graph generation tasks, demonstrating significant improvements in both relation prediction accuracy and the handling of imbalanced data distributions.",Sheng Zhong|Yang Cao|Qiaosen Chen|Jie Gong,South China Normal University|South China Normal University|South China Normal University|South China Normal University,0,Learning with semantic ambiguity for unbiased scene graph generation,https://doi.org/10.7717/peerj-cs.2639,0.0,FALSE,en,TRUE,gold,PeerJ Computer Science,journal,2025-01-23,article
https://openalex.org/W4406778829,,J. Chen|Xiaomeng Wang|Tong Xu|Shiwei Wu,University of Science and Technology of China||University of Science and Technology of China|University of Science and Technology of China,2,Towards Scene-Centric Multi-Level Interest Mining for Video Recommendation,https://doi.org/10.1145/3712600,19.32734649,FALSE,en,FALSE,closed,ACM Transactions on Multimedia Computing Communications and Applications,journal,2025-01-23,article
https://openalex.org/W4406956167,,Lakshita Agarwal|Bindu Verma,Delhi Technological University|Delhi Technological University,1,"Enriching image description generation through multi-modal fusion of VGG16, scene graphs and BiGRU",https://doi.org/10.1007/s00371-024-03790-9,4.77340731,FALSE,en,FALSE,closed,The Visual Computer,journal,2025-01-29,article
https://openalex.org/W4406938057,,Jiayi Tang|Xiaochong Tong|Chunping Qiu|Yuekun Sun|Haoshuai Song|Yaxian Lei|Yi Lei|Congzhou Guo,PLA Information Engineering University|PLA Information Engineering University|PLA Information Engineering University|PLA Information Engineering University|PLA Information Engineering University|PLA Information Engineering University|PLA Information Engineering University|PLA Information Engineering University,1,Remote sensing scene graph generation for improved retrieval based on spatial relationships,https://doi.org/10.1016/j.isprsjprs.2025.01.012,4.77340731,FALSE,en,FALSE,closed,ISPRS Journal of Photogrammetry and Remote Sensing,journal,2025-01-29,article
https://openalex.org/W4407146936,,Hongyun Wang|Jiachen Li|Xiang Xiang|Qing Xie|Yanchun Ma|Yongjian Liu,Wuhan University of Technology|Ministry of Natural Resources|Wuhan University of Technology|Ministry of Natural Resources|Huazhong University of Science and Technology|Wuhan University of Technology|Ministry of Natural Resources|Wuhan University of Technology|Ministry of Natural Resources|Wuhan University of Technology|Ministry of Natural Resources,0,GroupRF: Panoptic Scene Graph Generation with group relation tokens,https://doi.org/10.1016/j.jvcir.2025.104405,0.0,FALSE,en,TRUE,hybrid,Journal of Visual Communication and Image Representation,journal,2025-02-04,article
https://openalex.org/W4409494940,,Jianwei Zhang|Kang Zhou|Junjie Yang|Xiaojian Li,Southern University of Science and Technology|City University of Hong Kong|Chinese Academy of Sciences|Shenzhen Institutes of Advanced Technology|Shenzhen Institutes of Advanced Technology|Chinese Academy of Sciences,0,Latent 3D Scene Graph with Aligned Visual-Language Perception for Object-Goal Navigation,https://doi.org/10.1109/icmcr64890.2025.10962894,0.0,FALSE,en,FALSE,closed,,,2025-02-14,article
https://openalex.org/W4407598244,,Shufei Li|Zhijie Yan|Zuoxu Wang|Yiping Gao,City University of Hong Kong|Huazhong University of Science and Technology|Beihang University|Beihang University|Beihang University|Huazhong University of Science and Technology,11,VLM-MSGraph: Vision Language Model-enabled Multi-hierarchical Scene Graph for robotic assembly,https://doi.org/10.1016/j.rcim.2025.102978,40.88450581,FALSE,en,FALSE,closed,Robotics and Computer-Integrated Manufacturing,journal,2025-02-16,article
https://openalex.org/W4408611586,,Shu‐Kai Hsieh|Huey-Ing Liu,Fu Jen Catholic University|Fu Jen Catholic University,1,Generation of Scene Graph and Semantic Image: A Review and Challenge Ahead,https://doi.org/10.1109/icaiic64266.2025.10920847,4.77340731,FALSE,en,FALSE,closed,,,2025-02-18,review
https://openalex.org/W4407784524,"As a pivotal application of Augmented Reality (AR) technology, AR games empower players to bridge reality with virtuality, offering a distinct and immersive experience set apart from traditional games. However, when creating AR games, one of the most formidable challenges faced by designers pertains to the unpredictability of intricate real-world environments, which hinders crafting naturally integrated scenes where virtual objects harmoniously blend with the players' surroundings. In this paper, we introduce EverywhereAR, a system that is capable of flexibly realizing the designer's idea in various real-world scenes. It provides a designer-friendly Game Scene Template development interface, for designers to quickly graphify their inspirations. To achieve the best AR game scene, this work proposes a highly customizable integration method. According to the integrated AR scene graph, the system will arrange each virtual object in a reasonable position to make the generated game scene look natural. We conducted an experiment to evaluate our system's performance across various game scene templates and real-world environments. Results from the experiment indicated that our system was able to generate AR game scenes matching the quality of scenes manually created by professional designers. In addition, we conducted another experiment to assess the effectiveness and usability of the proposed interface. The experiment results showed that the interface was intuitive and efficient, allowing users to create a simple game scene within one minute.",Jia Liu|Renjie Zhang|Isidro Butaslac|Taishi Sawabe|Yuichiro Fujimoto|Masayuki Kanbara|Hirokazu Kato,Nara Institute of Science and Technology|Nara Institute of Science and Technology|Nara Institute of Science and Technology|Nara Institute of Science and Technology|Nara Institute of Science and Technology|Nara Institute of Science and Technology|Nara Institute of Science and Technology,1,EverywhereAR: A Visual Authoring System for Creating Adaptive AR Game Scenes,https://doi.org/10.1109/tvcg.2025.3544021,4.81974515,FALSE,en,TRUE,hybrid,IEEE Transactions on Visualization and Computer Graphics,journal,2025-02-20,article
https://openalex.org/W4407766431,,Guichang Wu|Qian Zhao|Xiushu Liu,Shanghai University of Electric Power|Shanghai University of Electric Power|People’s Hospital of Rizhao,2,Scene graph sorting and shuffle polishing based controllable image captioning,https://doi.org/10.1007/s11760-025-03921-2,9.54681462,FALSE,en,FALSE,closed,Signal Image and Video Processing,journal,2025-02-20,article
https://openalex.org/W4407825796,"A scene graph is a key image representation in visual reasoning. The generalisability of Scene Graph Generation (SGG) methods is crucial for reliable reasoning and real-world applicability. However, imbalanced training datasets limit this, underrepresenting meaningful visual relationships. Current SGG methods using external knowledge sources face limitations due to these imbalances or restricted relationship coverage, impacting their reasoning and generalisation capabilities. We propose a novel neurosymbolic approach that integrates data-driven object detection with heterogeneous knowledge graph-based object refinement and zero-shot relationship retrieval, highlighting the loosely coupled synergy between neural and symbolic components. This combination addresses the limitations of imbalanced training datasets in scene graph generation and enables effective prediction of unseen visual relationships. Objects are detected using a region-based deep neural network and refined based on their positional and structural similarity, followed by retrieval of pairwise visual relationships using a heterogeneous knowledge graph. The redundant and irrelevant visual relationships are discarded based on the similarity of relationship labels and node embeddings. Finally, the visual relationships are interlinked to generate the scene graph. The employed heterogeneous knowledge graph combines diverse knowledge sources, offering rich common sense knowledge about objects and their interactions in the world. Our method, evaluated using the benchmark Visual Genome dataset and zero-shot recall (zR@K) metric, shows a 59.96% improvement over existing state-of-the-art methods, highlighting its effectiveness in generalised SGG. The object refinement step effectively improved the object detection performance by 57.1%. Additional evaluation using the GQA dataset confirms the cross-dataset generalisability of our method. We also compared various knowledge sources and embedding models to determine an optimal combination for zero-shot SGG. The source code is available at https://github.com/jaleedkhan/zsrr-sgg.",Muhammad Jaleed Khan|John G. Breslin|Edward Curry,Ollscoil na Gaillimhe – University of Galway|Ollscoil na Gaillimhe – University of Galway|Trials Methodology Research Network|Ollscoil na Gaillimhe – University of Galway|Trials Methodology Research Network,3,KnowZRel: Common Sense Knowledge-Based Zero-Shot Relationship Retrieval for Generalized Scene Graph Generation,https://doi.org/10.1109/tai.2025.3544177,14.45923544,FALSE,en,TRUE,hybrid,IEEE Transactions on Artificial Intelligence,journal,2025-02-21,article
https://openalex.org/W4409262610,,Thanh-Son Nguyen|Hong Yang|Basura Fernando,"Institute of High Performance Computing|Agency for Science, Technology and Research|Agency for Science, Technology and Research|Institute of High Performance Computing|Institute of High Performance Computing|Agency for Science, Technology and Research",0,Effective Scene Graph Generation by Statistical Relation Distillation,https://doi.org/10.1109/wacv61041.2025.00816,0.0,FALSE,en,FALSE,closed,,,2025-02-26,article
https://openalex.org/W4409760891,,Taewook Ha|Selin Choi|Seonji Kim|Dooyoung Kim|Woontack Woo,Korea Advanced Institute of Science and Technology|Korea Advanced Institute of Science and Technology|Korea Advanced Institute of Science and Technology|Korea Advanced Institute of Science and Technology|Korea Advanced Institute of Science and Technology,0,Human-Scene Interaction Data Generation with Virtual Environment using User-Centric Scene Graph,https://doi.org/10.1109/vrw66409.2025.00357,0.0,FALSE,en,FALSE,closed,,,2025-03-08,article
https://openalex.org/W4408347185,,Jonghwan Hong|Seonghyeok Noh|Bonhwa Ku|Hanseok Ko,Korea University|Korea University|Korea University|Korea University,0,Less is more: Efficient Scene Graph Generation with reparameterization,https://doi.org/10.1109/icassp49660.2025.10888396,0.0,FALSE,en,FALSE,closed,,,2025-03-12,article
https://openalex.org/W4408351779,,Jinghang Chen|Chi Zhang|Yuehu Liu|Le Wang,Xi'an Jiaotong University|Xi'an Jiaotong University|Xi'an Jiaotong University|Xi'an Jiaotong University,2,Semantic Graph Embedded Energy Minimization Learning for Scene Graph Generation,https://doi.org/10.1109/icassp49660.2025.10889599,19.32734649,FALSE,en,FALSE,closed,,,2025-03-12,article
https://openalex.org/W4408532899,"The generation of images from scene graphs is an important area in computer vision, where structured object relationships are used to create detailed visual representations. While recent methods, such as generative adversarial networks (GANs), transformers, and diffusion models, have improved image quality, they still face challenges, like scalability issues, difficulty in generating complex scenes, and a lack of clear evaluation standards. Despite various approaches being proposed, there is still no unified way to compare their effectiveness, making it difficult to determine the best techniques for real-world applications. This review provides a detailed assessment of scene-graph-based image generation by organizing current methods into different categories and examining their advantages and limitations. We also discuss the datasets used for training, the evaluation measures applied to assess model performance, and the key challenges that remain, such as ensuring consistency in scene structure, handling object interactions, and reducing computational costs. Finally, we outline future directions in this field, highlighting the need for more efficient, scalable, and semantically accurate models. This review serves as a useful reference for researchers and practitioners, helping them understand current trends and identify areas for further improvement in scene-graph-based image generation.",Ijeoma Amuche Chikwendu|Xiaoling Zhang|Happy Nkanta Monday|Grace Ugochi Nneji|Chiagoziem C. Ukwuoma|Okechukwu Chinedum Chikwendu|Yeong Hyeon Gu|Mugahed A. Al–antari,University of Electronic Science and Technology of China|University of Electronic Science and Technology of China|Chengdu University of Technology|Chengdu University of Technology|Chengdu University of Technology|Federal University of Technology Owerri|Sejong University|Sejong University,3,"Advancements, Challenges, and Future Directions in Scene-Graph-Based Image Generation: A Comprehensive Review",https://doi.org/10.3390/electronics14061158,14.32022193,FALSE,en,TRUE,gold,Electronics,journal,2025-03-15,review
https://openalex.org/W4408507942,,Yufeng Hou|Qingguo Zhou|Hui Lv|Lan Guo|Li Yan|La Duo|Zhenyu He,Lanzhou University|Lanzhou University|Lanzhou University|Lanzhou University|Lanzhou University|Qinghai Normal University|Lanzhou University,2,VLSG-net: Vision-Language Scene Graphs network for Paragraph Video Captioning,https://doi.org/10.1016/j.neucom.2025.129976,9.54681462,FALSE,en,FALSE,closed,Neurocomputing,journal,2025-03-17,article
https://openalex.org/W4408611773,"Knowledge-based visual question answering (KB-VQA) requires reasoning about the visual grounding relations between the images and questions by incorporating external knowledge. Existing works typically retrieve knowledge from knowledge graphs by leveraging global multimodal representations of image–text pairs for graph convolution, which neglect contextual clues at hop granularity, resulting in suboptimal spreading and leveraging of contextual information. To this end, we propose a multi-hop graph reasoning network (MGRN) for KB-VQA, which consists of a knowledge graph constructor (KGC) module, a semantic-instructed graph reasoning (SGR) module, and an answering module. MGRN exploits multimodal semantics from given images and questions as instructions for graph reasoning to obtain the knowledge representation from either the scene graph or knowledge base. Specifically, KGC fuses the scene graph with triplets from ConceptNet and Comet to construct a contextual knowledge graph for retrieving knowledge representation. Furthermore, SGR conducts multi-hop graph reasoning to select top- K knowledge items for answering by passing and filtering interplay messages on contextual knowledge graphs under the guidance of multimodal semantic representation. Extensive experiments conducted on two public datasets show the effectiveness and outperformance of our method.",Zihan Hu|Jiuxiang You|Zhenguo Yang|Xiaoping Li|Haoran Xie|Qing Li|Wenyin Liu,Guangdong University of Technology|Hiroshima University of Economics|Higashihiroshima Medical Center|Hiroshima University|Guangdong University of Technology|Guangdong University of Technology||Hong Kong Polytechnic University|Guangdong University of Technology,2,A Multi-Hop Graph Reasoning Network for Knowledge-Based VQA,https://doi.org/10.1145/3724125,9.54681462,FALSE,en,TRUE,bronze,ACM Transactions on Intelligent Systems and Technology,journal,2025-03-19,article
https://openalex.org/W4410493023,,S. Berlin Shiny|A Abisheak|Pavan Balaji|K Rohit,,0,Image Captioning Using Scene Graph Generation,https://doi.org/10.1109/wispnet64060.2025.11005338,0.0,FALSE,en,FALSE,closed,,,2025-03-20,article
https://openalex.org/W4408859084,,Nan Xu|C.P. Chen|Liang Chu|Chong Guo|T. Pang,Jilin University|Jilin University|Jilin University|Jilin University|Beijing Institute of Technology,1,FSGA: Motion Prediction Method Based on Future Scene Graph Attention,https://doi.org/10.1109/tits.2025.3552078,4.77340731,FALSE,en,FALSE,closed,IEEE Transactions on Intelligent Transportation Systems,journal,2025-03-27,article
https://openalex.org/W4410771945,,Yong Sun|Yin Zhu|Yiran Tao|Binghui Li,|PLA Information Engineering University|PLA Information Engineering University|PLA Information Engineering University,0,Scene Graph Generation Approach with Integrated Environmental Contextual Information,https://doi.org/10.1109/cacml64929.2025.11010975,0.0,FALSE,en,FALSE,closed,,,2025-03-28,article
https://openalex.org/W4409145170,"Nowadays, the accessible and technologically advanced edit- ing tools, coupled with the surge in photo and video content pose a great risk to content authenticity. Manipulated content can be used to spread misinformation, cause harassment and infringe human rights. In this article, we compare the effectiveness of two approaches for video manipulation detection using micro and macro information, i.e., a Long Short-Term Memory (LSTM) architecture with frame-level features of videos and their respective ground truths as inputs and a Graph Con- volutional Network (GCN) with frame-level video scene graphs con- catenated using temporal edges. While the LSTM-based model cap- tures frame-level micro-information, the GCN model captures high-level macro-information inside a video.",R. Kurian,,0,Video Manipulation Detection using Sequence Learning and Convolution Networks: A Comparative Study,https://doi.org/10.52783/jisem.v10i30s.4894,0.0,FALSE,en,TRUE,diamond,Journal of Information Systems Engineering & Management,journal,2025-03-31,article
https://openalex.org/W4409110305,"The recording of metaverse experiences supports various use cases in collaboration, VR training, and more. Such Metaverse Recordings can be created as multimedia and time series data during the 3D rendering process of the audio–video stream for the user. To search in a collection of recordings, Multimedia Information Retrieval methods can be used. Also, querying and accessing Metaverse Recordings based on the recorded time series data is possible. The presentation of human-perceivable results of time-series-based Metaverse Recordings is a challenge. This paper demonstrates an approach to generating human-perceivable media from time-series-based Metaverse Recordings with the help of generative artificial intelligence. Our findings show the general feasibility of the approach and outline the current limitations and remaining challenges.",Patrick Steinert|Stefan Wagenpfeil|Ingo Frommholz|Matthias Hemmje,University of Hagen|Software (Germany)|PFH Private University of Applied Sciences|University of Wolverhampton|University of Hagen,1,Artificial-Intelligence-Based Image Generation from Scene Graphs for Metaverse Recording Retrieval,https://doi.org/10.3390/electronics14071427,4.77340731,FALSE,en,TRUE,gold,Electronics,journal,2025-04-01,article
https://openalex.org/W4409544063,,Fardifa Fathmiul Alam|Federico Luricich|Nianyi Li|Yunyi Jia|Bing Li,Clemson University|Clemson University|Clemson University|Clemson University|Clemson University,0,Hierarchical Feature-Based Localization Using Scene Graphs in Off-Road Navigation,https://doi.org/10.4271/2025-01-8046,0.0,FALSE,en,FALSE,closed,SAE technical papers on CD-ROM/SAE technical paper series,journal,2025-04-01,article
https://openalex.org/W4409919532,,Rongsen Wu|Jie Xu|Hao Zheng|Zhiyuan Xu|Zixuan Li|Shixue Cheng|Shumao Zhang,,0,Spatio-temporal features with global–local transformer model for video scene graph generation,https://doi.org/10.1016/j.dcan.2025.04.010,0.0,FALSE,en,TRUE,diamond,Digital Communications and Networks,journal,2025-04-01,article
https://openalex.org/W4409185170,,Chaoyang Tian,Shanghai Jiao Tong University,0,RSG-P: A Fast Global Path Planning Method Based on Route Scene Graph,https://doi.org/10.1007/s12204-025-2813-9,0.0,FALSE,en,FALSE,closed,Journal of Shanghai Jiaotong University (Science),journal,2025-04-04,article
https://openalex.org/W4409166906,,Hong‐Xiang Hu|Xu-Hua Yang|Yuping Zhao,,4,Scene graph generation based on lightweight entity pair object detection and relation classification ensemble,https://doi.org/10.1016/j.neucom.2025.130130,19.09362924,FALSE,en,FALSE,closed,Neurocomputing,journal,2025-04-04,article
https://openalex.org/W4409210580,,Sebastian Künzel|Tanja Munz|Pascal Tilli|Noel Schäfer|Sandeep Vidyapu|Ngoc Thang Vu|Daniel Weiskopf,|Stuttgart University of Applied Sciences|University of Stuttgart|University of Stuttgart|Institut für Mikroelektronik Stuttgart|Stuttgart University of Applied Sciences|University of Stuttgart|Stuttgart University of Applied Sciences|University of Stuttgart|Stuttgart University of Applied Sciences|University of Stuttgart|Institut für Mikroelektronik Stuttgart|Stuttgart University of Applied Sciences|University of Stuttgart|Stuttgart University of Applied Sciences,1,Visual explainable artificial intelligence for graph-based visual question answering and scene graph curation,https://doi.org/10.1186/s42492-025-00185-y,4.77340731,FALSE,en,TRUE,diamond,Visual Computing for Industry Biomedicine and Art,journal,2025-04-07,article
https://openalex.org/W4409314049,,Yilun Sun|Yunliang Chen|Xiaohui Huang|Yuewei Wang|Shaoqian Chen|Kangfei Yao|Ao Yang,,2,DGTM: Deriving Graph from transformer with Mamba for panoptic scene graph generation,https://doi.org/10.1016/j.array.2025.100394,9.54681462,FALSE,en,TRUE,gold,Array,journal,2025-04-09,article
https://openalex.org/W4409335385,,Yufan Hu|Fang Zhang|Ran Wei|Junling Gao,University of Science and Technology Beijing|Tianjin Polytechnic University|China Foreign Affairs University|Tianjin Polytechnic University,1,Learning semantic-unified cross-modal representations for open-vocabulary video scene graph generation,https://doi.org/10.1007/s00530-025-01767-9,4.77340731,FALSE,en,FALSE,closed,Multimedia Systems,journal,2025-04-10,article
https://openalex.org/W4409334164,"This paper discusses new advances in multimodal image captioning, and the focus lies on improving access, contextual understanding, and generalization across many datasets. Current state-of-the-art uni-modal approaches are no longer good enough, whereas innovative multimodal techniques combine the visual and textual features to yield more accurate captions and richness of the captions produced. This includes the attention mechanism, scene graphs, and pre-trained transformer models through which more contextual descriptions are made. Further, the paper addresses challenges in cross-dataset generalization and multilingual captioning, pointing to the necessity of systems that adapt to real-world variability and support diversity in linguistic backgrounds. Through synthesizing the research conducted so far, this work outlines future directions for the creation of more inclusive, robust, and effective image captioning technologies, especially for applications in accessibility",Mr. Nikhil Gopal Khodave|Mr. Prathamesh S. Powar,Human Resource Development Group|Human Resource Development Group,0,"Survey on Multimodal Image Captioning Approaches: Addressing Contextual Understanding, Cross-Dataset Generalization, and Multilingual Captioning",https://doi.org/10.48175/ijarsct-25104,0.0,FALSE,en,TRUE,diamond,International Journal of Advanced Research in Science Communication and Technology,journal,2025-04-10,article
https://openalex.org/W4409356664,,Mingtao Feng|Chan Kit Yan|Zijie Wu|Weisheng Dong|Yaonan Wang|Ajmal Mian,Xidian University|Xidian University|Hunan University|Xidian University|Hunan University|University of Western Australia,38,Hyperrectangle Embedding for Debiased 3D Scene Graph Prediction From RGB Sequences,https://doi.org/10.1109/tpami.2025.3560090,181.38947777,FALSE,en,FALSE,closed,IEEE Transactions on Pattern Analysis and Machine Intelligence,journal,2025-04-11,article
https://openalex.org/W4409367216,"With the beneft of explicit object-oriented reasoning capabilities of scene graphs, scene graph-to-image generation has made remarkable advancements in comprehending object coherence and interactive relations. Recent state-of-the-arts typically predict the scene layouts as an intermediate representation of a scene graph before synthesizing the image. Nevertheless, transforming a scene graph into an exact layout may restrict its representation capabilities, leading to discrepancies in interactive relationships (such as standing on, wearing, or covering) between the generated image and the input scene graph. In this paper, we propose a Scene Graph-Grounded Image Generation (SGG-IG) method to mitigate the above issues. Specifcally, to enhance the scene graph representation, we design a masked auto-encoder module and a relation embedding learning module to integrate structural knowledge and contextual information of the scene graph with a mask self-supervised manner. Subsequently, to bridge the scene graph with visual content, we introduce a spatial constraint and image-scene alignment constraint to capture the fne-grained visual correlation between the scene graph symbol representation and the corresponding image representation, thereby generating semantically consistent and high-quality images. Extensive experiments demonstrate the effectiveness of the method both quantitatively and qualitatively.",Fuyun Wang|Tong Zhang|Yuanzhi Wang|Xiaoya Zhang|Xin Liu|Zhen Cui,Nanjing University of Science and Technology|Nanjing University of Science and Technology|Nanjing University of Science and Technology|Nanjing University of Science and Technology|Cloud Computing Center|Nanjing University of Science and Technology,0,Scene Graph-Grounded Image Generation,https://doi.org/10.1609/aaai.v39i7.32823,0.0,FALSE,en,TRUE,diamond,Proceedings of the AAAI Conference on Artificial Intelligence,conference,2025-04-11,article
https://openalex.org/W4409366005,"Scene Graph Generation (SGG) aims to detect all objects and identify their pairwise relationships existing in the scene. Considering the substantial human labor costs, existing scene graph annotations are often sparse and biased, which result in confusion training with low-frequency predicates. In this work, we design a Semi-Supervised Clustering framework for Scene Graph Generation (SSC-SGG) that uses the sparse labeled data to guide the generation of effective pseudo-labels from unlabeled object pairs, thus enriching the labeled sample space, especially for low-frequency interaction samples. We approach from the perspective of clustering, reducing the problem of confirmation bias in a self-training manner. Specifically, we first enhance the model's robustness to feature extraction via prototype-based clustering, aggregating different relationship augmented features onto the same prototype. Secondly, we design a dynamic pseudo-label assignment algorithm based on a mini-batch, which adjusts the detection sensitivity to different frequency samples from the historical assignment. Finally, we conduct joint training on the pseudo-labels and the labeled data. We conduct experiments on various SGG models and achieve substantial overall performance improvements, demonstrating the effectiveness of SSC-SGG.",Jiarui Yang|Chuan Wang|Jun Zhang|Shuyi Wu|Jinjing Zhao|Zeming Liu|Liang Yang,Shanghai Institute of Computing Technology|Beijing Jiaotong University|Institute of Information Engineering|PLA Academy of Military Science|Information Technology Laboratory|Beihang University|Hebei University of Technology,0,Semi-Supervised Clustering Framework for Fine-grained Scene Graph Generation,https://doi.org/10.1609/aaai.v39i9.32998,0.0,FALSE,en,TRUE,diamond,Proceedings of the AAAI Conference on Artificial Intelligence,conference,2025-04-11,article
https://openalex.org/W4409535208,"In autonomous driving, retrieving a specific traffic scene in huge datasets is a significant challenge. Traditional scene retrieval methods struggle to cope with the semantic complexity and heterogeneity of traffic scenes and are unable to meet the variable needs of different users. This paper proposes “Query-by-Example”, a traffic scene retrieval approach based on Visual-Large Language Model (VLM)-generated Road Scene Graph (RSG) representation. Our method uses VLMs to generate structured scene graphs from video data, capturing high-level semantic attributes and detailed object relationships in traffic scenes. We introduce an extensible set of scene attributes and a graph-based scene description to quantify scene similarity. We also propose a RSG-LLM benchmark dataset containing 1000 traffic scenes, their corresponding natural language descriptions, and RSGs to evaluate the performance of LLMs in generating RSGs. Experiments show that our method can effectively retrieve semantically similar traffic scenes from large databases, supporting various query formats, including natural language, images, video clips, rosbag, etc. Our method provides a comprehensive and flexible framework for traffic scene retrieval, promoting its application in autonomous driving systems.",Yafu Tian|Alexander Carballo|Ruifeng Li|Simon Thompson|Kazuya Takeda,Nagoya University|Harbin Institute of Technology|Nagoya University|Gifu University|Harbin Institute of Technology|Nagoya University|Nagoya University,2,Query by Example: Semantic Traffic Scene Retrieval Using LLM-Based Scene Graph Representation,https://doi.org/10.3390/s25082546,9.54681462,FALSE,en,TRUE,gold,Sensors,journal,2025-04-17,article
https://openalex.org/W4409593436,,Ming Zhao|Jing Zhang,,2,Panoptic segmentation-based semantic embedding matching model for scene graph generation,https://doi.org/10.1016/j.patrec.2025.04.005,9.54681462,FALSE,en,FALSE,closed,Pattern Recognition Letters,journal,2025-04-19,article
https://openalex.org/W4409624932,,Minh Tam Nguyen|Quynh T. Nguyen|Minh Son Dao|Binh T. Nguyen,Vietnam National University Ho Chi Minh City|Hanoi University of Industry|National Institute on Consumer Education|Ho Chi Minh City University of Science|Vietnam National University Ho Chi Minh City,1,Multimodal scene-graph matching for cheapfakes detection,https://doi.org/10.1007/s13735-025-00365-9,4.77340731,FALSE,en,FALSE,closed,International Journal of Multimedia Information Retrieval,journal,2025-04-21,article
https://openalex.org/W4411272219,,Trey Woodlief|Felipe Toledo|Sebastian Elbaum|Matthew B. Dwyer,University of Virginia|University of Virginia|University of Virginia|University of Virginia,1,Closing the Gap Between Sensor Inputs and Driving Properties: A Scene Graph Generator for CARLA,https://doi.org/10.1109/icse-companion66252.2025.00017,4.77340731,FALSE,en,FALSE,closed,,,2025-04-27,article
https://openalex.org/W7114832162,,"Wang, Jun",,0,Automatic radiology report generation,,,FALSE,en,FALSE,closed,Warwick Research Archive Portal (University of Warwick),repository,2025-05-01,other
https://openalex.org/W4410015982,,Jian Luo|Jian Zhang|Bo Cai|Yaoxiang Yu|Aihua Ke,,4,Learning hierarchical scene graph and contrastive learning for object goal navigation,https://doi.org/10.1016/j.knosys.2025.113532,19.09362924,FALSE,en,FALSE,closed,Knowledge-Based Systems,journal,2025-05-01,article
https://openalex.org/W7110601951,"This thesis advances video understanding by enhancing Video Scene Graph Generation (VidSGG) through improved temporal modeling, the integration of long-range temporal dependencies via continuous updates to interaction histories, and the utilization of Large Language Models (LLMs) for scene graph reasoning. To this end, three novel datasets and corresponding approaches are introduced. First, the ASPIRe dataset incorporates interactivity annotations and leverages the Hierarchical Interlacement Graph (HIG) for hierarchical temporal modeling, providing deep insights into scene changes and effectively capturing intricate interactions. Next, the AeroEye dataset, focusing on drone videos, is paired with the Cyclic Graph Transformer (CYCLO), which establishes circular connectivity among video frames to model direct and long-range temporal relationships. Finally, the VSGR dataset, a new large-scale benchmark for advancing scene graph reasoning tasks, is introduced. Notably, the Multimodal LLMs on a Scene HyperGraph (HyperGLM) approach integrates hypergraph-based representations with LLMs, enabling more nuanced multimodal reasoning. These contributions significantly enhance dataset diversity, strengthen relationship modeling, and improve causal reasoning in VidSGG, resulting in state-of-the-art performance.","Nguyen, Trong-Thuan",,0,Towards Multimodal Scene Graph Generation Approaches to Video Understanding,,0.0,FALSE,,TRUE,green,Journal of the Arkansas Academy of Science,journal,2025-05-01,article
https://openalex.org/W7110599359,"Group Activity Recognition (GAR) has emerged as a crucial problem in computer vision, with wide-ranging applications in sports analysis, video surveillance, and social scene understanding. Unlike traditional action recognition focused on individuals, GAR requires understanding complex spatiotemporal relationships between multiple actors, their interactions, and the broader context in which these activities occur. This complexity introduces unique challenges, including the need for accurate actor localization, modeling of inter-actor dependencies, and understanding of temporal evolution in group behaviors. While recent advances have shown promise, existing approaches often rely heavily on extensive annotations such as ground-truth bounding boxes and action labels, creating significant barriers to practical deployment and scalability. Additionally, current methods struggle to capture the full spectrum of contextual factors that give meaning to group activities, particularly in real-world applications where multiple modalities of information are available. We first introduce Self-supervised Spatiotemporal Transformers Approach to Group Activity Recognition (SPARTAN) and Self-supervised Spatiotemporal Attention-based Social Group Activity Recognition (SoGAR), novel self-supervised frameworks that significantly reduce annotation requirements while maintaining high recognition accuracy. SPARTAN leverages multi-resolution temporal views to capture varied motion characteristics, while SoGAR implements temporal collaborative learning and spatiotemporal cooperative learning strategies. These approaches achieve state-of-the-art performance on multiple benchmark datasets, including JRDB-PAR, NBA, and Volleyball, without requiring person-level annotations. In the multimodal domain, we present three frameworks: Recognize Every Action Everywhere All At Once (REACT), which employs a Vision-Language Encoder for sparse spatial interactions; Hierarchical Attention-Flow Mechanism for Group-Activity Scene Graph Generation in Videos (HAtt-Flow), which introduces flow conservation principles in attention mechanisms; and LiDAR-Guided Hierarchical Transformer for Multi-Modal Group Activity Recognition (LiGAR), which utilizes LiDAR data as a structural backbone for processing visual and textual information. These frameworks demonstrate significant improvements in capturing cross-modal dependencies and spatial-temporal relationships. Finally, we extend our research to healthcare applications, particularly in analyzing tobacco-related content on social media platforms. We develop Public Health Advocacy Dataset: A Dataset of Tobacco Usage Videos from Social Media (PHAD), Flow-Attention Adaptive Semantic Hierarchical Fusion for Multimodal Tobacco Content Analysis (FLAASH), and A Large-scale 1M Dataset and Foundation Model for Tobacco Addiction Prevention (DEFEND) frameworks to address the limitations of current Large Language Models in processing video content. Our experimental results show substantial improvements over existing methods, achieving up to 10.6% gains in F1-score on JRDB-PAR and 5.9% improvement in Mean Per Class Accuracy on the NBA dataset. This thesis advances the field of group activity recognition by reducing reliance on extensive annotations, improving multimodal integration, and demonstrating practical applications in public health monitoring. The proposed frameworks provide a foundation for future research in automated understanding of complex group behaviors while addressing real-world challenges in data annotation and multimodal analysis.","Chappa, Naga Venkata Sai Raviteja",,0,Vision-Based Multimodal Frameworks for Human Behavioral Analysis: Applications in Group Activity Understanding and Public Health,,0.0,FALSE,,TRUE,green,Journal of the Arkansas Academy of Science,journal,2025-05-01,article
https://openalex.org/W4410055778,,Soohwan Jeong|Jongmin Park|Min-Gyu Choi|Yongjin Kwon|Sungsu Lim,Chungnam National University|Chungnam National University|Chungnam National University|Electronics and Telecommunications Research Institute|Chungnam National University,2,LLM-powered scene graph representation learning for image retrieval via visual triplet-based graph transformation,https://doi.org/10.1016/j.eswa.2025.127926,9.54681462,FALSE,en,FALSE,closed,Expert Systems with Applications,journal,2025-05-03,article
https://openalex.org/W4410144297,,Bo Sun|Zhuo Hao|Lejun Yu|Jun He,Beijing Normal University|Beijing Normal University|Beijing Normal University|Beijing Normal University,0,Mutual introspective distillation for unbiased scene graph generation,https://doi.org/10.1007/s11227-025-07246-2,0.0,FALSE,en,FALSE,closed,The Journal of Supercomputing,journal,2025-05-07,article
https://openalex.org/W4410232384,,Peng Wu|Jian Wang|Yang Liu|Jianyu Pan,,0,Attribute mining of multisource and multimodal data from social media based on generalized scene graphs,https://doi.org/10.1117/12.3059635,0.0,FALSE,en,FALSE,closed,,,2025-05-09,article
https://openalex.org/W4410432646,"Coastal environments are dynamic and ecologically significant, yet monitoring across multiple sites and analysis remain challenging due to the lack of domain-specific datasets tailored to their unique features. General-purpose models, including those used for scene graph generation, often fail to capture the semantic details necessary for meaningful comparisons in this context. This paper outlines the process of creating a domain-specific dataset for coastal environments, focusing on the challenges posed by crowdsourced imagery, such as variability in image sizes, lighting conditions, and camera quality. By leveraging scene graph generation to capture semantic meaning, this research seeks to create a domain-specific dataset suitable for the comparison of coastal environments. This work demonstrates how domain-specific datasets can drive innovation in computer vision and semantic understanding, contributing to the broader field of artificial intelligence by bridging the gap between generalized tools and specialized applications. Ultimately, this effort lays the groundwork for future planned research to develop a pipeline capable of generating comparison metrics based on the semantic content of scenes. Using raw standardized images of coastal environments from the Coastie Initiative, this pipeline aims to go beyond superficial appearance comparisons, offering more meaningful analyses that could enhance our understanding and support conservation efforts.",Nathan Cherry|Ziad Kobti|Chris Houser,University of Windsor|University of Windsor|University of Waterloo,0,Creating Domain-Specific Datasets for Intelligent Environmental Feature Comparison,https://doi.org/10.32473/flairs.38.1.138913,0.0,FALSE,en,TRUE,diamond,Proceedings of the ... International Florida Artificial Intelligence Research Society Conference,conference,2025-05-14,article
https://openalex.org/W4410454823,"Understanding the intricate interplay between actions and their consequential effects is a cornerstone of human intelligence and decision-making processes. Enabling artificial agents to emulate such capabilities is essential for fostering seamless interaction in dynamic, real-world environments. In response to this demand, we present a novel approach, termed Differential Effect-Aware Reasoner (DEAR), which systematically leverages the structured representations encapsulated within scene-graphs to model the nuanced outcomes of actions articulated in natural language. Unlike prior methods that predominantly rely on monolithic visual features paired with linguistic cues, DEAR capitalizes on observing relational differences across state transitions induced by actions. By employing paired scene-graphs reflecting pre-action and post-action states, our approach enhances the agent's sensitivity to subtle state variations. To empirically validate the effectiveness and robustness of DEAR, we conduct extensive evaluations on the CLEVR\_HYP dataset. The experimental results consistently demonstrate that DEAR surpasses baseline models in terms of reasoning accuracy, data efficiency, and cross-scenario generalization, thus underscoring its potential as a foundational mechanism for future action-effect reasoning systems.",Michelle Everett|Wyne Nasir|R. Gordon Cassidy,Tufts University|Tufts University|Tufts University,0,A Differential Effect-Aware Reasoner for Action Dynamics,https://doi.org/10.20944/preprints202505.1174.v1,,FALSE,en,TRUE,green,Preprints.org,repository,2025-05-15,preprint
https://openalex.org/W4410494570,,Jianwu Jiang|Jiawei Zhou|Yushi Yan|Yuefeng Wang|Lina Wang|Jingwen Li,Guilin University of Technology|Guilin University of Technology|Guilin University of Technology|Guilin University of Technology|Jiangxi Province Forestry Survey Planning Institute|Guilin University of Technology,0,A hierarchical indoor spatial-semantic reasoning-based scene graph construction for elderly-centric safety warnings,https://doi.org/10.1016/j.rineng.2025.105397,0.0,FALSE,en,TRUE,gold,Results in Engineering,journal,2025-05-19,article
https://openalex.org/W4413925104,,Simon Janzon|Carlos Medina-Sánchez|Alexander J. Golkowski|Marcus Handte|Pedro José Marrón,University of Duisburg-Essen|University of Duisburg-Essen|University of Duisburg-Essen|University of Duisburg-Essen|University of Duisburg-Essen,0,Enhancing 3D Scene Graphs with Real-Time Room Classification,https://doi.org/10.1109/icra55743.2025.11128432,0.0,FALSE,en,FALSE,closed,,,2025-05-19,article
https://openalex.org/W4413945312,,Qingfeng Li|Xinlei Zhang|Chen Chen|Haochen Zhao|Jianwei Niu,Beihang University|Beihang University|Beihang University|Beihang University|Beihang University,0,Interaction-Driven Updates: 3D Scene Graph Maintenance During Robot Task Execution,https://doi.org/10.1109/icra55743.2025.11128194,0.0,FALSE,en,FALSE,closed,,,2025-05-19,article
https://openalex.org/W4413944814,,Yaowen Zhang|Yuan Ruan|Miaoxin Pan|Yi Yang|Mengyin Fu,Beijing Institute of Technology|Beijing Institute of Technology|Beijing Institute of Technology|Beijing Institute of Technology|Nanjing University of Science and Technology,0,Parking-SG: Open-Vocabulary Hierarchical 3D Scene Graph Representation for Open Parking Environments,https://doi.org/10.1109/icra55743.2025.11128767,0.0,FALSE,en,FALSE,closed,,,2025-05-19,article
https://openalex.org/W4410604293,"Text-based image retrieval is one of the most common approaches for searching images acquired from vision sensors such as cameras. However, this method suffers from limitations in retrieval accuracy, particularly when the query contains limited information or involves previously unseen sentences. These challenges arise because keyword-based matching fails to adequately capture contextual and semantic meanings. To address these limitations, we propose a novel approach that transforms sentences and images into semantic graphs and scene graphs, enabling a quantitative comparison between them. Specifically, we utilize a graph neural network (GNN) to learn features of nodes and edges and generate graph embeddings, enabling image retrieval through natural language queries without relying on additional image metadata. We introduce a contrastive GNN-based framework that matches semantic graphs with scene graphs to retrieve semantically similar images. In addition, we incorporate a hard negative mining strategy, allowing the model to effectively learn from more challenging negative samples. The experimental results on the Visual Genome dataset show that the proposed method achieves a top nDCG@50 score of 0.745, improving retrieval performance by approximately 7.7 percentage points compared to random sampling with full graphs. This confirms that the model effectively retrieves semantically relevant images by structurally interpreting complex scenes.",Kim Jaehoon|Byoung Chul Ko,Keimyung University|Keimyung University,0,Scene Graph and Natural Language-Based Semantic Image Retrieval Using Vision Sensor Data,https://doi.org/10.3390/s25113252,0.0,FALSE,en,TRUE,gold,Sensors,journal,2025-05-22,article
https://openalex.org/W4410639351,,Yunfei Guo|Fei Yin|Xiao-Hui Li|Chenglin Liu,Chinese Academy of Sciences|Shandong Institute of Automation|Chinese Academy of Sciences|Shandong Institute of Automation|Shandong Institute of Automation|Chinese Academy of Sciences|Chinese Academy of Sciences|Shandong Institute of Automation,0,QDNet: Query-Denoising Network for Visual Traffic Knowledge Graph Generation,https://doi.org/10.1109/tpami.2025.3572944,0.0,FALSE,en,FALSE,closed,IEEE Transactions on Pattern Analysis and Machine Intelligence,journal,2025-05-23,article
https://openalex.org/W4410735516,"Constructing maps suitable for autonomous vehicles (AVs) is a critical research focus in autonomous driving and AI, extending cartography’s challenges. Building on cartographic principles, we propose the concept of a road scene map along with its modeling method that incorporates dynamic/static traffic elements with geometric/semantic features. Current limitations include unclear road scene graph relationships and a lack of integration among 3D traffic entity detection, map element detection, and scene relation extraction. To address these issues, we propose a method for constructing road scene maps: (1) A multi-task detection model identifies traffic entities and map elements directly in bird’s-eye-view (BEV) space, providing precise location, geometry, and attribute data; (2) A unified road scene relation pattern enables rule-based spatial/semantic relationship extraction. Experiments on nuScenes demonstrate improvements: the detection model achieves 1.5% and 1.9% accuracy gains in traffic entity and map element detection over state-of-the-art methods, while the relation extraction method covers broader perceptual ranges and more complex interactions. Results confirm the effective integration of 3D object detection, map element recognition, and scene relation extraction into a unified map. This integration delivers critical environmental information (locations, geometries, attributes, and spatial/semantic relationships) to AVs, significantly enhancing their perception and reasoning in dynamic road scenarios.",Juan Lei|Xiong You|Jiangpeng Tian|Jian Yang|Kuiliang Gao|Weitang Liu,PLA Information Engineering University|PLA Information Engineering University|PLA Information Engineering University|PLA Information Engineering University|PLA Information Engineering University|PLA Information Engineering University,1,Road scene map for autonomous driving and modeling method,https://doi.org/10.1080/17538947.2025.2505623,2.17737573,FALSE,en,TRUE,gold,International Journal of Digital Earth,journal,2025-05-26,article
https://openalex.org/W4410783191,"Abstract Context-awareness is a pivotal trend within the Internet of Things research area, facilitating the near real-time processing and interpretation of relevant sensor data to enhance data processing efficiency. Context Management Platforms (CMPs), as advanced IoT middleware platforms offer promising solutions to IoT applications by offering seamless interoperability between different IoT silos. However, significant gaps still remain in evaluating the performance of CMPs, especially the data retrieval aspect of CMPs. Existing IoT middleware evaluation approaches lack flexible, user-friendly approaches to generate query loads to address aforementioned limitation. This paper introduces ACOCA-G (Adaptive COntext CAching-Generator), a configurable automated framework designed to generate scenario-based context queries to evaluate data retrieval of CMPs. ACOCA-G transforms real-world scenarios into scene descriptions in a specified format, which are then converted into Context-aware Scene Graphs (CSGs). ACOCA-G framework introduces a novel situation reasoning approach that integrates concepts of Context Spaces Theory (CST) and enhanced Situation State Machines (SSM). CSG reasoning identifies situations, facilitating the generation of dynamic and context-specific query loads. The framework produces template-based queries of varied complexities, demonstrating ACOCA-G’s flexibility and adaptability in generating queries closely aligning with real-world query complexities. Empirical evaluations using real-world IoT datasets and scenes of diverse complexities assess ACOCA-G in terms of query completeness, volume, and generation efficiency, providing insights into accuracy, scalability, and effectiveness. As a pioneering solution for systematically evaluating CMP performance, ACOCA-G serves as a robust tool for addressing gaps in CMP data retrieval assessment.",Ravindi de Silva|Arkady Zaslavsky|Seng W. Loke|Prem Prakash Jayaraman,Deakin University|Deakin University|Deakin University|Swinburne University of Technology,0,ACOCA-G: scenario-based context query generation for evaluating performance of context management platforms,https://doi.org/10.1007/s43926-025-00159-9,0.0,FALSE,en,TRUE,gold,Discover Internet of Things,journal,2025-05-27,article
https://openalex.org/W4410831792,"Abstract Panoptic Scene Graph Generation (PSG) aims to segment objects and predict the relation triplets <subject, relation, object> within an image. Despite the impressive achievements in PSG, current methods still struggle to capture fine-grained visual context, eschewing spatial and situational information in favor of visual features related to object identity. This limitation naturally impedes the model’s ability to distinguish subtle visual differences between relation triplets, such as “cat-on-person” and “cat-lying on-person”. To address this challenge, we propose CVCPSG, a novel DETR-based method that uncovers composite visual clues for PSG. Specifically, drawing inspiration from how humans capture visual context using diverse visual clues, we first construct a composite visual clues bank based on three key aspects: object, spatial, and situational. Then, we introduce a multi-level visual extractor to align visual features from objects, interactions, and image levels with the composite visual clues bank. Additionally, we incorporate a cross-modal learning module with a multitower architecture to seamlessly integrate visual clues into the relation decoder, thereby improving PSG detection. Extensive experiments on two PSG benchmarks confirm the effectiveness and interpretability of CVCPSG.",Nanhao Liang|Xiaoyuan Yang|Yingwei Xia|Yong Liu,University of Science and Technology of China|Hefei Institutes of Physical Science|University of Science and Technology of China|Hefei Institutes of Physical Science|Hefei Institutes of Physical Science|Hefei Institutes of Physical Science,0,CVCPSG: Discovering Composite Visual Clues for Panoptic Scene Graph Generation,https://doi.org/10.1007/s44443-025-00063-w,0.0,FALSE,en,TRUE,gold,Journal of King Saud University - Computer and Information Sciences,journal,2025-05-28,article
https://openalex.org/W4410854624,"ABSTRACT In this manuscript, we introduce a semi-automatic scene graph an- notation tool for images, the GeneAnnotator. This software allows human annotators to describe the existing relationships between participators in the visual scene in the form of directed graphs, Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. MM ’21, October 20–24, 2021, Chengdu, China © 2021 Association for Computing Machinery. ACM ISBN 978-1-4503-XXXX-X/18/06... $15.00 https://doi.org/10.1145/1122445.1122456 hence enabling the learning and reasoning on visual relationships, e.g., image captioning, VQA and scene graph generation, etc. The annotations for certain image datasets could either be merged in a single VG150 data-format file to support most existing models for scene graph learning or transformed into a separated annotation file for each single image to build customized datasets. Moreover, GeneAnnotator provides a rule-based relationship recommending algorithm to reduce the heavy annotation workload. With GeneAn- notator, we propose Traffic Genome, a comprehensive scene graph dataset with 1000 diverse traffic images, which in return validates the effectiveness of the proposed software for scene graph annota- tion. The project source code, with usage examples and sample data",J Ashwini,,0,A Semi-automatic Annotation of Scene Graph,https://doi.org/10.55041/ijsrem49105,0.0,FALSE,en,TRUE,hybrid,INTERANTIONAL JOURNAL OF SCIENTIFIC RESEARCH IN ENGINEERING AND MANAGEMENT,journal,2025-05-29,article
https://openalex.org/W4413145643,,Jiawei Fu|Tiantian Zhang|Kai Chen|Qi Dou,Chinese University of Hong Kong|Chinese University of Hong Kong|Chinese University of Hong Kong|Chinese University of Hong Kong,1,Hybrid Reciprocal Transformer with Triplet Feature Alignment for Scene Graph Generation,https://doi.org/10.1109/cvpr52734.2025.00837,4.77340731,FALSE,en,FALSE,closed,,,2025-06-10,article
https://openalex.org/W4413157161,,Wenhuan Huang|Yi Ji|Guiqian Zhu|Ying Li|Chunping Liu,Soochow University|Soochow University|Soochow University|Soochow University|Soochow University,1,Navigating the Unseen: Zero-shot Scene Graph Generation via Capsule-Based Equivariant Features,https://doi.org/10.1109/cvpr52734.2025.02742,4.77340731,FALSE,en,FALSE,closed,,,2025-06-10,article
https://openalex.org/W4413145822,,Changsheng Lv|Mengshi Qi|Liang Liu|Huadóng Ma,Beijing University of Posts and Telecommunications|Beijing University of Posts and Telecommunications|Beijing University of Posts and Telecommunications|Beijing University of Posts and Telecommunications,2,T<sup>2</sup>SG: Traffic Topology Scene Graph for Topology Reasoning in Autonomous Driving,https://doi.org/10.1109/cvpr52734.2025.01603,9.54681462,FALSE,en,FALSE,closed,,,2025-06-10,article
https://openalex.org/W7110813183,"In the construction of the scene graph, the image regions from the visual input and the words from the textual input form the vertices. The intra-modal and cross-modal relationships between these vertices are represented by the weights of the edges. During the extraction of knowledge entries, we start from anchor objects, i.e., vertices that possess at least one cross-modal edge, and extract the corresponding knowledge entries associated with these anchors.",Zihan Guo (1774495)|Xiang Shen (127119)|Chongqing Chen (14185807),,0,The process of knowledge extraction.,https://doi.org/10.1371/journal.pone.0325543.g003,,FALSE,,TRUE,gold,,,2025-06-10,other
https://openalex.org/W4414198445,,Yu-Chen Lai|Motoharu Sonogashira|Itthisak Phueaksri|Yasutomo Kawanishi,Eiken Chemical (Japan)|Eiken Chemical (Japan)|Eiken Chemical (Japan)|Eiken Chemical (Japan),0,Scene-Specific Anomalous Relationship Detection Using Scene Graph Summarization,https://doi.org/10.1109/cvprw67362.2025.00384,0.0,FALSE,en,FALSE,closed,,,2025-06-11,article
https://openalex.org/W4415744687,,Yizhen Wang,Weifang University of Science and Technology,0,Enhancing Boosting-Guided Image Captioning with Scene-Graph-Based Vision Transformer Integrated with GAN,https://doi.org/10.1109/asip65980.2025.00024,,FALSE,,FALSE,closed,,,2025-06-13,article
https://openalex.org/W4411306396,,Yi-Wen Mo|Chiao‐Ling Kuo|Zhi-Sheng Lin,"Research Center for Humanities and Social Sciences, Academia Sinica|National Taiwan University|Research Center for Humanities and Social Sciences, Academia Sinica|Research Center for Humanities and Social Sciences, Academia Sinica",0,Enhancing arcade detection using bi-directed scene graphs with graph convolutional networks on street view imagery,https://doi.org/10.1016/j.rsase.2025.101628,0.0,FALSE,en,FALSE,closed,Remote Sensing Applications Society and Environment,journal,2025-06-14,article
https://openalex.org/W4411450263,,Trey Woodlief|Felipe Toledo|Matthew B. Dwyer|Sebastian Elbaum,University of Virginia|University of Virginia|University of Virginia|University of Virginia,0,Scene Flow Specifications: Encoding and Monitoring Rich Temporal Safety Properties of Autonomous Systems,https://doi.org/10.1145/3729382,0.0,FALSE,en,FALSE,closed,Proceedings of the ACM on software engineering.,journal,2025-06-19,article
https://openalex.org/W4413010317,,Akash Sonth|A. Lynn Abbott|Abhijit Sarkar,Virginia Tech|Virginia Tech|Virginia Tech,0,Intersection Safety Modeling Using Semantic Scene Graph and Graph Neural Network,https://doi.org/10.1109/iv64158.2025.11097800,0.0,FALSE,en,FALSE,closed,,,2025-06-22,article
https://openalex.org/W4411635655,,Yujun Hu|X. Y. Zhou|Changbo Wang|Weiliang Meng|Gaoqi He,East China Normal University|East China University of Science and Technology|East China Normal University|Shandong Institute of Automation|Chinese Academy of Sciences|East China Normal University|Chongqing Normal University,0,3D Scene Graph Generation with Cross-Modal Alignment and Adversarial Learning,https://doi.org/10.1145/3731715.3733257,0.0,FALSE,en,FALSE,closed,,,2025-06-25,article
https://openalex.org/W4411631848,,Zihan Kong|Haiwei Zhang,Nankai University|Nankai University,0,OpenSGen: Fine-Grained Relation-Aware Prompt for Open-Vocabulary Scene Graph Generation,https://doi.org/10.1145/3731715.3733409,0.0,FALSE,en,FALSE,closed,,,2025-06-25,article
https://openalex.org/W4413917387,,Lulu Wang|Weimin Jiang|Yi He|Te‐Cheng Pan|Yingna Li|Ruoyu Zhang|Zhentao Yu,Kunming University of Science and Technology|Kunming University of Science and Technology|China Tobacco|Kunming University of Science and Technology|Kunming University of Science and Technology|Kunming University of Science and Technology|Kunming University of Science and Technology,0,GLRM: Graph Learning and Regional Messaging for Unbiased Scene Graph Generation,https://doi.org/10.1109/crsit66101.2025.11141651,0.0,FALSE,en,FALSE,closed,,,2025-06-27,article
https://openalex.org/W4416251488,,Huzhenyu Zhang|Dmitry Yudin,Institute of Physics and Technology|Moscow Institute of Physics and Technology|Moscow Institute of Physics and Technology|Institute of Physics and Technology,0,LaMDEN: Addressing Elevator-Based Navigation with Large Language Models and 3D Scene Graphs,https://doi.org/10.1109/ijcnn64981.2025.11228224,,FALSE,,FALSE,closed,,,2025-06-30,article
https://openalex.org/W4415708481,,Yao Hao|F. Richard Yu|Yanhao Wang|Yuehua Li|Qingxu Deng|Yu Yuan|Chen Huang|Nan Che,Harbin Institute of Technology|Zhejiang Lab|East China Normal University|Zhejiang Lab|Institute for Advanced Study|Zhejiang Lab|Zhejiang Lab|Harbin University of Science and Technology,0,Open-Scene Understanding-oriented 3D Scene Graph Generation,https://doi.org/10.1109/icme59968.2025.11209525,,FALSE,,FALSE,closed,,,2025-06-30,article
https://openalex.org/W4415708310,,Weixin Chen|Yongyong Chen|Shichao Kan,Harbin Institute of Technology|Harbin Institute of Technology|Central South University,0,Scene Graph Generation with Large Vision-Language Model and Its Applications,https://doi.org/10.1109/icme59968.2025.11210017,,FALSE,,FALSE,closed,,,2025-06-30,article
https://openalex.org/W4411957542,,Shuang Liang|Long Zhang|Baihua Liu|Shushan Yan|Hongming Zhu,Tongji University|Tongji University|Tongji University|Tongji University|Tongji University,0,Front-door causal attention for unbiased panoptic scene graph generation,https://doi.org/10.1016/j.neucom.2025.130823,0.0,FALSE,en,FALSE,closed,Neurocomputing,journal,2025-07-02,article
https://openalex.org/W4411977062,,Trong-Thuan Nguyen|Minh–Triet Tran,Vietnam National University Ho Chi Minh City|Vietnam National University Ho Chi Minh City,0,LLaVA-SNIPPER: Scene-Graph-based Inference with Multimodal LLMs for Explainable Out-of-Context Misinformation,https://doi.org/10.1145/3709020.3734830,0.0,FALSE,en,FALSE,closed,,,2025-07-03,article
https://openalex.org/W4412441638,"The development of large language models (LLMs) holds the potential to significantly advance automation in the Architecture, Engineering, and Construction (AEC) industry. This paper explores the application of LLM-powered agent systems in drone-based visual inspection, focusing on three core aspects. First, a multi-agent framework is proposed, composed of five specialized sub-agents—Router, PathPlanner, Controller, Perceptioner, and Retriever—that collaboratively handle inspection subtasks such as 3D spatial reasoning, path planning, and UAV control. Second, a novel image-based pipeline is introduced to generate multi-criteria semantic point clouds and abstract them into 3D Scene Graphs (3DSGs), enabling spatial-semantic reasoning aligned with human intent. These 3DSGs act as both knowledge storage and reasoning engines. Third, the system is evaluated through simulations and laboratory experiments, demonstrating its ability to automate inspection workflows and provide a foundation for extensible AI-agent systems. The results highlight the advantages of LLM-based agents in flexible task delegation and high-level decision-making. While current implementations rely on general-purpose LLMs accessed via commercial APIs—introducing some latency and adaptation gaps—these aspects also point to promising directions for future optimization and domain-specific enhancement. Overall, the study presents an early yet promising step toward collaborative human–machine intelligence in infrastructure inspection, where autonomous agents augment human decision-making through interactive, context-aware support.",Jiucai Liu|Haijiang Li|Chengzhang Chai|Kehong Chen|Dalei Wang,,3,A LLM-informed multi-agent AI system for drone-based visual inspection for infrastructure,https://doi.org/10.1016/j.aei.2025.103643,18.41025599,FALSE,en,TRUE,hybrid,Advanced Engineering Informatics,journal,2025-07-15,article
https://openalex.org/W4412448819,,Jian Luo|Jian Zhang|Jie Yang|Siwei Huang|Bo Cai,Wuhan University||Wuhan University|Wuhan University|Wuhan University,2,Learning Bird’s Eye View scene graph and knowledge-inspired policy for embodied visual navigation,https://doi.org/10.1016/j.knosys.2025.113959,9.54681462,FALSE,en,FALSE,closed,Knowledge-Based Systems,journal,2025-07-15,article
https://openalex.org/W4413086988,,Ru‐Shan Wu|Kalama Bitur,Nanjing Tech University|South East European University,0,Exploration of Simulation Environments for Visual Question Answering: 3D Dense Captioning for Outdoor Scenes,https://doi.org/10.1142/s0218001425520202,0.0,FALSE,en,FALSE,closed,International Journal of Pattern Recognition and Artificial Intelligence,journal,2025-07-17,article
https://openalex.org/W4412726749,,Ruigeng Zeng|Wentao Ma|Tongqing Zhou|Shan Zhao|Xinjun Mao|Jie Liu,National University of Defense Technology|Anhui Agricultural University|Anhui University|National University of Defense Technology|Hefei University of Technology|Anhui Agricultural University|Anhui University|National University of Defense Technology,1,Hierarchical knowledge-guided reasoning for text-based person re-identification,https://doi.org/10.1016/j.neunet.2025.107888,4.77340731,FALSE,en,FALSE,closed,Neural Networks,journal,2025-07-19,article
https://openalex.org/W4412621907,,Eunbin Hong|June-Seong Yi,Ewha Womans University|Ewha Womans University,1,Sequence image layout generation for construction accident simulation using domain-tuned NER by ZSL-PLM and scene graph learning,https://doi.org/10.1016/j.aei.2025.103673,2.39630643,FALSE,en,FALSE,closed,Advanced Engineering Informatics,journal,2025-07-24,article
https://openalex.org/W4412692730,"The integration of Building Information Modeling (BIM) and 3D Geographic Information System (3D GIS) models provides high-precision spatial data for digital twin watersheds. To tackle the challenges of large data volumes and rendering latency in integrated models, this study proposes a three-step framework that uses Industry Foundation Classes (IFCs) as the base model and Open Scene Graph Binary (OSGB) as the target model: (1) geometric optimization through an angular weighting (AW)-controlled Quadric Error Metrics (QEM) algorithm; (2) Level of Detail (LOD) hierarchical mapping to establish associations between the IFC and OSGB models, and redesign scene paging logic; (3) coordinate registration by converting the IFC model’s local coordinate system to the global coordinate system and achieving spatial alignment via the seven-parameter method. Applied to the Santun River Basin digital twin project, experiments with 10 water gate models show that the AW-QEM algorithm reduces average loading time by 15% compared to traditional QEM, while maintaining 97% geometric accuracy, demonstrating the method’s efficiency in balancing precision and rendering performance.",Zhengbing Yang|Mahemujiang Aihemaiti|Beilikezi Abudureheman|Hongfei Tao,Xinjiang Institute of Water Resources and Hydropower Research|Xinjiang Agricultural University|Xinjiang Agricultural University|Xinjiang Institute of Water Resources and Hydropower Research|Xinjiang Agricultural University|Xinjiang Institute of Water Resources and Hydropower Research|Xinjiang Agricultural University|Xinjiang Institute of Water Resources and Hydropower Research,4,High-Precision Optimization of BIM-3D GIS Models for Digital Twins: A Case Study of Santun River Basin,https://doi.org/10.3390/s25154630,10.82310352,FALSE,en,TRUE,gold,Sensors,journal,2025-07-26,article
https://openalex.org/W4412675858,,Fei Yu|Hui Ji|Yuehua Li,Liaoning University of Technology|Jiangsu University|Zhejiang Lab,0,Cross-modality-enhanced visual Scene Graph Generation,https://doi.org/10.1016/j.inffus.2025.103430,0.0,FALSE,en,FALSE,closed,Information Fusion,journal,2025-07-27,article
https://openalex.org/W4412903872,,Changkai Feng|Tong Xu|Shiwei Wu|Derong Xu|Enhong Chen,University of Science and Technology of China|University of Science and Technology of China|University of Science and Technology of China|University of Science and Technology of China|University of Science and Technology of China,0,Adaptive Hierarchical Prompt for Open-Vocabulary Scene Graph Generation,https://doi.org/10.1145/3748318,0.0,FALSE,en,FALSE,closed,ACM Transactions on Asian and Low-Resource Language Information Processing,journal,2025-07-28,article
https://openalex.org/W4415047946,,Gen Li|Kaitian Cao,Shanghai Institute of Technology|Shanghai Institute of Technology,0,Enhancing Wireless Image Semcom with Scene Graphs and Text-to-Image Models,https://doi.org/10.23919/ccc64809.2025.11179628,0.0,FALSE,en,FALSE,closed,,,2025-07-28,article
https://openalex.org/W4415048174,,Xudong Li|Lizhen Wu|Yifeng Niu|Man Yuan,National University of Defense Technology|National University of Defense Technology|National University of Defense Technology|National University of Defense Technology,1,Lightweight Scene Graph Generation with Transformer for Resource-Constrained Platforms,https://doi.org/10.23919/ccc64809.2025.11179594,19.30239969,FALSE,en,FALSE,closed,,,2025-07-28,article
https://openalex.org/W4414979389,"Blind and low-vision users continue to face significant challenges when interacting with modern dynamic and visually complex web applications. Traditional screen readers often fall short due to the rapid changes in content, single-page applications, and intricate layouts. This paper introduces Screen Reader AI, a novel conversational web accessibility assistant implemented as a browser extension, designed to provide adaptive and context-rich support for non-visual navigation. Unlike conventional screen readers, Screen Reader AI constructs and continuously updates a live semantic scene graph by integrating the Document Object Model (DOM) and the Accessibility Object Model (AOM). Leveraging multimodal vision-language reasoning powered by GPT-4o, it generates detailed visual interpretations, detects interface structures and interactive elements, and conveys this information through natural, conversational dialogue. This approach allows users to request clarifications, discover relationships between interface components, and receive proactive notifications about dynamic content updates. The system features a modular architecture that ensures compatibility with evolving AI models and web standards, while maintaining an intuitive user interface. Core capabilities include adaptive task guidance, an interactive dashboard with contextual summaries, nested menus, live feeds, and predictive navigation assistance across diverse content types such as forms and multimedia. An evaluation framework outlines expected improvements in user experience, including reduced task completion times, enhanced understanding of page layouts, and greater autonomy during browsing. Initial findings suggest that conversational interaction can decrease cognitive load by reducing repetitive commands and streamlining information retrieval. Screen Reader AI represents a paradigm shift in digital accessibility by embedding adaptive intelligence into assistive technology, empowering independence and inclusivity while making accessibility an integral part of web innovation.",R. B. Patel,Institute for Advanced Journalism Studies,0,Screen Reader AI: A Conversational Web-Accessibility Assistant for Blind and Low-Vision Users,https://doi.org/10.52088/ijesty.v5i3.1562,0.0,FALSE,en,TRUE,diamond,International Journal of Engineering Science and Information Technology,journal,2025-07-28,article
https://openalex.org/W4412837250,,Nguyen Cong Luong|Nguyen Huu Sang|Nguyen Duc Hai|Nguyen Duc Duy Anh|Nguyễn Quốc Khánh|Xingwang Li|Dusit Niyato|Dong In Kim,Phenikaa University|Phenikaa University|Phenikaa University|Phenikaa University|Phenikaa University|Henan Polytechnic University|Nanyang Technological University|Sungkyunkwan University,0,Incentive Mechanisms for Data Relay and Scene Graph Transmission in UAV-Assisted Networks With Image Fidelity Awareness,https://doi.org/10.1109/tcomm.2025.3594777,0.0,FALSE,en,FALSE,closed,IEEE Transactions on Communications,journal,2025-08-01,article
https://openalex.org/W7116727619,,Nurhan Bulus Guran|Hanchi Ren|Yue Yang|Xianghua Xie,Swansea University|Swansea University|Swansea University|Swansea University,0,Scene Graph-Based Spatial Reasoning with VLM for High-Level Robotic Tasks,https://doi.org/10.1109/icarm65671.2025.11293747,0.0,FALSE,,FALSE,closed,,,2025-08-01,article
https://openalex.org/W4414117684,"In the 6G era, semantic communication (SemCom), which transmits high-level semantics instead of bit-wise data, shows great potential to improve transmission efficiency and robustness. However, existing methods often rely on deep feature extraction, lacking interpretability and may lead to redundant transmission. This paper proposes an interpretable SemCom framework based on scene graphs. Firstly, we use explicit scene graphs to represent semantic content, and a topology-aware codec to efficiently compress semantic structures while preserving topological relationships. To verify semantic consistency, the reconstructed scene graph is further utilized to generate images in various styles using a large language model (LLM). Simulation results show that, compared with traditional SemCom approaches, the proposed method achieves a 12.6% improvement in semantic consistency score (SCS), a 10.2% increase in structural similarity index (SSIM), and a 32.5% reduction in graph edit distance (GED), with a compression rate exceeding 95%. These results highlight the ability of the framework to balance efficiency and semantic fidelity, which offers an effective and interpretable solution for future SemCom.",Zijie Feng|Mingkai Chen|Xiaoming He|Ning Gao|Yao Sun|Shui Yu,Nanjing University of Posts and Telecommunications|Nanjing University of Posts and Telecommunications|Nanjing University of Posts and Telecommunications|Southeast University|University of Glasgow|University of Technology Sydney,0,Topology-aware Interpretable Image Semantic Communication,https://doi.org/10.1109/iccc65529.2025.11149288,0.0,FALSE,en,TRUE,green,,,2025-08-10,article
https://openalex.org/W4415597882,,Xinxin Liang|Zuoxu Wang|Mingrui Li|Jinming Yao|Jihong Liu,Beihang University|Beihang University|Beihang University|Beihang University|Beihang University,1,Scene Graph-Guided Rapid Construction of Industrial Multimodal Knowledge Graphs,https://doi.org/10.1115/detc2025-168851,4.81974515,FALSE,,FALSE,closed,,,2025-08-17,article
https://openalex.org/W4413275516,,Te‐Cheng Pan|Lulu Wang|Ruoyu Zhang|Zhengtao Yu|Yingna Li,Kunming University of Science and Technology|Yunnan University|Kunming University of Science and Technology|Kunming University of Science and Technology|Kunming University of Science and Technology|Yunnan University|Kunming University of Science and Technology|Yunnan University,0,Distribution-aware network with context and entity attention for scene graph generation,https://doi.org/10.1016/j.engappai.2025.111984,0.0,FALSE,en,FALSE,closed,Engineering Applications of Artificial Intelligence,journal,2025-08-18,article
https://openalex.org/W4413280241,"Abstract Scene Graph Generation (SGG) aims to represent visual scenes as structured graphs, where objects and their pairwise relationships are modeled as relational triples. However, conventional SGG methods often struggle with two crucial limitations: inadequate modeling of rich spatial relations beyond 2D layouts, and poor generalization in open-vocabulary scenarios. To address these limitations, we propose SPIN-SGG, a novel framework that explicitly integrates multi-dimensional spatial information—including planar geometry, relative depth, and topological structure—into an open-vocabulary SGG pipeline, without requiring ground-truth 3D annotations. Our approach builds upon a two-stage design. First, we generate pseudo-3D scene reconstructions from monocular images using multi-view synthesis and point cloud estimation, and construct a spatially enriched instruction-tuning dataset that includes fine-grained spatial predicates (e.g., left of, above, inside). Second, we propose a spatially aware visual-language model, trained with both static scene graph descriptions and dynamic spatial reasoning tasks such as question answering and multi-turn dialogue. To further enhance spatial layout consistency, we incorporate layer-aware clustering and object-level depth anchoring in the scene parsing module. Extensive experiments on the PSG benchmark and a newly curated SpatialSGG dataset demonstrate that SPIN-SGG significantly outperforms previous open-vocabulary SGG methods, with improvements of +1.3% in mR@50 on PSG and +2.7% QA accuracy on SpatialSGG, showcasing its robust and comprehensive spatial reasoning capabilities.",Nanhao Liang|Xiaoyuan Yang|Shengyi Wang|Yong Liu|Yingwei Xia,University of Science and Technology of China|Chinese Academy of Sciences|Hefei Institutes of Physical Science|University of Science and Technology of China|Chinese Academy of Sciences|Hefei Institutes of Physical Science|Hefei Institutes of Physical Science|Chinese Academy of Sciences|University of Science and Technology of China|Chinese Academy of Sciences|Hefei Institutes of Physical Science|Chinese Academy of Sciences|Hefei Institutes of Physical Science,0,SPIN-SGG: spatial integration for open-vocabulary scene graph generation,https://doi.org/10.1007/s44443-025-00203-2,0.0,FALSE,en,TRUE,gold,Journal of King Saud University - Computer and Information Sciences,journal,2025-08-18,article
https://openalex.org/W4413318938,,Wenyu Yang|Kang Liu|Zheng Tan|Li-Yu Lo|Yinglun Wang|Ka‐Hing Wong|Chih‐Yung Wen,Hong Kong Polytechnic University|Hong Kong Polytechnic University|Hong Kong Polytechnic University|Hong Kong Polytechnic University|Hong Kong Polytechnic University|Hong Kong Polytechnic University|Hong Kong Polytechnic University,0,Hierarchical 3-D Scene-Graph-Based Semantic-Metric SLAM for Plant Inspection and Fruit Counting in Intelligent Hydroponics System,https://doi.org/10.1109/jiot.2025.3600531,0.0,FALSE,en,FALSE,closed,IEEE Internet of Things Journal,journal,2025-08-19,article
https://openalex.org/W4413473452,,Zijian Zhou|Holger Caesar|Qijun Chen|Miaojing Shi,King's College London|Delft University of Technology|Tongji University|Tongji University,0,VLPrompt-PSG: Vision-Language Prompting for Panoptic Scene Graph Generation,https://doi.org/10.1007/s11263-025-02564-7,0.0,FALSE,en,FALSE,closed,International Journal of Computer Vision,journal,2025-08-23,article
https://openalex.org/W4413773197,"Panoptic scene graph generation (PSG) aims to simultaneously segment both foreground objects and background regions while predicting object relations for fine-grained scene modeling. Despite significant progress in panoptic scene understanding, current PSG methods face challenging problems: relation prediction often only relies on visual representations and is hindered by imbalanced relation category distributions. Accordingly, we propose IntegraPSG, a single-stage framework that integrates large language model (LLM) guidance with multimodal feature fusion. IntegraPSG introduces a multimodal sparse relation prediction network that efficiently integrates visual, linguistic, and depth cues to identify subject–object pairs most likely to form relations, enhancing the screening of subject–object pairs and filtering dense candidates into sparse, effective pairs. To alleviate the long-tail distribution problem of relations, we design a language-guided multimodal relation decoder where LLM is utilized to generate language descriptions for relation triplets, which are cross-modally attended with vision pair features. This design enables more accurate relation predictions for sparse subject–object pairs and effectively improves discriminative capability for rare relations. Experimental results show that IntegraPSG achieves steady and strong performance on the PSG dataset, especially with the R@100, mR@100, and mean reaching 38.7%, 28.6%, and 30.0%, respectively, indicating strong overall results and supporting the validity of the proposed method.",Yishuang Zhao|Qiang Zhang|Xueying Sun|Guanchen Liu,Jiangsu University of Science and Technology|Jiangsu University of Science and Technology|Jiangsu University of Science and Technology|Jiangsu University of Science and Technology,0,IntegraPSG: Integrating LLM Guidance with Multimodal Feature Fusion for Single-Stage Panoptic Scene Graph Generation,https://doi.org/10.3390/electronics14173428,0.0,FALSE,en,TRUE,gold,Electronics,journal,2025-08-28,article
https://openalex.org/W4413848595,,Guoqing Zhang|Shichao Kan|Yue Zhang|Yigang Cen|Wanru Xu|Yi Jin|Yidong Li,Beijing Jiaotong University|Central South University|Henan Normal University|Beijing Jiaotong University|Beijing Jiaotong University|Beijing Jiaotong University|Beijing Jiaotong University,0,Query-guided predicate decoupling and prototype approximation learning for scene graph generation,https://doi.org/10.1016/j.eswa.2025.129525,0.0,FALSE,en,FALSE,closed,Expert Systems with Applications,journal,2025-08-29,article
https://openalex.org/W4414359691,,Mingxin Li|Wenhao Wang|Hongru Ji|Xianghua Li|Chao Gao,Northwestern Polytechnical University|Northwestern Polytechnical University|Northwestern Polytechnical University|Northwestern Polytechnical University|Northwestern Polytechnical University,0,Can Retelling Have Adequate Information for Reasoning? An Enhancement Method for Imperfect Video Understanding with Large Language Model,https://doi.org/10.24963/ijcai.2025/906,0.0,FALSE,en,FALSE,closed,,,2025-09-01,article
https://openalex.org/W4414290299,,Huzhenyu Zhang,Moscow Institute of Physics and Technology,0,ElevNav: Large Language Model-Guided Robot Navigation via 3D Scene Graphs in Elevator Environments,https://doi.org/10.3103/s1060992x25700109,0.0,FALSE,en,FALSE,closed,Optical Memory and Neural Networks,journal,2025-09-01,article
https://openalex.org/W4413977001,"Abstract. Vector representation of geodata is widely used in various application due to high density of information and the advanced level of information representation, introduced by the human operator while creating a map. We can say that a map is a vector representation of understanding a scene based on its image. Scene understanding can be considered at different levels of depth, beginning from image classification and semantic segmentation and completing with rich semantic relationships between objects and retrieving its hierarchy. With the progress in machine learning methods and tools for obtaining and processing large amounts of data a set of neural network models has been developed that demonstrate state-of-the art performance (humanlike and better) in image classification and image semantic segmentation tasks. After object detection and recognition, the next step in scene understanding is retrieving the relations between objects and their hierarchy. This problem is known as scene graph generation, and recently it received notable attention by the scientific community. The developed approach incorporates the information about the structural and functional relationships between objects in the image, which, on the one hand, improves the quality of segmentation through the use of new a priori data, and on the other hand, reduces the time spent by the operator on subsequent processing of the results of the neural network algorithm. To train and evaluate the developed framework, a special dataset is collected and annotated. It contains more than 10k aerial photographs representing various types of objects taken in different years and seasons. The evaluation results on the created dataset proved the state-of-the-art performance of the developed framework.",V. A. Knyaz|V. V. Kniaz|Sergey Y. Zheltov|Anton V. Emelyanov|Е. С. Смирнов,State Scientific Research Institute of Aviation Systems|Moscow Institute of Physics and Technology|Moscow Institute of Physics and Technology|State Scientific Research Institute of Aviation Systems|State Scientific Research Institute of Aviation Systems|Moscow Institute of Physics and Technology|State Scientific Research Institute of Aviation Systems|State Scientific Research Institute of Aviation Systems,0,Hierarchical Scene Graph Generation and Vectorization of Aerial Images,https://doi.org/10.5194/isprs-archives-xlviii-2-w9-2025-161-2025,0.0,FALSE,en,TRUE,diamond,"The international archives of the photogrammetry, remote sensing and spatial information sciences/International archives of the photogrammetry, remote sensing and spatial information sciences",journal,2025-09-04,article
https://openalex.org/W4414260980,,Yiming Han|Y. Cui|Tinglun Song,Nanjing University of Aeronautics and Astronautics|Nanjing University of Aeronautics and Astronautics|Chery Automobile (China),0,SGVLM: Depth-Integrated Semantic Scene Graph Fusion for Enhanced Autonomous Driving Decision-Making,https://doi.org/10.22541/au.175774544.49992943/v1,0.0,FALSE,en,TRUE,gold,,,2025-09-13,preprint
https://openalex.org/W4414317082,"Understanding visual scenes as structured graphs of objects and their interactions is central to advancing high-level visual reasoning. Conventional scene graph generation methods rely on dense and carefully annotated supervision, where each subject-predicate-object triplet is coupled with explicit bounding box labels. Such supervision, however, is expensive to obtain and scales poorly to the open world. In contrast, natural image captions provide abundant descriptions of scenes at a fraction of the cost, though they remain weakly aligned and inherently noisy. In this work, we introduce \textbf{LINGGRAPH}, a new framework that transforms captions into an indirect yet powerful supervisory signal for scene graph generation. Unlike prior efforts that reduce supervision to isolated triplets, we exploit the global semantic organization encoded in captions—where entities, modifiers, and actions co-occur in narrative structures—to capture interdependent relationships and commonsense scene dynamics. LINGGRAPH extracts structured linguistic cues from captions, such as nominal groups, adjectival modifiers, and verbal relations, and leverages them to guide the detection and classification of graph components. To mitigate the noise and incompleteness of captions, we devise an iterative refinement process that progressively aligns textual spans with visual regions, discarding irrelevant associations while strengthening meaningful ones. Our study demonstrates that linguistic regularities encoded in captions can effectively substitute fine-grained annotations for training robust relational models. Experiments reveal that integrating both global narrative semantics and local syntactic features yields superior interpretability and accuracy in graph generation, surpassing existing weakly supervised baselines. By disambiguating visually similar entities and ensuring semantic coherence, our approach establishes captions as a scalable and practical form of weak supervision. This work highlights the potential of free-form language as a bridge for structured visual understanding, underscoring its role in unifying vision and language at the relational level.",Nguyễn Trường Sơn|Norman MacLeod|Arjun Patel|B. S. Monroe,Simon Fraser University|Simon Fraser University|Simon Fraser University|Simon Fraser University,0,Capturing Narrative Semantics from Captions for Relational Scene Abstraction,https://doi.org/10.20944/preprints202509.1607.v1,,FALSE,en,TRUE,green,Preprints.org,repository,2025-09-18,preprint
https://openalex.org/W4414444617,"The perception of 3D space by mobile robots is rapidly moving from flat metric 1 grid representations to hybrid metric-semantic graphs built from human-interpretable 2 concepts. While most approaches first build metric maps and then add semantic layers, 3 we explore an alternative concept-first architecture where spatial understanding emerges 4 from asynchronous concept agents that directly instantiate and manage semantic entities. 5 Our robot employs two spatial concepts—room and door—implemented as autonomous 6 processes within a cognitive distributed architecture. These concept agents cooperatively 7 build a shared scene graph representation of indoor layouts through active exploration 8 and incremental validation. The key architectural principle is hierarchical constraint 9 propagation: room instantiation provides geometric and semantic priors that constrain and 10 improve door detection within wall boundaries. The resulting structure is maintained by a 11 complementary functional principle: prediction-matching loops. This approach builds an 12 actionable, human-interpretable spatial representation without relying on a pre-existing 13 global metric map, enabling scalable operation and persistent, task-relevant understanding 14 in structured indoor environments.",Noé Zapata|Gerardo Pérez|Alejandro Torrejón|Pedro Núñez|Pablo Bustos,,0,"Concept-Guided Exploration: Building Persistent, Actionable Scene Graphs",https://doi.org/10.20944/preprints202509.1887.v1,,FALSE,en,TRUE,green,Preprints.org,repository,2025-09-23,preprint
https://openalex.org/W4415848069,"Today, autonomous mobile robots are widely used in all walks of life. Autonomous navigation, as a basic capability of robots, has become a research hotspot. Classical navigation techniques, which rely on pre-built maps, struggle to cope with complex and dynamic environments. With the development of artificial intelligence, learning-based navigation technology have emerged. Instead of relying on pre-built maps, the agent perceives the environment and make decisions through visual observation, enabling end-to-end navigation. A key challenge is to enhance the generalization ability of the agent in unfamiliar environments. To tackle this challenge, it is necessary to endow the agent with spatial intelligence. Spatial intelligence refers to the ability of the agent to transform visual observations into insights, insights into understanding, and understanding into actions. To endow the agent with spatial intelligence, relevant research uses scene graph to represent the environment. We refer to this method as scene graph-based object goal navigation. In this paper, we concentrate on scene graph, offering formal description, computational framework of object goal navigation. We provide a comprehensive summary of the methods for constructing and applying scene graph. Additionally, we present experimental evidence that highlights the critical role of scene graph in improving navigation success. This paper also delineates promising research directions, all aimed at sharpening the focus on scene graph. Overall, this paper shows how scene graph endows the agent with spatial intelligence, aiming to promote the importance of scene graph in the field of intelligent navigation.",Chi Guo|Aolin Li|Yuwei Meng,Wuhan University|Artificial Intelligence in Medicine (Canada)|Wuhan University|Wuhan University,0,Navigating with Spatial Intelligence: A Survey of Scene Graph-Based Object Goal Navigation,https://doi.org/10.1051/wujns/2025305405,,FALSE,,TRUE,hybrid,Wuhan University Journal of Natural Sciences,journal,2025-10-01,article
https://openalex.org/W4414713912,,Yuan Gao|Yaochen Li|Yujie Zang|Jingze Liu|Yuehu Liu,Xi'an Jiaotong University|Xi'an Jiaotong University|Xi'an Jiaotong University|Xi'an Jiaotong University|Xi'an Jiaotong University,1,Prototype-based multi-domain self-distillation for unbiased scene graph generation,https://doi.org/10.1016/j.neucom.2025.131625,4.77340731,FALSE,en,FALSE,closed,Neurocomputing,journal,2025-10-01,article
https://openalex.org/W4414758239,,Qixiang Ma|Runze Fan|Lizhi Zhao|Jian Wu|Sio‐Kei Im|Lili Wang,Beihang University|Beihang University|Beihang University|Beihang University|Macao Polytechnic University|Beihang University,0,SGSG: Stroke-Guided Scene Graph Generation,https://doi.org/10.1109/tvcg.2025.3616751,0.0,FALSE,en,FALSE,closed,IEEE Transactions on Visualization and Computer Graphics,journal,2025-10-02,article
https://openalex.org/W4416403908,,Jin-Seok Hong|Hyerim Park|Heejeong Ko|Woontack Woo,Korea Advanced Institute of Science and Technology|Korea Advanced Institute of Science and Technology|Korea Advanced Institute of Science and Technology|Universidad Villa Rica,0,Few-Shot Action Scene Graph Generation from Video via Multimodal Language Models for Structuring Spatial Experience,https://doi.org/10.1109/ismar-adjunct68609.2025.00027,,FALSE,,FALSE,closed,,,2025-10-08,article
https://openalex.org/W4415259654,"In this study, we propose an interactive environment-aware dialog and planning system for social robots in early childhood education, aimed at supporting the learning and social interaction of young children. The proposed architecture consists of three core modules. First, semantic simultaneous localization and mapping (SLAM) accurately perceives the environment by constructing a semantic scene representation that includes attributes such as position, size, color, purpose, and material of objects, as well as their positional relationships. Second, the automated planning system enables stable task execution even in changing environments through planning domain definition language (PDDL)-based planning and replanning capabilities. Third, the visual question answering module leverages scene graphs and SPARQL conversion of natural language queries to answer children’s questions and engage in context-based conversations. The experiment conducted in a real kindergarten classroom with children aged 6 to 7 years validated the accuracy of object recognition and attribute extraction for semantic SLAM, the task success rate of the automated planning system, and the natural language question answering performance of the visual question answering (VQA) module.The experimental results confirmed the proposed system’s potential to support natural social interaction with children and its applicability as an educational tool.",Jiyoun Moon|Sen Song,Chosun University|Chosun University,0,Interactive Environment-Aware Planning System and Dialogue for Social Robots in Early Childhood Education,https://doi.org/10.3390/app152011107,0.0,FALSE,en,TRUE,gold,Applied Sciences,journal,2025-10-16,article
https://openalex.org/W4416750350,,Xiangru Mu|F. Chen|Runhan Wang|Siyuan Chen|Jiyuan Cai|Jia Cai|Ming Yang|Tong Qin,Shanghai Jiao Tong University|Shanghai Jiao Tong University|Shanghai Jiao Tong University|Shanghai Jiao Tong University|Shanghai Jiao Tong University|Shanghai Jiao Tong University|Shanghai Jiao Tong University|Shanghai Jiao Tong University,0,AVP Scene Graph: Hierarchical Visual Language Mapping and Navigation for Autonomous Valet Parking,https://doi.org/10.1109/iros60139.2025.11246107,,FALSE,,FALSE,closed,,,2025-10-19,article
https://openalex.org/W4416749150,,Ruifei Ma|Yifan Xu|Yuze Li|Yanping Fang|Zhifei Yang|Jiaxing Qi|Xinyu Zhao|Chao Zhang,Digital China Health (China)|Beihang University|Beijing University of Posts and Telecommunications|Digital China Health (China)|Peking University|Beihang University|Beihang University|Digital China Health (China),0,CTSG: Integrating Context and Way Topology Into Scene Graph for Zero-shot Navigation,https://doi.org/10.1109/iros60139.2025.11246966,,FALSE,,FALSE,closed,,,2025-10-19,article
https://openalex.org/W4416748875,,Minglin Chen|Renshu Yang|Qingping Hu|Kaiwen Xue|Shunbo Zhou|Yulan Guo,Sun Yat-sen University|Sun Yat-sen University|Sun Yat-sen University|Cloud Computing Center|Huawei Technologies (United States)|Huawei Technologies (China)|Huawei Technologies (United States)|Cloud Computing Center|Huawei Technologies (China)|Sun Yat-sen University,0,Graph2Scene: Versatile 3D Indoor Scene Generation with Interaction-aware Scene Graph,https://doi.org/10.1109/iros60139.2025.11246595,,FALSE,,FALSE,closed,,,2025-10-19,article
https://openalex.org/W4416749650,,Wooseok Oh|Hogun Kee|Songhwai Oh,Seoul National University|Seoul National University|Seoul National University,0,Language-Guided Hierarchical Planning with Scene Graphs for Tabletop Object Rearrangement,https://doi.org/10.1109/iros60139.2025.11247525,,FALSE,,FALSE,closed,,,2025-10-19,article
https://openalex.org/W4416749247,,Xudong Li|Chang Wang|Yifeng Niu|Man Yuan|Lizhen Wu,National University of Defense Technology|National University of Defense Technology|National University of Defense Technology|National University of Defense Technology|National University of Defense Technology,0,Reducing Scene Graph Generation Parameters Towards UAV Understanding of Structured Environments,https://doi.org/10.1109/iros60139.2025.11247395,,FALSE,,FALSE,closed,,,2025-10-19,article
https://openalex.org/W7126214658,,Qi Hao|Jingyi Huang|Changhao Ding|Zeping Zhang,Southeast University|Southeast University|Southeast University|Southeast University,0,Semantic Heat Guided Relational Privacy Inference Based on Panoptic Scene Graph,https://doi.org/10.1109/raid67961.2025.00031,0.0,FALSE,,FALSE,closed,,,2025-10-19,article
https://openalex.org/W4415368608,,Xuejian Huang|Tinghuai Ma|Hao Tang|Huan Rong,Jiangxi University of Finance and Economics|Nanjing University of Information Science and Technology|Peking University|Nanjing University of Information Science and Technology,0,Knowledge-Enhanced Dynamic Scene Graph Attention Network for Fake News Video Detection,https://doi.org/10.1109/tmm.2025.3623491,0.0,FALSE,,FALSE,closed,IEEE Transactions on Multimedia,journal,2025-10-20,article
https://openalex.org/W4415395022,,Haojun Gao|Jiajun Wang|Xiaoling Wang|K. Lu|Xiangyun Meng|Le Deng,,0,Deep reinforcement learning coupled with topological scene graph for dynamic path planning of autonomous bulldozer in complex earthwork construction,https://doi.org/10.1016/j.autcon.2025.106617,0.0,FALSE,en,FALSE,closed,Automation in Construction,journal,2025-10-21,article
https://openalex.org/W4415401451,,Takahiro Komamizu,Nagoya University,0,"Prospective Analysis of Semantic Image Retrieval: Comparing Scene Graph, Visual Features, and Captions",https://doi.org/10.1145/3746266.3762160,0.0,FALSE,,FALSE,closed,,,2025-10-21,article
https://openalex.org/W4415465710,,Yun Meng|Hongling Zhang|Xinyi Liu|Xinru Liu,,0,Multi-Scale Feature Fusion Scene Graph Generation Network for Traffic Scene,https://doi.org/10.1061/9780784486269.035,0.0,FALSE,,FALSE,closed,,,2025-10-23,article
https://openalex.org/W4415465167,,Cong Yao|Hongwei Mo,Harbin Engineering University|Harbin Engineering University,0,Task-Guided Dynamic Visual Reasoning for Visual Question Answering,https://doi.org/10.1142/s0219843625400122,0.0,FALSE,en,FALSE,closed,International Journal of Humanoid Robotics,journal,2025-10-23,article
https://openalex.org/W7125508637,,Chen Zhang|Zhaoyang Du|Yangfei Lin|Jiale Wu|Wugedele Bao|Ha Si,University of Electro-Communications|University of Electro-Communications|University of Electro-Communications|University of Electro-Communications|Hohhot Minzu College|Hohhot Minzu College,0,Adaptive Semantic Compression for Scene Graph Communication with Deep Reinforcement Learning,https://doi.org/10.1109/meet67398.2025.11335821,0.0,FALSE,,FALSE,closed,,,2025-10-24,article
https://openalex.org/W4415539763,,Tao Ling|Siping Shi|Dan Wang,Hong Kong Polytechnic University|Hong Kong Polytechnic University|Hong Kong Polytechnic University,0,Accelerating Long Video Understanding via Compressed Scene Graph-Enabled Chain-of-Thought,https://doi.org/10.1145/3746027.3754765,0.0,FALSE,,TRUE,gold,,,2025-10-25,article
https://openalex.org/W4415721628,"<title>Abstract</title> Scene Graph Generation (SGG) is a fundamental task in computer vision. It entails classifying entities in images and predicting visual relationships among them. Existing scene graph generation research targets the identification of all possible relationships within an image. However, this approach may suffer from inefficiencies and generate redundant information, hindering the further processing of relationship data in downstream tasks. In contrast, identifying important relationships is more aligned with the practical requiements of downstream tasks. To address this issue, we introduce the Fuzzy Logic-Driven Important Relationship Learning Network (FLIRL-Net). Initially, we employ fuzzy logic to compute an importance score for each relationship. This computation accounts for factors affecting relationship significance and user requirements. Subsequently, we design the Relationship Label Loss Importance Weighting (RLIW) module. This module utilizes these importance scores as weights to adjust relationship sample losses. Such adjustments direct the model's focus towards important relationships. Thirdly, we develop the Importance-Based Entity Pair Feature PoolFormer (IEPFP) module to enhance the model's recognition of important entity pairs. Additionally, We propose metrics Important Relationship Recall (IR@K) and Important Relationship Precision (IP@K). These metrics assess the model's effectiveness in identifying important relationships. Experimental results demonstrate that our model excels at identifying important relationships in the VG150 dataset and effectively minimizes unimportant relationship outputs.",Jin Wang|Zilong Yang|Jialing Xu|Siyuan Wu,Nantong University|Nantong University|Nantong University|Nantong University,0,FLIRL-Net: Fuzzy Logic-Driven Important Relationship Learning for Scene Graph Generation,https://doi.org/10.21203/rs.3.rs-7500066/v1,,FALSE,,TRUE,gold,,,2025-10-31,preprint
https://openalex.org/W4415959810,"Abstract Small handgun detection in CCTV surveillance suffers from high false positives and negatives due to limited distinguishing features. We propose H-SGE (Hybrid Scene Graph Enrichment), combining Generative Adversarial Networks (GANs), scene graph enrichment, and multiple YOLO variants (YOLOv5, YOLOv7, YOLO10, YOLO11) for enhanced detection. H-SGE employs a five-stage pipeline: (1) enriched scene graph generation, (2) GAN-based feature enhancement, (3) context-aware RoI selection, (4) multi-YOLO detection, and (5) output fusion. Evaluation on a handgun dataset demonstrates significant F1-score improvements: YOLOv5 from 58% to 80%, YOLOv7 from 56% to 82%, YOLO10 from 62% to 85%, and YOLO11 from 64% to 87%, achieving over 20% accuracy gains. Results show that combining contextual reasoning with visual augmentation and hybrid detection effectively addresses small object detection challenges in surveillance systems.",Nasreen Jawaid|Najma Imtiaz Ali|Imtiaz Ali Korejo|Imtiaz Ali Brohi|Nor Hafeizah Hassan,"University of Sindh|Technical University of Malaysia Malacca|University of Sindh|Government College University, Lahore|Technical University of Malaysia Malacca",0,H-SGE: A hybrid model based on scene graph enrichment for automated Handgun detection in security surveillance,https://doi.org/10.1007/s11760-025-04926-7,,FALSE,en,TRUE,hybrid,Signal Image and Video Processing,journal,2025-11-06,article
https://openalex.org/W4416016894,,Xiaoyu Wang|Tao Sun|Gengchen Liu|Zhi Yang|Jiahui Liu|Zimeng Xu,Qilu University of Technology|Shandong Academy of Sciences|Shandong Academy of Sciences|Qilu University of Technology|Qilu University of Technology|Shandong Academy of Sciences|Shandong Academy of Sciences|Qilu University of Technology|Qilu University of Technology|Shandong Academy of Sciences|Shandong Academy of Sciences|Qilu University of Technology,0,MGFSG-EE: A Method based on Multi-grained Fusion and Scene Graph Enhancement for Event Extraction,https://doi.org/10.1145/3746252.3761235,,FALSE,,FALSE,closed,,,2025-11-07,article
https://openalex.org/W4416018080,,Haoyi Xiu|Xin Liu|Taehoon Kim|Kyoung‐Sook Kim,National Institute of Advanced Industrial Science and Technology|National Institute of Advanced Industrial Science and Technology|National Institute of Advanced Industrial Science and Technology|National Institute of Advanced Industrial Science and Technology,0,Exploring the Potential of Pre-Trained Language Models in Long-Term Semantic Scene Change Prediction Using Variable Scene Graphs,https://doi.org/10.1145/3746252.3760897,,FALSE,,TRUE,gold,,,2025-11-08,article
https://openalex.org/W4416166824,,Tingzhu Wang|Linlin Wang|Junwei Luo|Kang Wu|Yansheng Li,Wuhan University|Wuhan University|Wuhan University|Wuhan University|Wuhan University,0,Bias-aware learning for unbiased scene graph generation in remote sensing imagery,https://doi.org/10.1016/j.isprsjprs.2025.11.009,,FALSE,en,FALSE,closed,ISPRS Journal of Photogrammetry and Remote Sensing,journal,2025-11-13,article
https://openalex.org/W4416332881,"Visual question answering~(VQA) fundamentally requires a model to interpret heterogeneous semantic cues in an image and align them with a natural-language query. Traditional approaches benefit from scene graph representations, yet they often suffer from severe imbalances when handling rich semantic structures, especially when reasoning demands simultaneous consideration of objects, relations, and fine-grained attributes. Existing models frequently overlook the subtle interactions among these three information streams, leading to faulty attribute inference or overlooked relational cues. Addressing these long-standing limitations calls for a more principled integration of all semantic constituents within a unified and expressive reasoning space. In this paper, we introduce \textbf{\textsc{TriUnity-GNN}}, a tri-modal fusion framework that redefines scene graph reasoning by jointly enhancing object-centric, relation-centric, and attribute-centric representations under a unified graph neural paradigm. Instead of treating scene graphs as monolithic structures, our approach restructures the given graph into two complementary modalities, an object-dominant perspective and a relation-dominant perspective, thereby enabling the model to capture multi-granular semantics that are typically under-explored. To further strengthen the expressivity of these representations, \textsc{TriUnity-GNN} integrates attribute cues through an explicit fusion design, significantly enlarging the impact of attribute signals that are otherwise marginalized in classic architectures. Moreover, we design a novel message-passing enhancement module that substantially increases cross-type semantic exchange among objects, relations, and attributes, ensuring that all three modalities collectively shape the final reasoning embedding. We perform comprehensive evaluations on benchmark datasets including GQA, VG, and motif-VG. Across all benchmarks, \textsc{TriUnity-GNN} consistently surpasses prior graph-based VQA systems by a clear margin, demonstrating robustness in handling both straightforward and semantically composite queries. The results verify that a tri-modal, explicitly balanced graph reasoning mechanism is crucial for improving interpretability and accuracy in challenging visual question answering scenarios.",Jolien Van den Bossche|Thibault Clercq|Callum Hensley|Rune Peeters,,0,Relation-Sensitive VQA with A Unified Tri-Modal Graph Framework,https://doi.org/10.20944/preprints202511.1373.v1,,FALSE,,TRUE,green,Preprints.org,repository,2025-11-18,preprint
https://openalex.org/W4416513609,,Ruonan Zhang|Yiqing Hao|Feng Zhang|Gaoyun An|Binyang Song|Dapeng Wu,Beijing Jiaotong University|Beijing Jiaotong University|Southwest Jiaotong University|Beijing Jiaotong University|Beijing Jiaotong University|Nanyang Technological University|City University of Hong Kong,0,Human-Inspired Scene Understanding: A Grounded Cognition Method for Unbiased Scene Graph Generation,https://doi.org/10.1109/tpami.2025.3635152,,FALSE,en,FALSE,closed,IEEE Transactions on Pattern Analysis and Machine Intelligence,journal,2025-11-21,article
https://openalex.org/W7125019002,,Xiaoyi Yang|Xia Guo|Hairui Guo,Fudan University|Fudan University|Fudan University,0,MKSGG: Multi-Knowledge-Enhanced Scene Graph Generation in Remote Sensing Images,https://doi.org/10.1109/icicml67980.2025.11333526,0.0,FALSE,,FALSE,closed,,,2025-11-21,article
https://openalex.org/W4416509245,,Li Songju|Bin Sun|Shutao Li|Bin Yang,Hunan University|Hunan University|Hunan University|Hunan University,0,Multimodal large model driven pseudo labeling for unbiased scene graph generation,https://doi.org/10.1016/j.neucom.2025.132170,,FALSE,en,FALSE,closed,Neurocomputing,journal,2025-11-21,article
https://openalex.org/W4416717976,"Abstract. The generation of scene maps of remote sensing images is very important for the understanding of image depth, but its development is limited by the characteristics of wide image size, significant changes in target scale and dense distribution. In this paper, a scene graph generation method based on knowledge graph enhancement and relationship filtering is proposed. Firstly, the method takes the Reltr model as the backbone framework, constructs and integrates the domain knowledge graph, and uses the typical spatial relationships (such as orientation and topology) and semantic associations (such as functional constraints) of the encoded remote sensing targets as structured prior knowledge to guide the model to understand the more likely reasonable relationship patterns between targets. Secondly, the semantic information entropy mechanism was introduced to calculate the information entropy value of the probability distribution of the relationship class predicted by the model to quantify the uncertainty of the prediction. Based on this, an adaptive threshold is set to effectively filter and suppress low-confidence fuzzy or erroneous relationship predictions, focusing on mining deep complex relationships with high certainty between targets. Experiments on the STAR dataset show that the accuracy of the proposed method in the task of remote sensing scene map generation is significantly improved: the accuracy of the R@100, R@200, and R@500 reaches 23.1%, 25.6%, and 26.1%, respectively, and the mR@100, mR@200, and mR@500 reach 13.1%, 15.6%, and 17.1%, respectively, and the accuracy is better than that of the existing algorithms. This verifies the effectiveness of the method and provides a new and effective solution for the scene understanding of remote sensing images.",Yu Geng|Jingguo Lv,Beijing University of Civil Engineering and Architecture|Beijing University of Civil Engineering and Architecture,0,Remote Sensing Image Scene Graph Generation Method Based on Knowledge Graph Enhancement and Relationship Filtering,https://doi.org/10.5194/isprs-archives-xlviii-4-w14-2025-47-2025,,FALSE,en,TRUE,diamond,"The international archives of the photogrammetry, remote sensing and spatial information sciences/International archives of the photogrammetry, remote sensing and spatial information sciences",journal,2025-11-26,article
https://openalex.org/W4416684237,,Wentao Ma|Xiaoqian Wu|Shaofan Chen|Wei Liu|Shan Zhao|Qian Wan|Lichuan Gu,Anhui Agricultural University|Anhui Agricultural University|Anhui Agricultural University|Anhui University of Finance and Economics|Hefei University|Hefei University of Technology|Central China Normal University|Anhui Agricultural University,0,TSGR2: Image-text matching via triple-level scene graph relation reasoning,https://doi.org/10.1016/j.asoc.2025.114323,,FALSE,en,FALSE,closed,Applied Soft Computing,journal,2025-11-26,article
https://openalex.org/W4416833860,,Seok‐Hee Lee|Dae-Sik Ko|Seokil Song,,0,Implementation of a Real-Time Video Captioning System based on Scene Graphs and Templates for Construction Safety Monitoring,https://doi.org/10.14801/jkiit.2025.23.11.279,,FALSE,en,FALSE,closed,The Journal of Korean Institute of Information Technology,journal,2025-11-30,article
https://openalex.org/W7108730288,"Image captioning, the task of automatically generating natural language descriptions for visual content, has seen significant advancements with deep learning. However, existing models frequently suffer from critical limitations, including the generation of factually incorrect (hallucinated) content, lack of fine-grained detail, and insufficient verifiability of generated statements. These issues stem from an over-reliance on holistic image representations or isolated object detections, which fail to capture the intricate relationships and contextual semantics within a scene. To address these challenges, this paper proposes a novel framework for verifiable and fine-grained image captioning built upon graph-structured multimodal reasoning. Our approach leverages scene graphs to explicitly represent objects, their attributes, and their relationships, providing a rich, structured understanding of the visual scene. This structured representation is then integrated with a multimodal reasoning module that employs Graph Neural Networks (GNNs) and attention mechanisms to perform deep semantic inference. A key innovation is the incorporation of a verifiability component that grounds generated captions directly back to the extracted scene graph elements, thereby mitigating hallucinations and enhancing factual consistency. Furthermore, the model is designed to prioritize the inclusion of fine-grained details by focusing on attribute-rich nodes and complex relational predicates within the graph. Extensive experiments on benchmark datasets demonstrate that our graph-structured reasoning framework significantly improves both the factual accuracy and the level of detail in generated captions, outperforming state-of-the-art methods in verifiability and fine-grained metric evaluations. The proposed architecture marks a substantial step towards more reliable, interpretable, and human-like image understanding and description.","Revista, Zen|IA, 10",,0,Graph-Structured Multimodal Reasoning for Verifiable and Fine-Grained Image Captioning,https://doi.org/10.5281/zenodo.17816350,0.0,FALSE,,TRUE,green,Zenodo (CERN European Organization for Nuclear Research),repository,2025-12-04,article
https://openalex.org/W7108898999,"Text-to-image synthesis has seen remarkable advancements, transforming natural language descriptions into vivid visual content. However, existing models often struggle with fine-grained control over compositional elements and precise semantic manipulation, particularly when confronted with complex scenes or novel attribute bindings. This paper proposes a foundational model designed to address these limitations by integrating explicit compositional reasoning and multi-modal controllability mechanisms within a unified generative framework. Our conceptual model leverages an advanced hierarchical semantic encoder to parse textual prompts into structured representations, which then guide a cascaded generative process. This process includes a scene graph generation module for spatial arrangement and object interaction, followed by a multi-stage image generation network that refines visual details based on compositional constraints and user-defined control signals. We hypothesize that this architecture will enable unprecedented levels of control over object attributes, spatial relationships, and artistic style, moving beyond mere content generation to true visual conceptualization. The aim is to bridge the gap between abstract textual commands and concrete, controllable visual outcomes, laying the groundwork for more intuitive and powerful human-AI creative collaboration.","Revista, Zen|IA, 10",,0,Conceptualizing Visuals: A Foundational Model for Compositional and Controllable Text-to-Image Synthesis,https://doi.org/10.5281/zenodo.17827821,0.0,FALSE,,TRUE,green,Zenodo (CERN European Organization for Nuclear Research),repository,2025-12-05,article
https://openalex.org/W4417060726,,K. Krishnakishore|R. Vijayarangan|V. Jagan Naveen|V. Kannan,|Department of Biotechnology||,0,Scene Graph Generation from Static Images,https://doi.org/10.1002/9781394356362.ch2,,FALSE,en,FALSE,closed,,,2025-12-05,other
https://openalex.org/W4417085694,,Xiaotian Lv|Yue Zhao|Hanlong Yin|Yifei Chen|Jianxing Liu,,0,MSG-CLIP: Enhancing CLIP’s ability to learn fine-grained structural associations through multi-modal scene graph alignment,https://doi.org/10.1016/j.patcog.2025.112794,,FALSE,en,FALSE,closed,Pattern Recognition,journal,2025-12-06,article
https://openalex.org/W4417251981,,Xin Cai|Ji Wu|Shu Han|Yu Li|Wen Yang|Huai Yu|Gui-Song Xia,Wuhan University|Wuhan University|Wuhan University|Wuhan University|Wuhan University|Wuhan University|Wuhan University,0,SGE-GLoc: Semantic Gaussian Ellipsoid Scene Graphs for Efficient LiDAR Global Localization,https://doi.org/10.1109/lra.2025.3643266,,FALSE,,FALSE,closed,IEEE Robotics and Automation Letters,journal,2025-12-11,article
https://openalex.org/W7114912069,"Industrial deliverables in the AEC/FM sector are increasingly specified, validated, and governed by open standards. However, the machine-readable delivery specifications rarely propagate intact into the real-time collaborative 3D scene descriptions required by digital twins, XR, large-scale simulation, and visualization. This paper proposes a pipeline that transforms industrial deliverables into semantically faithful, queryable, and render-ready open scene descriptions. Unlike existing workflows that focus on geometric translation via connectors or intermediate formats, the proposed pipeline aligns defined delivery specifications with schema-aware USD composition so that contractual semantics remain executable in the scene. The pipeline comprises delivery specification, which records required objects, attributes, and provenance as versioned rule sets; semantically bound scene realization, which builds an open scene graph that preserves spatial hierarchy and identifiers, while linking rich properties through lightweight references; and interactive sustainment, which lets multiple engines render, analyze, and update the scene while allowing rules to be re-applied at any time. It presents a prototype and roadmap that make open scene description a streaming-ready execution layer for building deliverables, enabling consistent semantics, and reuse across diverse 3D engines.",Guoqian Ren|Chengzheng Huang|Tengxiang Su,Tongji University|Shanghai Tongji Urban Planning and Design Institute|Cardiff University|Cardiff University,0,From Building Deliverables to Open Scene Description: A Pipeline for Lifecycle 3D Interoperability,https://doi.org/10.3390/buildings15244503,0.0,FALSE,en,TRUE,gold,Buildings,journal,2025-12-12,article
https://openalex.org/W4417308578,"In response to the dynamic occlusion, lighting changes, and computational efficiency faced by 3D object recognition in the intelligent transformation of construction engineering, this study proposes an optimized algorithm that integrates the Improved Outdoor Dynamic Scene Graph (ODSG) with a Lightweight Transformer. The SFM trajectory decoupling algorithm enhances geometric constraints and utilizes the S2DNet network to extract deep features, thereby optimizing the 3D reconstruction process. Additionally, a three-stage Lightweight Transformer model is developed, integrating self-supervised depth estimation, feature selection, and multimodal fusion mechanisms. The research results showed that on the threedimensional benchmark dataset, the optimized outdoor dynamic scene image framework achieved a dense reconstruction accuracy of 32.29% at 1cm precision, which was 9.53% higher than that of the traditional Collection of COLMAP system. The F1 scores reached 4.23 and 54.34 at 1cm and 5cm precision, respectively. In terms of object recognition, the optimized 3D Transformer achieved an average accuracy index of 25.33%, 17.68%, and 14.72% for 3D object detection on joint datasets in Easy, Mod, and Hard modes, respectively, which was 2.38% higher than that of the Monocular 3D Object Detection with Flexible Representations. The average precision for bird's eye view reached 36.15% in Easy mode, representing a 36.1% improvement over the conventional M3D-RPN baseline (26.56%). The research provides an efficient 3D perception solution for monitoring and safety warning of automated equipment in intelligent construction.",Jing Meng,,0,Multi-Modal Lightweight 3D Transformer for Target Recognition and Reconstruction in Intelligent Construction,https://doi.org/10.31449/inf.v49i17.9808,,FALSE,,TRUE,diamond,Informatica,journal,2025-12-12,article
https://openalex.org/W4417438711,,Haechan Chong|Jongwon Lee|Hyemin Ahn,Ulsan National Institute of Science and Technology|Ulsan National Institute of Science and Technology|Pohang University of Science and Technology,0,Robust Task Planning via Failure Detection Using Scene Graph From Multi-View Images,https://doi.org/10.1109/lra.2025.3645659,,FALSE,,FALSE,closed,IEEE Robotics and Automation Letters,journal,2025-12-17,article
https://openalex.org/W7117263104,"""This dataset, titled VR-MultiNav, is a comprehensive multimodal collection designed for intent-based navigation within specific virtual reality environments. It comprises several thousand high-quality samples that synchronize natural language commands, normalized gesture coordinates, and head-orientation data representing the user's field-of-view. Each entry is mapped to a ground-truth navigation output formatted in JSON for seamless integration with structured scene graphs.""",Huiqin Chang,,0,"""VR-Nav""",https://doi.org/10.21227/0egh-cr30,,FALSE,,TRUE,green,IEEE DataPort,repository,2025-12-24,dataset
https://openalex.org/W7117241615,,Shanshan Du|Hanli Wang,Tongji University|Tongji University,0,Visual dialog with semantic consistency: An external knowledge-driven approach,https://doi.org/10.1016/j.neunet.2025.108523,0.0,FALSE,en,FALSE,closed,Neural Networks,journal,2025-12-25,article
https://openalex.org/W7117564981,,Linnan Lu|Guannan Si|Xinyu Liang|Mingshen Li|Fengyu Zhou,Shandong Jiaotong University|Shandong Jiaotong University|Shandong Jiaotong University|Shandong Jiaotong University|Shandong University,0,STVRM : Spatio-temporal relational modeling with vision transformer for dynamic scene graph generation,https://doi.org/10.1016/j.eswa.2025.131018,0.0,FALSE,en,FALSE,closed,Expert Systems with Applications,journal,2025-12-29,article
https://openalex.org/W7118561011,,Hojun Song|Chae-yeong Song|Dong-Hun Lee|Heejung Choi|Jinwoo Jeong|Sungjei Kim|Sang-hyo Park,Kyungpook National University|Kyungpook National University|Kyungpook National University|Kyungpook National University|Korea Electronics Technology Institute|Korea University of Technology and Education|Kyungpook National University,0,Compression Framework for Light 3D Scene Graph Generation via Pruning-As-Search and Distillation,https://doi.org/10.1109/tmm.2026.3651094,,FALSE,,FALSE,closed,IEEE Transactions on Multimedia,journal,2026-01-01,article
https://openalex.org/W7127446693,,Luca Bianchi|Marco Rossi|Giulia Conti|Alessandro De Santis,Sapienza University of Rome|Sapienza University of Rome|Sapienza University of Rome|Sapienza University of Rome,0,Dynamic Scene Graph Construction for Embodied Navigation in Interactive 3D Environments Authors,https://doi.org/10.2139/ssrn.6115351,,FALSE,,TRUE,green,SSRN Electronic Journal,repository,2026-01-01,preprint
https://openalex.org/W7124451324,,Ruonan Zhang|Gaoyun An|Yiqing Hao|Dapeng Wu,Beijing Jiaotong University|Beijing Jiaotong University|Beijing Jiaotong University|City University of Hong Kong,0,Hippocampal Memory-Like Separation-Completion Collaborative Network for Unbiased Scene Graph Generation,https://doi.org/10.1109/tip.2025.3650668,,FALSE,en,FALSE,closed,IEEE Transactions on Image Processing,journal,2026-01-01,article
https://openalex.org/W7118178921,,Yuhao ZHU|Long Sun,China Jiliang University|China Jiliang University,0,LLM4SGG: Large Language Model for Scene Graph Generation,https://doi.org/10.2139/ssrn.6017544,,FALSE,,TRUE,green,SSRN Electronic Journal,repository,2026-01-01,preprint
https://openalex.org/W7125662493,,Xiaoyan Xiao|Yuchen Zhou|Linkai Liu|Chao Gou,Sun Yat-sen University|Sun Yat-sen University|Sun Yat-sen University|Sun Yat-sen University,0,RG <sup>2</sup> : Reasoning With Scene Graphs and Traffic Rule Graphs for Interpretable Autonomous Driving Decisions,https://doi.org/10.1109/mits.2025.3649228,,FALSE,,FALSE,closed,IEEE Intelligent Transportation Systems Magazine,journal,2026-01-01,article
https://openalex.org/W7128174652,,Antoine Sanner,,0,Strukturiertes Schlussfolgern über intrakranielle Blutungen auf Voxelbasis,,,FALSE,en,FALSE,closed,Open MIND,repository,2026-01-01,other
https://openalex.org/W7119105327,"Autonomous Driving Systems (ADSs) are becoming increasingly widespread, with companies deploying them for various tasks such as taxi services, delivery, and personal transportation. As these systems become integral to daily life, ensuring their safe operation is crucial. However, recent high-profile safety incidents, including fatal collisions and traffic violations, have exposed the limitations of current validation approaches. These failures stem from three fundamental problems that span the entire ADS lifecycle. First, there exists a semantic gap between the high-dimensional sensor data that ADSs consume—such as camera images and LiDAR point clouds—and the high-level safe driving properties against which they must be validated, preventing the formal specification and automated evaluation of safety requirements. Second, safety property violations are often discovered late in ADS development, requiring costly remediation efforts such as data collection and model retraining. Third, during deployment, ADS lack mechanisms for continuous monitoring and real-time correction to prevent safe driving properties violations and avoid potential accidents. To address these problems, this dissertation introduces SD4AS (Safe Driving for Autonomous Systems), a framework designed to improve safe driving property conformance across the entire ADS lifecycle. SD4AS bridges the semantic gap by leveraging scene graphs—structured representations that capture entities and their spatial relationships—as an abstraction of raw sensor data. Combined with a specification language, this abstraction enables the definition and automated evaluation of complex safety properties, capturing 76% of the rules in the Virginia driving code. To improve conformance during development, SD4AS first quantifies the extent to which datasets contain scenarios necessary to validate properties, enabling developers to identify gaps in training data. Beyond data adequacy, for ADS that rely on machine learning components, SD4AS introduces property-aware optimization that treats safety rule violations as training errors, guiding the learning process to produce components that exhibit safe behaviors by construction, demonstrated by the fine-tuning two ADSs to reduce driving infractions. To maintain conformance during deployment, SD4AS synthesizes runtime monitors directly from property specifications, enabling continuous evaluation of safety rules during the ADS operation. To enable real-world monitoring, SD4AS integrates scene graph generators to extract entities and their spatial relationships in the driving domain directly from camera images. Lastly, SD4AS introduces a correction mechanism that proactively adjusts control outputs when violations are imminent, maintaining property conformance through real-time interventions, successfully reducing infractions of three different ADS architectures. Through the development of these approaches and their empirical evaluation across multiple ADS, this dissertation advances the state of the art in ADS safety assurance. The contributions span the entire validation pipeline—from bridging sensor inputs and safety specifications, through improving conformance during development, to enabling continuous monitoring and correction during deployment—bringing us closer to safe autonomous vehicles.",Felipe Toledo,University of Virginia,0,SD4AS: Safe Driving for Autonomous Systems,https://doi.org/10.18130/b41g-7b50,,FALSE,,TRUE,green,Libra,repository,2026-01-07,article
https://openalex.org/W7123358987,"Visual Relationship Prediction (VRP) is crucial for advancing structured scene understanding, yet existing methods struggle with ineffective multimodal fusion, static relationship representations, and a lack of logical consistency. To address these limitations, this paper proposes a Hierarchical CLIP model (H-CLIP) for structured scene parsing. Our approach leverages a pre-trained CLIP backbone to extract aligned visual, textual, and spatial features for entities and their union regions. A multi-head self-attention mechanism then performs deep, dynamic multimodal fusion. The core innovation is a consistency and reversibility verification mechanism, which imposes algebraic constraints as a regularization loss to enforce logical coherence in the learned relation space. Extensive experiments on the Visual Genome dataset demonstrate the superiority of the proposed method. H-CLIP significantly outperforms state-of-the-art baselines on the predicate classification task, achieving a Recall@50 score of 64.31% and a Mean Recall@50 of 36.02%, thereby validating its effectiveness in generating accurate and logically consistent scene graphs even under long-tailed distributions.",Yunhao Sun|Xiaoao Chen|Heng Chen|Yiduo Liang|Ruihua Qi,Dalian University of Foreign Languages|Dalian University of Foreign Languages|Dalian University of Foreign Languages|Dalian University of Foreign Languages|Dalian University of Foreign Languages,0,Structured Scene Parsing with a Hierarchical CLIP Model for Images,https://doi.org/10.3390/app16020788,,FALSE,en,TRUE,gold,Applied Sciences,journal,2026-01-12,article
https://openalex.org/W7124435494,"Scientific studies have demonstrated how certain insect species can be used as bioindicators and reverse environmental degradation through their behavior and organization. Studying these species involves capturing and extracting hundreds of insects from a colony for subsequent study, analysis, and observation. This allows researchers to classify the individuals and also determine the organizational structure and behavioral patterns of the insects within colonies. The miniaturization of hardware devices for data and image acquisition, coupled with new Artificial Intelligence techniques such as Scene Graph Generation (SGG), has evolved from the detection and recognition of objects in an image to the understanding of relationships between objects and the ability to produce textual descriptions based on image content and environmental parameters. This research paper presents the design and functionality of a distributed computing architecture for image and video acquisition of bees and ants in their natural environment, in addition to a parallel computing architecture that hosts two datasets with images of real environments from which scene graphs are generated to recognize, classify, and analyze the behaviors of bees and ants while preserving and protecting these species. The experiments that were carried out are classified into two categories, namely the recognition and classification of objects in the image and the understanding of the relationships between objects and the generation of textual descriptions of the images. The results of the experiments, conducted in real-life environments, show recognition rates above 70%, classification rates above 80%, and comprehension and generation of textual descriptions with an assertive rate of 85%.",Apolinar Velarde Velarde Martinez|Gilberto Gonzalez Gonzalez Rodriguez,Universidad Tecnológica de Aguascalientes|Universidad Tecnológica de Aguascalientes,0,Distributed Artificial Intelligence for Organizational and Behavioral Recognition of Bees and Ants,https://doi.org/10.3390/s26020622,,FALSE,en,TRUE,gold,Sensors,journal,2026-01-16,article
https://openalex.org/W7125531837,"Scene Graph Generation (SGG) aims to extract visual entities and their semantic relationships from images, providing a structured layout for scene understanding. Current models often suffer from insufficient multi-modal feature fusion and imbalanced predicate distributions, leading to biased predictions. To address these issues, we propose ReBalance-HCA, a unified framework that combines Hybrid Co-Attention Networks (HCA) with Predicate Reweighting (PR). HCA enhances intra-modal features and aligns cross-modal semantics, while PR dynamically adjusts the predicate distribution by modeling inter-predicate correlations. Extensive experiments on the Visual Genome and OpenImages datasets demonstrate that ReBalance-HCA achieves competitive mR@K scores compared to recent state-of-the-art methods in SGG sub-tasks. Our code and datasets are available at: https://github.com/LinusLing/ReBalance-HCA .",Fei Ling|Zheyan Zhu|Wen Fan,,0,Enhancing scene graph generation <i>via</i> hybrid co-attention and predicate reweighting for long-tail robustness,https://doi.org/10.7717/peerj-cs.3548,,FALSE,en,TRUE,gold,PeerJ Computer Science,journal,2026-01-23,article
https://openalex.org/W7125923953,"Understanding how movable objects affect navigability is critical for embodied agents operating in realistic environments. This study proposes a learning-based approach to infer traversable scene structures under object mobility constraints. A neural graph encoder is trained to predict passability relations between spatial regions conditioned on object states, using RGB-D observations and interaction feedback. The model is trained on 15,000 simulated navigation trajectories generated in rearranged indoor scenes. Quantitative evaluation shows that the learned scene structure reduces navigation failure due to blocked paths by 28.4% and improves average navigation efficiency by 16.7% compared with static scene graph representations.",James Walker|Emily A. Carter|Thomas Bennett|Sophie Hughes,,0,Learning Traversable Scene Structures for Embodied Navigation with Movable Object Constraints,https://doi.org/10.20944/preprints202601.1852.v1,,FALSE,,TRUE,green,Preprints.org,repository,2026-01-26,preprint
https://openalex.org/W7125809769,,Zhaodi Wang|Biao Leng|Shuo Zhang,Beihang University|Beihang University|Beijing Jiaotong University,0,DuoNet: Joint optimization of representation learning and prototype classifier for unbiased scene graph generation,https://doi.org/10.1016/j.patcog.2026.113152,,FALSE,en,FALSE,closed,Pattern Recognition,journal,2026-01-27,article
https://openalex.org/W7125824985,"Abstract. The map, as a way of representing geospatial data, is designed to reflect important information about the Earth as deeply and accurately as possible. To meet this requirement, maps are produced in different scales and different types, depending on the task being solved. Created by highly educated specialists, the map contains not only raw geospatial data, but also some high-level knowledge accumulated by people during the exploration of the Earth. The introduction of deep learning into the data analysis process has allowed the development of neural network models that can solve complex aerial image processing tasks, such as semantic image segmentation, object detection and recognition, and retrieving of semantic relations between objects in a scene. These advances created the background for moving to image (scene) understanding as a higher level of image analysis. The current study addresses to a problem of multi-scale scene graph generation from aerial images, similarly to creating maps of different scales.",Vladimir A. Knyaz|Vladimir V. Kniaz|Anton V. Emelyanov|Sergey Yu. Zheltov|Victor S. Aleksandrov,Moscow Institute of Physics and Technology|State Scientific Research Institute of Aviation Systems|Moscow Institute of Physics and Technology|State Scientific Research Institute of Aviation Systems|Moscow Institute of Physics and Technology|State Scientific Research Institute of Aviation Systems|State Scientific Research Institute of Aviation Systems|State Scientific Research Institute of Aviation Systems,0,Multi-scale scene graph generation for remote sensing imagery,https://doi.org/10.5194/isprs-archives-xlviii-4-w18-2025-215-2026,,FALSE,en,TRUE,diamond,"The international archives of the photogrammetry, remote sensing and spatial information sciences/International archives of the photogrammetry, remote sensing and spatial information sciences",journal,2026-01-27,article
https://openalex.org/W7126244221,,Felix Igelbrink|Lennart Niecksch|Marian Renz|Martin Günther|Martin Atzmueller,German Research Centre for Artificial Intelligence|Osnabrück University|German Research Centre for Artificial Intelligence|Osnabrück University|German Research Centre for Artificial Intelligence|German Research Centre for Artificial Intelligence|Osnabrück University|German Research Centre for Artificial Intelligence,0,LIEREx: Language-Image Embeddings for Robotic Exploration,https://doi.org/10.1007/s13218-026-00902-6,,FALSE,en,TRUE,green,KI - Künstliche Intelligenz,journal,2026-01-30,article
https://openalex.org/W7126247100,,Tao He|Xin Hu|Tongtong Wu|Dongyang Zhang|Ming Li|Yuan-Fang Li|Fei Richard Yu,Institute of Computing Technology|Institute of Computing Technology|Monash University|Institute of Computing Technology|Beijing Academy of Artificial Intelligence|Monash University|Beijing Academy of Artificial Intelligence,0,Lifelong scene graph generation,https://doi.org/10.1016/j.patcog.2026.113132,,FALSE,en,FALSE,closed,Pattern Recognition,journal,2026-01-31,article
https://openalex.org/W7127077934,"We address the fundamental challenge of collaborative multi-agent object-goal navigation for autonomous inspections in complex, real-world environments. While single- agent approaches to object-goal navigation have demonstrated considerable promise in recent years, scaling these methods to larger environments necessitates the coordination of multiple robots to achieve efficient coverage, faster task completion, and robust operation under uncertainty. We introduce SEEK-Multi, a comprehensive framework that extends semantic-guided object inspection to multi-robot systems through distributed belief sharing, collaborative planning, coordinated task allocation, and adaptive communication protocols. SEEK-Multi enables multiple agents to share semantic understanding and inspection findings through a distributed Relational Semantic Network (RSN) and a shared Dynamic Scene Graph (DSG), maintaining consistency across the team while accommodating communication constraints. We propose novel algorithms for collaborative exploration that leverage semantic priors, belief fusion using consensus protocols with provable convergence guarantees, and conflict-free task allocation based on auction mechanisms. Our extensive simulation analyses across diverse environment configurations demonstrate that SEEK-Multi achieves significant speedup over single-agent approaches while maintaining high success rates, with near-linear scaling efficiency for up to four agents and graceful degradation under communication failures. We validate our approach through comprehensive simulations including ablation studies, sensitivity analyses, and comparisons with state-of-the-art multi-agent coordination methods, demonstrating its practicality for real-world multi-robot inspection scenarios in industrial, search-and-rescue, and domestic environments. Code is available at: https://arrdel.github.io/seek-multi/",Adele Chinda,,0,SEEK-Multi: Collaborative Multi-Agent Semantic Reasoning for Object Goal Navigation in Inspection Tasks,https://doi.org/10.30574/ijsra.2026.18.1.0181,,FALSE,,TRUE,hybrid,International Journal of Science and Research Archive,journal,2026-01-31,article
https://openalex.org/W7127740496,"Zero-Shot Object Navigation in unknown environments poses significant challenges for Unmanned Aerial Vehicles (UAVs) due to the conflict between high-level semantic reasoning requirements and limited onboard computational resources. To address this, we present USS-Nav, a lightweight framework that incrementally constructs a Unified Spatio-Semantic scene graph and enables efficient Large Language Model (LLM)-augmented Zero-Shot Object Navigation in unknown environments. Specifically, we introduce an incremental Spatial Connectivity Graph generation method utilizing polyhedral expansion to capture global geometric topology, which is dynamically partitioned into semantic regions via graph clustering. Concurrently, open-vocabulary object semantics are instantiated and anchored to this topology to form a hierarchical environmental representation. Leveraging this hierarchical structure, we present a coarse-to-fine exploration strategy: LLM grounded in the scene graph's semantics to determine global target regions, while a local planner optimizes frontier coverage based on information gain. Experimental results demonstrate that our framework outperforms state-of-the-art methods in terms of computational efficiency and real-time update frequency (15 Hz) on a resource-constrained platform. Furthermore, ablation studies confirm the effectiveness of our framework, showing substantial improvements in Success weighted by Path Length (SPL). The source code will be made publicly available to foster further research.",Weiqi Gai|Yuman Gao|Yuan Zhou|Yufan Xie|Zhiyang Liu|Yuze Wu|Xin Zhou|Fei Gao|Zhijun Meng,,0,USS-Nav: Unified Spatio-Semantic Scene Graph for Lightweight UAV Zero-Shot Object Navigation,,,FALSE,,TRUE,green,arXiv (Cornell University),repository,2026-01-31,article
https://openalex.org/W7127407058,,Daozheng Qu|Yanfei Ma,University of Liverpool|University of Liverpool|Fairleigh Dickinson University,0,Fourier–Transformer Mixer Network for Efficient Video Scene Graph Prediction,https://doi.org/10.3390/engproc2025120016,,FALSE,,TRUE,gold,,,2026-02-02,article
https://openalex.org/W7127739951,"Despite rapid progress, embodied agents still struggle with long-horizon manipulation that requires maintaining spatial consistency, causal dependencies, and goal constraints. A key limitation of existing approaches is that task reasoning is implicitly embedded in high-dimensional latent representations, making it challenging to separate task structure from perceptual variability. We introduce Grounded Scene-graph Reasoning (GSR), a structured reasoning paradigm that explicitly models world-state evolution as transitions over semantically grounded scene graphs. By reasoning step-wise over object states and spatial relations, rather than directly mapping perception to actions, GSR enables explicit reasoning about action preconditions, consequences, and goal satisfaction in a physically grounded space. To support learning such reasoning, we construct Manip-Cognition-1.6M, a large-scale dataset that jointly supervises world understanding, action planning, and goal interpretation. Extensive evaluations across RLBench, LIBERO, GSR-benchmark, and real-world robotic tasks show that GSR significantly improves zero-shot generalization and long-horizon task completion over prompting-based baselines. These results highlight explicit world-state representations as a key inductive bias for scalable embodied reasoning.",Kewei Hu|Michael Zhang|Wei Ying|Tianhao Liu|Guoqiang Hao|Zimeng Li|Wanchan Yu|Jiajian Jing|Fangwen Chen|Hanwen Kang,,0,GSR: Learning Structured Reasoning for Embodied Manipulation,,,FALSE,,TRUE,green,arXiv (Cornell University),repository,2026-02-02,article
https://openalex.org/W7127541116,"Representing and understanding 3D environments in a structured manner is crucial for autonomous agents to navigate and reason about their surroundings. While traditional Simultaneous Localization and Mapping (SLAM) methods generate metric reconstructions and can be extended to metric-semantic mapping, they lack a higher level of abstraction and relational reasoning. To address this gap, 3D scene graphs have emerged as a powerful representation for capturing hierarchical structures and object relationships. In this work, we propose an enhanced hierarchical 3D scene graph that integrates open-vocabulary features across multiple abstraction levels and supports object-relational reasoning. Our approach leverages a Vision Language Model (VLM) to infer semantic relationships. Notably, we introduce a task reasoning module that combines Large Language Models (LLM) and a VLM to interpret the scene graph's semantic and relational information, enabling agents to reason about tasks and interact with their environment more intelligently. We validate our method by deploying it on a quadruped robot in multiple environments and tasks, highlighting its ability to reason about them.",Albert Gassol Puigjaner|Angelos Zacharia|Kostas Alexis,,0,Relationship-Aware Hierarchical 3D Scene Graph for Task Reasoning,,,FALSE,,TRUE,green,ArXiv.org,repository,2026-02-02,article
https://openalex.org/W7127739884,"While Open Set Semantic Mapping and 3D Semantic Scene Graphs (3DSSGs) are established paradigms in robotic perception, deploying them effectively to support high-level reasoning in large-scale, real-world environments remains a significant challenge. Most existing approaches decouple perception from representation, treating the scene graph as a derivative layer generated post hoc. This limits both consistency and scalability. In contrast, we propose a mapping architecture where the 3DSSG serves as the foundational backend, acting as the primary knowledge representation for the entire mapping process. Our approach leverages prior work on incremental scene graph prediction to infer and update the graph structure in real-time as the environment is explored. This ensures that the map remains topologically consistent and computationally efficient, even during extended operations in large-scale settings. By maintaining an explicit, spatially grounded representation that supports both flat and hierarchical topologies, we bridge the gap between sub-symbolic raw sensor data and high-level symbolic reasoning. Consequently, this provides a stable, verifiable structure that knowledge-driven frameworks, ranging from knowledge graphs and ontologies to Large Language Models (LLMs), can directly exploit, enabling agents to operate with enhanced interpretability, trustworthiness, and alignment to human concepts.",Martin Günther|Felix Igelbrink|Oscar Lima|Lennart Niecksch|Marian Renz|Martin Atzmueller,,0,A Scene Graph Backed Approach to Open Set Semantic Mapping,,,FALSE,,TRUE,green,arXiv (Cornell University),repository,2026-02-03,article
https://openalex.org/W7127341192,,Shengnan Ke|Shibin Li|Jun Gong|Lingxiang Liu|Jianjun Luo|Bing Wang|Shengjun Tang,Jiangxi Normal University|Jiangxi Normal University|Jiangxi Normal University|Shenzhen University|Urban Planning & Design Institute of Shenzhen (China)|Jiangxi Normal University|Hong Kong Polytechnic University|Shenzhen University|Urban Planning & Design Institute of Shenzhen (China),0,Dependency-aware indoor 3D scene graph prediction via multimodal feature learning,https://doi.org/10.1016/j.autcon.2026.106817,,FALSE,en,FALSE,closed,Automation in Construction,journal,2026-02-03,article
https://openalex.org/W7127739832,"We introduce SceneLinker, a novel framework that generates compositional 3D scenes via semantic scene graph from RGB sequences. To adaptively experience Mixed Reality (MR) content based on each user's space, it is essential to generate a 3D scene that reflects the real-world layout by compactly capturing the semantic cues of the surroundings. Prior works struggled to fully capture the contextual relationship between objects or mainly focused on synthesizing diverse shapes, making it challenging to generate 3D scenes aligned with object arrangements. We address these challenges by designing a graph network with cross-check feature attention for scene graph prediction and constructing a graph-variational autoencoder (graph-VAE), which consists of a joint shape and layout block for 3D scene generation. Experiments on the 3RScan/3DSSG and SG-FRONT datasets demonstrate that our approach outperforms state-of-the-art methods in both quantitative and qualitative evaluations, even in complex indoor environments and under challenging scene graph constraints. Our work enables users to generate consistent 3D spaces from their physical environments via scene graphs, allowing them to create spatial MR content. Project page is https://scenelinker2026.github.io.",Seok-Young Kim|Dooyoung Kim|Woojin Cho|Hail Song|Suji Kang|Woontack Woo,,0,SceneLinker: Compositional 3D Scene Generation via Semantic Scene Graph from RGB Sequences,,,FALSE,,TRUE,green,arXiv (Cornell University),repository,2026-02-03,article
https://openalex.org/W7128097472,"In partially known environments, robots must combine exploration to gather information with task planning for efficient execution. To address this challenge, we propose EPoG, an Exploration-based sequential manipulation Planning framework on Scene Graphs. EPoG integrates a graph-based global planner with a Large Language Model (LLM)-based situated local planner, continuously updating a belief graph using observations and LLM predictions to represent known and unknown objects. Action sequences are generated by computing graph edit operations between the goal and belief graphs, ordered by temporal dependencies and movement costs. This approach seamlessly combines exploration and sequential manipulation planning. In ablation studies across 46 realistic household scenes and 5 long-horizon daily object transportation tasks, EPoG achieved a success rate of 91.3%, reducing travel distance by 36.1% on average. Furthermore, a physical mobile manipulator successfully executed complex tasks in unknown and dynamic environments, demonstrating EPoG's potential for real-world applications.",Heqing Yang|Ziyuan Jiao|Shu Wang|Yida Niu|Si Liu|Hangxin Liu,,0,Integrated Exploration and Sequential Manipulation on Scene Graph with LLM-based Situated Replanning,,,FALSE,,TRUE,green,ArXiv.org,repository,2026-02-04,article
https://openalex.org/W7128097626,"Current 3D scene graph generation (3DSGG) approaches heavily rely on a single-agent assumption and small-scale environments, exhibiting limited scalability to real-world scenarios. In this work, we introduce Multi-Agent 3D Scene Graph Generation (MA3DSG) model, the first framework designed to tackle this scalability challenge using multiple agents. We develop a training-free graph alignment algorithm that efficiently merges partial query graphs from individual agents into a unified global scene graph. Leveraging extensive analysis and empirical insights, our approach enables conventional single-agent systems to operate collaboratively without requiring any learnable parameters. To rigorously evaluate 3DSGG performance, we propose MA3DSG-Bench-a benchmark that supports diverse agent configurations, domain sizes, and environmental conditions-providing a more general and extensible evaluation framework. This work lays a solid foundation for scalable, multi-agent 3DSGG research.",Yirum Kim|Jae-Woo Kim|Ue-Hwan Kim,,0,MA3DSG: Multi-Agent 3D Scene Graph Generation for Large-Scale Indoor Environments,,,FALSE,,TRUE,green,ArXiv.org,repository,2026-02-04,article
https://openalex.org/W7128096103,"Robots are finding wider adoption in human environments, increasing the need for natural human-robot interaction. However, understanding a natural language command requires the robot to infer the intended task and how to decompose it into executable actions, and to ground those actions in the robot's knowledge of the environment, including relevant objects, agents, and locations. This challenge can be addressed by combining the capabilities of Large language models (LLMs) to understand natural language with 3D scene graphs (3DSGs) for grounding inferred actions in a semantic representation of the environment. However, many 3DSGs lack explicit spatial relations between objects, even though humans often rely on these relations to describe an environment. This paper investigates whether incorporating open- or closed-vocabulary spatial relations into 3DSGs can improve the ability of LLMs to interpret natural language commands. To address this, we propose an LLM-based pipeline for target object grounding from open-vocabulary language commands and a vision language model (VLM)-based pipeline to add open-vocabulary spatial edges to 3DSGs from images captured while mapping. Finally, two LLMs are evaluated in a study assessing their performance on the downstream task of target object grounding. Our study demonstrates that explicit spatial relations improve the ability of LLMs to ground objects. Moreover, open-vocabulary relation generation with VLMs proves feasible from robot-captured images, but their advantage over closed-vocabulary relations is found to be limited.",Julia Christina Kühn|Francesco Verdoja|Tsvetomila Mihaylova|Ville Kyrki,,0,Relational Scene Graphs for Object Grounding of Natural Language Commands,,,FALSE,,TRUE,green,ArXiv.org,repository,2026-02-04,article
https://openalex.org/W7128408830,"Large language models (LLMs) have recently shown strong potential for Automated Program Repair (APR), yet most existing approaches remain unimodal and fail to leverage the rich diagnostic signals contained in visual artifacts such as screenshots and control-flow graphs. In practice, many bug reports convey critical information visually (e.g., layout breakage or missing widgets), but directly using such dense visual inputs often causes context loss and noise, making it difficult for MLLMs to ground visual observations into precise fault localization and executable patches. To bridge this semantic gap, we propose \textbf{SVRepair}, a multimodal APR framework with structured visual representation. SVRepair first fine-tunes a vision-language model, \textbf{Structured Visual Representation (SVR)}, to uniformly transform heterogeneous visual artifacts into a \emph{semantic scene graph} that captures GUI elements and their structural relations (e.g., hierarchy), providing normalized, code-relevant context for downstream repair. Building on the graph, SVRepair drives a coding agent to localize faults and synthesize patches, and further introduces an iterative visual-artifact segmentation strategy that progressively narrows the input to bug-centered regions to suppress irrelevant context and reduce hallucinations. Extensive experiments across multiple benchmarks demonstrate state-of-the-art performance: SVRepair achieves \textbf{36.47\%} accuracy on SWE-Bench M, \textbf{38.02\%} on MMCode, and \textbf{95.12\%} on CodeVision, validating the effectiveness of SVRepair for multimodal program repair.",Xiaoxuan Tang|Jincheng Wang|Liwei Luo|Jingxuan Xu|Sheng Zhou|Dajun Chen|Wei Jiang|Yong Li,,0,SVRepair: Structured Visual Reasoning for Automated Program Repair,,,FALSE,,TRUE,green,ArXiv.org,repository,2026-02-05,article
