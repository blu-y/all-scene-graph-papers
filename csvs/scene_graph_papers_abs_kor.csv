no,arxiv_url,title,authors,abstract_kor
1036,http://arxiv.org/abs/2602.20055 ,To Move or Not to Move: Constraint-based Planning Enables Zero-Shot Generalization for Interactive Navigation,"Apoorva Vashisth, Manav Kulshrestha, Pranav Bakshi, Damon Conover, Guillaume Sartoretti, Aniket Bera","시각적 탐색은 일반적으로 로봇이 발견/계획해야 하는 시작과 목표 사이에 장애물이 없는 경로가 하나 이상 존재한다고 가정합니다. 그러나 가정 환경 및 창고와 같은 실제 시나리오에서는 혼란으로 인해 모든 경로가 차단될 수 있습니다. 이러한 경우를 목표로 우리는 조작 능력이 있는 모바일 로봇이 복잡한 물건을 이동하여 순차적인 물건 배치 작업을 완료하기 위한 자체 경로를 만들 수 있는 평생 대화형 내비게이션 문제를 소개합니다. 각 작업에는 주어진 물건(예: 알람 시계, 베개)을 대상 물건(예: 식탁, 책상, 침대)에 배치하는 작업이 포함됩니다. 환경 변화의 영향이 누적되고 장기적인 영향을 미치는 이러한 평생 환경을 해결하기 위해 우리는 적극적인 인식을 갖춘 LLM 중심의 제약 기반 계획 프레임워크를 제안합니다. 우리의 프레임워크를 통해 LLM은 발견된 물체와 장애물의 구조화된 장면 그래프를 추론하여 이동할 물체, 배치할 위치, 작업 관련 정보를 발견하기 위해 다음에 볼 위치를 결정할 수 있습니다. 이러한 추론과 능동적 인식의 결합을 통해 에이전트는 환경을 철저하게 매핑하는 대신 작업 완료에 기여할 것으로 예상되는 영역을 탐색할 수 있습니다. 그런 다음 표준 모션 플래너는 해당 탐색-선택-위치 또는 우회 시퀀스를 실행하여 안정적인 하위 수준 제어를 보장합니다. 물리학 기반 ProcTHOR-10k 시뮬레이터에서 평가된 우리의 접근 방식은 비학습 및 학습 기반 기준보다 성능이 뛰어납니다. 우리는 실제 하드웨어에 대한 우리의 접근 방식을 질적으로 보여줍니다."
1035,http://arxiv.org/abs/2602.19974 ,RL-RIG: A Generative Spatial Reasoner via Intrinsic Reflection,"Tianyu Wang, Zhiyuan Ma, Qian Wang, Xinyi Zhang, Xinwei Long, Bowen Zhou","최근 이미지 생성의 발전으로 고품질 이미지를 생성하는 데 있어 인상적인 결과를 얻었습니다. 그러나 기존 이미지 생성 모델은 일반적으로 공간 추론 딜레마로 인해 여전히 어려움을 겪고 있으며, 프롬프트에서 세분화된 공간 관계를 정확하게 캡처하고 구조적 무결성이 있는 장면을 올바르게 생성하는 기능이 부족합니다. 이러한 딜레마를 완화하기 위해 우리는 반사 기반 이미지 생성을 위한 강화 학습 프레임워크인 RL-RIG를 제안합니다. 우리의 아키텍처는 딜레마를 해결하기 위해 이미지 생성에서 사고 사슬 추론 능력을 촉발하는 생성-반사-편집 패러다임에 따라 Diffuser, Checker, Actor 및 Inverse Diffuser의 네 가지 기본 구성 요소로 구성됩니다. 모델에 생성 궤적에 대한 더 나은 직관력을 제공하기 위해 Reflection-GRPO를 추가로 개발하여 편집 프롬프트에 대해 VLM 액터를 훈련하고 주어진 프롬프트에서 더 나은 이미지 품질을 위해 이미지 편집기를 각각 훈련합니다. 시각적으로 훌륭하지만 구조적으로 불합리한 콘텐츠만 생성하는 기존 접근 방식과 달리, 우리의 평가 지표는 장면 그래프 IoU를 활용하고 VLM-as-a-Judge 전략을 사용하여 LAION-SG 데이터 세트에서 생성된 이미지의 공간적 일관성을 평가하는 등 공간 정확도를 우선시합니다. 실험 결과에 따르면 RL-RIG는 이미지 생성 시 제어 가능하고 정확한 공간 추론 측면에서 기존의 최첨단 오픈 소스 모델보다 최대 11% 더 뛰어난 것으로 나타났습니다."
1034,http://arxiv.org/abs/2602.17555 ,GraphThinker: Reinforcing Video Reasoning with Event Graph Thinking,"Zixu Cheng, Da Li, Jian Hu, Ziquan Liu, Wei Li, Shaogang Gong",비디오 추론을 위해서는 비디오 속 사건 간의 인과 관계를 이해해야 합니다. 그러나 이러한 관계는 암시적이어서 수동으로 주석을 달기에는 비용이 많이 드는 경우가 많습니다. 기존 다중 모드 대형 언어 모델(MLLM)은 비디오 추론을 위해 조밀한 캡션이나 비디오 요약을 통해 이벤트 관계를 추론하는 경우가 많지만 이러한 모델링에는 여전히 인과 관계 이해가 부족합니다. 비디오 이벤트 내외의 명시적인 인과 구조 모델링이 없으면 이러한 모델은 비디오 추론 중에 환각에 시달립니다. 본 연구에서는 구조적 이벤트 수준 장면 그래프를 구성하고 시각적 기반을 강화하여 비디오 추론에서 환각을 공동으로 줄이는 강화 미세 조정 기반 방법인 GraphThinker를 제안합니다. 특히 우리는 먼저 MLLM을 사용하여 이벤트 내 관계와 이벤트 간 관계를 모두 명시적으로 모델링하는 이벤트 기반 비디오 장면 그래프(EVSG)를 구성하고 이렇게 형성된 장면 그래프를 중간 사고 과정으로 MLLM에 통합합니다. 또한 강화 미세 조정 중에 시각적 주의 보상을 도입하여 비디오 접지를 강화하고 환각을 더욱 완화합니다. 우리는 RexTime과 VidHalluc의 두 가지 데이터 세트에서 GraphThinker를 평가합니다. 여기서는 보다 정확한 이벤트 위치 파악을 통해 객체와 이벤트 관계를 캡처하는 탁월한 능력을 보여 이전 방법에 비해 비디오 추론에서 환각을 줄입니다.
1033,http://arxiv.org/abs/2602.16356 ,Articulated 3D Scene Graphs for Open-World Mobile Manipulation,"Martin Büchner, Adrian Röfer, Tim Engelbracht, Tim Welschehold, Zuria Bauer, Hermann Blum, Marc Pollefeys, Abhinav Valada","시맨틱스는 3D 장면 이해와 여유 기반 개체 상호 작용을 가능하게 했습니다. 그러나 실제 환경에서 작동하는 로봇은 물체가 어떻게 움직이는지 예측할 수 없다는 치명적인 한계에 직면합니다. 장거리 모바일 조작을 위해서는 의미론, 기하학, 운동학 사이의 격차를 줄여야 합니다. 이 작업에서 우리는 상호 작용할 수 있는 수많은 개체를 포함하는 연결된 장면의 의미 운동학적 3D 장면 그래프를 구축하기 위한 새로운 프레임워크인 MoMa-SG를 제시합니다. 여러 개체 관절이 포함된 RGB-D 시퀀스가 ​​주어지면 우리는 개체 상호 작용을 시간적으로 분할하고 폐색에 강한 점 추적을 사용하여 개체 동작을 추론합니다. 그런 다음 점 궤적을 3D로 리프트하고 단일 최적화 패스에서 회전 및 프리즘 관절 매개변수를 강력하게 추정하는 새로운 통합 비틀림 추정 공식을 사용하여 관절 모델을 추정합니다. 다음으로, 추정된 관절과 객체를 연결하고 식별된 개방 상태에서 부모-자식 관계를 추론하여 포함된 객체를 감지합니다. 또한 600개의 개체 상호 작용과 3개의 서로 다른 관찰 패러다임을 포함하는 62개의 실제 RGB-D 시퀀스에 걸쳐 부모-자식 관계 레이블을 포함한 계층적 개체 의미론을 개체 축 주석과 고유하게 결합하는 새로운 Arti4D-Semantic 데이터 세트를 소개합니다. 우리는 두 가지 데이터 세트에서 MoMa-SG의 성능을 광범위하게 평가하고 우리 접근 방식의 주요 설계 선택을 제거합니다. 또한 네 발 달린 동물과 이동식 조작기에 대한 실제 실험에서는 의미론적 운동학적 장면 그래프가 일상적인 가정 환경에서 연결된 개체를 강력하게 조작할 수 있음을 보여줍니다. 우리는 https://momasg.cs.uni-freiburg.de에서 코드와 데이터를 제공합니다."
1032,http://arxiv.org/abs/2602.13086 ,UniManip: General-Purpose Zero-Shot Robotic Manipulation with Agentic Operational Graph,"Haichao Liu, Yuanjiang Xue, Yuheng Zhou, Haoyuan Deng, Yinan Liang, Lihua Xie, Ziwei Wang","범용 로봇 조작을 달성하려면 로봇이 구조화되지 않은 환경에서 높은 수준의 의미론적 의도와 낮은 수준의 물리적 상호 작용을 원활하게 연결해야 합니다. 그러나 기존 접근 방식은 제로샷 일반화에서 불안정합니다. 엔드투엔드 VLA(Vision-Language-Action) 모델은 장거리 작업에 필요한 정밀도가 부족한 경우가 많은 반면, 기존 계층적 계획자는 개방형 변형에 직면할 때 의미론적 경직성으로 인해 어려움을 겪습니다. 이 문제를 해결하기 위해 우리는 의미론적 추론과 물리적 기반을 통합하는 Bi-level Agentic Operational Graph(AOG) 기반 프레임워크인 UniManip을 제시합니다. 작업 조정을 위한 높은 수준의 Agentic Layer와 동적 상태 표현을 위한 낮은 수준의 Scene Layer를 결합함으로써 시스템은 추상적 계획을 기하학적 제약 조건에 맞춰 지속적으로 조정하여 강력한 제로샷 실행을 가능하게 합니다. 정적 파이프라인과 달리 UniManip은 동적 에이전트 루프로 작동합니다. 즉, 구조화되지 않은 인식에서 객체 중심 장면 그래프를 적극적으로 인스턴스화하고, 안전 인식 로컬 플래너를 통해 이러한 표현을 충돌 없는 궤적으로 매개변수화하고, 구조화된 메모리를 활용하여 실행 실패를 자동으로 진단하고 복구합니다. 광범위한 실험을 통해 보이지 않는 물체와 작업에 대한 시스템의 강력한 제로샷 기능을 검증하여 최첨단 VLA 및 계층적 기준에 비해 각각 22.5% 및 25.0% 더 높은 성공률을 보여줍니다. 특히, 이 시스템은 미세 조정이나 재구성 없이 고정 베이스 설정에서 모바일 조작으로 직접 제로샷 전송을 가능하게 합니다. 우리의 오픈 소스 프로젝트 페이지는 https://henryhcliu.github.io/unimanip에서 찾을 수 있습니다."
1031,http://arxiv.org/abs/2602.12971 ,INHerit-SG: Incremental Hierarchical Semantic Scene Graphs with RAG-Style Retrieval,"YukTungSamuel Fang, Zhikang Shi, Jiabin Qiu, Zixuan Chen, Jieqi Shi, Hao Xu, Jing Huo, Yang Gao","기초 모델의 발전에 힘입어 의미론적 장면 그래프는 로봇 탐색에서 높은 수준의 3D 환경 추상화를 위한 주요 패러다임으로 부상했습니다. 그러나 기존 접근 방식은 구현된 작업의 요구 사항과 근본적으로 일치하지 않습니다. 오프라인 일괄 처리 또는 암시적 기능 임베딩에 의존하기 때문에 맵은 복잡한 환경에서 해석 가능한 인간 의도 추론을 거의 지원할 수 없습니다. 이러한 제한 사항을 해결하기 위해 INHerit-SG를 제시합니다. 우리는 인간의 의도에 더 잘 부합하기 위해 자연어 설명이 명시적인 의미론적 앵커로 도입되는 구조화된 RAG 지원 지식 기반으로 맵을 재정의합니다. 비동기식 이중 프로세스 아키텍처는 Floor-Room-Area-Object 계층 구조와 함께 시간이 많이 걸리는 의미 추론에서 기하학적 분할을 분리합니다. 이벤트로 트리거되는 지도 업데이트 메커니즘은 의미 있는 의미 이벤트가 발생할 때만 그래프를 재구성합니다. 이 전략을 사용하면 그래프가 상대적으로 낮은 계산 오버헤드로 장기적인 일관성을 유지할 수 있습니다. 검색을 위해 다중 역할 LLM(대형 언어 모델)을 배포하여 쿼리를 원자 제약 조건으로 분해하고 논리적 부정을 처리하며, 견고한 추론을 보장하기 위해 소프트 필터링 전략을 사용합니다. 이러한 명시적인 해석 가능성은 복잡한 검색의 성공률과 신뢰성을 향상시켜 시스템이 더 넓은 범위의 인간 상호 작용 작업에 적응할 수 있도록 해줍니다. 새로 구성된 데이터 세트인 HM3DSem-SQR과 실제 환경에서 INHerit-SG를 평가합니다. 실험은 우리 시스템이 복잡한 쿼리에 대해 최첨단 성능을 달성하고 다운스트림 탐색 작업에 대한 확장성을 보여줍니다. 프로젝트 페이지: https://fangyuktung.github.io/INHeritSG.github.io/"
1030,http://arxiv.org/abs/2602.12244 ,Any House Any Task: Scalable Long-Horizon Planning for Abstract Human Tasks,"Zhihong Liu, Yang Li, Rengming Huang, Cewu Lu, Panpan Cai","개방형 언어 조건 작업 계획은 대규모 가정 환경에서 작동하는 로봇에 매우 중요합니다. 최근의 많은 연구에서는 프롬프트 또는 교육을 통해 LLM(대형 언어 모델)을 사용하여 이 문제를 해결하려고 시도하지만 주요 과제는 확장성입니다. 환경 크기, 계획 길이, 명령의 모호성 및 제약 조건의 복잡성이 증가함에 따라 성능이 급속히 저하되는 경우가 많습니다. 본 연구에서는 인간의 지시가 모호한 대규모 환경에서 장기적인 계획을 세우는데 최적화된 가사 작업 플래너인 AHAT(Any House Any Task)를 제안합니다. 핵심적으로 AHAT는 작업 지침과 텍스트 장면 그래프를 PDDL(계획 도메인 정의 언어)에 정의된 기본 하위 목표에 매핑하도록 훈련된 LLM을 활용합니다. 이러한 하위 목표는 이후 명시적인 상징적 추론을 통해 실행 가능하고 최적의 장기 계획을 생성하기 위해 해결됩니다. 복잡하고 모호한 의도를 분해하는 모델의 능력을 향상시키기 위해 중간 추론 추적의 외부 수정을 그룹 상대 정책 최적화(GRPO)에 통합하는 새로운 강화 학습 알고리즘인 TGPO를 도입합니다. 실험에 따르면 AHAT는 특히 간단한 지침이 필요하지만 복잡한 실행 계획이 필요한 인간형 가사 작업에서 최첨단 프롬프트, 계획 및 학습 방법에 비해 상당한 성능 향상을 달성하는 것으로 나타났습니다."
1029,http://arxiv.org/abs/2602.09165 ,All-in-One Conditioning for Text-to-Image Synthesis,"Hirunima Jayasekara, Chuong Huynh, Yixuan Ren, Christabel Acquaye, Abhinav Shrivastava","여러 개체, 속성 및 공간 관계가 포함된 복잡한 프롬프트를 정확하게 해석하고 시각적으로 표현하는 것은 텍스트-이미지 합성에서 중요한 과제입니다. 사실적인 출력 생성의 최근 발전에도 불구하고 현재 모델은 복잡한 텍스트 입력을 처리할 때 의미 충실도와 구조적 일관성을 유지하는 데 어려움을 겪는 경우가 많습니다. 우리는 기존 모델의 구성 능력을 향상시키는 것을 목표로 장면 그래프 구조의 프레임워크 내에서 텍스트-이미지 합성을 기반으로 하는 새로운 접근 방식을 제안합니다. 이전 접근 방식에서는 프롬프트에서 파생된 미리 정의된 레이아웃 맵을 사용하여 이 문제를 해결하려고 시도했지만 이러한 엄격한 제약으로 인해 구성의 유연성과 다양성이 제한되는 경우가 많습니다. 이와 대조적으로 추론 중에 부드러운 시각적 안내를 생성하는 제로샷 장면 그래프 기반 조건 조정 메커니즘을 도입합니다. 우리 방법의 핵심에는 경량 언어 모델을 통해 시각적 조건을 생성하고 추론 시간 최적화를 통해 확산 기반 생성을 안내하는 ASQL(속성-크기-수량-위치) 컨디셔너가 있습니다. 이를 통해 모델은 가볍고 일관되며 다양한 이미지 합성을 지원하면서 텍스트-이미지 정렬을 유지할 수 있습니다."
1028,http://arxiv.org/abs/2602.07541 ,Differentiate-and-Inject: Enhancing VLAs via Functional Differentiation Induced by In-Parameter Structural Reasoning,"Jingyi Hou, Leyu Zhou, Chenchen Jing, Jinghan Yang, Xinbo Yu, Wei He","로봇은 점점 더 다양한 작업을 수행할 것으로 예상되므로 낮은 수준의 작업뿐만 아니라 작업 전개 방식을 결정하는 높은 수준의 구조도 이해해야 합니다. 기존 VLA(비전-언어-행동) 모델은 이러한 형태의 작업 수준 추론에 어려움을 겪습니다. 이는 불안정하고 언어적 변화에 민감한 프롬프트 기반의 컨텍스트 내 분해 또는 대규모 시연이 필요하고 작업 수준 추론을 낮은 수준 제어와 얽히게 하는 엔드투엔드 장거리 훈련에 의존합니다. 우리는 매개변수 내 구조적 추론에 의해 유도된 기능적 차별화를 통해 VLA 모델을 향상시키기 위한 프레임워크인 매개변수 내 구조적 작업 추론(iSTAR)을 제시합니다. VLA를 모놀리식 정책으로 처리하는 대신 iSTAR는 작업 수준 의미 구조를 모델 매개변수에 직접 내장하여 외부 계획자나 직접 만든 프롬프트 입력 없이 차별화된 작업 수준 추론을 가능하게 합니다. 이 주입된 구조는 매개변수 공간에서 개체 관계, 하위 작업 의미 및 작업 수준 종속성을 캡처하는 암시적 동적 장면 그래프 지식의 형태를 취합니다. 다양한 조작 벤치마크에서 iSTAR는 상황 내 및 종단 간 VLA 기준보다 더 안정적인 작업 분해와 더 높은 성공률을 달성하여 기능 차별화를 위한 매개변수 공간 구조적 추론의 효율성을 입증하고 작업 변형 전반에 걸쳐 일반화를 개선했습니다."
1027,http://arxiv.org/abs/2602.06090 ,SVRepair: Structured Visual Reasoning for Automated Program Repair,"Xiaoxuan Tang, Jincheng Wang, Liwei Luo, Jingxuan Xu, Sheng Zhou, Dajun Chen, Wei Jiang, Yong Li","LLM(대형 언어 모델)은 최근 APR(자동 프로그램 복구)에 대한 강력한 잠재력을 보여 주었지만 대부분의 기존 접근 방식은 단봉으로 남아 있으며 스크린샷 및 제어 흐름 그래프와 같은 시각적 아티팩트에 포함된 풍부한 진단 신호를 활용하지 못합니다. 실제로 많은 버그 보고서는 중요한 정보(예: 레이아웃 손상 또는 누락된 위젯)를 시각적으로 전달하지만 이러한 조밀한 시각적 입력을 직접 사용하면 컨텍스트 손실 및 노이즈가 발생하여 MLLM이 시각적 관찰을 통해 정확한 결함 위치 파악 및 실행 가능한 패치를 기반으로 하기가 어렵습니다. 이러한 의미적 격차를 해소하기 위해 우리는 구조화된 시각적 표현을 갖춘 다중 모드 APR 프레임워크인 \textbf{SVRepair}를 제안합니다. SVRepair는 먼저 비전 언어 모델인 \textbf{Structured Visual Representation(SVR)}을 미세 조정하여 이기종 시각적 아티팩트를 GUI 요소와 그 구조적 관계(예: 계층 구조)를 캡처하는 \emph{semantic scene graph}로 균일하게 변환하고 다운스트림 복구를 위한 정규화된 코드 관련 컨텍스트를 제공합니다. 그래프를 기반으로 SVRepair는 코딩 에이전트를 구동하여 결함 위치를 파악하고 패치를 합성하며, 추가로 버그 중심 영역으로 입력을 점진적으로 좁혀 관련 없는 컨텍스트를 억제하고 환각을 줄이는 반복적인 시각적 아티팩트 분할 전략을 도입합니다. 여러 벤치마크에 대한 광범위한 실험에서 최첨단 성능이 입증되었습니다. SVRepair는 SWE-Bench M에서 \textbf{36.47\%} 정확도, MMCode에서 \textbf{38.02\%}, CodeVision에서 \textbf{95.12\%}를 달성하여 다중 모드 프로그램 복구에 대한 SVRepair의 효율성을 검증했습니다."
1026,http://arxiv.org/abs/2602.04635 ,Relational Scene Graphs for Object Grounding of Natural Language Commands,"Julia Kuhn, Francesco Verdoja, Tsvetomila Mihaylova, Ville Kyrki","로봇은 인간 환경에서 더 폭넓게 채택되어 자연스러운 인간-로봇 상호 작용의 필요성이 증가하고 있습니다. 그러나 자연어 명령을 이해하려면 로봇이 의도한 작업과 이를 실행 가능한 작업으로 분해하는 방법을 추론하고 관련 개체, 에이전트 및 위치를 포함한 환경에 대한 로봇의 지식을 기반으로 이러한 작업을 기반으로 해야 합니다. 이 과제는 자연어를 이해하는 LLM(대형 언어 모델) 기능과 환경의 의미론적 표현에서 추론된 작업을 기반으로 하는 3D 장면 그래프(3DSG)를 결합하여 해결할 수 있습니다. 그러나 인간이 환경을 설명하기 위해 이러한 관계에 의존하는 경우가 많음에도 불구하고 많은 3DSG에는 개체 간의 명시적인 공간 관계가 부족합니다. 이 논문에서는 개방형 또는 폐쇄형 어휘 공간 관계를 3DSG에 통합하면 자연어 명령을 해석하는 LLM의 능력을 향상시킬 수 있는지 여부를 조사합니다. 이를 해결하기 위해 우리는 개방형 어휘 언어 명령에서 대상 개체 접지를 위한 LLM 기반 파이프라인과 매핑 중에 캡처된 이미지에서 개방형 어휘 공간 에지를 3DSG에 추가하는 VLM(비전 언어 모델) 기반 파이프라인을 제안합니다. 마지막으로, 대상 객체 접지의 다운스트림 작업에 대한 성능을 평가하는 연구에서 두 개의 LLM이 평가됩니다. 우리의 연구는 명시적인 공간 관계가 지상 물체에 대한 LLM의 능력을 향상시킨다는 것을 보여줍니다. 더욱이, VLM을 사용한 개방형 어휘 관계 생성은 로봇이 캡처한 이미지에서 실현 가능한 것으로 입증되었지만 폐쇄형 어휘 관계에 비해 장점은 제한적인 것으로 나타났습니다."
1025,http://arxiv.org/abs/2602.04419 ,Integrated Exploration and Sequential Manipulation on Scene Graph with LLM-based Situated Replanning,"Heqing Yang, Ziyuan Jiao, Shu Wang, Yida Niu, Si Liu, Hangxin Liu",부분적으로 알려진 환경에서 로봇은 정보를 수집하기 위한 탐색과 효율적인 실행을 위한 작업 계획을 결합해야 합니다. 이러한 문제를 해결하기 위해 우리는 장면 그래프의 탐색 기반 순차 조작 계획 프레임워크인 EPoG를 제안합니다. EPoG는 그래프 기반 글로벌 플래너를 LLM(대형 언어 모델) 기반 상황 로컬 플래너와 통합하여 관측 및 LLM 예측을 사용하여 믿음 그래프를 지속적으로 업데이트하여 알려진 개체와 알려지지 않은 개체를 나타냅니다. 동작 시퀀스는 시간적 종속성과 이동 비용에 따라 정렬된 목표 그래프와 신념 그래프 간의 그래프 편집 작업을 계산하여 생성됩니다. 이 접근 방식은 탐색과 순차적 조작 계획을 완벽하게 결합합니다. 46개의 실제 가정 장면과 5개의 장거리 일일 물체 운송 작업에 대한 절제 연구에서 EPoG는 91.3%의 성공률을 달성하여 이동 거리를 평균 36.1% 줄였습니다. 또한 물리적 모바일 조작기는 알려지지 않은 동적 환경에서 복잡한 작업을 성공적으로 실행하여 실제 응용 프로그램에 대한 EPoG의 잠재력을 보여주었습니다.
1024,http://arxiv.org/abs/2602.04152 ,MA3DSG: Multi-Agent 3D Scene Graph Generation for Large-Scale Indoor Environments,"Yirum Kim, Jaewoo Kim, Ue-Hwan Kim","현재 3D 장면 그래프 생성(3DSGG) 접근 방식은 단일 에이전트 가정과 소규모 환경에 크게 의존하므로 실제 시나리오에 대한 확장성이 제한되어 있습니다. 이 작업에서는 다중 에이전트를 사용하여 이러한 확장성 문제를 해결하기 위해 설계된 최초의 프레임워크인 다중 에이전트 3D 장면 그래프 생성(MA3DSG) 모델을 소개합니다. 우리는 개별 에이전트의 부분 쿼리 그래프를 통합된 글로벌 장면 그래프로 효율적으로 병합하는 훈련이 필요 없는 그래프 정렬 알고리즘을 개발합니다. 광범위한 분석과 경험적 통찰력을 활용하는 당사의 접근 방식을 통해 기존 단일 에이전트 시스템은 학습 가능한 매개변수 없이도 공동으로 작동할 수 있습니다. 3DSGG 성능을 엄격하게 평가하기 위해 우리는 보다 일반적이고 확장 가능한 평가 프레임워크를 제공하는 다양한 에이전트 구성, 도메인 크기 및 환경 조건을 지원하는 벤치마크인 MA3DSG-Bench를 제안합니다. 이 작업은 확장 가능한 다중 에이전트 3DSGG 연구를 위한 견고한 기반을 마련합니다."
1023,http://arxiv.org/abs/2602.03781 ,A Scene Graph Backed Approach to Open Set Semantic Mapping,"Martin Günther, Felix Igelbrink, Oscar Lima, Lennart Niecksch, Marian Renz, Martin Atzmueller","Open Set Semantic Mapping 및 3DSSG(3D Semantic Scene Graph)는 로봇 인식에서 확립된 패러다임이지만 대규모 실제 환경에서 높은 수준의 추론을 지원하기 위해 효과적으로 배포하는 것은 여전히 ​​중요한 과제로 남아 있습니다. 대부분의 기존 접근 방식은 인식과 표현을 분리하여 장면 그래프를 사후 생성된 파생 레이어로 처리합니다. 이로 인해 일관성과 확장성이 모두 제한됩니다. 이와 대조적으로 우리는 3DSSG가 전체 매핑 프로세스에 대한 기본 지식 표현 역할을 하는 기본 백엔드 역할을 하는 매핑 아키텍처를 제안합니다.   우리의 접근 방식은 증분 장면 그래프 예측에 대한 이전 작업을 활용하여 환경을 탐색하면서 실시간으로 그래프 구조를 추론하고 업데이트합니다. 이를 통해 대규모 설정에서 확장된 작업 중에도 지도가 위상학적으로 일관되고 계산적으로 효율적으로 유지됩니다. 평면적 토폴로지와 계층적 토폴로지를 모두 지원하는 명시적이고 공간적으로 기반이 있는 표현을 유지함으로써 하위 기호 원시 센서 데이터와 상위 수준 기호 추론 간의 격차를 해소합니다. 결과적으로 이는 지식 그래프 및 온톨로지에서 대형 언어 모델(LLM)에 이르는 지식 기반 프레임워크가 직접 활용할 수 있는 안정적이고 검증 가능한 구조를 제공하여 에이전트가 향상된 해석 가능성, 신뢰성 및 인간 개념에 대한 정렬을 통해 작동할 수 있도록 합니다."
1022,http://arxiv.org/abs/2602.02974 ,SceneLinker: Compositional 3D Scene Generation via Semantic Scene Graph from RGB Sequences,"Seok-Young Kim, Dooyoung Kim, Woojin Cho, Hail Song, Suji Kang, Woontack Woo",RGB 시퀀스의 의미론적 장면 그래프를 통해 구성적인 3D 장면을 생성하는 새로운 프레임워크인 SceneLinker를 소개합니다. 각 사용자의 공간을 기반으로 혼합 현실(MR) 콘텐츠를 적응적으로 경험하려면 주변의 의미 단서를 콤팩트하게 포착하여 실제 레이아웃을 반영하는 3D 장면을 생성하는 것이 필수적입니다. 이전 작업에서는 객체 간의 맥락적 관계를 완전히 포착하는 데 어려움을 겪었거나 주로 다양한 모양을 합성하는 데 중점을 두어 객체 배열에 맞춰 정렬된 3D 장면을 생성하는 것이 어려웠습니다. 우리는 장면 그래프 예측을 위한 교차 검사 기능 주의를 통해 그래프 네트워크를 설계하고 3D 장면 생성을 위한 조인트 모양 및 레이아웃 블록으로 구성된 그래프-VAE(graph-variational autoencoder)를 구성함으로써 이러한 문제를 해결합니다. 3RScan/3DSSG 및 SG-FRONT 데이터 세트에 대한 실험은 복잡한 실내 환경과 까다로운 장면 그래프 제약 조건에서도 정량적 및 정성적 평가 모두에서 우리의 접근 방식이 최첨단 방법보다 성능이 우수하다는 것을 보여줍니다. 우리의 작업을 통해 사용자는 장면 그래프를 통해 물리적 환경에서 일관된 3D 공간을 생성하여 공간 MR 콘텐츠를 만들 수 있습니다. 프로젝트 페이지는 https://scenelinker2026.github.io입니다.
1021,http://arxiv.org/abs/2602.02456 ,Relationship-Aware Hierarchical 3D Scene Graph for Task Reasoning,"Albert Gassol Puigjaner, Angelos Zacharia, Kostas Alexis",3D 환경을 구조화된 방식으로 표현하고 이해하는 것은 자율 에이전트가 주변 환경을 탐색하고 추론하는 데 매우 중요합니다. 전통적인 SLAM(Simultaneous Localization and Mapping) 방법은 메트릭 재구성을 생성하고 메트릭-의미론적 매핑으로 확장될 수 있지만 더 높은 수준의 추상화 및 관계형 추론이 부족합니다. 이러한 격차를 해결하기 위해 3D 장면 그래프는 계층 구조와 개체 관계를 캡처하는 강력한 표현으로 등장했습니다. 이 작업에서 우리는 여러 추상화 수준에 걸쳐 개방형 어휘 기능을 통합하고 객체 관계 추론을 지원하는 향상된 계층적 3D 장면 그래프를 제안합니다. 우리의 접근 방식은 VLM(Vision Language Model)을 활용하여 의미론적 관계를 추론합니다. 특히 LLM(대형 언어 모델)과 VLM을 결합하여 장면 그래프의 의미 및 관계 정보를 해석하는 작업 추론 모듈을 도입하여 에이전트가 작업에 대해 추론하고 환경과 보다 지능적으로 상호 작용할 수 있도록 합니다. 우리는 여러 환경과 작업에서 4족 로봇에 배포하여 이를 추론하는 능력을 강조함으로써 방법을 검증합니다.
1020,http://arxiv.org/abs/2602.01930 ,LIEREx: Language-Image Embeddings for Robotic Exploration,"Felix Igelbrink, Lennart Niecksch, Marian Renz, Martin Günther, Martin Atzmueller","의미 지도를 통해 로봇은 알려진 환경 탐색, 특정 물체 찾기, 매핑되지 않은 영역 탐색과 같은 작업을 수행하기 위해 주변 환경을 추론할 수 있습니다. 전통적인 매핑 접근 방식은 정확한 기하학적 표현을 제공하지만 사전 설계된 기호 어휘에 의해 제한되는 경우가 많습니다. 고정된 개체 클래스에 의존하면 디자인 타임에 정의되지 않은 배포되지 않은 지식을 처리하는 것이 비현실적입니다. CLIP과 같은 Vision-Language Foundation 모델의 최근 발전으로 개체가 고정 레이블이 아닌 고차원 임베딩으로 인코딩되는 개방형 매핑이 가능해졌습니다. LIEREx에서는 이러한 VLFM을 확립된 3D 의미론적 장면 그래프와 통합하여 부분적으로 알려지지 않은 환경에서 자율 에이전트에 의한 목표 지향 탐색을 가능하게 합니다."
1019,http://arxiv.org/abs/2602.01693 ,GSR: Learning Structured Reasoning for Embodied Manipulation,"Kewei Hu, Michael Zhang, Wei Ying, Tianhao Liu, Guoqiang Hao, Zimeng Li, Wanchan Yu, Jiajian Jing, Fangwen Chen, Hanwen Kang","빠른 발전에도 불구하고 구체화된 에이전트는 공간적 일관성, 인과적 종속성 및 목표 제약을 유지해야 하는 장거리 조작으로 여전히 어려움을 겪고 있습니다. 기존 접근 방식의 주요 한계는 작업 추론이 고차원 잠재 표현에 암시적으로 포함되어 있어 작업 구조를 지각적 가변성과 분리하는 것이 어렵다는 것입니다. 우리는 의미론적으로 기초한 장면 그래프에 대한 전환으로 세계 상태 진화를 명시적으로 모델링하는 구조화된 추론 패러다임인 GSR(Grounded Scene-graph Reasoning)을 소개합니다. GSR은 인식을 행동에 직접 매핑하는 대신 객체 상태와 공간 관계에 대해 단계적으로 추론함으로써 물리적 기반 공간에서 행동 전제 조건, 결과 및 목표 만족에 대한 명시적인 추론을 가능하게 합니다. 이러한 추론 학습을 지원하기 위해 우리는 세계 이해, 행동 계획 및 목표 해석을 공동으로 감독하는 대규모 데이터 세트인 Manip-Cognition-1.6M을 구축합니다. RLBench, LIBERO, GSR 벤치마크 및 실제 로봇 작업에 대한 광범위한 평가를 통해 GSR이 메시지 기반 기준에 비해 제로 샷 일반화 및 장거리 작업 완료를 크게 향상시키는 것으로 나타났습니다. 이러한 결과는 확장 가능한 구체화된 추론을 위한 주요 귀납적 편향으로서 명시적인 세계 상태 표현을 강조합니다."
1018,http://arxiv.org/abs/2602.00708 ,USS-Nav: Unified Spatio-Semantic Scene Graph for Lightweight UAV Zero-Shot Object Navigation,"Weiqi Gai, Yuman Gao, Yuan Zhou, Yufan Xie, Zhiyang Liu, Yuze Wu, Xin Zhou, Fei Gao, Zhijun Meng",알 수 없는 환경에서의 제로샷 객체 탐색은 높은 수준의 의미론적 추론 요구 사항과 제한된 온보드 계산 리소스 간의 충돌로 인해 무인 항공기(UAV)에 심각한 문제를 제기합니다. 이 문제를 해결하기 위해 우리는 통합 공간 의미론적 장면 그래프를 점진적으로 구성하고 알 수 없는 환경에서 효율적인 LLM(Large Language Model)으로 강화된 Zero-Shot 객체 탐색을 가능하게 하는 경량 프레임워크인 USS-Nav를 제시합니다. 특히 그래프 클러스터링을 통해 의미 영역으로 동적으로 분할되는 전역 기하학적 토폴로지를 캡처하기 위해 다면체 확장을 활용하는 증분 공간 연결 그래프 생성 방법을 소개합니다. 동시에 개방형 어휘 개체 의미론이 인스턴스화되고 이 토폴로지에 고정되어 계층적 환경 표현을 형성합니다. 이러한 계층적 구조를 활용하여 우리는 장면 그래프의 의미론에 기초한 LLM을 통해 대략적인 탐색 전략을 제시하여 전역 대상 지역을 결정하고 로컬 계획자는 정보 획득을 기반으로 프론티어 적용 범위를 최적화합니다. 실험 결과는 우리 프레임워크가 리소스가 제한된 플랫폼에서 계산 효율성 및 실시간 업데이트 빈도(15Hz) 측면에서 최첨단 방법보다 성능이 우수하다는 것을 보여줍니다. 또한 절제 연구는 프레임워크의 효율성을 확인하여 SPL(Success Weighted by Path Length)의 상당한 개선을 보여줍니다. 소스 코드는 추가 연구를 촉진하기 위해 공개적으로 제공될 예정입니다.
1017,http://arxiv.org/abs/2602.00637 ,VIZOR: Viewpoint-Invariant Zero-Shot Scene Graph Generation for 3D Scene Reasoning,"Vivek Madhavaram, Vartika Sengar, Arkadipta De, Charu Sharma","장면 이해 및 추론은 3D 컴퓨터 비전의 근본적인 문제로, 모델이 객체, 해당 속성, 객체 간의 공간적 또는 비교 관계를 식별해야 합니다. 기존 접근 방식에서는 2D 이미지, 깊이 맵, 개체 레이블, 특정 참조 뷰의 주석이 달린 관계 등 여러 입력을 사용하여 장면 그래프를 생성함으로써 이를 가능하게 합니다. 그러나 이러한 방법은 종종 일반화에 어려움을 겪고 ""왼쪽/오른쪽""과 같은 부정확한 공간 관계를 생성하여 다양한 관점에서 일관되지 않습니다. 이러한 한계를 해결하기 위해 우리는 VIZOR(3D 장면 추론)을 위한 Viewpoint-Invariant Zero-shot 장면 그래프 생성을 제안합니다. VIZOR는 원시 3D 장면에서 직접 조밀하고 시점 불변의 3D 장면 그래프를 구성하는 교육이 필요 없는 엔드 투 엔드 프레임워크입니다. 생성된 장면 그래프는 각 객체의 정면 방향을 기준으로 공간 관계가 정의되어 참조 뷰에 관계없이 일관성을 유지하므로 명확합니다. 또한 주석이 달린 훈련 데이터 없이 장면 객체 간의 공간 및 근접 관계를 설명하는 개방형 어휘 관계를 추론합니다. 우리는 장면 그래프 생성 및 쿼리 기반 객체 접지와 같은 다운스트림 작업에서 VIZOR의 효율성을 평가하기 위해 광범위한 정량적 및 정성적 평가를 수행합니다. VIZOR는 최첨단 방법보다 성능이 뛰어나 장면 그래프 생성이 확실히 향상되었으며 Replica 및 Nr3D 데이터 세트에서 제로샷 접지 정확도가 각각 22% 및 4.81% 향상되었습니다."
1016,http://arxiv.org/abs/2602.00414 ,Toward Autonomous Laboratory Safety Monitoring with Vision Language Models: Learning to See Hazards Through Scene Structure,"Trishna Chakraborty, Udita Ghosh, Aldair Ernesto Gongora, Ruben Glatt, Yue Dong, Jiachen Li, Amit K. Roy-Chowdhury, Chengyu Song","실험실은 사소한 불안전한 행동으로 인해 심각한 부상을 입는 경향이 있지만, 의무적인 실험실 전 안전 교육을 넘어서는 지속적인 안전 모니터링은 사람의 가용성에 따라 제한됩니다. 비전 언어 모델(VLM)은 자율적인 실험실 안전 모니터링을 약속하지만 대부분의 안전 사고가 주로 구조화되지 않은 텍스트로 문서화되므로 시각적 평가 데이터가 부족하여 현실적인 설정에서의 효과가 불분명합니다. 이러한 격차를 해결하기 위해 먼저 대규모 언어 모델을 장면 그래프 설계자로 사용하고 이미지 생성 모델을 렌더러로 사용하여 텍스트 실험실 시나리오를 정렬된 삼중(이미지, 장면 그래프, 지상 진실)으로 변환하는 구조화된 데이터 생성 파이프라인을 도입합니다. 362개의 고유한 시나리오와 7개의 오픈 소스 및 비공개 소스 모델에 걸쳐 1,207개 샘플의 합성 데이터 세트에 대한 우리의 실험에서는 VLM이 주어진 텍스트 장면 그래프에서 효과적으로 수행되지만 시각적 전용 설정에서는 성능이 크게 저하되어 구조화된 개체 관계를 픽셀에서 직접 추출하는 데 어려움이 있음을 보여줍니다. 이를 극복하기 위해 우리는 시각적 입력을 VLM 추론과 더 잘 일치하는 구조화된 장면 그래프로 변환하여 시각적 전용 설정에서 위험 감지 성능을 향상시킴으로써 VLM의 지각 격차를 메우는 훈련 후 컨텍스트 엔지니어링 접근 방식인 장면 그래프 기반 정렬을 제안합니다."
1015,http://arxiv.org/abs/2601.21498 ,SimGraph: A Unified Framework for Scene Graph-Based Image Generation and Editing,"Thanh-Nhan Vo, Trong-Thuan Nguyen, Tam V. Nguyen, Minh-Triet Tran","최근 GenAI(Generative Artificial Intelligence)의 발전으로 이미지 생성 및 편집 기능이 크게 향상되었습니다. 그러나 현재 접근 방식에서는 이러한 작업을 별도로 처리하는 경우가 많아 생성된 콘텐츠와 편집 간의 공간적 일관성과 의미론적 일관성을 유지하는 데 있어 비효율성과 어려움이 발생합니다. 더욱이, 주요 장애물은 객체 관계 및 공간 배치에 대한 구조적 제어가 부족하다는 것입니다. 개체와 개체의 상호 관계를 구조화된 형식으로 나타내는 장면 그래프 기반 방법은 이미지 생성과 편집 모두에서 구성과 상호 작용을 보다 효과적으로 제어할 수 있는 솔루션을 제공합니다. 이 문제를 해결하기 위해 우리는 장면 그래프 기반 이미지 생성 및 편집을 통합하여 개체 상호 작용, 레이아웃 및 공간 일관성을 정밀하게 제어할 수 있는 통합 프레임워크인 SimGraph를 소개합니다. 특히, 우리의 프레임워크는 단일 장면 그래프 기반 모델 내에서 토큰 기반 생성 및 확산 기반 편집을 통합하여 고품질의 일관된 결과를 보장합니다. 광범위한 실험을 통해 우리는 우리의 접근 방식이 기존의 최첨단 방법보다 성능이 우수하다는 것을 경험적으로 보여줍니다."
1014,http://arxiv.org/abs/2601.20355 ,CURVE: Learning Causality-Inspired Invariant Representations for Robust Scene Understanding via Uncertainty-Guided Regularization,"Yue Liang, Jiatong Du, Ziyi Yang, Yanjun Huang, Hong Chen","장면 그래프는 장면 이해를 위한 구조화된 추상화를 제공하지만 가짜 상관 관계에 과적합되는 경우가 많아 분포 외 일반화를 심각하게 방해합니다. 이러한 한계를 해결하기 위해 우리는 변동성 불확실성 모델링과 불확실성 기반 구조 정규화를 통합하여 높은 분산성, 환경별 관계를 억제하는 인과성 기반 프레임워크인 CURVE를 제안합니다. 구체적으로, 우리는 프로토타입 조건화된 편향성 제거를 적용하여 환경 의존적 변형으로부터 불변 상호 작용 역학을 분리하여 희박하고 도메인이 안정적인 토폴로지를 촉진합니다. 경험적으로 우리는 제로샷 전송 및 저데이터 시뮬레이션-실제 적응에서 CURVE를 평가하여 도메인 안정 희소 토폴로지를 학습하는 능력을 검증하고 분포 변화 시 위험 예측을 지원하기 위한 신뢰할 수 있는 불확실성 추정치를 제공합니다."
1013,http://arxiv.org/abs/2601.19832 ,Information-Theoretic Detection of Bimanual Interactions for Dual-Arm Robot Plan Generation,"Elena Merlo, Marta Lagomarsino, Arash Ajoudani","시연을 통한 프로그래밍은 사람의 시연을 통해 비전문가를 위한 로봇 프로그래밍 프로세스를 단순화하는 전략입니다. 그러나 양손 작업에 대한 채택은 손 조정의 복잡성으로 인해 아직 탐구되지 않은 문제이며, 이로 인해 데이터 기록도 방해됩니다. 본 논문에서는 양팔 로봇 시스템의 실행 계획을 생성하기 위해 양손 작업 시연의 단일 RGB 비디오를 처리하는 새로운 원샷 방법을 제시합니다. 손 조정 정책을 탐지하기 위해 Shannon의 정보 이론을 적용하여 장면 요소 간의 정보 흐름을 분석하고 장면 그래프 속성을 활용합니다. 생성된 계획은 원하는 무기 조정에 따라 다양한 구조를 가정하는 모듈식 동작 트리입니다. 우리는 수집하여 오픈 소스로 만들고 공개적으로 사용 가능한 외부 데이터 세트의 데이터를 활용하는 여러 주제의 비디오 데모를 통해 이 프레임워크의 효율성을 검증했습니다. 기존 방법과 비교하면 두 가지 시스템을 조정하기 위한 중앙 집중식 실행 계획을 생성하는 데 상당한 개선이 나타났습니다."
1012,http://arxiv.org/abs/2601.19433 ,RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming,"Jisheng Chu, Wenrui Li, Rui Zhao, Wangmeng Zuo, Shifeng Chen, Xiaopeng Fan","텍스트에서 몰입형 3D 장면을 생성하는 것은 컴퓨터 비전의 핵심 작업이며 가상 현실 및 게임 개발 애플리케이션에 매우 중요합니다. 2D 확산 사전 활용 가능성에도 불구하고 기존 방법은 공간적 실명 문제를 겪고 있으며 주요 개체 간의 내부 관계를 활용하지 못하는 미리 정의된 궤적에 의존합니다. 결과적으로 이러한 접근 방식은 의미 체계 레이아웃을 이해할 수 없으므로 가려진 콘텐츠를 추론하기 위해 장면을 적응적으로 탐색할 수 없습니다. 더욱이 현재 인페인팅 모델은 2D 이미지 공간에서 작동하므로 카메라 움직임으로 인해 발생하는 구멍을 그럴듯하게 채우는 데 어려움을 겪습니다. 이러한 한계를 해결하기 위해 의미론적 안내와 공간 생성 간의 격차를 해소하는 새로운 프레임워크인 RoamScene3D를 제안합니다. 우리의 방법은 객체 간의 의미 관계를 추론하고 일관되고 사실적인 장면을 생성합니다. 특히 우리는 VLM(비전 언어 모델)을 사용하여 객체 관계를 인코딩하는 장면 그래프를 구성하고, 카메라가 두드러진 객체 경계를 인식하고 적응형 로밍 궤적을 계획하도록 안내합니다. 또한 정적 2D 사전의 한계를 완화하기 위해 실제 카메라 궤적을 통합하는 합성 파노라마 데이터 세트에서 미세 조정되어 카메라 모션에 적응할 수 있는 모션 주입 인페인팅 모델을 도입합니다. 광범위한 실험을 통해 의미론적 추론과 기하학적 제약을 통해 우리의 방법이 일관되고 사실적인 장면을 생성하는 데 있어 최첨단 접근 방식보다 훨씬 뛰어난 것으로 나타났습니다. 우리 코드는 https://github.com/JS-CHU/RoamScene3D에서 확인할 수 있습니다."
1011,http://arxiv.org/abs/2601.18765 ,Goal-oriented Communication for Fast and Robust Robotic Fault Detection and Recovery,"Shutong Chen, Adnan Aijaz, Yansha Deng","자율 로봇 시스템은 스마트 공장에 널리 배포되며 짧은 대기 시간과 강력한 오류 감지 및 복구(FDR)가 필요한 역동적이고 불확실하며 인간이 관여하는 환경에서 작동합니다. 그러나 기존 FDR 프레임워크는 통신 및 계산의 상당한 지연, 로봇 동작/궤적 생성의 불안정성 등 다양한 제한 사항을 나타냅니다. 이는 주로 통신-계산-제어(3C) 루프가 다운스트림 FDR 목표를 고려하지 않고 설계되었기 때문입니다. 이 문제를 해결하기 위해 우리는 로봇 작업(예: 공작물 정렬) 성공률을 최대화하면서 FDR 시간을 최소화하는 것을 목표로 빠르고 강력한 로봇 FDR에 맞춰진 3C 루프를 공동 설계하는 새로운 목표 지향 통신(GoC) 프레임워크를 제안합니다. 결함 감지를 위해 GoC 프레임워크는 설계된 표현 추출기를 통해 3D 장면 그래프(3D-SG)를 의미론적 표현으로 혁신적으로 정의 및 추출하고, 3D-SG의 공간적 관계 변화를 모니터링하여 결함을 감지합니다. 오류 복구를 위해 LoRA(Low-Rank Adaptation)를 통해 SLM(Small Language Model)을 미세 조정하고 지식 증류를 통해 추론 및 일반화 기능을 강화하여 로봇의 복구 동작을 생성합니다. 또한 디지털 트윈 재구성을 위해 작업 관련 객체 윤곽만 사용하여 세밀한 로봇 제어가 필요할 때 SLM에서 생성된 복구 동작을 개선하기 위해 경량 목표 지향 디지털 트윈 재구성 모듈을 설계합니다. 광범위한 시뮬레이션을 통해 우리의 GoC 프레임워크는 오류 감지를 위한 비전 언어 모델과 오류 복구를 위한 대규모 언어 모델에 의존하는 최첨단 프레임워크에 비해 FDR 시간을 최대 82.6%까지 줄이고 작업 성공률을 최대 76%까지 향상시키는 것으로 나타났습니다."
1010,http://arxiv.org/abs/2601.18157 ,Agentic Very Long Video Understanding,"Aniket Rege, Arka Sadhu, Yuliang Li, Kejie Li, Ramya Korlakai Vinayak, Yuning Chai, Yong Jae Lee, Hyo Jin Kim","스마트 안경과 같이 하루 종일 착용할 수 있는 장치를 통해 상시 작동되는 개인 AI 비서의 출현은 짧고 고립된 이벤트를 넘어 자기 중심적인 비디오의 지속적이고 종단적인 흐름을 포괄하는 새로운 수준의 상황 이해를 요구합니다. 이러한 비전을 달성하려면 시스템이 며칠 또는 몇 주에 걸쳐 시각적 및 오디오 정보를 해석하고 기억해야 하는 장거리 비디오 이해의 발전이 필요합니다. 대규모 언어 모델 및 검색 증강 생성을 포함한 기존 방법은 제한된 컨텍스트 창으로 인해 제한되며 매우 긴 비디오 스트림에 대해 구성적 다중 홉 추론을 수행하는 기능이 부족합니다. 이 작업에서 우리는 시간에 따른 사람, 장소, 개체 및 이들의 관계를 나타내는 엔터티 장면 그래프를 중심으로 한 향상된 에이전트 프레임워크인 EGAgent를 통해 이러한 문제를 해결합니다. 우리 시스템은 계획 담당자에게 이러한 그래프에 대한 구조화된 검색 및 추론을 위한 도구는 물론 하이브리드 시각 및 오디오 검색 기능을 제공하여 상세하고 교차 모달이며 시간적으로 일관된 추론을 가능하게 합니다. EgoLifeQA 및 Video-MME(Long) 데이터 세트에 대한 실험에서는 우리의 방법이 복잡한 종단적 비디오 이해 작업에 대해 EgoLifeQA(57.5%)에서 최첨단 성능을 달성하고 Video-MME(Long)(74.1%)에서 경쟁력 있는 성능을 달성한다는 것을 보여줍니다."
1009,http://arxiv.org/abs/2601.16093 ,SAMTok: Representing Any Mask with Two Words,"Yikang Zhou, Tao Zhang, Dengxian Gong, Yuanzheng Wu, Ye Tian, Haochen Wang, Haobo Yuan, Jiacong Wang, Lu Qi, Hao Fei, Anran Wang, Zhuochen Wang, Yujing Wang, Cheng Chen, Shunping Ji, Xiangtai Li","대화형 지능형 시스템을 구축하려면 픽셀 단위의 기능이 필수적입니다. 그러나 MLLM(픽셀별 다중 모달 LLM)은 복잡한 지역 수준 인코더, 특수 분할 디코더 및 호환되지 않는 훈련 목표로 인해 확장하기가 여전히 어렵습니다. 이러한 문제를 해결하기 위해 우리는 모든 영역 마스크를 두 개의 특수 토큰으로 변환하고 이러한 토큰을 사용하여 높은 충실도로 마스크를 재구성하는 개별 마스크 토크나이저인 SAMTok을 제시합니다. SAMTok은 마스크를 새로운 언어 토큰으로 처리함으로써 기본 MLLM(예: QwenVL 시리즈)이 아키텍처 수정 및 특수 손실 설계 없이 표준 다음 토큰 예측 및 간단한 강화 학습을 통해 픽셀별 기능을 학습할 수 있도록 합니다. SAMTok은 SAM2를 기반으로 하며 마스크 인코더와 잔여 벡터 양자화기를 사용하여 209M의 다양한 마스크에 대해 교육을 받아 이산적이고 컴팩트하며 정보가 풍부한 토큰을 생성합니다. 500만 개의 SAMTok 형식 마스크 이해 및 생성 데이터 샘플을 통해 QwenVL-SAMTok은 영역 캡션, 영역 VQA, 기반 대화, 참조 분할, 장면 그래프 구문 분석 및 다중 라운드 대화형 분할에 대한 최첨단 또는 유사한 결과를 얻습니다. 또한 마스크 생성을 위한 효율적인 강화 학습을 가능하게 하고 GRES 및 GCG 벤치마크에서 상당한 개선을 제공하는 텍스트 답변 일치 보상을 소개합니다. 우리의 결과는 MLLM에 강력한 픽셀 단위 기능을 장착하기 위한 확장 가능하고 간단한 패러다임을 보여줍니다. 우리의 코드와 모델을 사용할 수 있습니다."
1008,http://arxiv.org/abs/2601.15025 ,ExPrIS: Knowledge-Level Expectations as Priors for Object Interpretation from Sensor Data,"Marian Renz, Martin Günther, Felix Igelbrink, Oscar Lima, Martin Atzmueller","딥 러닝이 로봇 객체 인식을 크게 발전시켰지만, 순수한 데이터 기반 접근 방식은 의미론적 일관성이 부족하고 환경에 대한 기존의 가치 있는 지식을 활용하지 못하는 경우가 많습니다. 이 보고서는 지식 수준 기대치가 센서 데이터의 개체 해석을 향상시키는 데 어떻게 도움이 될 수 있는지 조사하여 이러한 과제를 해결하는 ExPrIS 프로젝트를 제시합니다. 우리의 접근 방식은 3DSSG(3D Semantic Scene Graph)의 점진적 구성을 기반으로 합니다. 우리는 과거 관찰로부터 얻은 맥락적 사전 지식과 ConceptNet과 같은 외부 그래프로부터 얻은 의미론적 지식이라는 두 가지 소스로부터의 기대치를 통합합니다. 이는 이기종 그래프 신경망(GNN)에 내장되어 기대 편향 추론 프로세스를 생성합니다. 이 방법은 정적 프레임별 분석을 넘어 시간이 지남에 따라 장면 이해의 견고성과 일관성을 향상시킵니다. 보고서는 이 아키텍처와 평가를 자세히 설명하고 모바일 로봇 플랫폼에 대한 통합 계획을 간략하게 설명합니다."
1007,http://arxiv.org/abs/2601.11460 ,Learning Semantic-Geometric Task Graph-Representations from Human Demonstrations,"Franziska Herbert, Vignesh Prasad, Han Liu, Dorothea Koert, Georgia Chalvatzaki","인간의 시연에서 구조화된 작업 표현을 학습하는 것은 장거리 조작 동작을 이해하는 데 필수적입니다. 특히 작업 순서, 개체 참여 및 상호 작용 기하학이 크게 다를 수 있는 양손 설정에서 더욱 그렇습니다. 주요 과제는 작업의 개별 의미 구조와 작업 진행에 대한 추론을 지원하는 형태로 객체 중심 기하학적 관계의 시간적 진화를 공동으로 포착하는 데 있습니다. 이 연구에서는 객체 정체성, 객체 간 관계 및 인간 시연의 시간적 기하학적 진화를 인코딩하는 의미 기하학적 작업 그래프 표현을 소개합니다. 이 공식을 바탕으로 MPNN(Message Passing Neural Network) 인코더와 Transformer 기반 디코더를 결합하여 작업 진행에 대한 동작 조건 추론에서 장면 표현 학습을 분리하는 학습 프레임워크를 제안합니다. 인코더는 시간적 장면 그래프에서만 작동하여 구조화된 표현을 학습하는 반면, 디코더는 동작 컨텍스트를 조건으로 확장된 시간 범위에 걸쳐 미래 동작 시퀀스, 관련 객체 및 객체 모션을 예측합니다. 인간 데모 데이터세트에 대한 광범위한 평가를 통해 우리는 의미론적 기하학적 작업 그래프 표현이 단순한 시퀀스 기반 모델이 작업 진행 상황을 포착하는 데 어려움을 겪는 높은 작업 및 개체 가변성이 있는 작업에 특히 유용하다는 것을 보여줍니다. 마지막으로, 작업 그래프 표현이 실제 양손 로봇으로 전송되어 온라인 작업 선택에 사용될 수 있음을 보여줌으로써 조작 시스템의 다운스트림 의사 결정을 위한 재사용 가능한 작업 추상화로서의 잠재력을 강조합니다."
1006,http://arxiv.org/abs/2601.10168 ,RAG-3DSG: Enhancing 3D Scene Graphs with Re-Shot Guided Retrieval-Augmented Generation,"Yue Chang, Rufeng Chen, Zhaofan Zhang, Yi Chen, Sihong Xie","개방형 어휘 3D 장면 그래프(3DSG) 생성은 구조화된 의미론적 표현을 활용하여 조작 및 탐색과 같은 로봇 공학의 다양한 다운스트림 작업을 향상시킬 수 있습니다. 3DSG는 객체가 노드로 표시되고 관계가 가장자리로 표시되는 장면의 여러 이미지로 구성됩니다. 그러나 개방형 어휘 3DSG 생성을 위한 기존 작업은 주로 제한된 시점, 폐색 및 중복 표면 밀도로 인해 객체 수준 인식 정확도와 속도가 낮다는 문제를 겪고 있습니다. 이러한 문제를 해결하기 위해 우리는 재촬영 유도 불확실성 추정을 통해 집계 노이즈를 완화하고 신뢰할 수 있는 낮은 불확실성 개체를 통해 개체 수준 RAG(검색 증강 생성)를 지원하는 RAG-3DSG를 제안합니다. 또한 적응형 세분성으로 이미지 간 개체 집계를 가속화하기 위한 동적 다운샘플링 매핑 전략을 제안합니다. Replica 데이터 세트에 대한 실험에서는 RAG-3DSG가 바닐라 버전에 비해 매핑 시간을 2/3로 줄이면서 3DSG 생성에서 노드 캡션 정확도를 크게 향상시키는 것으로 나타났습니다."
1005,http://arxiv.org/abs/2601.11644 ,Predicting When to Trust Vision-Language Models for Spatial Reasoning,"Muhammad Imran, Yugyung Lee","VLM(Vision-Language Model)은 다중 모달 작업 전반에 걸쳐 인상적인 기능을 보여 주지만 체계적인 공간 추론 실패를 나타내어 기본 방향 관계에 대해 49%(CLIP) ~ 54%(BLIP-2) 정확도만을 달성합니다. 로봇 공학 및 자율 시스템의 안전한 배포를 위해서는 모든 출력을 수용하기보다는 VLM 공간 예측을 신뢰할 시기를 예측해야 합니다. 우리는 객체 감지를 사용한 독립적인 기하학적 검증을 통해 VLM 예측을 검증하는 비전 기반 신뢰도 추정 프레임워크를 제안합니다. 자체 평가에 의존하는 텍스트 기반 접근 방식과 달리, 우리의 방법은 그래디언트 부스팅을 통해 VLM 주장과 좌표 간의 기하학적 정렬, 중복으로 인한 공간적 모호성, 감지 품질 및 VLM 내부 불확실성이라는 네 가지 신호를 융합합니다. 생성 및 분류 아키텍처 전반에 걸쳐 일반화하여 BLIP-2에서 0.674 AUROC(텍스트 기반 기준선에 비해 34.0% 개선), CLIP에서 0.583 AUROC(16.1% 개선)를 달성했습니다. 우리 프레임워크는 선택적 예측을 가능하게 합니다. 60% 목표 정확도에서 BLIP-2의 기준선 27.6%(2.2배 개선) 대비 61.9% 적용 범위를 달성합니다. 특징 분석에 따르면 비전 기반 신호는 모델 중요도의 87.4%, VLM 신뢰도의 12.7%를 기여하여 외부 기하학적 검증이 자체 평가보다 성능이 우수하다는 것을 입증합니다. 신뢰도 기반 가지치기가 68.2%의 가장자리를 유지하면서 정밀도를 52.1%에서 78.3%로 향상시키는 신뢰할 수 있는 장면 그래프 구성을 보여줍니다."
1004,http://arxiv.org/abs/2601.11632 ,KG-ViP: Bridging Knowledge Grounding and Visual Perception in Multi-modal LLMs for Visual Question Answering,"Zhiyang Li, Ao Ke, Yukun Cao, Xike Xie",VQA(시각적 질문 응답)를 위한 MLLM(다중 모드 대형 언어 모델)은 지식 환각과 불충분한 세밀한 시각적 인식이라는 이중 제한으로 인해 어려움을 겪는 경우가 많습니다. 결정적으로 우리는 상식 그래프와 장면 그래프가 풍부한 외부 지식을 제공하고 세밀한 시각적 세부 정보를 캡처함으로써 이러한 각각의 결함에 대한 정확한 보완 솔루션을 제공한다는 것을 확인했습니다. 그러나 이전 연구에서는 일반적으로 이들을 분리하여 다루며 시너지 잠재력을 간과했습니다. 이러한 격차를 해소하기 위해 우리는 장면 그래프와 상식 그래프를 융합하여 MLLM을 강화하는 통합 프레임워크인 KG-ViP를 제안합니다. KG-ViP 프레임워크의 핵심은 쿼리를 의미론적 브리지로 활용하여 두 그래프를 점진적으로 통합하고 신뢰할 수 있는 다중 모드 추론을 용이하게 하는 통일된 구조적 컨텍스트를 합성하는 새로운 검색 및 융합 파이프라인입니다. FVQA 2.0+ 및 MVQA 벤치마크에 대한 광범위한 실험을 통해 KG-ViP가 기존 VQA 방법보다 훨씬 뛰어난 성능을 보여줍니다.
1003,http://arxiv.org/abs/2601.08728 ,Salience-SGG: Enhancing Unbiased Scene Graph Generation with Iterative Salience Estimation,"Runfeng Qu, Ole Hall, Pia K Bideau, Julie Ouerfelli-Ethier, Martin Rolfs, Klaus Obermayer, Olaf Hellwich","SGG(장면 그래프 생성)는 소수의 조건자 클래스가 지배적인 반면 다른 많은 클래스는 과소 대표되는 긴 꼬리 분포로 인해 드문 관계에서 성능이 저하되는 편향된 모델이 발생합니다. Unbiased-SGG 방법은 편향성 제거 전략을 구현하여 이 문제를 해결하지만 종종 공간적 이해를 희생하여 의미론적 사전에 지나치게 의존하게 됩니다. 우리는 두드러진 공간 구조를 가진 삼중항을 강조하는 ISD(Iterative Salience Decoder)를 특징으로 하는 새로운 프레임워크인 Salience-SGG를 소개합니다. 이를 지원하기 위해 우리는 ISD를 안내하는 의미론적 돌출 레이블을 제안합니다. Visual Genome, Open Images V6 및 GQA-200에 대한 평가에서는 Salience-SGG가 최첨단 성능을 달성하고 쌍별 현지화 평균 정밀도에서 입증된 것처럼 공간 이해에서 기존 Unbiased-SGG 방법을 향상시키는 것으로 나타났습니다."
1002,http://arxiv.org/abs/2601.07219 ,VENUS: Visual Editing with Noise Inversion Using Scene Graphs,"Thanh-Nhan Vo, Trong-Thuan Nguyen, Tam V. Nguyen, Minh-Triet Tran","최첨단 텍스트 기반 이미지 편집 모델은 배경 보존과 의미적 일관성의 균형을 맞추는 데 어려움을 겪는 경우가 많으며, 이로 인해 완전히 새로운 이미지가 합성되거나 의도한 편집을 실현하지 못하는 출력이 나오는 경우가 많습니다. 대조적으로, 장면 그래프 기반 이미지 편집은 의미적 개체와 그 관계의 구조화된 표현을 제공하여 향상된 제어 가능성을 제공함으로써 이러한 제한을 해결합니다. 그러나 기존 장면 그래프 편집 방법은 일반적으로 모델 미세 조정에 의존하므로 계산 비용이 많이 들고 확장성이 제한됩니다. 이를 위해 장면 그래프 기반 이미지 편집을 위한 교육이 필요 없는 프레임워크인 VENUS(Visual Editing with Noise inversion Using Scene Graphs)를 소개합니다. 특히 VENUS는 편집 대상 개체를 배경 컨텍스트에서 분리하는 동시에 편집되지 않은 영역의 충실도를 유지하기 위해 노이즈 반전을 활용하는 분할 프롬프트 조건화 전략을 사용합니다. 또한 제안된 접근 방식은 추가 교육 없이 다중 모드 대형 언어 모델에서 추출된 장면 그래프를 확산 백본과 통합합니다. 경험적으로 VENUS는 최첨단 장면 그래프 편집 모델(SGEdit)에 비해 PSNR을 22.45에서 24.80으로, SSIM을 0.79에서 0.84로, LPIPS를 0.100에서 0.070으로 줄임으로써 PIE-Bench의 배경 보존과 의미 정렬을 모두 크게 개선했습니다. 또한 VENUS는 CLIP 유사성(24.97 대 24.19)으로 측정된 의미적 일관성을 향상시킵니다. EditVal에서 VENUS는 0.87 DINO 점수로 가장 높은 충실도를 달성했으며, 결정적으로 이미지당 실행 시간을 6~10분에서 20~30초로 줄였습니다. 장면 그래프 기반 편집 외에도 VENUS는 LEDIT++ 및 P2P+DirInv와 같은 강력한 텍스트 기반 편집 기준을 능가하여 두 패러다임 모두에서 일관된 개선을 보여줍니다."
1001,http://arxiv.org/abs/2601.06806 ,SpatialNav: Leveraging Spatial Scene Graphs for Zero-Shot Vision-and-Language Navigation,"Jiwen Zhang, Zejun Li, Siyuan Wang, Xiangyu Shi, Zhongyu Wei, Qi Wu","학습 기반 비전 및 언어 탐색(VLN) 에이전트는 대규모 교육 데이터에서 암시적으로 공간 지식을 학습할 수 있지만 제로 샷 VLN 에이전트에는 이 프로세스가 부족하여 주로 탐색을 위한 로컬 관찰에 의존하므로 비효율적인 탐색과 상당한 성능 격차가 발생합니다. 문제를 해결하기 위해 에이전트가 작업 실행 전에 환경을 완전히 탐색할 수 있도록 허용하는 제로샷 VLN 설정을 고려합니다. 그런 다음 탐색된 환경에서 전역 공간 구조와 의미를 명시적으로 포착하기 위해 공간 장면 그래프(SSG)를 구성합니다. SSG를 기반으로 효율적인 탐색을 위해 에이전트 중심 공간 맵, 나침반 정렬 시각적 표현 및 원격 개체 위치 파악 전략을 통합하는 제로샷 VLN 에이전트인 SpatialNav를 소개합니다. 개별 환경과 연속 환경 모두에서 포괄적인 실험을 통해 SpatialNav가 기존 제로샷 에이전트보다 훨씬 뛰어난 성능을 발휘하고 최첨단 학습 기반 방법과의 격차를 확실히 줄이는 것으로 나타났습니다. 이러한 결과는 일반화 가능한 탐색을 위한 전역 공간 표현의 중요성을 강조합니다."
1000,http://arxiv.org/abs/2601.06415 ,Semantic Enrichment of CAD-Based Industrial Environments via Scene Graphs for Simulation and Reasoning,"Nathan Pascal Walus, Ranulfo Bezerra, Shotaro Kojima, Tsige Tadesse Alemayoh, Satoshi Tadokoro, Kazunori Ohno","디스플레이, 대화형 밸브 등 산업 환경에서 기능적 요소를 활용하면 로봇 교육에 효과적인 가능성을 제공할 수 있습니다. 높은 수준의 장면 이해가 필요한 로봇이나 애플리케이션에 대한 시뮬레이션을 준비할 때 시뮬레이션 환경도 마찬가지로 상세해야 합니다. 이러한 환경을 위한 CAD 파일은 형상과 시각적 요소에 대한 정확한 설명을 제공하지만 일반적으로 의미, 관계 및 기능 정보가 부족하여 시뮬레이션 및 교육 가능성이 제한됩니다. 3D 장면 그래프는 LVLM(Large Vision-Language Model)을 통해 환경을 풍부하게 하여 의미론적, 공간적, 기능적 정보를 구성할 수 있습니다. 본 논문에서는 CAD 환경에서 상세한 3D 장면 그래프를 생성하는 오프라인 접근 방식을 제시합니다. 이는 기능적 요소와 실행 가능한 요소의 관계를 포함하는 기초 역할을 하며, 이는 동적 시뮬레이션과 추론에 사용될 수 있습니다. 이 연구의 주요 결과에는 생성된 의미 라벨의 정량적 결과뿐만 아니라 장면 그래프의 정성적 결과, 특히 파이프 구조와 식별된 기능적 관계에 대한 사후 이해가 포함됩니다. 모든 코드, 결과 및 환경은 https://cad-scenegraph.github.io에서 확인할 수 있습니다."
999,http://arxiv.org/abs/2601.05600 ,SceneAlign: Aligning Multimodal Reasoning to Scene Graphs in Complex Visual Scenes,"Chuhan Wang, Xintong Li, Jennifer Yuntong Zhang, Junda Wu, Chengkai Huang, Lina Yao, Julian McAuley, Jingbo Shang","다중 모드 대형 언어 모델은 복잡한 엔터티와 관계가 각 단계에서 정확한 시각적 기반을 요구하는 복잡한 시각적 장면에서 충실한 추론에 어려움을 겪는 경우가 많습니다. 이러한 추론의 불성실함은 환각에 빠진 실체, 잘못된 관계, 건너뛴 단계, 지나치게 구체화된 추론으로 자주 나타납니다. 일반적으로 텍스트 변동이나 답변 조건 근거에 의존하는 기존 선호도 기반 접근 방식은 모델이 시각적 기반을 우회하기 위해 언어 사전을 활용할 수 있도록 허용하므로 이러한 문제를 해결하지 못합니다. 이 문제를 해결하기 위해 우리는 장면 그래프를 구조화된 시각적 정보로 활용하여 제어 가능한 구조적 개입을 수행하는 프레임워크인 SceneAlign을 제안합니다. 추론에 중요한 노드를 식별하고 일반적인 접지 오류를 모방하는 4가지 목표 전략을 통해 이를 교란함으로써 SceneAlign은 언어적으로는 타당하지만 부정확한 시각적 사실에 기초한 확실한 부정적 근거를 구성합니다. 이러한 대조 쌍은 Direct Preference Optimization에서 모델을 세밀하고 구조에 충실한 추론으로 유도하는 데 사용됩니다. 7개의 시각적 추론 벤치마크 전반에 걸쳐 SceneAlign은 응답 정확도와 추론 충실도를 지속적으로 향상하여 다중 모드 추론을 위한 접지 인식 정렬의 효율성을 강조합니다."
998,http://arxiv.org/abs/2601.02905 ,LOST-3DSG: Lightweight Open-Vocabulary 3D Scene Graphs with Semantic Tracking in Dynamic Environments,"Sara Micol Ferraina, Michele Brienza, Francesco Argenziano, Emanuele Musumeci, Vincenzo Suriani, Domenico D. Bloisi, Daniele Nardi","동적 환경 내에서 움직이는 물체를 추적하는 것은 로봇 공학의 핵심 과제입니다. 최근 연구에서는 이 주제를 크게 발전시켰습니다. 그러나 많은 기존 접근 방식은 무거운 기초 모델에 의존하기 때문에 여전히 비효율적입니다. 이러한 한계를 해결하기 위해 우리는 실제 환경에서 동적 객체를 추적하도록 설계된 경량 개방형 어휘 3D 장면 그래프인 LOST-3DSG를 제안합니다. 우리의 방법은 word2vec 및 문장 임베딩을 기반으로 하는 엔터티 추적에 대한 의미론적 접근 방식을 채택하여 조밀한 CLIP 시각적 기능을 저장할 필요성을 피하면서 개방형 어휘 표현을 가능하게 합니다. 결과적으로 LOST-3DSG는 고차원 시각적 임베딩에 의존하는 접근 방식에 비해 우수한 성능을 달성합니다. 우리는 TIAGo 로봇을 이용하여 실제 3D 환경에서 진행되는 정성적, 정량적 실험을 통해 우리의 방법을 평가합니다. 결과는 동적 객체 추적에서 LOST-3DSG의 효과와 효율성을 보여줍니다. 코드와 보충 자료는 프로젝트 웹사이트(https://lab-rococo-sapienza.github.io/lost-3dsg/)에서 공개적으로 제공됩니다."
997,http://arxiv.org/abs/2601.02792 ,Textile IR: A Bidirectional Intermediate Representation for Physics-Aware Fashion CAD,"Petteri Teikari, Neliana Fuenmayor","제조에 유효한 CAD, 물리 기반 시뮬레이션, 패션 디자인의 라이프사이클 평가를 연결하는 양방향 중간 표현인 Textile IR을 소개합니다. 패턴 소프트웨어가 재봉 가능한 출력을 보장하지만 드레이프에 대해 전혀 이해하지 못하고 물리 시뮬레이션이 동작을 예측하지만 패턴을 자동으로 수정할 수 없는 기존 사일로 도구와 달리 Textile IR은 값싼 구문 검사(패턴 마감, 솔기 호환성)부터 값비싼 물리 검증(드레이프 시뮬레이션, 응력 분석)에 이르기까지 7계층 검증 래더를 통해 통합을 위한 의미론적 접착제를 제공합니다. 이 아키텍처는 양방향 피드백을 가능하게 합니다. 시뮬레이션 실패는 패턴 수정을 제안합니다. 재료 대체는 지속 가능성 추정치를 실시간으로 업데이트합니다. 불확실성은 명시적인 신뢰 한계를 통해 파이프라인 전체에 전파됩니다. 우리는 패션 엔지니어링을 세 가지 영역에 대한 제약 조건 충족으로 공식화하고 Textile IR의 장면 그래프 표현을 통해 AI 시스템이 의류를 픽셀 배열이 아닌 구조화된 프로그램으로 조작할 수 있는 방법을 보여줍니다. 프레임워크는 복합 불확실성 문제를 해결합니다. 재료 테스트의 측정 오류, 시뮬레이션 근사치 및 LCA 데이터베이스 격차가 결합되면 명시적인 불확실성 추적 없이는 지속 가능성 주장을 신뢰할 수 없게 됩니다. 우리는 6가지 연구 우선 순위를 제안하고 통합 워크플로가 전문 엔지니어링 요구 사항을 줄이는 패션 중소기업을 위한 배포 고려 사항을 논의합니다. 주요 기여: 엔지니어링 제약 조건을 인지하고, 조작할 수 있으며, 즉각적으로 결과를 가져올 수 있는 형식적 표현 - 디자이너가 비용이 많이 드는 물리적 프로토타입 제작 후 충돌을 발견하는 대신 지속 가능성, 제조 가능성 및 미적 균형을 동시에 탐색할 수 있도록 해줍니다."
996,http://arxiv.org/abs/2601.01872 ,CausalNav: A Long-term Embodied Navigation System for Autonomous Mobile Robots in Dynamic Outdoor Scenarios,"Hongbo Duan, Shangyi Luo, Zhiyuan Deng, Yanbo Chen, Yuanhao Chiang, Yi Liu, Fangming Liu, Xueqian Wang","의미론적 추론, 동적 조건 및 장기적인 안정성의 어려움으로 인해 대규모 실외 환경에서 자율적인 언어 안내 탐색은 모바일 로봇 공학의 주요 과제로 남아 있습니다. 우리는 역동적인 실외 환경에 맞춰진 최초의 장면 그래프 기반 의미론적 탐색 프레임워크인 CausalNav를 제안합니다. 우리는 대략적인 지도 데이터를 세분화된 객체 엔터티와 계층적으로 통합하는 Embodied Graph라고 하는 LLM을 사용하여 다단계 의미론적 장면 그래프를 구성합니다. 구성된 그래프는 검색 증강 생성(RAG)을 위한 검색 가능한 지식 기반 역할을 하여 개방형 어휘 쿼리에서 의미론적 탐색 및 장기 계획을 가능하게 합니다. 실시간 인식과 오프라인 지도 데이터를 융합함으로써 Embodied Graph는 역동적인 야외 환경에서 다양한 공간적 세분화에 걸쳐 강력한 탐색을 지원합니다. 동적 개체는 장면 그래프 구성 및 계층적 계획 모듈 모두에서 명시적으로 처리됩니다. Embodied Graph는 환경 변화를 반영하고 실시간 의미 탐색을 지원하기 위해 시간 창 내에서 지속적으로 업데이트됩니다. 시뮬레이션과 실제 설정 모두에서 광범위한 실험을 통해 탁월한 견고성과 효율성이 입증되었습니다."
995,http://arxiv.org/abs/2601.01181 ,GenCAMO: Scene-Graph Contextual Decoupling for Environment-aware and Mask-free Camouflage Image-Dense Annotation Generation,"Chenglizhao Chen, Shaojiang Yuan, Xiaoxue Lu, Mengke Song, Jia Song, Zhenyu Wu, Wenfeng Song, Shuai Li","은폐 밀도 예측(CDP), 특히 RGB-D 위장 물체 감지 및 개방형 어휘 위장 물체 분할은 복잡한 위장 장면에 대한 이해와 추론을 발전시키는 데 중요한 역할을 합니다. 그러나 조밀한 주석이 포함된 고품질 및 대규모 위장 데이터 세트는 값비싼 데이터 수집 및 라벨링 비용으로 인해 여전히 부족합니다. 이 과제를 해결하기 위해 우리는 생성 모델을 활용하여 세밀한 표현, 사전 지식 및 보조 추론을 통해 CDP 모델을 훈련하기 위한 사실적인 위장 이미지 밀도 데이터를 합성하는 방법을 탐구합니다. 구체적으로 우리의 기여는 세 가지입니다. (i) 깊이 맵, 장면 그래프, 속성 설명 및 텍스트 프롬프트를 포함한 다중 모드 주석이 있는 대규모 위장 데이터 세트인 GenCAMO-DB를 소개합니다. (ii) 우리는 고충실도 위장 이미지 밀도 주석을 생성하는 환경 인식 및 마스크 없는 생성 프레임워크인 GenCAMO를 제시합니다. (iii) 여러 양식에 걸친 광범위한 실험에서는 GenCAMO가 고품질 합성 데이터를 제공하여 복잡한 위장 장면에서 조밀한 예측 성능을 크게 향상시키는 것으로 나타났습니다. 코드와 데이터 세트는 논문 승인 후 공개됩니다."
994,http://arxiv.org/abs/2601.06097 ,Semantic Event Graphs for Long-Form Video Question Answering,"Aradhya Dixit, Tianxi Liang","실제 토큰 및 컴퓨팅 예산을 초과하지 않고 시간 규모의 영상을 추론하는 데 어려움을 겪는 최신 비전 언어 모델에서는 긴 형식의 비디오 질문 답변이 여전히 어려운 과제입니다. 기존 시스템은 일반적으로 프레임을 다운샘플링하거나 대규모 컨텍스트 언어 모델에 조밀한 시각적 임베딩을 제공하여 시간적 범위와 비용을 절충합니다. 우리는 원시 프레임을 컴팩트한 시간적 상호 작용 로그로 대체하는 비디오와 언어 간의 경량 기호 인터페이스인 Semantic Event Graphs(SEG)를 제안합니다. 우리의 파이프라인은 YOLOv11을 사용하여 객체를 감지 및 추적하고 근접 패턴을 START/END 인간-객체 이벤트로 변환하고 이를 TSG(Temporal Scene Graph)로 구성합니다. 추론 시 쿼리 인식 정리 모듈은 앵커 엔터티와 어휘 관련 이벤트를 식별하여 답변 생성을 위해 음성으로 전달되고 Gemini 2.5 Flash에 전달되는 작은 하위 그래프만 반환합니다. 5개의 YouTube 동영상(각각 300~500개의 상호 작용)과 120개의 자동 생성된 장거리 질문에서 SEG는 쿼리당 3.47,000개의 토큰만 사용하여 65.0%의 정확도를 달성하고 전체 로그 기준(40.39,000개의 토큰에서 62.5%)과 거의 일치하는 동시에 토큰 사용량을 91.4% 줄입니다. 마지막 30초로 제한된 짧은 컨텍스트 기준선은 2.5% 정확도로 축소되어 명시적 시간 기억의 필요성을 강조합니다. 이러한 결과는 상징적 시간 그래프가 기성 비전 언어 모델을 위한 효과적인 플러그 앤 플레이 메모리 계층 역할을 할 수 있으며 장거리 추론 능력을 보존하는 동시에 긴 형식의 비디오 질문에 대한 답변을 훨씬 더 토큰 및 비용 효율적으로 만들 수 있음을 보여줍니다. 재현성을 위해 코드, 로그 및 이벤트 추출 도구가 출시될 예정입니다."
993,http://arxiv.org/abs/2512.24845 ,ArtiSG: Functional 3D Scene Graph Construction via Human-demonstrated Articulated Objects Manipulation,"Qiuyi Gu, Yuze Sheng, Jincheng Yu, Jiahao Tang, Xiaolong Shan, Zhaoyang Shen, Tinghao Yi, Xiaodan Liang, Xinlei Chen, Yu Wang","3D 장면 그래프는 탐색 및 계획에 대한 의미론적 이해를 로봇에 부여했지만, 특히 관절로 연결된 물체와 관련하여 물리적 조작에 필요한 기능 정보가 부족한 경우가 많습니다. 정적 관측으로부터 관절 메커니즘을 추론하기 위한 기존 접근 방식은 시각적으로 모호한 경향이 있는 반면, 상태 변화에서 매개변수를 추정하는 방법은 일반적으로 고정된 카메라 및 방해받지 않는 뷰와 같은 제한된 설정에 의존합니다. 또한 일반 물체 감지기에서는 작은 손잡이와 같은 세밀한 기능 요소를 놓치는 경우가 많습니다. 이러한 격차를 해소하기 위해 우리는 인간의 시연을 구조화된 로봇 메모리로 인코딩하여 기능적인 3D 장면 그래프를 구성하는 프레임워크인 ArtiSG를 제시합니다. 우리의 접근 방식은 휴대용 설정을 활용하는 강력한 관절 데이터 수집 파이프라인을 활용하여 카메라 자아 움직임에서도 6-DoF 관절 궤적과 축을 정확하게 추정합니다. 우리는 시각적 인식에서 놓친 눈에 띄지 않는 기능적 요소를 발견하기 위해 상호 작용 데이터를 활용하면서 이러한 운동학적 사전을 계층적 및 개방형 어휘 그래프에 통합합니다. 광범위한 실제 실험을 통해 ArtiSG가 기능 요소 리콜 및 관절 추정 정밀도에서 기준선보다 훨씬 뛰어난 성능을 발휘한다는 사실이 입증되었습니다. 또한, 우리는 구성된 그래프가 다양한 관절 개체가 포함된 실제 환경에서 로봇이 언어 중심 조작 작업을 수행하도록 효과적으로 안내하는 신뢰할 수 있는 기능 메모리 역할을 한다는 것을 보여줍니다."
992,http://arxiv.org/abs/2512.23024 ,With Great Context Comes Great Prediction Power: Classifying Objects via Geo-Semantic Scene Graphs,"Ciprian Constantinescu, Marius Leordeanu","인간은 공간적 관계, 물질적 특성, 다른 물체의 동시 발생을 포함하여 주변 장면에 대한 풍부한 이해를 활용하여 쉽게 물체를 식별합니다. 대조적으로, 대부분의 계산 객체 인식 시스템은 고립된 의미가 없는 고립된 이미지 영역에서 작동하므로 이러한 중요한 상황 정보를 무시합니다. 이 논문은 맥락의 중요한 역할을 주장하고 맥락적 객체 분류를 위한 새로운 프레임워크를 소개합니다. 먼저 단일 단안 이미지에서 GSCG(Geo-Semantic Contextual Graph)를 구성합니다. 이 풍부하고 구조화된 표현은 미터법 깊이 추정기를 통합된 Panoptic 및 재료 분할 모델과 통합하여 구축되었습니다. GSCG는 객체를 상세한 기하학적, 색채 및 재료 속성을 가진 노드로 인코딩하고 공간적 관계를 가장자리로 인코딩합니다. 이 명시적인 그래프 구조는 모델의 추론 프로세스를 본질적으로 해석 가능하게 만듭니다. 그런 다음 대상 개체, 바로 이웃 및 전역 장면 컨텍스트의 특징을 집계하여 클래스를 예측하는 특수 그래프 기반 분류기를 제안합니다. 광범위한 제거 연구를 통해 우리는 상황 인식 모델이 73.4%의 분류 정확도를 달성하여 상황에 구애받지 않는 버전(최저 38.4%)보다 훨씬 뛰어난 성능을 발휘함을 보여줍니다. 또한 GSCG 기반 접근 방식은 미세 조정된 ResNet 모델(최대 53.5%) 및 최첨단 다중 모드 LLM(대형 언어 모델), Llama 4 Scout를 포함하여 강력한 기준을 크게 능가합니다. 이는 객체에 대한 자세한 설명과 함께 전체 이미지가 제공되는 경우에도 최대 42.3%입니다. COCO 2017 열차/발 분할에 대한 이러한 결과는 객체 인식 작업에 대한 명시적으로 구조화되고 해석 가능한 컨텍스트의 우수성을 강조합니다."
991,http://arxiv.org/abs/2512.22693 ,Instance Communication System for Intelligent Connected Vehicles: Bridging the Gap from Semantic to Instance-Level Transmission,"Daiqi Zhang, Bizhu Wang, Wenqi Zhang, Chen Sun, Xiaodong Xu",지능형 연결 차량(ICV)은 효율적이고 안전이 중요한 서비스를 위해 고속 데이터 전송을 사용합니다. 그러나 무선 리소스의 부족으로 인해 ICV의 기능이 제한됩니다. SemCom(의미론적 커뮤니케이션) 시스템은 전체 원시 데이터 대신 의미론적 정보라고 하는 작업 관련 정보를 추출하고 전송함으로써 이 문제를 완화할 수 있습니다. 그럼에도 불구하고 우리는 동일한 의미 범주에 속하는 모든 인스턴스가 다운스트림 작업에 똑같이 중요하지는 않은 SemCom 시스템 내에서 잔여 중복성이 지속된다는 것을 밝힙니다. 이 문제를 해결하기 위해 우리는 ICV의 의미 수준에서 인스턴스 수준으로 통신을 향상시키는 인스턴스 통신(InsCom)을 도입했습니다. 특히 InsCom은 장면 그래프 생성 모델을 사용하여 모든 이미지 인스턴스를 식별하고 상호 관계를 분석하여 의미상 동일한 인스턴스를 구별합니다. 또한 주제 의미론 및 관계-객체 쌍을 기반으로 사용자가 구성할 수 있는 작업에 중요한 기준을 적용하여 인식된 인스턴스를 필터링합니다. 결과적으로 InsCom은 작업에 중요한 인스턴스만 전송함으로써 데이터 중복성을 크게 줄여 제한된 무선 리소스 내에서 전송 효율성을 크게 향상시킵니다. 다양한 데이터 세트와 무선 채널 조건에 대한 평가를 통해 InsCom은 최첨단 SemCom 시스템과 비교하여 7.82배 이상의 데이터 볼륨 감소와 1.75~14.03dB 범위의 품질 개선을 달성한 것으로 나타났습니다.
990,http://arxiv.org/abs/2512.21243 ,LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation,"Anatoly O. Onishchenko, Alexey K. Kovalev, Aleksandr I. Panov","작업에 따른 구체화된 교육을 위한 계획자로 LLM(대형 언어 모델)을 사용하는 방법이 널리 보급되었습니다. 작업을 성공적으로 완료하려면 LLM이 로봇이 작동하는 환경에 접지되어 있어야 합니다. 한 가지 해결책은 필요한 모든 정보가 포함된 장면 그래프를 사용하는 것입니다. 최신 방법은 사전 구축된 장면 그래프에 의존하며 모든 작업 관련 정보를 계획 시작 시 사용할 수 있다고 가정합니다. 그러나 이러한 접근 방식은 그래프 구성과 작업 실행 사이에 발생할 수 있는 환경 변화를 고려하지 않습니다. 우리는 정적 자산과 객체 사전순으로 구성된 장면 그래프를 활용하는 방법인 LookPlanGraph를 제안합니다. 계획 실행 중에 LookPlanGraph는 기존 사전을 확인하거나 새로운 엔터티를 검색하여 관련 개체로 그래프를 지속적으로 업데이트합니다. 이는 비전 언어 모델을 사용하여 에이전트의 자기 중심적 카메라 보기를 처리함으로써 달성됩니다. 우리는 변경된 객체 위치 VirtualHome 및 OmniGibson 시뮬레이션 환경을 사용하여 실험을 수행하여 LookPlanGraph가 사전 정의된 정적 장면 그래프를 기반으로 하는 방법보다 성능이 우수하다는 것을 입증했습니다. 우리 접근 방식의 실제 적용 가능성을 입증하기 위해 실제 환경에서 실험도 수행했습니다. 또한 SayPlan Office, BEHAVIOR-1K 및 VirtualHome RobotHow에서 가져온 514개의 작업으로 구성된 자동 검증 프레임워크를 갖춘 GraSIF(Graph Scenes for Instruction Follow) 데이터 세트를 소개합니다. 프로젝트 페이지는 https://lookplangraph.github.io에서 확인할 수 있습니다."
989,http://arxiv.org/abs/2512.21133 ,SparScene: Efficient Traffic Scene Representation via Sparse Graph Learning for Large-Scale Trajectory Generation,"Xiaoyu Mo, Jintian Ge, Zifan Wang, Chen Lv, Karl Henrik Johansson","다중 에이전트 궤적 생성은 자율주행 및 지능형 교통 시스템의 핵심 문제입니다. 그러나 복잡한 장면에서 수많은 도로 사용자와 인프라 간의 동적 상호 작용을 효율적으로 모델링하는 것은 아직 해결되지 않은 문제로 남아 있습니다. 기존 방법은 일반적으로 거리 기반 또는 완전히 연결된 조밀한 그래프 구조를 사용하여 상호 작용 정보를 캡처합니다. 이는 많은 수의 중복 에지를 도입할 뿐만 아니라 인코딩을 위해 복잡하고 매개변수화된 네트워크가 필요하므로 훈련 및 추론 효율성이 낮고 크고 복잡한 교통 장면에 대한 확장성이 제한됩니다. 기존 방법의 한계를 극복하기 위해 효율적이고 확장 가능한 교통 상황 표현을 위해 설계된 희소 그래프 학습 프레임워크인 SparScene을 제안합니다. 거리 임계값에 의존하는 대신 SparScene은 차선 그래프 토폴로지를 활용하여 에이전트와 차선 사이의 구조 인식 희소 연결을 구성하여 효율적이면서도 유익한 장면 그래프 표현을 가능하게 합니다. SparScene은 에이전트-맵 및 에이전트-에이전트 상호 작용을 효율적으로 집계하는 경량 그래프 인코더를 채택하여 효율성과 확장성이 크게 향상된 컴팩트한 장면 표현을 생성합니다. WOMD(Waymo Open Motion Dataset)의 모션 예측 벤치마크에서 SparScene은 놀라운 효율성으로 경쟁력 있는 성능을 달성했습니다. 5ms 이내에 한 장면에서 200개 이상의 에이전트에 대한 궤적을 생성하고 2.9GB의 GPU 메모리로 단 54ms의 추론 시간으로 5,000개 이상의 에이전트, 17,000개 레인으로 확장되어 대규모 교통 장면에 대한 탁월한 확장성을 강조합니다."
988,http://arxiv.org/abs/2512.19221 ,From Pixels to Predicates Structuring urban perception with scene graphs,"Yunlong Liu, Shuyang Li, Pengyuan Liu, Yu Zhang, Rudi Stouffs","인식 연구는 거리 풍경을 사용하여 점점 더 모델링되고 있지만 많은 접근법은 여전히 ​​픽셀 특징이나 객체 동시 발생 통계에 의존하고 인간 인식을 형성하는 명시적인 관계를 간과합니다. 본 연구에서는 6가지 지각 지표를 예측하기 위해 스트리트 뷰 이미지(SVI)를 구조화된 표현으로 변환하는 3단계 파이프라인을 제안합니다. 첫 번째 단계에서는 개방형 Panoptic Scene Graph 모델(OpenPSG)을 사용하여 각 이미지를 구문 분석하여 개체 조건자 개체 삼중항을 추출합니다. 두 번째 단계에서는 이종 그래프 자동 인코더(GraphMAE)를 통해 컴팩트 장면 수준 임베딩을 학습합니다. 세 번째 단계에서는 신경망이 이러한 임베딩을 통해 인식 점수를 예측합니다. 우리는 정확성, 정밀성 및 도시 간 일반화 측면에서 이미지 전용 기준에 대해 제안된 접근 방식을 평가합니다. 결과는 (i) 우리의 접근 방식이 기본 모델에 비해 인식 예측 정확도를 평균 26% 향상시키고 (ii) 도시 간 예측 작업에서 강력한 일반화 성능을 유지한다는 것을 나타냅니다. 또한 구조화된 표현은 벽의 낙서, 보도에 주차된 자동차와 같은 도시 장면에서 어떤 관계 패턴이 낮은 인식 점수에 기여하는지 명확하게 보여줍니다. 전반적으로, 이 연구는 그래프 기반 구조가 도시 인식을 모델링하고 인간 중심 및 상황 인식 도시 분석을 발전시키기 위한 표현적이고 일반화 가능하며 해석 가능한 신호를 제공한다는 것을 보여줍니다."
987,http://arxiv.org/abs/2512.18613 ,Text2Graph VPR: A Text-to-Graph Expert System for Explainable Place Recognition in Changing Environments,"Saeideh Yousefzadeh, Hamidreza Pourreza","장기 배포 시 시각적 장소 인식(VPR)에는 픽셀 유사성 이상의 추론이 필요합니다. 시스템은 조명, 날씨 및 계절 변화에도 견고하게 유지되는 투명하고 해석 가능한 결정을 내려야 합니다. 우리는 이미지 시퀀스를 텍스트 장면 설명으로 변환하고, 해당 설명을 구조화된 장면 그래프로 구문 분석하고, 결과 그래프에 대한 이유를 파악하여 장소를 식별하는 설명 가능한 의미론적 현지화 시스템인 Text2Graph VPR을 제시합니다. 장면 그래프는 개체, 속성 및 쌍별 관계를 캡처합니다. 우리는 프레임별 그래프를 압축된 장소 표현으로 집계하고 구조적 일치를 위해 학습된 GAT(Graph Attention Network) 임베딩과 SP(Shortest-Path) 커널을 융합하는 이중 유사성 메커니즘을 사용하여 검색을 수행합니다. 이 하이브리드 설계는 학습된 의미론적 일치와 토폴로지 인식 비교를 모두 가능하게 하며, 결정적으로 진단 분석을 지원하고 의사 결정 프로세스의 투명성을 향상시키는 사람이 읽을 수 있는 중간 표현을 생성합니다. 우리는 Oxford RobotCar 및 MSLS(암만/샌프란시스코) 벤치마크에서 시스템을 검증하고 인간 텍스트 쿼리를 사용한 제로샷 작업과 함께 심각한 모양 변화 하에서 강력한 검색을 보여줍니다. 결과는 의미론적 그래프 기반 추론이 장소 인식을 위한 실행 가능하고 해석 가능한 대안이며 특히 안전에 민감하고 자원이 제한된 설정에 적합하다는 것을 보여줍니다."
986,http://arxiv.org/abs/2512.18448 ,Object-Centric Framework for Video Moment Retrieval,"Zongyao Li, Yongkang Wong, Satoshi Yamazaki, Jianquan Liu, Mohan Kankanhalli","대부분의 기존 비디오 순간 검색 방법은 주로 전역 시각적 및 의미 정보를 인코딩하는 프레임 또는 클립 수준 기능의 시간 순서에 의존합니다. 그러나 이러한 표현은 특정 엔터티 및 해당 상호 작용과 관련된 개체 지향 쿼리에서 설명하는 순간을 지역화하는 데 중요한 세부적인 개체 의미 체계 및 모양을 캡처하지 못하는 경우가 많습니다. 특히, 객체 수준의 시간적 역학이 크게 간과되어 자세한 객체 수준 추론이 필요한 시나리오에서 기존 접근 방식의 효율성이 제한되었습니다. 이러한 한계를 해결하기 위해 우리는 순간 검색을 위한 새로운 객체 중심 프레임워크를 제안합니다. 우리의 방법은 먼저 장면 그래프 파서를 사용하여 쿼리 관련 개체를 추출한 다음 비디오 프레임에서 장면 그래프를 생성하여 이러한 개체와 그 관계를 나타냅니다. 장면 그래프를 기반으로 풍부한 시각적 및 의미 정보를 인코딩하는 객체 수준 기능 시퀀스를 구성합니다. 이러한 시퀀스는 시간 경과에 따른 객체 간의 시공간 상관 관계를 모델링하는 관계형 트랙렛 변환기에 의해 처리됩니다. 객체 수준 상태 변경을 명시적으로 캡처함으로써 우리의 프레임워크는 객체 지향 쿼리에 맞춰진 순간의 보다 정확한 지역화를 가능하게 합니다. 우리는 Charades-STA, QVHighlights 및 TACoS의 세 가지 벤치마크에서 방법을 평가했습니다. 실험 결과는 우리의 방법이 모든 벤치마크에서 기존의 최첨단 방법보다 성능이 우수하다는 것을 보여줍니다."
985,http://arxiv.org/abs/2512.18407 ,Through the PRISm: Importance-Aware Scene Graphs for Image Retrieval,"Dimitrios Georgoulopoulos, Nikolaos Chaidos, Angeliki Dimitriou, Giorgos Stamou","의미론적으로 유사한 이미지를 정확하게 검색하는 것은 컴퓨터 비전의 근본적인 과제로 남아 있습니다. 전통적인 방법은 장면의 관계적, 맥락적 뉘앙스를 포착하지 못하는 경우가 많기 때문입니다. 두 가지 새로운 구성 요소를 통해 이미지 간 검색을 향상시키는 다중 모드 프레임워크인 PRISm(Pruning-based Image Retrieval via Importance Prediction on Semantic Graphs)을 소개합니다. 첫째, 중요도 예측 모듈은 관련 없는 요소를 잘라내면서 이미지 내에서 가장 중요한 개체와 관계형 삼중항을 식별하고 유지합니다. 둘째, Edge-Aware Graph Neural Network는 관계형 구조를 명시적으로 인코딩하고 전역 시각적 기능을 통합하여 의미론적으로 정보가 있는 이미지 임베딩을 생성합니다. PRISm은 객체의 의미론적 중요성과 상호 작용을 명시적으로 모델링함으로써 인간의 인식과 밀접하게 일치하는 이미지 검색을 달성합니다. 이는 이전 접근 방식에서는 거의 볼 수 없었던 기능입니다. 그 아키텍처는 관계형 추론과 시각적 표현을 효과적으로 결합하여 의미론적으로 기반한 검색을 가능하게 합니다. 벤치마크 및 실제 데이터세트에 대한 광범위한 실험을 통해 일관되게 우수한 성능을 입증하는 한편, 정성적 분석을 통해 PRISm이 주요 개체와 상호 작용을 정확하게 포착하여 해석 가능하고 의미론적으로 의미 있는 결과를 생성하는 것으로 나타났습니다."
984,http://arxiv.org/abs/2512.17445 ,LangDriveCTRL: Natural Language Controllable Driving Scene Editing with Multi-modal Agents,"Yun He, Francesco Pittaluga, Ziyu Jiang, Matthias Zwicker, Manmohan Chandraker, Zaid Tasneem","LangDriveCTRL은 다양한 교통 시나리오를 종합하기 위해 실제 운전 비디오를 편집하기 위한 자연어 제어 가능 프레임워크입니다. 이는 명시적인 3D 장면 분해를 활용하여 운전 비디오를 정적 배경과 동적 개체가 포함된 장면 그래프로 표현합니다. 세밀한 편집과 현실감을 구현하기 위해 Orchestrator가 사용자 지침을 전문 에이전트와 도구를 조정하는 실행 그래프로 변환하는 에이전트 파이프라인을 통합합니다. 특히, 객체 접지 에이전트는 자유 형식 텍스트 설명과 장면 그래프의 대상 객체 노드 간의 대응 관계를 설정합니다. 행동 편집 에이전트는 언어 명령으로부터 다중 객체 궤적을 생성합니다. 행동 검토 에이전트는 생성된 궤적을 반복적으로 검토하고 개선합니다. 편집된 장면 그래프는 렌더링된 다음 비디오 확산 도구를 사용하여 개선되어 객체 삽입 및 중요한 뷰 변경으로 인해 발생하는 아티팩트를 해결합니다. LangDriveCTRL은 단일 자연어 명령에서 개체 노드 편집(제거, 삽입 및 교체)과 다중 개체 동작 편집을 모두 지원합니다. 정량적으로는 뛰어난 구조 보존, 사실적, 교통 사실성으로 이전 SoTA보다 거의 $2\times$ 더 높은 지시 정렬을 달성합니다. 프로젝트 페이지는 https://yunhe24.github.io/langdrivectrl/에서 확인할 수 있습니다."
983,http://arxiv.org/abs/2512.16909 ,MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning,"Yuanchen Ju, Yongyuan Liang, Yen-Jen Wang, Nandiraju Gireesh, Yuanliang Ju, Seungjae Lee, Qiao Gu, Elvis Hsieh, Furong Huang, Koushil Sreenath","가정의 모바일 조작기는 탐색과 조작을 모두 수행해야 합니다. 이를 위해서는 객체의 위치, 객체의 기능, 실행 가능한 부분을 캡처하는 간결하고 의미가 풍부한 장면 표현이 필요합니다. 장면 그래프는 자연스러운 선택이지만 이전 작업에서는 종종 공간적 관계와 기능적 관계를 분리하고, 장면을 개체 상태나 시간적 업데이트 없이 정적 스냅샷으로 처리하고, 현재 작업을 수행하는 데 가장 관련된 정보를 간과했습니다. 이러한 제한 사항을 해결하기 위해 공간 기능 관계와 부분 수준 대화형 요소를 통합하는 구현된 에이전트에 대한 통합 장면 표현인 MomaGraph를 소개합니다. 그러나 이러한 표현을 발전시키려면 적절한 데이터와 엄격한 평가가 모두 필요하지만 이는 대부분 누락되었습니다. 따라서 우리는 가정 환경에서 풍부한 주석이 달린 작업 중심 장면 그래프의 최초 대규모 데이터 세트인 MomaGraph-Scenes와 높은 수준의 계획부터 세밀한 장면 이해에 이르기까지 6가지 추론 기능을 포괄하는 체계적인 평가 제품군인 MomaGraph-Bench를 제공합니다. 이러한 기반을 바탕으로 우리는 MomaGraph-Scenes에 대한 강화 학습으로 훈련된 7B 비전 언어 모델인 MomaGraph-R1을 추가로 개발합니다. MomaGraph-R1은 작업 중심 장면 그래프를 예측하고 Graph-then-Plan 프레임워크에서 제로샷 작업 플래너 역할을 합니다. 광범위한 실험을 통해 우리 모델이 공개 벤치마크 전반에 걸쳐 일반화하고 실제 로봇 실험으로 효과적으로 전환하는 동시에 벤치마크에서 71.6%의 정확도(최고 기준보다 +11.4%)에 도달하여 오픈 소스 모델 중에서 최첨단 결과를 달성하는 것으로 나타났습니다."
982,http://arxiv.org/abs/2512.16461 ,SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning,"Tin Stribor Sohn, Maximilian Dillitzer, Jason J. Corso, Eric Sax","자율 로봇 시스템은 안정적인 탐색 및 상호 작용을 보장하기 위해 동적 환경에 대한 시공간적 이해가 필요합니다. VLM(Vision-Language Model)은 개방형 의미론적 사전 정보를 제공하지만 3D 기하학 및 시간 역학에 대한 기반이 부족합니다. 반대로 기하학적 인식은 구조와 동작을 포착하지만 의미론적으로는 희박합니다. 우리는 VLM 파생 의미론을 포인트 클라우드 기하학 및 시간적 일관성과 통합하는 통합 4D 장면 이해를 위한 교육이 필요 없고 백본에 구애받지 않는 프레임워크인 SNOW(Scene Understanding with Open-World Knowledge)를 제안합니다. SNOW는 HDBSCAN 클러스터링을 사용하여 동기화된 RGB 이미지와 3D 포인트 클라우드를 처리하여 SAM2 기반 분할을 안내하는 객체 수준 제안을 생성합니다. 분할된 각 영역은 제안된 STEP(Spatio-Temporal Tokenized Patch Encoding)를 통해 인코딩되어 지역화된 의미, 기하학적 및 시간 속성을 캡처하는 다중 모드 토큰을 생성합니다. 이러한 토큰은 4DSG(4D 장면 그래프)에 점진적으로 통합되어 다운스트림 추론을 위한 4D 사전 역할을 합니다. 경량 SLAM 백엔드는 모든 STEP 토큰을 환경에 공간적으로 고정하여 전역 참조 정렬을 제공하고 시간이 지남에 따라 명확한 공간 접지를 보장합니다. 결과 4DSG는 VLM이 공간적 장면 구조와 시간적 역학을 직접 해석할 수 있는 쿼리 가능한 통합 세계 모델을 형성합니다. 다양한 벤치마크 세트에 대한 실험은 SNOW가 정확한 4D 장면 이해와 공간 기반 추론을 가능하게 함으로써 여러 설정에서 새로운 최첨단 성능을 설정하고 구체화된 추론과 자율 로봇 공학을 위한 구조화된 4D 사전의 중요성을 강조한다는 것을 보여줍니다."
981,http://arxiv.org/abs/2512.15957 ,Seeing is Believing (and Predicting): Context-Aware Multi-Human Behavior Prediction with Vision Language Models,"Utsav Panchal, Yuchen Liu, Luigi Palmieri, Ilche Georgievski, Marco Aiello","인간이 거주하는 환경에서 작동하는 모바일 로봇의 경우 인간 행동을 정확하게 예측하는 것이 중요합니다. 이전 연구는 주로 자기 중심적 관점에서 단일 인간 시나리오의 행동을 예측하는 데 중점을 두었지만, 여러 로봇 응용 프로그램에서는 제3자 관점에서 여러 인간 행동을 이해해야 합니다. 이를 위해 우리는 시각적 입력의 맥락적 특징과 장면 그래프의 공간 인식을 통합하여 인간과 장면의 상호 작용에 대한 예측을 향상시키는 VLM(Vision Language Model) 기반 프레임워크인 CAMP-VLM(Context-Aware Multi-human Behavior Prediction)을 제시합니다. 관찰자 관점에서 다중 인간 행동 예측을 위한 적합한 데이터 세트가 부족하기 때문에 우리는 사실적 시뮬레이터에서 생성된 합성 인간 행동 데이터로 CAMP-VLM을 미세 조정하고 합성 및 실제 시퀀스 모두에서 결과 모델을 평가하여 일반화 기능을 평가합니다. SFT(Supervised Fine-Tuning) 및 DPO(Direct Preference Optimization)를 활용하는 CAMP-VLM은 예측 정확도에서 최고 성능 기준을 최대 66.9% 능가합니다."
980,http://arxiv.org/abs/2512.15047 ,HERO: Hierarchical Traversable 3D Scene Graphs for Embodied Navigation Among Movable Obstacles,"Yunheng Wang, Yixiao Feng, Yuetong Fang, Shuning Zhang, Tan Jing, Jian Li, Xiangrui Jiang, Renjing Xu","3D 장면 그래프(3DSG)는 엔터티 간의 복잡한 공간적, 의미적, 기능적 관계를 명시적으로 모델링하고 에이전트가 환경과 지능적으로 상호 작용하고 다양한 동작을 실행할 수 있도록 기본 이해를 제공하는 능력으로 구별되는 물리적 세계의 강력한 표현을 구성합니다. 이러한 기능의 중요한 구성 요소인 구현된 탐색은 3DSG의 컴팩트하고 표현력이 풍부한 특성을 활용하여 복잡한 대규모 환경에서 장기적인 추론과 계획을 가능하게 합니다. 그러나 이전 작업은 정적 공간 레이아웃만을 기반으로 횡단 가능한 공간을 정의하고 상호 작용 가능한 장애물을 횡단 불가능한 것으로 처리하는 정적 세계 가정에 의존합니다. 이러한 근본적인 제한으로 인해 실제 시나리오에서의 효율성이 심각하게 저하되어 도달 가능성이 제한되고 효율성이 떨어지며 확장성이 떨어집니다. 이러한 문제를 해결하기 위해 우리는 작동 가능한 장애물을 경로로 모델링하고 물리적 상호 작용, 기능적 의미 및 장면의 관계 계층을 캡처하여 횡단 가능성을 재정의하는 계층적 횡단 가능 3DSG를 구성하기 위한 새로운 프레임워크인 HERO를 제안합니다. 결과에 따르면 HERO는 기준선에 비해 부분적으로 막힌 환경에서 PL을 35.1% 줄이고 완전히 막힌 환경에서 SR을 79.4% 높여 효율성과 도달 가능성이 훨씬 더 높은 것으로 나타났습니다."
979,http://arxiv.org/abs/2512.14092 ,ProtoFlow: Interpretable and Robust Surgical Workflow Modeling with Learned Dynamic Scene Graph Prototypes,"Felix Holm, Ghazal Ghazaei, Nassir Navab","목적: AI 보조 수술을 발전시키기 위해서는 상세한 수술 인식이 중요하지만, 높은 주석 비용, 데이터 부족, 해석 가능한 모델 부족으로 인해 진행이 방해를 받습니다. 장면 그래프는 수술 이벤트에 대한 구조화된 추상화를 제공하지만 그 잠재력은 아직 활용되지 않은 상태로 남아 있습니다. 이 작업에서는 동적 장면 그래프 프로토타입을 학습하여 해석 가능하고 강력한 방식으로 복잡한 수술 워크플로우를 모델링하는 새로운 프레임워크인 ProtoFlow를 소개합니다.   방법: ProtoFlow는 프로토타입 기반 미세 조정 단계와 풍부한 표현 학습을 위한 자가 지도 사전 학습을 결합한 그래프 신경망(GNN) 인코더-디코더 아키텍처를 활용합니다. 이 프로세스는 반복적이고 임상적으로 의미 있는 수술 상호 작용 패턴을 캡슐화하는 핵심 프로토타입을 발견하고 개선하여 작업 흐름 분석을 위한 설명 가능한 기반을 형성합니다.   결과: 세분화된 CAT-SG 데이터 세트에 대한 접근 방식을 평가합니다. ProtoFlow는 전체 정확도에서 표준 GNN 기준선보다 성능이 뛰어날 뿐만 아니라 제한된 데이터, 소수의 샷 시나리오에서 탁월한 견고성을 보여주어 단 하나의 수술 비디오로 훈련할 때에도 강력한 성능을 유지합니다. 우리의 질적 분석은 학습된 프로토타입이 뚜렷한 수술 하위 기술을 성공적으로 식별하고 작업 흐름 편차 및 드문 합병증에 대한 명확하고 해석 가능한 통찰력을 제공한다는 것을 추가로 보여줍니다.   결론: ProtoFlow는 강력한 표현 학습과 고유한 설명 가능성을 결합함으로써 보다 투명하고 신뢰할 수 있으며 데이터 효율적인 AI 시스템을 개발하고 수술 훈련, 실시간 의사 결정 지원 및 작업 흐름 최적화에서 임상 채택 가능성을 가속화하는 중요한 단계를 나타냅니다."
978,http://arxiv.org/abs/2512.13290 ,LINA: Learning INterventions Adaptively for Physical Alignment and Generalization in Diffusion Models,"Shu Yu, Chaochao Lu","확산 모델(DM)은 이미지 및 비디오 생성 분야에서 놀라운 성공을 거두었습니다. 그러나 그들은 여전히 ​​(1) 물리적 정렬 및 (2) OOD(배포 외) 지시 따르기에 어려움을 겪고 있습니다. 우리는 이러한 문제가 모델이 인과 방향을 학습하지 못하고 새로운 재조합에 대한 인과 요인을 풀지 못하는 데서 비롯된다고 주장합니다. 진단 개입을 가능하게 하는 CSG(인과 장면 그래프) 및 PAP(물리적 정렬 프로브) 데이터 세트를 소개합니다. 이 분석은 세 가지 주요 통찰력을 제공합니다. 첫째, DM은 프롬프트에서 명시적으로 결정되지 않은 요소에 대한 다중 홉 추론으로 어려움을 겪습니다. 둘째, 프롬프트 임베딩에는 질감과 물리학에 대한 분리된 표현이 포함되어 있습니다. 셋째, 시각적 인과 구조는 계산적으로 제한된 초기 잡음 제거 단계에서 불균형적으로 확립됩니다. 이러한 결과를 바탕으로 우리는 (1) 프롬프트 및 시각적 잠재 공간의 타겟 지침과 (2) 재할당된 인과관계 인식 노이즈 제거 일정을 사용하는 프롬프트별 개입을 예측하는 방법을 학습하는 새로운 프레임워크인 LINA(Learning INInterventions Adaptively)를 소개합니다. 우리의 접근 방식은 이미지 및 비디오 DM에 따라 물리적 정렬과 OOD 지침을 모두 적용하여 까다로운 인과 생성 작업과 Winoground 데이터 세트에서 최첨단 성능을 달성합니다. 우리 프로젝트 페이지는 https://opencausalab.github.io/LINA에 있습니다."
977,http://arxiv.org/abs/2512.10342 ,CoSPlan: Corrective Sequential Planning via Scene Graph Incremental Updates,"Shresth Grover, Priyank Pathak, Akash Kumar, Vibhav Vineet, Yogesh S Rawat","대규모 비전-언어 모델(VLM)은 인상적인 복잡한 추론 능력을 보여주지만 시각적 순차 계획, 즉 목표를 향한 다단계 작업 실행에서는 아직 탐구되지 않은 상태로 남아 있습니다. 또한 실제 순차 계획에는 최적이 아닌(잘못된) 단계가 포함되는 경우가 많아 VLM이 이러한 단계를 감지하고 수정하는 데 어려움을 겪습니다. 우리는 미로 탐색, 블록 재배열, 이미지 재구성 및 개체 재구성의 4개 영역에 걸쳐 오류가 발생하기 쉬운 비전 기반 순차 계획 작업에서 VLM을 평가하기 위해 CoSPlan(수정 순차 계획 벤치마크)을 제안합니다. CoSPlan은 오류 감지(최적화되지 않은 작업 식별)와 단계 완료(목표 달성을 위해 작업 시퀀스 수정 및 완료)라는 두 가지 주요 기능을 평가합니다. 사고 사슬 및 장면 그래프와 같은 최첨단 추론 기술을 사용함에도 불구하고 VLM(예: Intern-VLM 및 Qwen2)은 CoSPlan에서 어려움을 겪으며 상황별 단서를 활용하여 목표를 달성하지 못합니다. 이 문제를 해결하기 위해 우리는 초기 상태와 목표 상태 사이의 중간 추론 단계를 도입하는 새로운 훈련 없는 방법인 장면 그래프 증분 업데이트(SGI)를 제안합니다. SGI는 VLM이 시퀀스를 추론하는 데 도움을 주어 평균 5.2%의 성능 향상을 가져옵니다. SGI는 올바른 순차적 계획의 신뢰성을 높이는 것 외에도 Plan-Bench 및 VQA와 같은 전통적인 계획 작업을 일반화합니다. 프로젝트 페이지 : https://shroglck.github.io/cos_plan/"
976,http://arxiv.org/abs/2512.11903 ,Aion: Towards Hierarchical 4D Scene Graphs with Temporal Flow Dynamics,"Iacopo Catalano, Eduardo Montijano, Javier Civera, Julio A. Placed, Jorge Pena-Queralta",동적 환경에서의 자율 탐색에는 의미 구조와 시간적 진화를 모두 포착하는 공간 표현이 필요합니다. 3D 장면 그래프(3DSG)는 기하학과 의미를 인코딩하는 계층적 다중 해상도 추상화를 제공하지만 역학에 대한 기존 확장은 주로 개별 개체나 에이전트에 중점을 둡니다. 이와 동시에 MoD(Maps of Dynamics)는 일반적인 동작 패턴과 시간적 규칙성을 모델링하지만 일반적으로 의미 인식이 부족하고 대규모 환경에 맞게 확장되지 않는 그리드 기반 이산화에 묶여 있습니다. 본 논문에서는 시간 흐름 역학을 계층적 3DSG 내에 직접 내장하여 시간 차원을 효과적으로 통합하는 프레임워크인 Aion을 소개합니다. Aion은 그래프 기반 희소 MoD 표현을 사용하여 임의의 시간 간격에 걸쳐 모션 흐름을 캡처하고 이를 장면 그래프의 탐색 노드에 연결하여 복잡한 동적 환경에서 계획 및 상호 작용을 개선하는 보다 해석 가능하고 확장 가능한 예측을 생성합니다.
975,http://arxiv.org/abs/2512.09215 ,View-on-Graph: Zero-shot 3D Visual Grounding via Vision-Language Reasoning on Scene Graphs,"Yuanyuan Liu, Haiyang Mei, Dongyang Zhan, Jiayue Zhao, Dongsheng Zhou, Bo Dong, Xin Yang","3DVG(3D 시각적 접지)는 언어 설명을 통해 3D 장면의 개체를 식별합니다. 기존 제로샷 접근 방식은 3D 공간 정보(SI)를 VLM 처리에 적합한 형식으로 변환하여 2D 비전 언어 모델(VLM)을 활용합니다. 일반적으로 지정된 뷰 렌더링 또는 중첩된 객체 마커가 있는 비디오 시퀀스와 같은 복합 입력입니다. 그러나 이 VLM + SI 패러다임은 VLM이 복잡한 큐 전체를 처리하도록 강요하는 얽힌 시각적 표현을 생성하므로 공간 의미론적 관계를 효과적으로 활용하기가 어렵습니다. 본 연구에서는 3D SI를 VLM이 추론 중에 필요한 것만 점진적으로 검색할 수 있는 형식으로 외부화하는 새로운 VLM x SI 패러다임을 제안합니다. 우리는 장면을 다중 모달, 다중 레이어 장면 그래프로 구성하고 VLM이 장면을 통과할 때 필요한 큐에 선택적으로 액세스하는 활성 에이전트로 작동할 수 있도록 하는 새로운 VoG(View-on-Graph) 방법으로 이 패러다임을 인스턴스화합니다. 이 디자인은 두 가지 본질적인 이점을 제공합니다. (i) VLM을 조밀하게 얽힌 시각적 입력과 혼동하는 대신 3D 컨텍스트를 공간적 및 의미론적으로 일관된 장면 그래프로 구조화함으로써 VLM의 추론 난이도를 낮춥니다. (ii) 장면 그래프를 적극적으로 탐색하고 추론함으로써 해석 가능한 3DVG에 대한 투명한 단계별 추적을 자연스럽게 생성합니다. 광범위한 실험을 통해 VoG는 최첨단 제로샷 성능을 달성하고 구조화된 장면 탐색을 제로샷 3DVG 발전을 위한 유망한 전략으로 확립하는 것으로 나타났습니다."
974,http://arxiv.org/abs/2512.05524 ,VOST-SGG: VLM-Aided One-Stage Spatio-Temporal Scene Graph Generation,"Chinthani Sugandhika, Chen Li, Deepu Rajan, Basura Fernando","ST-SGG(시공간 장면 그래프 생성)는 비디오 프레임 전반에 걸쳐 개체와 객체의 진화 관계를 모델링하는 것을 목표로 하며 비디오 캡션 및 시각적 질문 답변과 같은 다운스트림 추론 작업에 대한 해석 가능한 표현을 가능하게 합니다. DETR 스타일 단일 스테이지 ST-SGG 모델의 최근 발전에도 불구하고 여전히 몇 가지 주요 제한 사항이 있습니다. 첫째, 이러한 모델은 주의 기반 학습 가능 쿼리를 핵심 구성 요소로 사용하지만 이러한 학습 가능 쿼리는 의미상 정보가 없으며 인스턴스에 구애받지 않고 초기화됩니다. 둘째, 이러한 모델은 술어 분류를 위해 단봉형 시각적 특징에만 의존합니다. 이러한 과제를 해결하기 위해 우리는 VLM(비전 언어 모델)의 상식 추론 기능을 ST-SGG 파이프라인에 통합하는 VLM 지원 1단계 ST-SGG 프레임워크인 VOST-SGG를 제안합니다. 먼저, 무엇을 어디에 주의해야 하는지를 분리하여 의미론적으로 기반한 무엇-어디 추론을 가능하게 하는 이중 소스 쿼리 초기화 전략을 소개합니다. 또한, 우리는 향상된 술어 분류를 위해 VLM에서 파생된 시각적, 텍스트 및 공간적 단서를 융합하는 다중 모드 기능 은행을 제안합니다. Action Genome 데이터 세트에 대한 광범위한 실험은 우리의 접근 방식이 최첨단 성능을 달성하고 ST-SGG에 대한 VLM 지원 의미 사전 및 다중 모드 기능 통합의 효율성을 검증함을 보여줍니다. https://github.com/LUNAProject22/VOST에서 코드를 공개할 예정입니다."
973,http://arxiv.org/abs/2512.03913 ,Hierarchical Vision Language Action Model Using Success and Failure Demonstrations,"Jeongeun Park, Jihwan Yoon, Byungwoo Jeon, Juhan Park, Jinwoo Shin, Namhoon Cho, Kyungjae Lee, Sangdoo Yun, Sungjoon Choi","이전 VLA(Vision-Language-Action) 모델은 일반적으로 데이터 수집 중에 자연스럽게 발생하는 수많은 실패한 시도를 삭제하는 동시에 원격 조작으로 성공한 데모에 대해 교육을 받았습니다. 그러나 이러한 실패는 정책이 취약할 수 있는 위치와 방법, 견고성을 향상하기 위해 활용할 수 있는 정보를 인코딩합니다. 우리는 혼합 품질 데이터 세트를 활용하여 계획 시 실패 인식 추론을 학습함으로써 이 문제를 해결합니다. 계층적 강화 학습 형식 하에서 높은 수준의 추론(시스템 2)과 낮은 수준의 제어(시스템 1)를 분리하여 실패를 시끄러운 감독이 아닌 구조화된 학습 신호로 사용할 수 있게 만드는 계층적 비전-언어-행동 모델인 VINE을 소개합니다. 시스템 2는 2D 장면 그래프 추상화를 통해 타당성 기반 트리 검색을 수행합니다. 하위 목표 전환을 제안하고, 성공과 실패 모두에서 성공 확률을 예측하고, 실행 전에 부서지기 쉬운 가지를 잘라내어 계획 평가를 타당성 점수로 효과적으로 캐스팅합니다. 선택한 하위 목표 시퀀스는 에이전트의 핵심 기술을 수정하지 않고 낮은 수준의 작업을 실행하는 시스템 1로 전달됩니다. 오프라인 원격조작 데이터를 통해 전적으로 교육받은 VINE은 부정적인 경험을 의사결정 루프에 직접 통합합니다. 까다로운 조작 작업 전반에 걸쳐 이 접근 방식은 성공률과 견고성을 지속적으로 향상시켜 실패 데이터가 VLA의 광범위한 역량을 강력한 실행으로 전환하는 데 필수적인 리소스임을 입증합니다."
972,http://arxiv.org/abs/2512.03474 ,Procedural Mistake Detection via Action Effect Modeling,"Wenliang Guo, Yujiang Pu, Yu Kong","절차적 작업의 실수 감지는 학습 및 작업 실행을 지원하는 지능형 시스템을 구축하는 데 필수적입니다. 기존 접근 방식은 주로 작업이 수행되는 방식을 분석하는 반면, 작업이 생성하는 것, 즉 \textbf{액션 효과}를 간과합니다. 그러나 많은 오류는 실행 자체가 아니라 의도하지 않은 객체 상태나 잘못된 공간 배치와 같은 결과 결과에서 나타납니다. 이러한 격차를 해소하기 위해 우리는 확률적 공식을 통해 작업 실행과 그 결과를 공동으로 포착하는 통합 프레임워크인 AEM(Action Effect Modeling)을 제안합니다. AEM은 먼저 의미적 관련성과 시각적 품질을 기반으로 가장 유익한 효과 프레임을 선택하여 작업의 결과를 식별합니다. 그런 다음 시각적 기반 및 상징적 장면 그래프에서 보완적인 단서를 추출하여 공유된 잠재 공간에 정렬하여 강력한 효과 인식 표현을 형성합니다. 실수를 감지하기 위해 작업별 프롬프트를 통합하고 각 작업 세그먼트를 의도된 실행 의미에 맞게 정렬하는 프롬프트 기반 감지기를 추가로 설계합니다. 우리의 접근 방식은 까다로운 OCC(One-Class Classification) 설정에서 EgoPER 및 CaptainCook4D 벤치마크에서 최첨단 성능을 달성합니다. 이러한 결과는 실행과 결과를 모두 모델링하면 보다 안정적인 실수 감지가 가능하고 효과 인식 표현의 잠재력을 강조하여 광범위한 다운스트림 애플리케이션에 이점을 제공한다는 것을 보여줍니다."
971,http://arxiv.org/abs/2512.03422 ,What Is The Best 3D Scene Representation for Robotics? From Geometric to Foundation Models,"Tianchen Deng, Yue Pan, Shenghai Yuan, Dong Li, Chen Wang, Mingrui Li, Long Chen, Lihua Xie, Danwei Wang, Jingchuan Wang, Javier Civera, Hesheng Wang, Weidong Chen","이 백서에서는 포인트 클라우드, 복셀, 부호 있는 거리 함수(SDF), 장면 그래프와 같은 전통적인 표현뿐만 아니라 NeRF(Neural Radiance Fields), 3DGS(3D Gaussian Splatting) 및 새로운 기초 모델과 같은 최신 신경 표현을 다루는 로봇공학의 기존 장면 표현 방법에 대한 포괄적인 개요를 제공합니다. 현재 SLAM 및 위치 파악 시스템은 주로 포인트 클라우드 및 복셀과 같은 희소 표현에 의존하지만 조밀한 장면 표현은 탐색 및 장애물 회피와 같은 다운스트림 작업에서 중요한 역할을 할 것으로 예상됩니다. 또한 NeRF, 3DGS 및 기초 모델과 같은 신경 표현은 높은 수준의 의미론적 특징과 언어 기반 사전 정보를 통합하는 데 매우 적합하여 보다 포괄적인 3D 장면 이해 및 구체화된 지능을 가능하게 합니다. 본 논문에서는 로봇공학의 핵심 모듈을 5가지 부분(Perception, Mapping, Localization, Navigation, Manipulation)으로 분류했습니다. 우리는 다양한 장면 표현 방법의 표준 공식을 제시하고 다양한 모듈에 걸쳐 장면 표현의 장점과 단점을 비교하는 것으로 시작합니다. 이 설문조사는 다음 질문에 중점을 두고 있습니다: 로봇 공학을 위한 최고의 3D 장면 표현은 무엇입니까? 그런 다음 3D 기초 모델이 미래 로봇 애플리케이션을 위한 통합 솔루션으로서 현재 방법을 대체할 수 있는 방법에 특히 중점을 두고 3D 장면 표현의 미래 개발 동향에 대해 논의합니다. 이 모델을 완전히 실현하는 데 남은 과제도 살펴봅니다. 우리는 신입 연구원과 숙련된 연구원 모두에게 3D 장면 표현의 미래와 로봇 공학에서의 적용을 탐색할 수 있는 귀중한 리소스를 제공하는 것을 목표로 합니다. 우리는 GitHub에 오픈 소스 프로젝트를 게시했으며 이 프로젝트에 새로운 작업과 기술을 계속 추가할 것입니다."
970,http://arxiv.org/abs/2512.01975 ,SGDiff: Scene Graph Guided Diffusion Model for Image Collaborative SegCaptioning,"Xu Zhang, Jin Yuan, Hanwang Zhang, Guojin Zhong, Yongsheng Zang, Jiacheng Lin, Zhiyong Li","캡션 또는 분할과 같은 제어 가능한 이미지 의미 이해 작업에서는 사용자가 고유한 결과를 예측하기 위해 프롬프트(예: 텍스트 또는 경계 상자)를 입력해야 하므로 비용이 많이 드는 프롬프트 입력 또는 제한된 정보 출력과 같은 문제가 발생합니다. 이 논문에서는 객체 주위의 경계 상자와 같은 간단한 프롬프트를 (캡션, 마스크) 쌍으로 표시되는 다양한 의미 해석으로 변환하여 사용자가 유연한 결과를 선택할 수 있도록 하는 것을 목표로 하는 새로운 작업 ""이미지 협업 분할 및 캡션""(SegCaptioning)을 소개합니다. 이 작업은 최소한의 프롬프트에서 사용자의 의도를 정확하게 포착하는 동시에 의미상 정렬된 여러 캡션 단어와 마스크를 예측하는 등 중요한 과제를 제기합니다. 기술적으로 우리는 상관된 마스크 캡션 예측을 위해 구조화된 장면 그래프 기능을 활용하는 새로운 장면 그래프 유도 확산 모델을 제안합니다. 처음에는 사용자의 프롬프트를 장면 그래프에 매핑하여 사용자의 의도를 효과적으로 포착하는 Prompt-Centric Scene Graph Adapter를 도입했습니다. 이어서 장면 그래프 유도 바이모달 변환기를 통합한 확산 프로세스를 사용하여 상호 연관된 캡션-마스크 쌍 사이의 복잡한 상관 관계를 찾아 예측합니다. 정확한 정렬을 보장하기 위해 모달 간 유사성을 고려하여 시각적 및 텍스트 엔터티를 명시적으로 정렬하도록 다중 엔터티 대조 학습 손실을 설계하여 잘 정렬된 캡션-마스크 쌍을 생성합니다. 두 개의 데이터 세트에 대해 수행된 광범위한 실험에서는 SGDiff가 SegCaptioning에서 탁월한 성능을 달성하여 최소한의 프롬프트 입력으로 캡션 및 분할 작업 모두에서 유망한 결과를 얻을 수 있음을 보여줍니다."
969,http://arxiv.org/abs/2512.00936 ,SceneProp: Combining Neural Network and Markov Random Field for Scene-Graph Grounding,"Keita Otani, Tatsuya Harada","여러 개체 및 관계를 사용하여 복잡하고 구성적인 시각적 쿼리를 기반으로 하는 것은 비전 언어 모델의 근본적인 과제입니다. 표준 구문 접지 방법은 단일 개체를 지역화하는 데 탁월하지만 복잡한 관계 설명을 구문 분석하는 구조적 귀납적 편향이 부족하여 쿼리가 더 설명적이 되면서 종종 실패합니다. 이러한 구조적 결함을 해결하기 위해 우리는 쿼리가 객체와 그 관계에 대한 명시적인 그래프인 강력하지만 덜 탐구된 공식인 장면 그래프 접지에 중점을 둡니다. 그러나 이 작업을 위한 기존 방법도 어려움을 겪고 있습니다. 역설적으로 쿼리 그래프가 커짐에 따라 성능이 저하되어 접지를 더 쉽게 만드는 바로 그 정보를 활용하지 못합니다. MRF(Markov Random Field)의 MAP(Maximum a Posteriori) 추론 문제로 장면 그래프 접지를 재구성하여 이 문제를 해결하는 새로운 방법인 SceneProp을 소개합니다. 전체 쿼리 그래프에 대해 전역 추론을 수행함으로써 SceneProp은 모든 제약 조건을 공동으로 충족하는 노드에 대한 이미지 영역의 최적 할당을 찾습니다. 이는 신념 전파 알고리즘의 차별화 가능한 구현을 통해 엔드투엔드 프레임워크 내에서 달성됩니다. 4가지 벤치마크에 대한 실험에서는 장면 그래프 접지 공식에 중점을 두어 SceneProp이 이전 작업보다 훨씬 뛰어난 성능을 발휘할 수 있음을 보여줍니다. 비판적으로, 정확도는 쿼리 그래프의 크기와 복잡성에 따라 지속적으로 향상되며, 더 많은 관계형 컨텍스트가 더 나은 기반으로 이어질 수 있고 그래야 한다는 것을 처음으로 보여줍니다. 코드는 https://github.com/keitaotani/SceneProp에서 확인할 수 있습니다."
968,http://arxiv.org/abs/2512.00565 ,Describe Anything Anywhere At Any Moment,"Nicolas Gorlo, Lukas Schmid, Luca Carlone","증강 현실부터 대규모 환경의 로봇 자율성에 이르는 컴퓨터 비전 및 로봇 공학 애플리케이션에는 정확한 언어 기반뿐만 아니라 의미론적 세부 사항을 위한 기하학적 구조를 모두 캡처하는 시공간 메모리 프레임워크가 필요합니다. 기존 방법은 풍부한 개방형 어휘 설명을 생성하는 것이 이러한 설명을 3D에 기반해야 할 때 실시간 성능을 희생한다는 절충점에 직면해 있습니다. 이러한 문제를 해결하기 위해 우리는 대규모 실시간 4D 장면 이해를 위한 새로운 시공간 메모리 프레임워크인 DAAAM(Describe Anything, Anywhere, at Any Moment)을 제안합니다. DAAAM은 DAM(Describe Anything Model)과 같은 지역화된 캡션 모델에서 자세한 의미 체계 설명을 추론하기 위한 새로운 최적화 기반 프런트엔드를 도입하고 일괄 처리를 활용하여 온라인 처리에 대한 추론 속도를 몇 배나 높입니다. 이러한 의미론적 이해를 활용하여 계층적 4D 장면 그래프(SG)를 구축합니다. 이는 효과적인 전역적으로 공간적, 시간적으로 일관된 메모리 표현 역할을 합니다. DAAAM은 실시간 성능을 유지하면서 자세하고 기하학적으로 기반이 되는 설명으로 4D SG를 구성합니다. 우리는 DAAAM의 4D SG가 추론 및 추론을 위한 도구 호출 에이전트와 잘 인터페이스된다는 것을 보여줍니다.   우리는 NaVQA 벤치마크에 대한 시공간 질문 답변의 복잡한 작업에서 DAAAM을 철저하게 평가하고 SG3D 벤치마크를 기반으로 하는 순차 작업에 대한 일반화 기능을 보여줍니다. 우리는 대규모 및 장기 평가를 위해 확장된 OC-NaVQA 벤치마크를 추가로 관리합니다. DAAAM은 가장 경쟁적인 기준에 비해 OC-NaVQA 질문 정확도를 각각 53.6%, 위치 오류를 21.9%, 시간 오류를 21.6%, SG3D 작업 접지 정확도를 27.8% 향상시켜 두 작업 모두에서 최첨단 결과를 달성했습니다. 우리는 데이터와 코드를 오픈 소스로 공개합니다."
967,http://arxiv.org/abs/2512.00294 ,Words into World: A Task-Adaptive Agent for Language-Guided Spatial Retrieval in AR,"Lixing Guo, Tobias Höllerer","기존의 증강 현실(AR) 시스템은 주로 고정 클래스 감지기 또는 기준 마커에 의존하므로 복잡하고 개방적인 어휘 자연어 쿼리를 해석하는 능력이 제한됩니다. 우리는 MLLM(Multimodal Large Language Model)을 기반 비전 모델과 통합하여 공간에서의 관계 추론과 물리적 환경에서 언어 조건부 공간 검색을 가능하게 하는 모듈형 AR 에이전트 시스템을 제시합니다. 당사의 적응형 작업 에이전트는 MLLM과 좌표 인식 도구를 조정하여 간단한 객체 식별부터 다중 객체 관계 추론에 이르기까지 다양한 쿼리 복잡성을 해결하는 동시에 미터 단위의 정확한 3D 앵커를 반환합니다. 9가지 유형의 관계(공간, 구조적 의미, 인과적 기능)를 인코딩하는 동적 AR 장면 그래프를 구성하여 MLLM이 객체가 존재하는 것뿐만 아니라 객체가 3D 공간에서 어떻게 관련되고 상호 작용하는지 이해할 수 있도록 합니다. 작업 적응형 관심 영역 강조 및 상황별 공간 검색을 통해 시스템은 인간의 관심을 정보 밀도가 높은 영역으로 안내하는 동시에 인간 참여형(Human-In-The-Loop) 개선을 지원합니다. 에이전트는 물리적 작업에서 복잡한 쿼리 선택, 측정, 비교 및 ​​작동 기반 언어 이해를 위한 좌표 인식 도구를 동적으로 호출합니다. 모듈식 아키텍처는 재교육 없이 플러그 앤 사용 비전 언어 모델을 지원하여 AR 에이전트를 대화형 장면 이해를 위한 실제 공간 지능으로 MLLM을 강화하는 중개자로 설정합니다. 또한 다양한 환경에 걸친 언어 중심의 실제 지역화 및 관계 접지를 위한 평가 프레임워크인 GroundedAR-Bench를 소개합니다."
966,http://arxiv.org/abs/2511.23304 ,Multi-Modal Scene Graph with Kolmogorov-Arnold Experts for Audio-Visual Question Answering,"Zijian Fu, Changsheng Lv, Mengshi Qi, Huadong Ma","본 논문에서는 SHRIKE(Audio-Visual Question Answering)를 위한 Kolmogorov-Arnold 전문가 네트워크를 사용한 새로운 다중 모드 장면 그래프를 제안합니다. 이 작업은 시청각 장면에서 정보를 추출하고 융합하여 인간의 추론을 모방하는 것을 목표로 하며, 주요 과제는 복잡한 시청각 콘텐츠에서 질문 관련 단서를 식별하는 것입니다. 기존 방법은 비디오 내의 구조적 정보를 캡처하지 못하고 다중 모드 기능에 대한 세밀한 모델링이 불충분합니다. 이러한 문제를 해결하기 위해 우리는 시청각 장면의 시각적 기반, 구조적 표현으로 객체와 객체의 관계를 명시적으로 모델링하는 새로운 다중 모드 장면 그래프를 최초로 도입했습니다. 또한 KAN(Kolmogorov-Arnold Network) 기반의 MoE(Mixture of Experts)를 설계하여 시간적 통합 단계의 표현력을 향상시킵니다. 이를 통해 질문 인식 융합 시청각 표현 내에서 교차 모달 상호 작용을 보다 세밀하게 모델링할 수 있어 더욱 풍부하고 미묘한 패턴을 포착한 다음 시간적 추론 성능을 향상시킬 수 있습니다. 우리는 확립된 MUSIC-AVQA 및 MUSIC-AVQA v2 벤치마크에서 모델을 평가하여 최첨단 성능을 달성합니다. 코드 및 모델 체크포인트는 공개적으로 공개됩니다."
965,http://arxiv.org/abs/2511.20937 ,ENACT: Evaluating Embodied Cognition with World Modeling of Egocentric Interaction,"Qineng Wang, Wenlong Huang, Yu Zhou, Hang Yin, Tianwei Bao, Jianwen Lyu, Weiyu Liu, Ruohan Zhang, Jiajun Wu, Li Fei-Fei, Manling Li","구체화된 인지는 지능이 수동적 관찰보다는 감각운동 상호작용에서 발생한다고 주장합니다. 이는 흥미로운 질문을 제기합니다. 주로 실체화되지 않은 방식으로 훈련된 현대 시각 언어 모델(VLM)이 구체화된 인지의 징후를 나타냅니까? 우리는 시각적 질문 응답(VQA) 형식의 자기 중심적 상호 작용을 통해 구현된 인지를 세계 모델링으로 평가하는 벤치마크인 ENACT를 소개합니다. 작업이 장면 그래프 변경인 부분적으로 관찰 가능한 마르코프 결정 프로세스(POMDP)로 구성된 ENACT는 두 가지 보완적인 시퀀스 재정렬 작업, 즉 순방향 모델링(작업이 지정된 셔플된 관찰 순서 변경)과 역세계 모델링(관찰된 셔플된 작업 재정렬)으로 구성됩니다. 개념적으로는 간단하지만 이러한 과제를 해결하려면 평가를 혼란스럽게 할 수 있는 낮은 수준의 이미지 합성을 피하면서 부분적으로 관찰 가능한 자기중심적 입력에서 구체화된 인지-순응성 인식, 행동 효과 추론, 구체화된 인식 및 대화형 장지평 기억에 핵심적인 기능이 암묵적으로 요구됩니다. 우리는 로봇 시뮬레이션(BEHAVIOR)에서 QA 쌍을 합성하고 장기적인 가정 규모 활동에 걸쳐 8,972개의 QA 쌍에 대한 모델을 평가하는 확장 가능한 파이프라인을 제공합니다. 실험을 통해 프론티어 VLM과 인간 사이의 성능 격차가 상호 작용 범위에 따라 확대되는 것으로 나타났습니다. 모델은 전진 작업보다 역 작업에서 일관되게 더 나은 성능을 발휘하며 오른손잡이 작업에 대한 선호와 카메라 고유 기능 또는 관점이 인간의 시각에서 벗어날 때 성능 저하를 포함하여 인간 중심적 편견을 나타냅니다. 웹사이트 https://enact-embodied-cognition.github.io/."
964,http://arxiv.org/abs/2511.20201 ,GHR-VQA: Graph-guided Hierarchical Relational Reasoning for Video Question Answering,"Dionysia Danai Brilli, Dimitrios Mallis, Vassilis Pitsikalis, Petros Maragos","우리는 비디오 시퀀스 내에서 복잡한 인간-객체 상호 작용을 포착하기 위해 장면 그래프를 통합하는 새로운 인간 중심 프레임워크인 비디오 질문 응답(비디오 QA)을 위한 그래프 기반 계층 관계 추론인 GHR-VQA를 제안합니다. 기존의 픽셀 기반 방식과 달리 각 프레임은 장면 그래프로 표현되고, 프레임 전체의 휴먼 노드는 글로벌 루트에 연결되어 비디오 수준의 그래프를 형성하고 휴먼 액터 중심의 프레임 간 추론이 가능합니다. 그런 다음 비디오 수준 그래프는 그래프 신경망(GNN)에 의해 처리되어 효율적인 처리를 위해 풍부한 상황 인식 임베딩으로 변환됩니다. 마지막으로 이러한 임베딩은 다양한 추상화 수준에서 작동하는 계층적 네트워크의 질문 기능과 통합되어 비디오 콘텐츠에 대한 로컬 및 글로벌 이해를 향상시킵니다. 인간에 뿌리를 둔 이 명시적인 구조는 행동을 인간-대상 상호 작용으로 분해하여 해석 가능성을 향상시키고 시공간 역학에 대한 보다 심오한 이해를 가능하게 합니다. 우리는 AGQA(Action Genome Question Answering) 데이터 세트에 대한 접근 방식을 검증하여 최첨단 객체 관계 추론의 7.3% 향상을 포함하여 상당한 성능 개선을 달성했습니다."
963,http://arxiv.org/abs/2511.18817 ,Disc3D: Automatic Curation of High-Quality 3D Dialog Data via Discriminative Object Referring,"Siyuan Wei, Chunjie Wang, Xiao Liu, Xiaosheng Yan, Zhishan Zhou, Rui Huang","3D 다중 모드 대형 언어 모델(MLLM)은 대규모의 고품질 3D 장면-대화 데이터 세트가 여전히 부족하기 때문에 여전히 2D 동료 모델에 비해 뒤떨어져 있습니다. 사전 노력은 비용이 많이 드는 인간 주석에 달려 있으며 두 가지 주요 모호함, 즉 공간 언어가 알 수 없는 카메라 포즈를 추정하는 시점 모호성과 비배타적 설명으로 인해 대상과 산만함 사이의 경계가 모호해지는 객체 참조 모호함이라는 두 가지 주요 모호성을 해결되지 않은 상태로 둡니다. 따라서 우리는 원시 3D 스캔을 이전 비용의 일부만으로 명확한 고품질 대화 데이터로 변환하는 완전 자동화된 파이프라인을 제시합니다. 규칙 기반 제약 조건을 2D MLLM 및 LLM과 시너지 효과를 발휘함으로써 파이프라인은 사람의 개입 없이 제어 및 확장 가능한 생성을 가능하게 합니다. 파이프라인은 (1) 개체, 프레임 및 장면 수준 캡션을 수집하는 메타 주석 수집, (2) 근접 개체 관계를 캡처하기 위한 관계 수정이 포함된 장면 그래프 구성, (3) 배타적이고 간결한 설명을 생성하는 식별 개체 참조, (4) 다양한 대화를 합성하는 다중 작업 데이터 생성의 4단계로 구성됩니다. 우리의 파이프라인은 소스 데이터 세트의 본질적인 결함을 체계적으로 완화하고 최종 Disc3D 데이터 세트, 25K 하이브리드 3D 장면, 장면, 뷰 및 개체 캡션, 시각적 접지 및 5가지 개체 중심 QA 작업에 대한 200만 개 이상의 샘플을 생성합니다. 광범위한 실험을 통해 Disc3D를 사용한 교육을 통해 공개 벤치마크와 다각적인 Disc3D-QA 작업 모두에서 일관되고 상당한 개선이 이루어졌음을 보여줍니다. 코드, 데이터, 모델은 공개적으로 이용 가능합니다."
962,http://arxiv.org/abs/2511.18734 ,Yo'City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion,"Keyang Lu, Sifan Zhou, Hongbin Xu, Gang Xu, Zhifei Yang, Yikai Wang, Zhen Xiao, Jieyi Long, Ming Li","사실적인 3D 도시 생성은 가상 현실과 디지털 트윈을 포함한 광범위한 애플리케이션의 기본입니다. 그러나 대부분의 기존 방법은 단일 확산 모델 학습에 의존하므로 개인화되고 무한한 도시 규모 장면을 생성하는 능력이 제한됩니다. 본 논문에서는 기성 대형 모델의 추론 및 구성 기능을 활용하여 사용자 맞춤형 및 무한 확장 가능한 3D 도시 생성을 가능하게 하는 새로운 에이전트 프레임워크인 Yo'City를 제시합니다. 구체적으로 Yo'City는 먼저 계층적인 ""City-District-Grid"" 구조를 정의하는 하향식 계획 전략을 통해 도시를 개념화합니다. 글로벌 플래너는 전체 레이아웃과 잠재적인 기능 구역을 결정하고, 로컬 디자이너는 상세한 그리드 수준 설명을 통해 각 구역을 더욱 구체화합니다. 그 후, 그리드 수준 3D 생성은 ""생성-정제-평가"" 아이소메트릭 이미지 합성 루프를 통해 달성되고, 이어서 이미지-3D 생성이 이루어집니다. 지속적인 도시 진화를 시뮬레이션하기 위해 Yo'City는 장면 그래프 기반 거리 및 의미 인식 레이아웃 최적화를 수행하여 공간적으로 일관된 도시 성장을 보장하는 사용자 대화형 관계 기반 확장 메커니즘을 추가로 도입합니다. 우리의 방법을 종합적으로 평가하기 위해 다양한 벤치마크 데이터세트를 구성하고 의미론, 기하학, 질감 및 레이아웃의 관점에서 생성 품질을 평가하는 6개의 다차원 측정항목을 설계합니다. 광범위한 실험을 통해 Yo'City가 모든 평가 측면에서 기존의 최첨단 방법보다 지속적으로 뛰어난 성능을 발휘한다는 사실이 입증되었습니다."
961,http://arxiv.org/abs/2511.18378 ,Synthetic Curriculum Reinforces Compositional Text-to-Image Generation,"Shijian Wang, Runhao Fu, Siyi Zhao, Qingqin Zhan, Xingjian Wang, Jiarui Jin, Yuan Lu, Hanqian Wu, Cunjian Chen","T2I(텍스트-이미지) 생성은 오랫동안 공개된 문제였으며 구성 합성이 특히 어려운 문제로 남아 있습니다. 이 작업을 수행하려면 다양한 속성과 복잡한 공간 및 의미 관계를 나타내는 여러 객체가 포함된 복잡한 장면을 정확하게 렌더링해야 하며, 정확한 객체 배치와 일관된 객체 간 상호 작용이 모두 필요합니다. 본 논문에서는 기존 T2I 모델의 구성적 약점을 해결하는 CompGen이라는 새로운 구성 커리큘럼 강화 학습 프레임워크를 제안합니다. 특히 장면 그래프를 활용하여 구성 능력에 대한 새로운 난이도 기준을 설정하고 해당 적응형 Markov Chain Monte Carlo 그래프 샘플링 알고리즘을 개발합니다. 이러한 난이도 인식 접근 방식을 통해 강화 학습을 통해 T2I 모델을 점진적으로 최적화하는 교육 커리큘럼 데이터를 합성할 수 있습니다. 우리는 커리큘럼 학습 접근 방식을 GRPO(그룹 상대 정책 최적화)에 통합하고 다양한 커리큘럼 일정 전략을 조사합니다. 우리의 실험에 따르면 CompGen은 다양한 커리큘럼 일정 전략 하에서 뚜렷한 스케일링 곡선을 나타내며, 쉽고 어려운 가우스 샘플링 전략은 무작위 샘플링에 비해 우수한 스케일링 성능을 제공합니다. 광범위한 실험을 통해 CompGen이 확산 기반 및 자동 회귀 T2I 모델 모두에 대한 구성 생성 기능을 크게 향상시켜 구성 T2I 생성 시스템을 개선하는 효과가 강조되었음을 보여줍니다."
960,http://arxiv.org/abs/2511.15948 ,Click2Graph: Interactive Panoptic Video Scene Graphs from a Single Click,"Raphael Ruschel, Hardikkumar Prajapati, Awsafur Rahman, B. S. Manjunath","최첨단 VSGG(비디오 장면 그래프 생성) 시스템은 구조화된 시각적 이해를 제공하지만 사람의 안내를 통합할 수 없는 폐쇄형 피드포워드 파이프라인으로 작동합니다. 대조적으로, SAM2와 같은 신속한 세분화 모델은 정확한 사용자 상호 작용을 가능하게 하지만 의미론적 또는 관계형 추론이 부족합니다. 시각적 메시지와 공간적, 시간적, 의미적 이해를 통합하는 PVSG(Panoptic Video Scene Graph Generation)를 위한 최초의 대화형 프레임워크인 Click2Graph를 소개합니다. 클릭이나 경계 상자와 같은 단일 사용자 큐에서 Click2Graph는 시간에 따라 주제를 분할 및 추적하고, 상호 작용하는 개체를 자동으로 발견하고, <주체, 개체, 조건자> 삼중항을 예측하여 시간적으로 일관된 장면 그래프를 형성합니다. 우리의 프레임워크는 두 가지 주요 구성 요소를 도입합니다. 주제 조건 개체 프롬프트를 생성하는 동적 상호 작용 발견 모듈과 공동 엔터티와 술어 추론을 수행하는 의미 분류 헤드입니다. OpenPVSG 벤치마크에 대한 실험은 Click2Graph가 사용자 안내 PVSG를 위한 강력한 기반을 구축하고 제어 가능하고 해석 가능한 비디오 장면 이해를 가능하게 하기 위해 인간 프롬프트가 어떻게 팬옵틱 접지 및 관계형 추론과 결합될 수 있는지 보여줍니다."
959,http://arxiv.org/abs/2511.15288 ,Edge-Centric Relational Reasoning for 3D Scene Graph Prediction,"Yanni Ma, Hao Liu, Yulan Guo, Theo Gevers, Martin R. Oswald","3D 장면 그래프 예측은 복잡한 3D 환경을 객체와 객체의 쌍별 관계로 구성된 구조화된 그래프로 추상화하는 것을 목표로 합니다. 기존 접근 방식은 일반적으로 연결된 객체 노드에서 메시지를 집계하여 관계 가장자리 기능이 반복적으로 업데이트되는 객체 중심 그래프 신경망을 채택합니다. 그러나 이 설계는 본질적으로 관계 표현을 쌍 개체 컨텍스트로 제한하므로 정확한 관계 예측에 필수적인 고차 관계 종속성을 캡처하기 어렵습니다. 이러한 한계를 해결하기 위해 우리는 관계 수준 컨텍스트에서 개체 수준 이해까지 점진적인 추론을 가능하게 하는 객체 인식 융합을 갖춘 링크 유도 에지 중심 관계 추론 프레임워크, 즉 LEO를 제안합니다. 구체적으로 LEO는 먼저 객체 쌍 간의 잠재적 링크를 예측하여 관련 없는 에지를 억제한 다음 원본 장면 그래프를 각 관계가 노드로 처리되는 선 그래프로 변환합니다. 선 그래프 신경망을 적용하여 에지 중심 관계 추론을 수행하여 상호 관계 컨텍스트를 포착합니다. 강화된 관계 특징은 이후에 원래의 객체 중심 그래프에 통합되어 객체 수준 추론을 향상하고 관계 예측을 향상시킵니다. 우리의 프레임워크는 모델에 구애받지 않으며 기존의 객체 중심 방법과 통합될 수 있습니다. 두 가지 경쟁 기준선을 사용하는 3DSSG 데이터 세트에 대한 실험은 일관된 개선을 보여 에지-객체 추론 패러다임의 효율성을 강조합니다."
958,http://arxiv.org/abs/2511.14884 ,GeoSceneGraph: Geometric Scene Graph Diffusion Model for Text-guided 3D Indoor Scene Synthesis,"Antonio Ruiz, Tao Wu, Andrew Melnik, Qing Cheng, Xuqin Wang, Lu Liu, Yongliang Wang, Yanfeng Zhang, Helge Ritter","텍스트 프롬프트에서 실내 3D 장면을 합성하는 방법은 영화 제작, 인테리어 디자인, 비디오 게임, 가상 현실 및 구현된 에이전트 교육을 위한 합성 데이터 생성에 광범위하게 적용됩니다. 기존 접근 방식은 일반적으로 생성 모델을 처음부터 학습하거나 VLM(비전 언어 모델)을 활용합니다. VLM은 특히 복잡하거나 개방형 프롬프트의 경우 강력한 성능을 달성하지만 확장 현실(XR) 안경이나 휴대폰과 같이 리소스가 제한된 장치에 배포하려면 더 작은 작업별 모델이 필요합니다. 그러나 처음부터 훈련하는 많은 생성적 접근 방식은 장면 일관성과 사실성을 제한할 수 있는 실내 장면의 고유한 그래프 구조를 간과합니다. 반대로, 장면 그래프를 통합하는 방법은 일반적으로 불편하고 제한적인 사용자 제공 의미 그래프를 요구하거나 실제 관계 주석에 의존하여 보다 다양한 개체 상호 작용을 캡처하는 능력을 제한합니다. 이러한 과제를 해결하기 위해 사전 정의된 관계 클래스에 의존하지 않고 3D 장면의 그래프 구조와 기하학적 대칭을 활용하여 텍스트 프롬프트에서 3D 장면을 합성하는 방법인 GeoSceneGraph를 소개합니다. 실측 관계를 사용하지 않음에도 불구하고 GeoSceneGraph는 사용하는 방법에 필적하는 성능을 달성합니다. 우리 모델은 EGNN(등변 그래프 신경망)을 기반으로 구축되었지만 기존 EGNN 접근 방식은 일반적으로 저차원 조건화로 제한되며 텍스트와 같은 복잡한 양식을 처리하도록 설계되지 않았습니다. 우리는 텍스트 특징에 따라 EGNN을 조절하기 위한 간단하고 효과적인 전략을 제안하고 절제 연구를 통해 디자인을 검증합니다."
957,http://arxiv.org/abs/2511.14430 ,Abstract Scene Graphs: Formalizing and Monitoring Spatial Properties of Automated Driving Functions,"Ishan Saxena, Bernd Westphal, Martin Fränzle",자동 운전 기능(ADF)은 공공 도로에서 운전하는 동안 다양한 복잡성의 공간 속성을 준수해야 합니다. 이러한 상황은 본질적으로 안전이 중요하므로 ADF가 공간 특성을 준수하는지 지속적으로 확인해야 합니다. 복잡성으로 인해 이러한 공간 속성은 자동화된 검사가 가능하도록 공식화되어야 합니다. 장면 그래프(SG)를 사용하면 교통 장면에 존재하는 객체와 객체 간의 공간적 관계를 명시적으로 구조적으로 표현할 수 있습니다. 본 논문에서는 SG 구성을 기반으로 ADF의 공간 속성을 공식화하기 위한 ASG(Abstract Scene Graph) 형식을 제안합니다. ASG를 사용하여 공간 속성을 공식화하는 방법을 실제 사례를 통해 보여줍니다. 마지막으로 ASG를 사용하여 ADF의 런타임 모니터링을 수행하는 프레임워크를 제시합니다. 이를 위해 ASG로 공식화된 공간 속성이 ADF 시스템 동작에 의해 어떻게 충족될 수 있는지 알고리즘적으로 보여줍니다.
956,http://arxiv.org/abs/2511.14004 ,Searching in Space and Time: Unified Memory-Action Loops for Open-World Object Retrieval,"Taijing Chen, Sateesh Kumar, Junhong Xu, Georgios Pavlakos, Joydeep Biswas, Roberto Martín-Martín","서비스 로봇은 요청이 속성(""빨간 머그""), 공간적 맥락(""테이블 위의 머그"") 또는 과거 상태(""어제 여기에 있었던 머그"")를 참조할 수 있는 동적 개방형 환경에서 개체를 검색해야 합니다. 기존 접근 방식은 이 문제의 일부만 포착합니다. 장면 그래프는 공간 관계를 캡처하지만 시간 기반을 무시하고, 시간 추론 방법은 역학을 모델링하지만 구현된 상호 작용을 지원하지 않으며, 동적 장면 그래프는 두 가지를 모두 처리하지만 고정된 어휘를 사용하여 닫힌 세계로 유지됩니다. 우리는 단일 결정 루프 내에서 메모리 쿼리와 구현된 작업을 통합하는 프레임워크인 STAR(SpatioTemporal Active Retrieval)를 제시합니다. STAR는 비모수적 장기 기억과 작업 기억을 활용하여 효율적인 회상을 지원하고 시각 언어 모델을 사용하여 각 단계에서 시간적 또는 공간적 행동을 선택합니다. 시뮬레이션 환경과 실제 환경 전반에 걸친 시공간 개체 검색 작업의 벤치마크인 STARBench를 소개합니다. STARBench와 Tiago 로봇의 실험에서는 STAR가 장면 그래프 및 메모리 전용 기준선보다 지속적으로 뛰어난 성능을 보여 시간 검색과 공간 검색을 통합 문제로 처리하는 이점을 보여줍니다. 자세한 내용은 https://amrl.cs.utexas.edu/STAR를 참조하세요."
955,http://arxiv.org/abs/2511.13970 ,Scene Graph-Guided Generative AI Framework for Synthesizing and Evaluating Industrial Hazard Scenarios,"Sanjay Acharjee, Abir Khan Ratul, Diego Patino, Md Nazmus Sakib","작업장의 위험을 정확하게 감지하기 위한 비전 모델을 훈련하려면 사고로 이어질 수 있는 안전하지 않은 조건에 대한 현실적인 이미지가 필요합니다. 그러나 사고를 유발하는 시나리오를 발생 즉시 포착하는 것이 거의 불가능하기 때문에 이러한 데이터 세트를 획득하는 것은 어렵습니다. 이러한 한계를 극복하기 위해 본 연구에서는 역사적 산업안전보건청(OSHA) 사고 보고서를 기반으로 위험 시나리오의 사실적인 이미지를 합성하는 새로운 장면 그래프 기반 생성 AI 프레임워크를 제시합니다. OSHA 내러티브는 GPT-4o를 사용하여 분석되어 구조화된 위험 추론을 추출하고, 이는 위험을 이해하는 데 필수적인 공간적, 맥락적 관계를 포착하는 객체 수준 장면 그래프로 변환됩니다. 이 그래프는 텍스트-이미지 확산 모델을 안내하여 구성적으로 정확한 위험 장면을 생성합니다. 생성된 데이터의 사실성과 의미 충실도를 평가하기 위해 시각적 질문 응답(VQA) 프레임워크가 도입되었습니다. 4가지 최첨단 생성 모델에서 제안된 VQA 그래프 점수는 엔트로피 기반 검증을 기반으로 하는 CLIP 및 BLIP 측정항목보다 성능이 우수하여 더 높은 차별적 민감도를 확인합니다."
954,http://arxiv.org/abs/2511.12676 ,BridgeEQA: Virtual Embodied Agents for Real Bridge Inspections,"Subin Varghese, Joshua Gao, Asad Ur Rahman, Vedhus Hoskere","현실적인 실제 환경에서 주변 환경에 대한 질문에 대답할 수 있는 구체화된 에이전트를 배포하는 것은 부분적으로 실제 작동 조건을 충실하게 포착하는 벤치마크가 부족하기 때문에 여전히 어렵습니다. 우리는 공개 어휘 EQA(Embodied Question Answering)를 위한 강력한 도메인으로 인프라 검사를 제안합니다. 이는 자연스럽게 다중 규모 추론, 장거리 공간 이해 및 복잡한 의미 관계를 요구하는 동시에 표준화된 NBI(National Bridge Inventory) 상태 등급(0-9), 전문 검사 보고서 및 자기중심적 이미지를 통해 고유한 평가 이점을 제공합니다.   장면당 평균 47.93개의 이미지가 포함된 200개의 실제 교량 장면에 대한 전문 검사 보고서를 기반으로 한 2,200개의 개방형 어휘 질문-답변 쌍(OpenEQA 스타일)의 벤치마크인 BridgeEQA를 소개합니다. 질문에는 여러 이미지에 걸쳐 시각적 증거를 합성하고 응답을 NBI 상태 등급에 맞춰 정렬해야 합니다. 우리는 관련 이미지를 인용하는 모델의 능력을 평가하기 위해 새로운 EQA 측정 기준인 이미지 인용 관련성을 제안합니다.   최첨단 비전 언어 모델을 평가하면 에피소드 메모리 EQA 설정에서 상당한 성능 차이가 있음이 드러납니다. 이 문제를 해결하기 위해 우리는 이미지 기반 장면 그래프에 대한 순차적 탐색으로 검사를 공식화하는 EMVR(Embodied Memory Visual Reasoning)을 제안합니다. 이미지는 노드이고 에이전트는 Markov 결정 프로세스 내에서 뷰를 순회하고, 증거를 비교하고, 추론하는 작업을 수행합니다. EMVR은 기준선에 비해 강력한 성능을 보여줍니다. 우리는 데이터세트와 코드를 모두 공개적으로 공개합니다."
953,http://arxiv.org/abs/2511.11266 ,GraphPilot: Grounded Scene Graph Conditioning for Language-Based Autonomous Driving,"Fabian Schmidt, Markus Enzweiler, Abhinav Valada","비전 언어 모델은 최근 공간 구조에 대한 토폴로지 인식 추론과 다중 모드 입력의 동적 상호 작용에 성공이 달려 있는 자율 주행을 위한 유망한 계획자로 부상했습니다. 그러나 기존 모델은 일반적으로 이러한 관계 종속성을 명시적으로 인코딩하는 감독 없이 훈련되므로 에이전트와 기타 트래픽 엔터티가 원시 센서 데이터에서 서로 영향을 미치는 방식을 추론하는 기능이 제한됩니다. 이 작업에서 우리는 교통 장면 그래프 형태의 구조화된 관계형 컨텍스트에 따라 언어 기반 운전 모델을 조절하는 새로운 모델 불가지론적 방법으로 이러한 격차를 해소합니다. 우리는 다양한 추상화 수준과 형식으로 장면 그래프를 직렬화하고 이를 구조화된 프롬프트 템플릿을 통해 모델에 통합하여 관계 감독이 언제, 어떻게 가장 유익한지에 대한 체계적인 분석을 가능하게 합니다. 공개 LangAuto 벤치마크에 대한 광범위한 평가에서는 최첨단 접근 방식의 장면 그래프 조정이 주행 성능을 크고 지속적으로 향상시키는 것으로 나타났습니다. 특히, 우리는 LMDrive의 운전 점수가 최대 15.6% 증가하고 BEVDriver의 경우 17.5% 증가한 것을 관찰했습니다. 이는 테스트 시 장면 그래프 입력이 필요하지 않은 경우에도 모델이 장면 그래프 조건 훈련을 통해 관계형 사전 분석을 더 잘 내부화하고 기반화할 수 있음을 나타냅니다. 코드, 미세 조정된 모델 및 장면 그래프 데이터 세트는 https://github.com/iis-esslingen/GraphPilot에서 공개적으로 제공됩니다."
952,http://arxiv.org/abs/2511.10376 ,MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation,"Xun Huang, Shijia Zhao, Yunxiang Wang, Xin Lu, Wanfa Zhang, Rongsheng Qu, Weixin Li, Yunhong Wang, Chenglu Wen","내장된 내비게이션은 로봇 에이전트 작동을 위한 기본 기능입니다. 실제 배포에는 개방형 어휘 일반화와 낮은 교육 오버헤드가 필요하므로 작업별 RL 교육보다는 제로샷 방법에 동기를 부여합니다. 그러나 명시적인 3D 장면 그래프를 구축하는 기존 제로샷 방법은 풍부한 시각적 관찰을 텍스트 전용 관계로 압축하는 경우가 많으며, 이는 높은 구성 비용, 되돌릴 수 없는 시각적 증거 손실 및 제한된 어휘로 이어집니다. 이러한 제한 사항을 해결하기 위해 텍스트 관계를 대체하여 시각적 단서를 보존하는 다중 모드 3D 장면 그래프(M3DSG)를 소개합니다."
951,http://arxiv.org/abs/2511.08651 ,RS-Net: Context-Aware Relation Scoring for Dynamic Scene Graph Generation,"Hae-Won Jo, Yeong-Jun Cho",DSGG(동적 장면 그래프 생성)는 비디오에서 시간이 지남에 따라 개체 관계가 어떻게 발전하는지 모델링합니다. 그러나 기존 방법은 주석이 달린 개체 쌍에 대해서만 학습되고 관련되지 않은 쌍에 대한 지침이 부족하여 추론 중에 의미 있는 관계를 식별하기 어렵습니다. 본 논문에서는 공간적 상호작용과 장거리 시간적 맥락을 모두 사용하여 객체 쌍의 맥락적 중요성을 평가하는 모듈식 프레임워크인 Relation Scoring Network(RS-Net)를 제안합니다. RS-Net은 학습 가능한 컨텍스트 토큰이 있는 공간 컨텍스트 인코더와 비디오 수준 정보를 집계하는 시간 인코더로 구성됩니다. 결과 관계 점수는 관계 예측을 향상시키기 위해 통합된 삼중 점수 매기기 메커니즘에 통합됩니다. RS-Net은 아키텍처 변경 없이 기존 DSGG 모델에 쉽게 통합될 수 있습니다. Action Genome 데이터세트에 대한 실험에서는 RS-Net이 다양한 기준선에 걸쳐 재현율과 정밀도를 지속적으로 향상시키고 평균 재현율이 눈에 띄게 향상되어 관계의 긴 꼬리 분포를 처리하는 능력이 강조되었음을 보여줍니다. 매개변수 수가 증가했음에도 불구하고 RS-Net은 경쟁력 있는 효율성을 유지하여 최첨단 방법보다 우수한 성능을 달성합니다.
950,http://arxiv.org/abs/2511.07813 ,Sparse3DPR: Training-Free 3D Hierarchical Scene Parsing and Task-Adaptive Subgraph Reasoning from Sparse RGB Views,"Haida Feng, Hao Wei, Zewen Xu, Haolin Wang, Chade Li, Yihong Wu","최근에는 3차원 장면 이해를 위해 LLM(Large Language Model)이 널리 연구되고 있습니다. 그 중에서 training-free 접근 방식은 training-based 방법에 비해 유연성과 일반화 측면에서 주목을 받고 있습니다. 그러나 일반적으로 실제 배포에서는 정확성과 효율성이 떨어지는 문제가 있습니다. 문제를 해결하기 위해 사전 훈련된 LLM의 추론 기능을 활용하고 희소 보기 RGB 입력만 필요한 개방형 장면 이해를 위한 새로운 훈련 없는 프레임워크인 Sparse3DPR을 제안합니다. 구체적으로, 우리는 개방형 어휘를 지원하고 지배적인 평면 구조를 공간 앵커로 채택하여 보다 명확한 추론 체인과 보다 신뢰할 수 있는 상위 수준 추론을 가능하게 하는 계층적 평면 강화 장면 그래프를 소개합니다. 또한 쿼리와 관련 없는 정보를 동적으로 필터링하여 상황별 노이즈를 줄이고 3D 장면 추론 효율성과 정확성을 향상시키는 작업 적응형 하위 그래프 추출 방법을 설계합니다. 실험 결과는 Space3D-Bench의 ConceptGraphs와 비교하여 28.7% EM@1 개선과 78.2% 속도 향상을 달성한 Sparse3DPR의 우수성을 보여줍니다. 또한 Sparse3DPR은 견고성과 일반화 기능을 확인하는 추가 실제 실험을 통해 ScanQA의 교육 기반 방법과 비슷한 성능을 얻습니다."
949,http://arxiv.org/abs/2511.07403 ,SpatialThinker: Reinforcing 3D Reasoning in Multimodal LLMs via Spatial Rewards,"Hunar Batra, Haoqin Tu, Hardy Chen, Yuanze Lin, Cihang Xie, Ronald Clark",다중 모드 대형 언어 모델(MLLM)은 비전 언어 작업에서 놀라운 발전을 이루었지만 공간 이해에 계속 어려움을 겪고 있습니다. 기존 공간 MLLM은 명시적인 3D 입력 또는 아키텍처별 수정에 의존하는 경우가 많으며 대규모 데이터 세트 또는 희박한 감독으로 인해 여전히 제약을 받습니다. 이러한 제한 사항을 해결하기 위해 구조화된 공간 접지와 다단계 추론을 통합하기 위해 RL로 훈련된 3D 인식 MLLM인 SpatialThinker를 소개합니다. 이 모델은 작업 관련 객체와 공간 관계의 장면 그래프를 구성하고 조밀한 공간 보상을 통해 답을 추론함으로써 인간과 같은 공간 인식을 시뮬레이션합니다. SpatialThinker는 (1) 고품질 공간 VQA 데이터세트인 STVQA-7K를 생성하는 데이터 합성 파이프라인과 (2) 공간 접지를 시행하는 다중 목표 조밀 공간 보상을 갖춘 온라인 RL이라는 두 가지 주요 기여로 구성됩니다. SpatialThinker-7B는 공간 이해 및 실제 VQA 벤치마크에서 감독된 미세 조정 및 희소 RL 기준을 능가하여 희소 RL에 비해 기본 모델 이득을 거의 두 배로 늘리고 GPT-4o를 능가합니다. 이러한 결과는 제한된 데이터로 강력한 3D 공간 이해를 가능하게 하고 MLLM을 인간 수준의 시각적 추론으로 발전시키는 데 있어 공간 감독과 보상 정렬 추론을 결합하는 효과를 보여줍니다.
948,http://arxiv.org/abs/2511.05935 ,Interaction-Centric Knowledge Infusion and Transfer for Open-Vocabulary Scene Graph Generation,"Lin Li, Chuhan Zhang, Dong Zhang, Chong Sun, Chen Li, Long Chen","OVSGG(개방형 어휘 장면 그래프 생성)는 사전에 훈련된 대규모 모델의 지식을 활용하고 사전 정의된 범주를 넘어서는 새로운 객체와 관계를 인식함으로써 기존 SGG를 확장합니다. 기존 OVSGG 방법은 항상 2단계 파이프라인을 채택합니다. 1) \textit{지식 주입}을 대규모 데이터 세트에 대한 사전 교육을 통해 대규모 모델에 적용합니다. 2) 감독된 미세 조정 중에 완전히 주석이 달린 장면 그래프가 있는 사전 훈련된 모델의 \textit{지식 전송}. 그러나 명시적인 상호 작용 모델링이 부족하기 때문에 이러한 방법은 동일한 개체 범주의 상호 작용하는 인스턴스와 상호 작용하지 않는 인스턴스를 구별하는 데 어려움을 겪습니다. 이러한 제한은 OVSGG의 두 단계 모두에서 중요한 문제를 유발합니다. 즉, 지식 주입 중에 일치하지 않는 개체로부터 시끄러운 의사 감독을 생성하고 지식 전송 중에 모호한 쿼리 일치를 유발합니다. 이를 위해 본 논문에서는 이러한 불일치를 최소화하기 위해 상호작용 중심 패러다임의 inter\textbf{AC}tion-\textbf{C}엔트릭 엔드 투 엔드 OVSGG 프레임워크(\textbf{ACC})를 제안합니다. \textit{상호작용 중심 지식 주입}의 경우 ACC는 모델의 상호작용 지식을 향상시키기 위해 강력한 의사 감독 생성을 위한 양방향 상호작용 프롬프트를 사용합니다. \textit{상호작용 중심 지식 전달}의 경우 ACC는 먼저 상호작용하는 객체 쌍을 우선시하여 상호작용하지 않는 객체의 간섭을 줄이는 상호작용 기반 쿼리 선택을 채택합니다. 그런 다음 일반 지식을 유지하면서 관계형 전경을 배경에서 멀리 밀어 견고성을 강화하기 위해 상호 작용 일관성 지식 증류를 통합합니다. 세 가지 벤치마크에 대한 광범위한 실험 결과는 ACC가 최첨단 성능을 달성하여 실제 애플리케이션에 대한 상호 작용 중심 패러다임의 잠재력을 입증한다는 것을 보여줍니다."
947,http://arxiv.org/abs/2511.05894 ,Open-World 3D Scene Graph Generation for Retrieval-Augmented Reasoning,"Fei Yu, Quan Deng, Shengeng Tang, Yuehua Li, Lechao Cheng","오픈 월드 환경에서 3D 장면을 이해하는 것은 특히 폐쇄형 어휘 감독 및 정적 주석의 한계로 인해 비전 및 로봇공학에 근본적인 과제를 안겨줍니다. 이 문제를 해결하기 위해 우리는 일반화 가능하고 대화형 3D 장면 이해를 가능하게 하는 검색 증강 추론을 갖춘 오픈 월드 3D 장면 그래프 생성을 위한 통합 프레임워크를 제안합니다. 우리의 방법은 VLM(Vision-Language Model)을 검색 기반 추론과 통합하여 다중 모드 탐색 및 언어 기반 상호 작용을 지원합니다. 프레임워크는 두 가지 주요 구성 요소로 구성됩니다. (1) 고정 레이블 세트 없이 객체를 감지하고 의미론적 관계를 추론하는 동적 장면 그래프 생성 모듈, (2) 텍스트/이미지 조건 쿼리를 지원하기 위해 장면 그래프를 벡터 데이터베이스로 인코딩하는 검색 증강 추론 파이프라인. 우리는 다양한 환경에서 강력한 일반화와 우수한 성능을 보여주는 4가지 작업 현장 질문 응답, 시각적 접지, 인스턴스 검색 및 작업 계획 전반에 걸쳐 3DSSG 및 Replica 벤치마크에 대한 방법을 평가합니다. 우리의 결과는 확장 가능한 3D 장면 이해를 위해 개방형 어휘 인식과 검색 기반 추론을 결합하는 효과를 강조합니다."
946,http://arxiv.org/abs/2511.04357 ,GraSP-VLA: Graph-based Symbolic Action Representation for Long-Horizon Planning with VLA Policies,"Maëlic Neau, Zoe Falomir, Paulo E. Santos, Anne-Gwenn Bosser, Cédric Buche","시연을 통해 새로운 기술을 배울 수 있는 자율 로봇을 배치하는 것은 현대 로봇 공학의 중요한 과제입니다. 기존 솔루션은 VLA(Vision-Language Action) 모델을 사용한 엔드투엔드 모방 학습이나 AML(Action Model Learning)을 사용한 상징적 접근 방식을 적용하는 경우가 많습니다. 한편으로, 현재 VLA 모델은 높은 수준의 상징적 계획이 부족하여 장기적인 작업 능력을 방해하는 제한을 받습니다. 반면, AML의 상징적 접근 방식에는 일반화 및 확장성 관점이 부족합니다. 이 논문에서는 연속 장면 그래프 표현을 사용하여 인간 시연의 상징적 표현을 생성하는 프레임워크인 새로운 신경 기호 접근 방식인 GraSP-VLA를 제시합니다. 이 표현은 추론 중에 새로운 계획 도메인을 생성하는 데 사용되며 하위 수준 VLA 정책에 대한 조정자 역할을 하여 연속적으로 재현할 수 있는 작업 수를 확장합니다. 우리의 결과는 GraSP-VLA가 관찰로부터 자동 계획 도메인 생성 작업에 대한 상징적 표현을 모델링하는 데 효과적이라는 것을 보여줍니다. 또한 실제 실험 결과는 장기적인 작업에서 낮은 수준의 VLA 정책을 조율하는 연속 장면 그래프 표현의 잠재력을 보여줍니다."
945,http://arxiv.org/abs/2511.08609 ,Case Study: Transformer-Based Solution for the Automatic Digitization of Gas Plants,"I. Bailo, F. Buonora, G. Ciarfaglia, L. T. Consoli, A. Evangelista, M. Gabusi, M. Ghiani, C. Petracca Ciavarella, F. Picariello, F. Sarcina, F. Tuosto, V. Zullo, L. Airoldi, G. Bruno, D. D. Gobbo, S. Pezzenati, G. A. Tona","에너지 전환은 환경 지속 가능성의 미래를 결정하는 지난 수십 년 동안의 핵심 주제이며, 이러한 중요성을 지닌 영역에서는 디지털화, 혁신 및 사용 가능한 새로운 기술 도구를 무시할 수 없습니다. 이는 이탈리아와 유럽의 선도적인 가스 운송 회사인 SNAM 에너지 인프라의 플랜트 구조 획득을 자동화하기 위해 Engineering Ingegneria Informatica SpA가 개발한 본 문서에 설명된 생성 인공 지능 모델이 배치되는 맥락입니다. 가스 플랜트의 디지털화는 관련 문서의 해석을 통해 모든 관련 정보를 등록하는 것으로 구성됩니다. 따라서 이 작업의 목적은 MGM 사용자의 일상 작업을 간소화하기 위해 식물의 디지털화에 필요한 정보 추출을 자동화하는 인공 지능 기술을 기반으로 하는 효과적인 솔루션을 설계하는 것입니다. 이 솔루션은 플랜트의 P&ID를 각각 PDF 형식으로 입력받고 OCR, Vision LLM, 개체 감지, 관계 추론 및 최적화 알고리즘을 사용하여 관련 설계 데이터의 구조화된 개요와 플랜트의 계층적 프레임워크라는 두 가지 정보 세트로 구성된 출력을 반환합니다. 설득력 있는 결과를 얻기 위해 우리는 플랜트 구성 요소 간의 복잡한 관계에 대한 분석을 심화할 목적으로 새로운 Transformer 아키텍처를 도입하는 장면 그래프 생성을 위한 최첨단 모델을 확장합니다. 나열된 AI 기반 기술의 시너지적 사용을 통해 표준화 부족으로 인해 데이터의 다양성으로 인해 발생하는 많은 장애물을 극복할 수 있었습니다. 설계 데이터와 관련된 텍스트 정보 추출에서 91%의 정확도가 달성되었습니다. 플랜트 토폴로지와 관련하여 구성요소의 93%가 올바르게 식별되었으며 계층 구조는 약 80%의 정확도로 추출되었습니다."
944,http://arxiv.org/abs/2511.03819 ,SILVI: Simple Interface for Labeling Video Interactions,"Ozan Kanbertay, Richard Vogg, Elif Karakoc, Peter M. Kappeler, Claudia Fichtel, Alexander S. Ecker","카메라 트랩, 드론 또는 야생 동물의 직접 관찰을 통해 수집된 대용량 비디오 데이터의 자동 분석에 컴퓨터 비전 방법이 점점 더 많이 사용되고 있습니다. 최근의 발전은 주로 개별 행동을 감지하는 데 중점을 두었지만 사회적 및 개별화된 동물 행동을 이해하는 데 중요한 측면인 상호 작용의 감지 및 주석을 다루는 작업은 훨씬 적습니다. 기존 오픈 소스 주석 도구는 개인의 위치를 ​​파악하지 않고 행동 라벨링을 지원하거나 상호 작용을 포착할 수 없는 위치를 지원합니다. 이러한 격차를 해소하기 위해 우리는 두 기능을 모두 통합한 오픈 소스 라벨링 소프트웨어인 SILVI를 선보입니다. SILVI를 사용하면 연구자는 비디오 데이터 내에서 직접 행동과 상호 작용에 주석을 달고 컴퓨터 비전 모델을 훈련하고 검증하는 데 적합한 구조화된 출력을 생성할 수 있습니다. SILVI는 행동 생태학과 컴퓨터 비전을 연결함으로써 세밀한 행동 분석을 위한 자동화된 접근 방식의 개발을 촉진합니다. SILVI는 주로 동물 행동의 맥락에서 개발되었지만 동적 장면 그래프를 추출해야 하는 다른 비디오에서 인간 상호 작용에 주석을 추가하는 데 더 광범위하게 유용할 수 있습니다. 문서 및 다운로드 지침과 함께 소프트웨어는 https://gitlab.gwdg.de/kanbertay/interaction-labelling-app에서 제공됩니다."
943,http://arxiv.org/abs/2510.27558 ,Toward Accurate Long-Horizon Robotic Manipulation: Language-to-Action with Foundation Models via Scene Graphs,"Sushil Samuel Dinesh, Shinkyu Park","이 문서에서는 도메인별 교육 없이 로봇 조작을 위해 사전 교육된 기초 모델을 활용하는 프레임워크를 제시합니다. 프레임워크는 기성 모델을 통합하여 기초 모델의 다중 모드 인식과 강력한 작업 순서 지정이 가능한 범용 추론 모델을 결합합니다. 프레임워크 내에서 동적으로 유지 관리되는 장면 그래프는 공간 인식을 제공하고 환경에 대한 일관된 추론을 가능하게 합니다. 프레임워크는 일련의 탁상형 로봇 조작 실험을 통해 평가되었으며, 결과는 기성 기초 모델 위에 직접 로봇 조작 시스템을 구축할 수 있는 잠재력을 강조합니다."
942,http://arxiv.org/abs/2511.00107 ,AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency,Piyushkumar Patel,"텍스트-비디오 생성은 생성 인공 지능의 중요한 개척지로 부상했지만 기존 접근 방식은 시간적 일관성, 구성 이해 및 시각적 내러티브에 대한 세밀한 제어를 유지하는 데 어려움을 겪고 있습니다. 우리는 비디오 합성에 대한 고충실도 텍스트를 위한 시간 인식 확산 모델과 구성적 장면 이해를 통합하는 새로운 계층적 프레임워크인 MOVAI(Multimodal Original Video AI)를 제시합니다. 우리의 접근 방식은 세 가지 주요 혁신을 도입합니다. (1) 텍스트 설명을 시간적 주석이 포함된 계층적 장면 그래프로 분해하는 CSP(Compositional Scene Parser), (2) 공간적 세부 정보를 보존하면서 프레임 전체에서 일관된 모션 역학을 보장하는 TSAM(Temporal-Spatial Attention Mechanism), (3) 다중 규모 시간적 추론을 통해 비디오 품질을 반복적으로 향상시키는 PVR(Progressive Video Refinement) 모듈입니다. 표준 벤치마크에 대한 광범위한 실험을 통해 MOVAI는 최첨단 성능을 달성하여 기존 방법에 비해 LPIPS에서 15.3%, FVD에서 12.7%, 사용자 선호도 연구에서 18.9%의 비디오 품질 지표를 향상시키는 것으로 나타났습니다. 우리의 프레임워크는 사실적인 시간 역학과 세밀한 의미 제어를 통해 복잡한 다중 객체 장면을 생성하는 데 특별한 강점을 보여줍니다."
941,http://arxiv.org/abs/2510.27033 ,A Multi-Modal Neuro-Symbolic Approach for Spatial Reasoning-Based Visual Grounding in Robotics,"Simindokht Jahangard, Mehrzad Mohammadi, Abhinav Dhall, Hamid Rezatofighi","시각적 추론, 특히 공간적 추론은 복잡한 환경, 특히 로봇 공학 영역 내에서 객체 관계와 상호 작용을 이해해야 하는 어려운 인지 작업입니다. 기존 비전_언어 모델(VLM)은 인식 작업에 탁월하지만 암시적, 상관 기반 추론 및 이미지에만 의존하기 때문에 세분화된 공간 추론에 어려움을 겪습니다. 우리는 공간적, 논리적 관계를 명시적으로 모델링하기 위해 신경 인식과 상징적 추론을 결합하여 파노라마 이미지와 3D 포인트 클라우드 정보를 모두 통합하는 새로운 Neuro_symbolic 프레임워크를 제안합니다. 우리의 프레임워크는 개체를 감지하고 속성을 추출하기 위한 인식 모듈과 정확하고 해석 가능한 쿼리를 지원하기 위해 구조화된 장면 그래프를 구성하는 추론 모듈로 구성됩니다. JRDB-Reasoning 데이터 세트로 평가된 우리의 접근 방식은 로봇 공학 및 구현된 AI 애플리케이션에 적합한 경량 설계를 유지하면서 혼잡하고 인간이 만든 환경에서 탁월한 성능과 안정성을 보여줍니다."
940,http://arxiv.org/abs/2510.23190 ,Evaluation of Vision-LLMs in Surveillance Video,"Pascal Benschop, Cristian Meo, Justin Dauwels, Jelte P. Mense","우리 사회에서 카메라의 광범위한 사용으로 인해 인간이 모니터링할 수 있는 용량을 훨씬 초과하는 압도적인 양의 영상 데이터가 생성되었습니다. 효과적인 대응과 예방을 위해서는 변칙적이거나 범죄적인 사건을 적시에 감지하는 것이 중요하기 때문에 이는 공공 안전과 보안에 중요한 과제를 제시합니다. 예상치 못한 사건을 인식하는 구체화된 에이전트의 능력은 근본적으로 공간 추론 능력과 연결되어 있습니다. 이 논문에서는 비정상적인 동작 인식을 제로샷, 언어 기반 작업으로 구성하여 희박한 2D 비디오에서 동적 3D 장면을 해석하는 구현된 인식 문제를 해결함으로써 비전 언어 모델(VLM)의 공간 추론을 조사합니다. 구체적으로 우리는 사전 훈련된 작은 비전인 LLM이 비디오를 텍스트 설명으로 변환하고 텍스트 수반을 통해 레이블을 채점함으로써 공간적으로 기반을 둔 제로샷 이상 탐지기 역할을 할 수 있는지 조사합니다. 우리는 프롬프트 및 개인 정보 보호 조건 하에서 UCF-Crime 및 RWF-2000에 대한 4가지 개방형 모델을 평가합니다. 몇 장의 예시는 일부 모델의 정확도를 향상시킬 수 있지만 거짓 긍정을 증가시킬 수 있으며 개인 정보 보호 필터(특히 전신 GAN 변환)는 정확도를 저하시키는 불일치를 초래합니다. 이러한 결과는 현재 비전인 LLM이 성공하는 위치(간단하고 공간적으로 중요한 이벤트)와 실패하는 위치(시끄러운 공간 단서, 신원 난독화)를 차트로 보여줍니다. 앞으로 우리는 구조 인식 프롬프트, 클립 전체의 경량 공간 메모리, 설명 중 장면 그래프 또는 3D 포즈 사전, 동작 관련 기하학을 보존하는 개인 정보 보호 방법 등 작업별 교육 없이 공간 기반을 강화할 수 있는 구체적인 경로를 설명합니다. 이는 제로샷, 언어 기반 파이프라인을 구현된 실제 비디오 이해를 위한 적응형 빌딩 블록으로 자리매김합니다. VLM 평가를 위한 구현은 https://github.com/pascalbenschopTU/VLLM_AnomalyRecognition에서 공개적으로 제공됩니다."
939,http://arxiv.org/abs/2510.22897 ,Charting the Design Space of Neural Graph Representations for Subgraph Matching,"Vaibhav Raj, Indradyumna Roy, Ashwin Ramachandran, Soumen Chakrabarti, Abir De","하위 그래프 매칭은 지식 그래프(KG) 질문 답변, 분자 설계, 장면 그래프, 코드 및 회로 검색 등에 필수적입니다. 신경 방법은 하위 그래프 매칭에 대한 유망한 결과를 보여주었습니다. 최근 시스템에 대한 우리의 연구는 이를 그래프 매칭 네트워크를 위한 통합 설계 공간으로 리팩토링하는 것을 제안합니다. 기존 방법은 이 공간에서 몇 개의 고립된 패치만을 차지하며, 이는 대부분 미지의 상태로 남아 있습니다. 우리는 쿼리와 코퍼스 그래프 사이의 주의 기반 대 소프트 순열 기반 상호 작용, 노드 대 에지 정렬, 그래프의 신경 표현을 통합하는 최종 채점 네트워크의 형태와 같은 축을 특징으로 하는 이 공간에 대한 최초의 포괄적인 탐색을 수행합니다. 우리의 광범위한 실험에 따르면 이 분야에서 지금까지 탐구되지 않은 현명한 선택 조합이 큰 성능 이점을 가져오는 것으로 나타났습니다. 더 나은 성능을 넘어, 우리의 연구는 가치 있는 통찰력을 발견하고 더 많은 관심을 끌 수 있는 신경 그래프 표현 및 상호 작용에 대한 일반적인 설계 원칙을 확립합니다."
938,http://arxiv.org/abs/2510.22538 ,Iteratively Refined Early Interaction Alignment for Subgraph Matching based Graph Retrieval,"Ashwin Ramachandran, Vaibhav Raj, Indrayumna Roy, Soumen Chakrabarti, Abir De","하위 그래프 동형성을 기반으로 한 그래프 검색에는 장면 그래프 검색, 분자 지문 감지 및 회로 설계와 같은 여러 실제 응용 프로그램이 있습니다. Royet al. [35]는 먼저 쌍 그래프와 독립적으로 각 그래프의 노드 및 에지 임베딩을 계산한 다음 훈련 가능한 정렬 맵을 계산하는 하위 그래프 매칭을 위한 후기 상호 작용 모델인 IsoNet을 제안했습니다. 여기에서는 여러 기술 혁신을 기반으로 한 초기 상호작용 그래프 신경망(GNN)인 IsoNet++를 소개합니다. 먼저, 노드 간의 주입적 정렬에 따라 두 개의 입력 그래프 내에서 메시지를 전달하여 모든 노드의 임베딩을 계산합니다. 둘째, 우리는 여러 라운드에 걸쳐 이 정렬을 게으른 방식으로 업데이트합니다. 각 라운드 내에서 현재 정렬 상태를 기반으로 레이어별 GNN을 처음부터 실행합니다. GNN의 한 라운드가 완료된 후 마지막 레이어 임베딩을 사용하여 정렬을 업데이트하고 다음 라운드로 진행합니다. 셋째, IsoNet++는 노드 쌍 파트너 상호 작용이라는 새로운 개념을 통합합니다. 기존의 초기 상호 작용은 노드와 다른 그래프의 잠재적 파트너 간의 주의를 계산한 다음 그래프를 통해 전달되는 메시지를 제어합니다. 이와 대조적으로 우리는 노드 쌍(단일 노드가 아님)을 잠재적 파트너로 간주합니다. 한 그래프의 노드 사이에 에지가 존재하고 다른 그래프에 존재하지 않는 것은 정렬을 개선하는 데 중요한 신호를 제공합니다. 여러 데이터 세트에 대한 실험에 따르면 정렬은 연속 라운드를 통해 점진적으로 개선되어 기존 방법보다 검색 성능이 훨씬 향상되는 것으로 나타났습니다. 우리는 세 가지 혁신이 모두 정확도 향상에 기여함을 보여줍니다. 우리의 코드와 데이터 세트는 https://github.com/structlearning/isonetpp에서 공개적으로 제공됩니다."
937,http://arxiv.org/abs/2510.22204 ,Human-Inspired Neuro-Symbolic World Modeling and Logic Reasoning for Interpretable Safe UAV Landing Site Assessment,"Weixian Qian, Tianyi Yang, Sebastian Schroder, Yao Deng, Jiaohong Yao, Xiao Cheng, Richard Han, Xi Zheng","배송, 검사, 감시 등 실제 애플리케이션에 무인 항공기(UAV)를 배치하려면 구조화되지 않은 환경에서 안전한 착륙 지점을 안정적으로 평가하는 것이 필수적입니다. 기존 학습 기반 접근 방식은 공변량 변화로 인해 품질이 저하되고 투명성이 제한되는 경우가 많으므로 리소스가 제한된 플랫폼에서 결정을 해석하고 검증하기가 어렵습니다. 우리는 인식 기반 세계 모델링을 논리 기반 안전 추론과 명시적으로 분리하는 마커 없는 UAV 착륙 지점 안전 평가를 위한 신경 기호 프레임워크인 NeuroSymLand를 제시합니다. 경량 분할 모델은 객체, 속성 및 공간 관계를 인코딩하는 확률적 의미론적 장면 그래프를 점진적으로 구성합니다. Human-In-The-Loop 개선을 통해 대규모 언어 모델을 통해 오프라인으로 합성된 상징적 안전 규칙은 런타임 시 이 세계 모델에서 직접 실행되어 화이트박스 추론을 수행하고 기본 안전 제약 조건에 대한 사람이 읽을 수 있는 설명과 함께 순위가 매겨진 착륙 후보를 생성합니다. 72개의 시뮬레이션 및 하드웨어 루프 착륙 시나리오에서 NeuroSymLand는 61개의 성공적인 평가를 달성하여 37~57개의 성공을 달성하는 4개의 경쟁 기준을 능가합니다. 정성적 분석은 탁월한 해석 가능성과 투명한 추론을 강조하는 반면, 배포 시 무시할 수 있는 에지 오버헤드가 발생합니다. 우리의 결과는 UAV 착륙 지점 평가를 통해 입증된 것처럼 명시적 세계 모델링과 상징적 추론을 결합하면 모바일 시스템에서 정확하고 해석 가능하며 엣지 배치 가능한 안전 평가를 지원할 수 있음을 시사합니다."
936,http://arxiv.org/abs/2510.21069 ,ZING-3D: Zero-shot Incremental 3D Scene Graphs via Vision-Language Models,"Pranav Saxena, Jimmy Chiun","복잡한 3D 환경을 이해하고 추론하려면 객체뿐만 아니라 의미론적, 공간적 관계도 포착하는 구조화된 장면 표현이 필요합니다. 3D 장면 그래프 생성에 대한 최근 작업에서는 작업별 미세 조정 없이 사전 훈련된 VLM을 활용했지만 대부분 단일 뷰 설정에 국한되어 있으며 새로운 관찰이 도착할 때 증분 업데이트를 지원하지 못하고 3D 공간에서 명시적인 기하학적 접지가 부족합니다. 이 모든 것이 구현된 시나리오에 필수적입니다. 본 논문에서는 사전 훈련된 기초 모델에 대한 방대한 지식을 활용하여 개방형 어휘 인식을 가능하게 하고 제로 샷 방식으로 장면의 풍부한 의미론적 표현을 생성하는 동시에 3D 공간에서 증분 업데이트 및 기하학적 접지를 가능하게 하여 다운스트림 로봇 공학 애플리케이션에 적합하게 만드는 프레임워크인 ZING-3D를 제안합니다. 우리의 접근 방식은 VLM 추론을 활용하여 깊이 정보를 사용하여 3D에 기반을 둔 풍부한 2D 장면 그래프를 생성합니다. 노드는 특징, 3D 위치 및 의미론적 맥락을 갖춘 개방형 어휘 객체를 나타내는 반면, 가장자리는 객체 간 거리를 통해 공간 및 의미론적 관계를 포착합니다. Replica 및 HM3D 데이터 세트의 장면에 대한 실험에서는 ZING-3D가 작업별 교육 없이도 공간 및 관계형 지식을 캡처하는 데 효과적이라는 것을 보여줍니다."
935,http://arxiv.org/abs/2510.19060 ,PoSh: Using Scene Graphs To Guide LLMs-as-a-Judge For Detailed Image Descriptions,"Amith Ananthram, Elias Stengel-Eskin, Lorena A. Bradford, Julia Demarest, Adam Purvis, Keith Krut, Robert Stein, Rina Elster Pantalony, Mohit Bansal, Kathleen McKeown","VLM(비전 언어 모델)이 상세한 이미지 설명으로 발전했지만 평가는 여전히 어려운 과제입니다. 표준 지표(예: CIDEr, SPICE)는 짧은 텍스트용으로 설계되었으며 객체 오인과 같이 현재 흔하지 않은 오류를 인식하도록 조정되었습니다. 대조적으로, 긴 텍스트는 오류를 특정 텍스트 범위에 국한시키는 속성 및 관계 첨부와 점수에 대한 민감도를 요구합니다. 이 연구에서는 장면 그래프를 구조화된 루브릭으로 사용하여 판사로서의 LLM을 안내하고 세밀한 오류(예: 구성 이해의 실수)를 바탕으로 종합 점수를 생성하는 상세한 이미지 설명을 위한 측정 기준인 PoSh를 소개합니다. PoSh는 복제 가능하고 해석 가능하며 기존 지표(GPT4o-a-Judge 포함)보다 인간 평가자를 위한 더 나은 프록시입니다. PoSh를 검증하기 위해 우리는 도전적인 새로운 데이터 세트인 DOCENT를 도입합니다. 이 새로운 벤치마크에는 전문가가 작성한 참고 자료와 모델 생성 설명이 결합된 예술 작품이 포함되어 있으며 미술사 학생들의 품질에 대한 세부적이고 대략적인 판단이 강화되었습니다. 따라서 DOCENT를 사용하면 도전적인 새로운 영역에서 상세한 이미지 설명 지표와 상세한 이미지 설명 자체를 모두 평가할 수 있습니다. 우리는 PoSh가 최고의 개방형 대안보다 DOCENT의 인간 판단과 더 강한 상관관계(+0.05 Spearman $ρ$)를 달성하고 이미지 유형(웹 이미지의 기존 데이터 세트인 CapArena 사용)에 강력하며 유능한 보상 기능을 제공하여 표준 감독 미세 조정을 능가한다는 것을 보여줍니다. 그런 다음 PoSh를 사용하여 DOCENT의 그림, 스케치 및 조각상을 설명할 때 개방형 모델과 폐쇄형 모델의 성능을 특성화하고 기초 모델이 풍부한 장면 역학으로 이미지를 오류 없이 완벽하게 커버하는 데 어려움을 겪고 있음을 발견하여 VLM 진행 상황을 측정하기 위한 까다로운 새 작업을 설정했습니다. PoSh와 DOCENT를 통해 보조 텍스트 생성과 같은 중요한 영역에서 발전을 이룰 수 있기를 바랍니다."
934,http://arxiv.org/abs/2510.18697 ,Event-Grounding Graph: Unified Spatio-Temporal Scene Graph from Robotic Observations,"Phuoc Nguyen, Francesco Verdoja, Ville Kyrki",일상 생활에서 인간을 지원할 수 있는 지능형 자율 로봇을 구축하기 위한 기본 측면은 풍부한 환경 표현을 구축하는 것입니다. 의미론적 장면 표현의 발전으로 로봇 장면 이해가 강화되었지만 현재 접근 방식에는 공간 특징과 동적 이벤트 간의 연결이 부족합니다. 예를 들어 파란색 머그를 머그를 씻는 이벤트에 연결합니다. 이 연구에서는 장면의 공간적 특징에 대한 이벤트 상호 작용을 기반으로 하는 프레임워크인 이벤트 기반 그래프(EGG)를 소개합니다. 이 표현을 통해 로봇은 복잡한 시공간 쿼리를 인식하고 추론하고 응답할 수 있습니다. 실제 로봇 데이터를 사용한 실험은 관련 정보를 검색하고 환경 및 사건에 관한 인간의 질문에 정확하게 응답하는 EGG의 능력을 보여줍니다. 또한 EGG 프레임워크의 소스 코드와 평가 데이터 세트는 https://github.com/aalto-intelligent-robotics/EGG에서 오픈 소스로 공개됩니다.
933,http://arxiv.org/abs/2510.17363 ,M2H: Multi-Task Learning with Efficient Window-Based Cross-Task Attention for Monocular Spatial Perception,"U. V. B. L Udugama, George Vosselman, Francesco Nex","에지 장치에 실시간 공간 인식을 배포하려면 계산 오버헤드를 최소화하면서 보완적인 작업 정보를 활용하는 효율적인 다중 작업 모델이 필요합니다. 본 논문에서는 단일 단안 이미지로부터 의미론적 분할과 깊이, 가장자리 및 표면 법선 추정을 위해 설계된 새로운 다중 작업 학습 프레임워크인 M2H(Multi-Mono-Hydra)를 소개합니다. 독립적인 단일 작업 모델이나 공유 인코더-디코더 아키텍처에 의존하는 기존 접근 방식과 달리 M2H는 작업별 세부 정보를 보존하면서 구조화된 기능 교환을 가능하게 하는 창 기반 교차 작업 주의 모듈을 도입하여 작업 전반에 걸쳐 예측 일관성을 향상시킵니다. 경량 ViT 기반 DINOv2 백본을 기반으로 구축된 M2H는 실시간 배포에 최적화되어 있으며 동적 환경에서 3D 장면 그래프 구성을 지원하는 단안 공간 인식 시스템의 기반 역할을 합니다. 종합적인 평가에 따르면 M2H는 NYUDv2의 최첨단 다중 작업 모델보다 성능이 뛰어나고, Hypersim의 단일 작업 깊이와 의미 기준을 능가하며, Cityscapes 데이터 세트에서 뛰어난 성능을 달성하는 동시에 노트북 하드웨어의 계산 효율성을 유지하는 것으로 나타났습니다. 벤치마크를 넘어 M2H는 실제 데이터에서 검증되어 공간 인식 작업에서 실용성을 보여줍니다."
932,http://arxiv.org/abs/2510.16643 ,Structured Interfaces for Automated Reasoning with 3D Scene Graphs,"Aaron Ray, Jacob Arkin, Harel Biggie, Chuchu Fan, Luca Carlone, Nicholas Roy","사용자의 자연어 입력을 이해하고 반응할 수 있는 능력을 로봇에 제공하려면 자연어가 로봇의 기본 세계 표현과 연결되어야 합니다. 최근에는 LLM(대형 언어 모델)과 3DSG(3D 장면 그래프)가 자연어를 기반으로 하고 세계를 표현하는 데 널리 사용됩니다. 이 작업에서 우리는 3DSG와 함께 LLM을 사용하여 자연어를 기반으로 하는 문제를 해결합니다. 기존 방법은 장면 그래프를 LLM의 컨텍스트 창 내에서 직렬화된 텍스트로 인코딩하지만 이 인코딩은 크거나 풍부한 3DSG로 확장되지 않습니다. 대신, 우리는 작업과 관련된 3DSG의 하위 집합을 선택하기 위해 검색 증강 생성 형식을 사용할 것을 제안합니다. 우리는 그래프 데이터베이스에서 3DSG를 인코딩하고 언어 기반에 대한 관련 데이터를 검색할 수 있는 LLM에 도구로 쿼리 언어 인터페이스(Cypher)를 제공합니다. 우리는 지침 따르기 및 장면 질문 응답 작업에 대한 접근 방식을 평가하고 기본 컨텍스트 창 및 코드 생성 방법과 비교합니다. 우리의 결과는 Cypher를 3D 장면 그래프에 대한 인터페이스로 사용하면 로컬 및 클라우드 기반 모델 모두에서 크고 풍부한 그래프로 훨씬 더 잘 확장된다는 것을 보여줍니다. 이는 기반 언어 작업의 성능을 크게 향상시키는 동시에 장면 그래프 콘텐츠의 토큰 수를 크게 줄입니다. 비디오 보충 자료는 https://www.youtube.com/watch?v=zY_YI9giZSA에서 볼 수 있습니다."
931,http://arxiv.org/abs/2510.15564 ,Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation,"Xiaoming Zhu, Xu Huang, Qinghongbing Xie, Zhi Deng, Junsheng Yu, Yirui Guan, Zhongyuan Liu, Lin Zhu, Qijun Zhao, Ligang Liu, Long Zeng","예술적이고 일관된 3D 장면 레이아웃을 생성하는 것은 디지털 콘텐츠 제작에 매우 중요합니다. 기존의 최적화 기반 방법은 번거로운 수동 규칙으로 인해 제약을 받는 경우가 많은 반면, 심층 생성 모델은 풍부하고 다양성이 있는 콘텐츠를 생성하는 데 어려움을 겪습니다. 더욱이 대규모 언어 모델을 활용하는 접근 방식은 견고성이 부족하고 복잡한 공간 관계를 정확하게 포착하지 못하는 경우가 많습니다. 이러한 문제를 해결하기 위해 이 문서에서는 새로운 비전 기반 3D 레이아웃 생성 시스템을 제시합니다. 먼저 2,037개의 장면 자산과 147개의 3D 장면 레이아웃을 포함하는 고품질 자산 라이브러리를 구축합니다. 그 후, 이미지 생성 모델을 사용하여 프롬프트 표현을 이미지로 확장하고 이를 자산 라이브러리에 맞게 미세 조정합니다. 그런 다음 시각적 의미와 기하학적 정보를 기반으로 장면의 3D 레이아웃을 복구하기 위한 강력한 이미지 구문 분석 모듈을 개발합니다. 마지막으로 장면 그래프와 전반적인 시각적 의미를 사용하여 장면 레이아웃을 최적화하여 이미지와의 논리적 일관성 및 정렬을 보장합니다. 광범위한 사용자 테스트를 통해 우리의 알고리즘이 레이아웃의 풍부함과 품질 측면에서 기존 방법보다 훨씬 뛰어난 것으로 나타났습니다. 코드와 데이터 세트는 https://github.com/HiHiAllen/Imaginarium에서 확인할 수 있습니다."
930,http://arxiv.org/abs/2510.15319 ,Traversability-aware Consistent Situational Graphs for Indoor Localization and Mapping,"Jeewon Kim, Minho Oh, Hyun Myung","장면 그래프는 방, 객체 등 다양한 공간 요소 간의 관계를 이해함으로써 로봇 공학의 3D 매핑 기능을 향상시킵니다. 최근 연구에서는 장면 그래프를 계층적 레이어로 확장하여 이러한 수준 전체에 제약 조건을 추가하고 활용합니다. 이 접근 방식은 포즈 그래프 최적화와 긴밀하게 통합되어 위치 파악과 매핑 정확도를 동시에 향상시킵니다. 그러나 공간 특성을 분할할 때 시점의 변화와 센서의 제한된 시야(FOV)로 인해 일관되게 방을 인식하는 것이 어렵습니다. 예를 들어, 기존 실시간 접근 방식은 큰 공간을 시간 의존적 방법으로 인해 위치 파악 및 매핑에 유용하지 않은 더 작은 비기능 공간으로 과도하게 분할하는 경우가 많습니다. 반대로, 복셀 기반 룸 분할 방법은 지상 로봇이나 인간이 횡단할 수 없는 완전히 밀폐되지 않은 3D 공간과 같은 복잡한 경우에 종종 과소 분할되어 포즈 그래프 최적화에 잘못된 제약이 발생합니다. 우리는 횡단 가능성 정보의 일관된 타당성과 함께 로봇과 주변 환경 간의 상호 작용을 고려하는 횡단 가능성 인식 공간 분할 방법을 제안합니다. 이는 포즈 그래프 최적화의 의미론적 일관성과 계산 효율성을 모두 향상시킵니다. 향상된 성능은 동일한 경로를 따라 동일한 공간을 반복적으로 탐색하는 데이터 세트에서 동일한 공간의 재감지 빈도와 최적화 시간 소모를 통해 입증됩니다."
929,http://arxiv.org/abs/2510.12101 ,Gaussian Semantic Field for One-shot LiDAR Global Localization,"Pengyu Yin, Shenghai Yuan, Haozhi Cao, Xingyu Ji, Ruofei Bai, Siyu Chen, Lihua Xie",우리는 경량의 3중 장면 그래프를 기반으로 의미 명확성 기능을 갖춘 원샷 LiDAR 전역 위치 파악 알고리즘을 제시합니다. 랜드마크 의미론적 등록 기반 방법은 기하학적인 방법에 비해 전역 지역화에서 유망한 성능 향상을 보여주었지만 랜드마크는 통신 설정에 대해 반복적이고 오해의 소지가 있을 수 있습니다. 우리는 가우스 프로세스 모집단에서 학습된 연속 함수를 사용하여 의미 분포를 모델링하여 이 문제를 완화할 것을 제안합니다. 개별 의미 체계 라벨과 비교하여 연속 함수는 더욱 세밀한 지리적 의미 정보를 캡처하고 대응 설정을 위한 보다 자세한 측정 정보를 제공합니다. 우리는 이 연속 함수를 개체 계층과 메트릭 의미 계층 사이의 중간 계층으로 삽입하여 3개 계층의 3D 장면 그래프를 형성하고 원샷 위치 파악을 위한 가볍지만 성능이 뛰어난 백엔드 역할을 합니다. 우리는 글로벌 현지화 파이프라인 Outram-GSF(Gaussian semantic field)라는 용어를 사용하고 공개적으로 사용 가능한 데이터 세트에 대한 광범위한 실험을 수행하여 현재 최첨단 기술에 비해 우수한 성능을 검증합니다.
928,http://arxiv.org/abs/2510.10778 ,Real2USD: Scene Representations in Universal Scene Description Language,"Christopher D. Hsu, Pratik Chaudhari","LLM(대형 언어 모델)은 로봇이 추상 작업 사양을 추론하는 데 도움이 될 수 있습니다. 이를 위해서는 자연어 기반 사전 분석을 통해 로봇이 사용하는 환경의 고전적 표현을 강화해야 합니다. 이를 위한 기존 접근 방식이 많이 있지만 탐색을 위한 시각적 언어 모델, 매핑을 위한 언어 유도 신경 방사 필드 등과 같은 특정 작업에 맞게 조정되었습니다. 이 문서에서는 USD(Universal Scene Description) 언어가 LLM 기반 로봇 작업 환경에서 기하학적, 측광적, 의미론적 정보를 효과적이고 일반적으로 표현한다고 주장합니다. 우리의 주장은 간단합니다. USD는 LLM과 인간 모두가 읽을 수 있고 기본적으로 모든 작업을 지원할 수 있을 만큼 풍부한 XML 기반 장면 그래프입니다. Pixar는 자산, 장면, 심지어 영화까지 저장하기 위해 이 언어를 개발했습니다. 우리는 LiDAR과 RGB 카메라를 장착한 Unitree Go2 4족 로봇을 사용하여 (i) 유리가 많이 포함된 다양한 물체와 까다로운 설정으로 실내 환경에 대한 명시적인 USD 표현을 구축하고 (ii) Google의 Gemini를 사용하여 USD를 구문 분석하여 장면 이해, 복잡한 추론 및 계획을 시연하는 ""Real to USD"" 시스템을 시연합니다. 또한 Nvidia의 Issac Sim을 사용하여 시뮬레이션된 창고 및 병원 환경에서 이 시스템의 다양한 측면을 연구합니다. 코드는 https://github.com/grasp-lyrl/Real2USD에서 확인할 수 있습니다."
927,http://arxiv.org/abs/2510.15963 ,ESCA: Contextualizing Embodied Agents via Scene-Graph Generation,"Jiani Huang, Amish Sethi, Matthew Kuo, Mayank Keoliya, Neelay Velingker, JungHo Jung, Ser-Nam Lim, Ziyang Li, Mayur Naik","다중 모드 대형 언어 모델(MLLM)은 범용 구현 에이전트를 향해 빠르게 발전하고 있습니다. 그러나 기존 MLLM은 낮은 수준의 시각적 특징과 높은 수준의 텍스트 의미 간의 세밀한 연결을 안정적으로 포착하지 못하여 기반이 약하고 인식이 부정확합니다. 이러한 문제를 극복하기 위해 우리는 시공간 장면 그래프에 인식을 기반으로 구현된 에이전트를 맥락화하는 프레임워크인 ESCA를 제안합니다. 그 핵심에는 CLIP을 기반으로 장면 그래프를 생성하기 위한 새로운 개방형 도메인의 신속한 기반 모델인 SGCLIP이 있습니다. SGCLIP은 자동으로 생성된 캡션을 모델 자체에서 생성된 장면 그래프와 정렬하는 신경 기호 파이프라인을 사용하여 87,000개가 넘는 공개 도메인 비디오에 대해 교육되어 사람이 레이블을 붙인 주석이 필요하지 않습니다. 우리는 SGCLIP이 프롬프트 기반 추론과 작업별 미세 조정 모두에서 탁월하여 장면 그래프 생성 및 동작 현지화 벤치마크에서 최첨단 결과를 달성한다는 것을 입증합니다. SGCLIP이 포함된 ESCA는 오픈 소스 및 상용 MLLM을 기반으로 구현된 에이전트에 대한 인식을 개선하여 두 구현된 환경에서 최첨단 성능을 달성합니다. 특히 ESCA는 에이전트 인식 오류를 크게 줄이고 오픈 소스 모델이 독점 기준을 능가할 수 있도록 해줍니다. 우리는 https://github.com/video-fm/LASER에서 SGCLIP 모델 훈련을 위한 소스 코드를 공개하고 https://github.com/video-fm/ESCA에서 구현된 에이전트를 위한 소스 코드를 공개합니다."
926,http://arxiv.org/abs/2510.10194 ,B2N3D: Progressive Learning from Binary to N-ary Relationships for 3D Object Grounding,"Feng Xiao, Hongbin Xu, Hai Ci, Wenxiong Kang",로봇 장면을 이해하려면 자연어를 사용하여 3D 객체의 위치를 ​​파악하는 것이 필수적입니다. 설명에는 유사한 객체를 구별하기 위해 여러 공간 관계가 포함되는 경우가 많아 3D 언어 정렬이 어렵습니다. 현재 방법은 다중 모드 관계 이해에서 n-ary 조합의 전체적인 지각적 중요성을 무시하고 쌍별 개체에 대한 관계만 모델링합니다. 이를 해결하기 위해 우리는 3D 객체 접지를 위한 새로운 진보적 관계형 학습 프레임워크를 제안합니다. 우리는 관계형 학습을 바이너리에서 n-ary로 확장하여 전 세계적으로 참조 설명과 일치하는 시각적 관계를 식별합니다. 훈련 데이터에서 참조된 객체에 대한 특정 주석이 없다는 점을 고려하여 n-ary 관계형 학습을 용이하게 하기 위해 그룹화된 감독 손실을 설계합니다. n항 관계로 생성된 장면 그래프에서 우리는 하이브리드 주의 메커니즘을 갖춘 다중 모드 네트워크를 사용하여 n항 조합 내에서 대상을 더욱 지역화합니다. ReferIt3D 및 ScanRefer 벤치마크에 대한 실험 및 절제 연구는 우리의 방법이 최첨단 방법보다 성능이 뛰어나고 3D 위치 파악에서 n-ary 관계 인식의 장점을 입증합니다.
925,http://arxiv.org/abs/2510.09483 ,"FOGMACHINE -- Leveraging Discrete-Event Simulation and Scene Graphs for Modeling Hierarchical, Interconnected Environments under Partial Observations from Mobile Agents","Lars Ohnemus, Nils Hantke, Max Weißer, Kai Furmans","동적 장면 그래프(DSG)는 계층적이고 상호 연결된 환경의 구조화된 표현을 제공하지만 현재 접근 방식은 확률적 역학, 부분 관찰 가능성 및 다중 에이전트 활동을 캡처하는 데 어려움을 겪고 있습니다. 이러한 측면은 에이전트가 불확실성과 지연된 인식 하에서 행동해야 하는 구체화된 AI에 매우 중요합니다. DSG를 이산 이벤트 시뮬레이션과 융합하여 객체 역학, 에이전트 관찰 및 대규모 상호 작용을 모델링하는 오픈 소스 프레임워크인 FOGMACHINE을 소개합니다. 이 설정을 통해 불확실성 전파, 제한된 인식 하에서의 계획 및 긴급 다중 에이전트 동작에 대한 연구를 가능하게 합니다. 도시 시나리오의 실험은 현실적인 시간적, 공간적 패턴을 보여 주는 동시에 희소한 관찰 하에서 신념 추정의 어려움을 드러냅니다. 구조화된 표현과 효율적인 시뮬레이션을 결합함으로써 FOGMACHINE은 복잡하고 불확실한 환경에서 벤치마킹, 모델 교육 및 구현된 AI 발전을 위한 효과적인 도구를 구축합니다."
924,http://arxiv.org/abs/2510.08512 ,Have We Scene It All? Scene Graph-Aware Deep Point Cloud Compression,"Nikolaos Stathoulopoulos, Christoforos Kanellakis, George Nikolakopoulos","3D 포인트 클라우드 데이터의 효율적인 전송은 중앙 집중식 및 분산형 다중 에이전트 로봇 시스템의 고급 인식에 매우 중요하며, 특히 엣지 및 클라우드 기반 처리에 대한 의존도가 높아지는 요즘에는 더욱 그렇습니다. 그러나 포인트 클라우드의 크고 복잡한 특성으로 인해 대역폭 제약과 간헐적인 연결 문제가 발생하여 시스템 성능이 저하되는 경우가 많습니다. 우리는 의미론적 장면 그래프를 기반으로 한 심층 압축 프레임워크를 제안합니다. 이 방법은 포인트 클라우드를 의미상 일관된 패치로 분해하고 이를 FiLM(Feature-wise Linear Modulation)으로 조절된 의미 인식 인코더를 사용하여 압축된 잠재 표현으로 인코딩합니다. 잠재 기능과 그래프 노드 속성에 따라 안내되는 폴딩 기반 디코더를 통해 구조적으로 정확한 재구성이 가능합니다. SemanticKITTI 및 nuScenes 데이터세트에 대한 실험에서는 프레임워크가 구조적 충실도와 의미론적 충실도를 모두 유지하면서 데이터 크기를 최대 98%까지 줄이는 최첨단 압축률을 달성하는 것으로 나타났습니다. 또한 다중 로봇 포즈 그래프 최적화 및 지도 병합과 같은 다운스트림 애플리케이션을 지원하여 원시 LiDAR 스캔으로 얻은 것과 비슷한 궤적 정확도 및 지도 정렬을 달성합니다."
923,http://arxiv.org/abs/2510.07053 ,Introspection in Learned Semantic Scene Graph Localisation,"Manshika Charvi Bissessur, Efimia Panagiotaki, Daniele De Martini","이 작업은 의미론이 학습된 자기 감독, 대조 의미론적 지역화 프레임워크에서 지역화 성능 및 견고성에 어떻게 영향을 미치는지 조사합니다. 원본 지도와 교란된 지도 모두에서 위치 파악 네트워크를 훈련한 후 모델이 환경 소음을 필터링하고 일상적인 혼란보다 독특한 랜드마크를 우선시하는지 여부를 조사하기 위해 철저한 사후 내성 분석을 수행합니다. 다양한 해석 가능성 방법을 검증하고 비교 신뢰성 분석을 제시합니다. 통합 경사도와 주의 가중치는 학습된 행동에 대한 가장 신뢰할 수 있는 프로브로 지속적으로 등장합니다. 의미론적 클래스 제거는 자주 사용되는 객체의 가중치가 낮아지는 암시적 가중치를 더욱 드러냅니다. 전반적으로, 결과는 모델이 장소 정의에 대한 잡음에 강하고 의미론적으로 두드러진 관계를 학습함으로써 까다로운 시각적 및 구조적 변형 하에서 설명 가능한 등록을 가능하게 함을 나타냅니다."
922,http://arxiv.org/abs/2510.21744 ,FORGE-Tree: Diffusion-Forcing Tree Search for Long-Horizon Robot Manipulation,"Yanjia Huang, Shuo Liu, Sheng Liu, Qingxiao Xu, Mingyang Wu, Xiangbo Gao, Zhengzhong Tu","장거리 로봇 조작 작업은 드리프트 및 노출 편향으로 인해 VLA(Vision-Language-Action) 정책에 있어 여전히 어려운 과제로 남아 있으며, 종종 고정된 하이퍼파라미터로 전체 궤적의 노이즈를 제거하여 작은 기하학적 오류가 여러 단계에 걸쳐 복합적으로 발생하고 여유 공간이 부족한 경우 추가 테스트 시간 컴퓨팅을 할당할 메커니즘을 제공하지 않습니다. 이러한 문제를 해결하기 위해 스테이지 정렬된 DF(확산 강제) 헤드와 테스트 시간 MCTD(Monte Carlo Tree Diffusion)를 결합하는 플러그인 제어 레이어인 FORGE-Tree를 소개합니다. 고정된 VLA 인코더를 사용하면 DF는 시간 단계를 하위 작업 단계에 맞춥니다. 추론 중에 다른 토큰은 고정된 상태로 유지하면서 대상 세그먼트만 부분적으로 노이즈 제거하여 궤적 개선을 일련의 로컬 편집으로 전환합니다. 그런 다음 Monte Carlo Tree Diffusion을 적용하여 개선할 다음 세그먼트를 선택합니다. 장면 그래프는 확장을 위한 사전 정보와 롤아웃을 위한 지오메트리 관계 인식 점수를 제공하여 실행된 접두사를 유지하면서 검색 예산에 따라 성능이 확장되는 트리 구조의 노이즈 제거 기능을 제공합니다. LIBERO 평가에서 FORGE-Tree는 OpenVLA 및 Octo-Base를 모두 사용하여 기본 VLA 기준에 비해 성공률을 13.4~17.2pp 향상합니다. 특히 장거리 변형에서 비슷한 컴퓨팅 예산 하에서 이익이 일관되게 유지됩니다. 동영상 제공: https://taco-group.github.io/FORGE-Tree/"
921,http://arxiv.org/abs/2510.05742 ,Vipera: Blending Visual and LLM-Driven Guidance for Systematic Auditing of Text-to-Image Generative AI,"Yanwei Huang, Wesley Hanwen Deng, Sijia Xiao, Motahhare Eslami, Jason I. Hong, Arpit Narechania, Adam Perer","향상된 기능에도 불구하고 텍스트-이미지 생성 AI 시스템은 편파적이고 공격적이며 문제가 있는 출력을 생성하는 것으로 알려져 있습니다. 최근의 발전으로 생성 AI의 테스트 및 감사가 지원되었지만, 기존 감사 방법은 구조화된 방식으로 AI 생성 출력의 방대한 공간을 효과적으로 탐색하도록 지원하는 데 여전히 어려움에 직면해 있습니다. 이러한 격차를 해소하기 위해 우리는 5명의 AI 감사자와 함께 형성 연구를 수행하고 체계적인 AI 감사를 지원하기 위한 5가지 설계 목표를 종합했습니다. 이러한 통찰력을 바탕으로 우리는 장면 그래프를 포함한 여러 시각적 단서를 사용하여 이미지 감지를 촉진하고 감사자가 감사 기준을 탐색하고 계층적으로 구성하도록 영감을 주는 대화형 감사 인터페이스인 Vipera를 개발했습니다. 또한 Vipera는 LLM 기반 제안을 활용하여 탐구되지 않은 감사 방향의 탐색을 촉진합니다. AI 감사 경험이 있는 24명의 참가자를 대상으로 한 통제된 실험을 통해 우리는 감사자가 다양한 기준을 사용하면서 대규모 AI 출력 공간을 탐색하고 분석을 구성하는 데 도움이 되는 Vipera의 효율성을 보여줍니다."
920,http://arxiv.org/abs/2510.05560 ,HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video,"Hongchi Xia, Chih-Hao Lin, Hao-Yu Hsu, Quentin Leboutet, Katelyn Gao, Michael Paulitsch, Benjamin Ummenhofer, Shenlong Wang","실제 세계를 정확한 시뮬레이션 지원 가상 환경으로 디지털화하면 증강 및 가상 현실, 게임, 로봇 공학과 같은 다양한 분야에서 중요한 기회를 얻을 수 있습니다. 그러나 현재의 3D 재구성 및 장면 이해 방법은 일반적으로 지오메트리 완성도, 객체 상호 작용성, 물리적 타당성, 사실적인 렌더링 또는 신뢰할 수 있는 동적 시뮬레이션을 위한 현실적인 물리적 특성과 같은 하나 이상의 중요한 측면에서 부족합니다. 이러한 제한 사항을 해결하기 위해 이러한 요구 사항을 동시에 달성하는 새로운 대화형 3D 재구성 프레임워크인 HoloScene을 소개합니다. HoloScene은 포괄적인 대화형 장면 그래프 표현을 활용하여 계층적 및 개체 간 관계와 함께 개체 형상, 모양 및 물리적 속성을 인코딩합니다. 재구성은 에너지 기반 최적화 문제로 공식화되어 관측 데이터, 물리적 제약 및 생성적 사전 변수를 통일되고 일관된 목표로 통합합니다. 최적화는 샘플링 기반 탐색과 그래디언트 기반 개선을 결합한 하이브리드 접근 방식을 통해 효율적으로 수행됩니다. 그 결과 디지털 트윈은 완전하고 정확한 기하학, 물리적 안정성 및 새로운 관점에서 사실적인 렌더링을 보여줍니다. 여러 벤치마크 데이터 세트에 대해 수행된 평가에서는 우수한 성능이 입증되었으며, 대화형 게임 및 실시간 디지털 트윈 조작의 실제 사용 사례에서는 HoloScene의 광범위한 적용 가능성과 효율성이 입증되었습니다. 프로젝트 페이지: https://xiahongchi.github.io/HoloScene."
919,http://arxiv.org/abs/2510.05430 ,Active Semantic Perception,"Huayi Tang, Pratik Chaudhari","우리는 탐색과 같은 작업을 위해 장면의 의미론을 사용하는 것을 의미하는 능동적 의미론적 인식에 대한 접근 방식을 개발합니다. 우리는 다양한 추상화 수준에서 크고 복잡한 실내 환경(예: 방, 물체, 벽, 창문 등에 해당하는 노드)과 해당 기하학적 구조의 세밀한 세부 정보를 나타낼 수 있는 컴팩트하고 계층적인 다층 장면 그래프를 구축합니다. 우리는 장면의 부분 관찰과 일치하는 관찰되지 않은 영역의 그럴듯한 장면 그래프를 샘플링하기 위해 LLM(대형 언어 모델)을 기반으로 하는 절차를 개발합니다. 이러한 샘플은 정교한 공간 추론을 위한 잠재적인 중간지점의 정보 획득을 계산하는 데 사용됩니다. 예를 들어 거실에 있는 두 개의 문은 부엌이나 침실로 연결될 수 있습니다. 우리는 시뮬레이션을 통해 복잡하고 사실적인 3D 실내 환경에서 이 접근 방식을 평가합니다. 우리는 우리의 접근 방식이 기본 접근 방식보다 더 빠르고 정확하게 환경의 의미를 파악할 수 있다는 것을 정성적 및 정량적 실험을 통해 보여줍니다."
918,http://arxiv.org/abs/2510.04714 ,Object-Centric Representation Learning for Enhanced 3D Scene Graph Prediction,"KunHo Heo, GiHyun Kim, SuYeon Kim, MyeongAh Cho","3D 의미론적 장면 그래프 예측은 3D 장면에서 객체와 의미론적 관계를 감지하는 것을 목표로 하며 로봇 공학 및 AR/VR 애플리케이션의 중요한 기술로 부상했습니다. 이전 연구에서는 데이터 세트 제한 사항을 해결하고 Open-Vocabulary 설정을 포함한 다양한 접근 방식을 탐색했지만 개체의 표현 용량 및 관계 기능을 최적화하지 못하는 경우가 많아 식별 기능이 부족함에도 불구하고 그래프 신경망에 과도하게 의존하는 것으로 나타났습니다. 이 작업에서 우리는 광범위한 분석을 통해 객체 특징의 품질이 전체 장면 그래프 정확도를 결정하는 데 중요한 역할을 한다는 것을 보여줍니다. 이 문제를 해결하기 위해 우리는 식별력이 뛰어난 객체 특징 인코더를 설계하고 장면 그래프 예측에서 객체 표현 학습을 분리하는 대조 사전 학습 전략을 사용합니다. 이 디자인은 객체 분류 정확도를 향상시킬 뿐만 아니라 관계 예측도 직접적으로 향상시킵니다. 특히, 사전 훈련된 인코더를 기존 프레임워크에 연결하면 모든 평가 지표에서 상당한 성능 향상이 관찰됩니다. 또한 기존 접근 방식은 관계 정보의 통합을 완전히 활용하지 못했지만 기하학적 및 의미론적 특징을 효과적으로 결합하여 우수한 관계 예측을 달성합니다. 3DSSG 데이터 세트에 대한 포괄적인 실험은 우리의 접근 방식이 이전의 최첨단 방법보다 훨씬 뛰어난 성능을 보여줍니다. 우리 코드는 https://github.com/VisualScienceLab-KHU/OCRL-3DSSG-Codes에서 공개적으로 제공됩니다."
917,http://arxiv.org/abs/2510.03727 ,Bridging the Gap Between Multimodal Foundation Models and World Models,Xuehai He,"인간은 다양한 감각 양상의 통합을 통해 세상을 이해하며, 이를 통해 역동적인 물리적 과정을 인지하고, 추론하고, 상상할 수 있습니다. 이 기능에 영감을 받아 다중 모드 기반 모델(MFM)이 다중 모드 이해 및 생성을 위한 강력한 도구로 등장했습니다. 그러나 오늘날의 MFM은 효과적인 세계 모델 역할을 하기에는 부족합니다. 반사실 추론 수행, 역학 시뮬레이션, 시공간 정보 이해, 생성된 시각적 결과 제어, 다면적 추론 수행과 같은 필수 능력이 부족합니다. 우리는 다중 모드 기반 모델과 월드 모델 간의 격차를 해소하는 데 필요한 것이 무엇인지 조사합니다. 우리는 차별적 작업을 통해 MFM의 추론 능력을 향상시키고 인과 추론, 반사실적 사고, 시공간 추론과 같은 구조화된 추론 기술을 MFM에 장착하여 표면 상관 관계를 넘어 시각적 및 텍스트 데이터 내의 더 깊은 관계를 이해할 수 있도록 합니다. 다음으로, 구조화되고 제어 가능한 생성을 위한 새로운 프레임워크를 도입하여 이미지와 비디오 형식 모두에 걸쳐 다중 모드 기반 모델의 생성 기능을 탐색합니다. 우리의 접근 방식은 장면 그래프, 다중 모드 조건화, 다중 모드 정렬 전략을 통합하여 생성 프로세스를 안내하고 높은 수준의 의미 체계와 세분화된 사용자 의도와의 일관성을 보장합니다. 우리는 이러한 기술을 제어 가능한 4D 생성으로 더욱 확장하여 시간과 공간에 따라 대화형, 편집 및 변형 가능한 객체 합성을 가능하게 합니다."
916,http://arxiv.org/abs/2510.01049 ,KeySG: Hierarchical Keyframe-Based 3D Scene Graphs,"Abdelrhman Werby, Dennis Rotondi, Fabio Scaparro, Kai O. Arras","최근 몇 년 동안 3D 장면 그래프는 기하학적 정확성과 의미론적 풍부함을 모두 제공하는 강력한 세계 표현으로 등장했습니다. 3D 장면 그래프와 대규모 언어 모델을 결합하면 로봇은 복잡한 인간 중심 환경에서 추론하고 계획하고 탐색할 수 있습니다. 그러나 3D 장면 그래프를 구성하기 위한 현재 접근 방식은 의미상 사전 정의된 관계 집합으로 제한되며 대규모 환경에서의 직렬화는 LLM의 컨텍스트 창을 쉽게 초과할 수 있습니다. 3D 장면을 바닥, 방, 개체 및 기능 요소로 구성된 계층적 그래프로 표현하는 프레임워크인 KeySG를 소개합니다. 여기서 노드는 기하학적 및 시각적 범위를 최적화하기 위해 선택된 키프레임에서 추출된 다중 모드 정보로 보강됩니다. 키프레임을 사용하면 VLM을 효율적으로 활용하여 장면 정보를 추출할 수 있으므로 개체 간의 관계 가장자리를 명시적으로 모델링할 필요성이 줄어들고 보다 일반적이고 작업에 구애받지 않는 추론 및 계획이 가능해집니다. 우리의 접근 방식은 계층적 RAG(Retrieval-Augmented Generation) 파이프라인을 활용하여 그래프에서 관련 컨텍스트를 추출함으로써 대규모 장면 그래프와 관련된 확장성 문제를 완화하는 동시에 복잡하고 모호한 쿼리를 처리할 수 있습니다. 3D 개체 분할 및 복잡한 쿼리 검색을 포함한 4가지 개별 벤치마크에서 평가된 KeySG는 대부분의 측정 항목에서 이전 접근 방식보다 뛰어난 성능을 발휘하여 뛰어난 의미론적 풍부함과 효율성을 보여줍니다."
915,http://arxiv.org/abs/2509.26457 ,Attention over Scene Graphs: Indoor Scene Representations Toward CSAI Classification,"Artur Barros, Carlos Caetano, João Macedo, Jefersson A. dos Santos, Sandra Avila","실내 장면 분류는 로봇 공학부터 CSAI(아동 성적 학대 이미지) 분류와 같은 민감한 콘텐츠 분석에 이르기까지 광범위한 애플리케이션을 사용하는 컴퓨터 비전의 중요한 작업입니다. 이 문제는 객체와 복잡한 공간 레이아웃 간의 복잡한 관계로 인해 특히 어렵습니다. 이 작업에서 우리는 원시 픽셀 대신 구조화된 그래프 표현에서 작동하는 새로운 프레임워크인 ASGRA(Attention over Scene Graphs for Sensitive Content Analysis)를 제안합니다. 먼저 이미지를 장면 그래프로 변환한 다음 추론을 위해 Graph Attention Network를 사용함으로써 ASGRA는 장면 구성 요소 간의 상호 작용을 직접 모델링합니다. 이 접근 방식은 (i) 객체 및 관계 식별을 통한 고유한 설명 가능성, (ii) 민감한 이미지에 직접 액세스하지 않고도 모델 교육을 가능하게 하는 개인 정보 보호라는 두 가지 주요 이점을 제공합니다. Places8에서는 이미지 기반 방법을 능가하는 81.27%의 균형 잡힌 정확도를 달성합니다. 법 집행 기관을 통한 실제 CSAI 평가는 74.27%의 균형 잡힌 정확도를 제공합니다. 우리의 결과는 구조화된 장면 표현을 실내 장면 분류 및 CSAI 분류를 위한 강력한 패러다임으로 설정합니다. 코드는 https://github.com/tutuzeraa/ASGRA에서 공개적으로 제공됩니다."
914,http://arxiv.org/abs/2509.24966 ,Social 3D Scene Graphs: Modeling Human Actions and Relations for Interactive Service Robots,"Ermanno Bartoli, Dennis Rotondi, Buwei He, Patric Jensfelt, Kai O. Arras, Iolanda Leite","로봇이 사회적으로 순응하고 상황을 인식하는 방식으로 작동하려면 사람들이 주변 환경 및 서로 상호 작용하는 방식을 이해하는 것이 필수적입니다. 3D 장면 그래프는 장면 이해를 위한 강력한 의미론적 표현으로 등장했지만, 기존 접근 방식은 주석이 달린 인간-환경 관계가 부족하기 때문에 장면에 있는 인간을 대부분 무시합니다. 더욱이 기존 방법은 일반적으로 단일 이미지 프레임에서 개방형 어휘 관계만 캡처하므로 관찰된 콘텐츠를 넘어서는 장거리 상호 작용을 모델링하는 능력이 제한됩니다. 개방형 어휘 프레임워크를 사용하여 로컬 및 원격 환경에서 인간, 인간의 속성, 활동 및 관계를 캡처하는 증강 3D 장면 그래프 표현인 소셜 3D 장면 그래프를 소개합니다. 또한 3D에서 사회적 장면 이해를 평가하기 위한 포괄적인 인간 장면 관계 주석과 다양한 유형의 쿼리를 갖춘 합성 환경으로 구성된 새로운 벤치마크를 소개합니다. 실험은 우리의 표현이 인간 활동 예측과 인간-환경 관계에 대한 추론을 향상시켜 사회적으로 지능적인 로봇을 향한 길을 닦는다는 것을 보여줍니다."
913,http://arxiv.org/abs/2509.23563 ,RAVEN: Resilient Aerial Navigation via Open-Set Semantic Memory and Behavior Adaptation,"Seungchan Kim, Omar Alama, Dmytro Kurdydyk, John Keller, Nikhil Keetha, Wenshan Wang, Yonatan Bisk, Sebastian Scherer","공중 실외 의미론적 탐색에는 로봇이 대상 물체를 찾기 위해 대규모의 구조화되지 않은 환경을 탐색해야 합니다. 의미론적 탐색의 최근 발전으로 실내 설정에서 개방형 개체-목표 탐색이 입증되었지만 이러한 방법은 제한된 공간 범위와 구조화된 레이아웃으로 인해 여전히 제한되어 있어 장거리 실외 검색에 적합하지 않습니다. 실외 의미론적 탐색 접근 방식이 존재하지만 근시안적인 동작을 생성하는 경향이 있는 현재 관찰을 기반으로 하는 반응적 정책에 의존하거나 탐색을 위해 오프라인으로 장면 그래프를 미리 계산하여 온라인 배포에 대한 적응성을 제한합니다. 우리는 구조화되지 않은 실외 환경에서 공중 의미론적 항법을 위한 3D 메모리 기반 행동 트리 프레임워크인 RAVEN을 제시합니다. (1) 공간적으로 일관된 의미론적 복셀 광선 맵을 영구 메모리로 사용하여 장거리 계획을 가능하게 하고 순전히 반응적인 동작을 방지합니다. (2) 단거리 복셀 검색과 장거리 광선 검색을 결합하여 대규모 환경으로 확장합니다. (3) 대규모 비전 언어 모델을 활용하여 보조 신호를 제안하고 야외 대상의 희박성을 완화합니다. 이러한 구성 요소는 강력한 작동을 위해 동작을 적응적으로 전환하는 동작 트리에 의해 조정됩니다. 우리는 단일 객체 검색, 다중 클래스, 다중 인스턴스 탐색 및 순차적 작업 변경을 포함하는 100개 이상의 의미론적 작업이 넘는 10개의 사실적인 야외 시뮬레이션 환경에서 RAVEN을 평가합니다. 결과에 따르면 RAVEN은 시뮬레이션에서 기준선을 85.25% 능가하고 실외 현장 테스트에서 공중 로봇에 배포하여 실제 적용 가능성을 입증했습니다."
912,http://arxiv.org/abs/2509.23107 ,Open-Vocabulary Spatio-Temporal Scene Graph for Robot Perception and Teleoperation Planning,"Yi Wang, Zeyu Xue, Mujie Liu, Tongqin Zhang, Yan Hu, Zhou Zhao, Chenguang Yang, Zhenyu Lu",자연어를 통한 원격 조작은 운영자의 작업량을 줄이고 위험도가 높은 환경이나 원격 환경에서 안전성을 향상시킵니다. 그러나 동적 원격 장면에서는 양방향 통신 중 전송 지연으로 인해 원격 인식 상태와 운영자 의도 사이에 격차가 발생하여 명령 오해와 잘못된 실행이 발생합니다. 이를 완화하기 위해 시간 역학 및 가벼운 대기 시간 주석을 통해 개방형 어휘 인식을 강화하는 표현인 ST-OVSG(Spatio-Temporal Open-Vocabulary Scene Graph)를 도입합니다. ST-OVSG는 LVLM을 활용하여 개방형 어휘 3D 객체 표현을 구성하고 시간적 일치 비용을 사용하여 헝가리어 할당을 통해 시간 영역으로 확장하여 통합된 시공간 장면 그래프를 생성합니다. LVLM 플래너가 과거 장면 상태를 소급하여 쿼리할 수 있도록 대기 시간 태그가 내장되어 있어 전송 지연으로 인한 로컬-원격 상태 불일치를 해결할 수 있습니다. 중복성을 더욱 줄이고 작업 관련 단서를 강조하기 위해 플래너를 위한 간결한 입력을 생성하는 작업 중심 하위 그래프 필터링 전략을 제안합니다. ST-OVSG는 새로운 카테고리를 일반화하고 미세 조정 없이도 전송 대기 시간에 대한 계획 견고성을 향상시킵니다. 실험에 따르면 우리의 방법은 Replica 벤치마크에서 74%의 노드 정확도를 달성하여 ConceptGraph를 능가하는 것으로 나타났습니다. 특히 지연 시간 견고성 실험에서 ST-OVSG의 지원을 받는 LVLM 플래너는 70.5%의 계획 성공률을 달성했습니다.
911,http://arxiv.org/abs/2509.22281 ,MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning,"Jinkun Hao, Naifu Liang, Zhen Luo, Xudong Xu, Weipeng Zhong, Ran Yi, Yichen Jin, Zhaoyang Lyu, Feng Zheng, Lizhuang Ma, Jiangmiao Pang","인간의 지시를 해석하고 조작 작업을 실행하는 로봇의 능력은 훈련을 위한 작업 관련 테이블 장면의 가용성을 필요로 합니다. 그러나 이러한 장면을 만드는 전통적인 방법은 시간이 많이 걸리는 수동 레이아웃 디자인이나 순전히 무작위 레이아웃에 의존하며 타당성이나 작업 정렬 측면에서 제한됩니다. 본 논문에서는 높은 수준의 작업 지침과 테이블탑 장면 사이의 상당한 격차로 인해 상당한 어려움을 겪는 작업 중심의 테이블탑 장면 생성이라는 새로운 작업을 공식화합니다. 이러한 까다로운 작업에 대한 연구를 지원하기 위해 우리는 현실적인 레이아웃과 복잡한 객체 간 관계를 보장하는 수동으로 제작된 레이아웃을 갖춘 약 10,700개의 합성 테이블탑 장면으로 구성된 대규모 데이터 세트인 MesaTask-10K를 소개합니다. 작업과 장면 사이의 격차를 해소하기 위해 생성 과정을 객체 추론, 공간 상호 관계 추론 및 최종 3D 레이아웃을 위한 장면 그래프 구성으로 분해하는 공간 추론 체인을 제안합니다. 우리는 이 추론 체인을 활용하고 DPO 알고리즘으로 더욱 강화되어 주어진 작업 설명과 잘 일치하는 물리적으로 그럴듯한 테이블톱 장면을 생성하는 LLM 기반 프레임워크인 MesaTask를 제시합니다. 철저한 실험은 현실적인 레이아웃으로 작업에 적합한 테이블 장면을 생성할 때 기준선과 비교하여 MesaTask의 우수한 성능을 보여줍니다. 프로젝트 페이지는 https://mesatask.github.io/에 있습니다."
910,http://arxiv.org/abs/2509.22014 ,Lightweight Structured Multimodal Reasoning for Clinical Scene Understanding in Robotics,"Saurav Jha, Stefan K. Ehrlich","의료 로봇에는 역동적인 임상 환경에서 안전을 보장하기 위한 강력한 다중 모드 인식과 추론이 필요합니다. 현재 비전-언어 모델(VLM)은 강력한 범용 기능을 보여주지만 로봇 계획에 필요한 시간적 추론, 불확실성 추정 및 구조화된 출력에서는 여전히 제한적입니다. 우리는 비디오 기반 장면 이해를 위한 경량 에이전트 멀티모달 프레임워크를 제시합니다. Qwen2.5-VL-3B-Instruct 모델과 SmolAgent 기반 오케스트레이션 레이어를 결합하여 사고 사슬 추론, 음성 비전 융합 및 동적 도구 호출을 지원합니다. 프레임워크는 구조화된 장면 그래프를 생성하고 해석 가능하고 적응 가능한 추론을 위해 하이브리드 검색 모듈을 활용합니다. Video-MME 벤치마크 및 맞춤형 임상 데이터 세트에 대한 평가는 최첨단 VLM에 비해 경쟁력 있는 정확성과 향상된 견고성을 보여 로봇 보조 수술, 환자 모니터링 및 의사 결정 지원에 적용할 수 있는 잠재력을 보여줍니다."
909,http://arxiv.org/abs/2509.21928 ,SAGE: Scene Graph-Aware Guidance and Execution for Long-Horizon Manipulation Tasks,"Jialiang Li, Wenzheng Wu, Gaojing Zhang, Yifan Han, Wenzhao Lian","장거리 조작 작업을 성공적으로 해결하는 것은 여전히 ​​근본적인 과제로 남아 있습니다. 이러한 작업에는 확장된 작업 순서와 복잡한 개체 상호 작용이 포함되며, 이는 높은 수준의 상징적 계획과 낮은 수준의 지속적인 제어 사이에 중요한 격차를 나타냅니다. 이러한 격차를 해소하려면 강력한 장기 작업 계획과 효과적인 목표 조건 조작이라는 두 가지 필수 기능이 필요합니다. 기존 및 LLM 기반 접근 방식을 포함한 기존 작업 계획 방법은 제한된 일반화 또는 희박한 의미 추론을 나타내는 경우가 많습니다. 한편, 이미지 조절 제어 방법은 보이지 않는 작업에 적응하는 데 어려움을 겪습니다. 이러한 문제를 해결하기 위해 우리는 장수평 조작 작업에서 장면 그래프 인식 안내 및 실행을 위한 새로운 프레임워크인 SAGE를 제안합니다. SAGE는 장면 상태에 대한 구조적 표현으로 의미론적 장면 그래프를 활용합니다. 구조적 장면 그래프를 사용하면 작업 수준의 의미 추론과 픽셀 수준의 시각 운동 제어를 연결할 수 있습니다. 이는 또한 정확하고 새로운 하위 목표 이미지의 제어 가능한 합성을 용이하게 합니다. SAGE는 두 가지 주요 구성 요소로 구성됩니다. (1) VLM 및 LLM을 사용하여 물리적 기반 장면 상태 전환 시퀀스에 대한 환경과 추론을 구문 분석하는 장면 그래프 기반 작업 플래너와 (2) 이미지 인페인팅 및 구성을 통해 각 대상 하위 목표 그래프를 해당 이미지로 제어 가능하게 변환하는 분리된 구조적 이미지 편집 파이프라인입니다. 광범위한 실험을 통해 SAGE가 뚜렷한 장거리 작업에서 최첨단 성능을 달성한다는 것이 입증되었습니다."
908,http://arxiv.org/abs/2509.21783 ,Prompt-guided Disentangled Representation for Action Recognition,"Tianci Wu, Guangming Zhu, Jiang Lu, Siyuan Wang, Ning Wang, Nuoye Xiong, Zhang Liang",동작 인식은 비디오 이해의 기본 작업입니다. 기존 방법은 일반적으로 하나의 비디오에서 모든 동작을 처리하기 위해 통합된 기능을 추출하므로 다중 동작 시나리오에서 서로 다른 개체 간의 상호 작용을 모델링하기가 어렵습니다. 이 문제를 완화하기 위해 우리는 효과적인 솔루션으로 복잡한 장면에서 특정 동작을 분리하는 방법을 모색합니다. 본 논문에서는 다중 액션 장면에서 특정 액션을 분리하는 새로운 프레임워크인 ProDA(Prompt-guided Disentangled Representation for Action Recognition)를 제안합니다. ProDA는 SSG(시공간 장면 그래프)를 활용하고 DPM(동적 프롬프트 모듈)을 도입하여 GPNN(그래프 구문 분석 신경망)이 작업별 표현을 생성하도록 안내합니다. 또한 동적 가중치를 사용하여 정보를 집계하는 비디오 적응 GPNN을 설계합니다. 비디오 동작 인식 실험은 최첨단 방법과 비교할 때 우리 접근 방식의 효율성을 보여줍니다. 우리의 코드는 https://github.com/iamsnaping/ProDA.git에서 찾을 수 있습니다.
907,http://arxiv.org/abs/2509.21576 ,"Vision Language Models Cannot Plan, but Can They Formalize?","Muyu He, Yuxi Zheng, Yuchen Liu, Zijian An, Bill Cai, Jiani Huang, Lifeng Zhou, Feng Liu, Ziyang Li, Li Zhang","비전 언어 모델(VLM)의 발전으로 구현된 에이전트는 간단한 다중 모드 계획 작업을 수행할 수 있지만 긴 일련의 작업이 필요한 장거리 작업은 수행할 수 없습니다. 텍스트 전용 시뮬레이션에서는 LLM의 역할을 재배치함으로써 장기적인 계획이 크게 개선되었습니다. 작업 시퀀스를 직접 생성하는 대신 LLM은 계획 영역과 문제를 PDDL(계획 도메인 정의 언어)과 같은 공식적인 계획 언어로 변환합니다. PDDL은 공식적인 해결자를 호출하여 검증 가능한 방식으로 계획을 도출할 수 있습니다. 다중 모드 환경에서 VLM-as-formalizer에 대한 연구는 거의 남아 있지 않으며 일반적으로 사전 정의된 객체 어휘 또는 지나치게 유사한 소수의 예제와 같은 총체적인 단순화를 포함합니다. 이 연구에서는 일회성, 개방형 어휘 및 다중 모드 PDDL 형식화를 다루는 5개의 VLM 형식화기 파이프라인 제품군을 제시합니다. 우리는 기존 벤치마크에서 이를 평가하는 동시에 원본, 다중 뷰 및 저품질 이미지를 사용한 계획을 설명하는 또 다른 두 가지를 처음으로 제시합니다. 우리는 VLM-as-formalizer가 종단 간 계획 생성보다 훨씬 뛰어난 성능을 발휘한다는 결론을 내렸습니다. VLM이 필요한 개체 관계의 전체 세트를 캡처하지 못하는 경우가 많기 때문에 병목 현상이 언어보다는 비전임을 밝힙니다. 캡션이나 장면 그래프와 같은 중간 텍스트 표현을 생성하는 동안 성능을 부분적으로 보상하지만 일관되지 않은 이득은 다중 모드 계획 공식화에 대한 향후 연구 방향에 대한 여유를 남깁니다."
906,http://arxiv.org/abs/2509.20941 ,Decoding the Surgical Scene: A Scoping Review of Scene Graphs in Surgery,"Angelo Henriques, Korab Hoxha, Daniel Zapp, Peter C. Issa, Nassir Navab, M. Ali Nasseri","장면 그래프(SG)는 복잡하고 역동적인 수술 환경을 디코딩하는 데 중요한 구조화된 관계형 표현을 제공합니다. 이 PRISMA-ScR 기반 범위 지정 검토는 수술 분야에서 SG 연구의 진화하는 환경을 체계적으로 매핑하고 해당 응용 프로그램, 방법론적 발전 및 미래 방향을 차트로 표시합니다. 우리의 분석은 급속한 성장을 보여주지만 중요한 '데이터 격차'를 드러냅니다. 내부 보기 연구(예: 삼중항 인식)는 거의 독점적으로 실제 2D 비디오를 사용하는 반면, 외부 보기 4D 모델링은 시뮬레이션된 데이터에 크게 의존하여 주요 변환 연구 격차를 드러냅니다. 방법론적으로 이 분야는 기본 그래프 신경망에서 이제 외과적 맥락에서 일반의 대규모 비전 언어 모델보다 훨씬 뛰어난 성능을 발휘하는 전문 기반 모델로 발전했습니다. 이러한 발전으로 인해 SG는 작업 흐름 인식 및 자동화된 안전 모니터링과 같은 분석과 제어 가능한 수술 시뮬레이션과 같은 생성 작업 모두를 위한 초석 기술로 자리 잡았습니다. 데이터 주석 및 실시간 구현에 대한 문제가 지속되고 있지만 새로운 기술을 통해 적극적으로 해결되고 있습니다. 수술 SG는 필수적인 의미 연결로 성숙되어 차세대 지능형 시스템을 통해 수술 안전성, 효율성 및 교육을 향상시킵니다."
905,http://arxiv.org/abs/2509.22720 ,LayoutAgent: A Vision-Language Agent Guided Compositional Diffusion for Spatial Layout Planning,"Zezhong Fan, Xiaohan Li, Luyi Ma, Kai Zhao, Liang Peng, Topojoy Biswas, Evren Korpeoglu, Kaushiki Nag, Kannan Achan","사실적인 다중 객체 장면을 디자인하려면 이미지를 생성하는 것뿐만 아니라 의미론적 관계와 물리적 타당성을 존중하는 공간 레이아웃을 계획해야 합니다. 최근 확산 모델의 발전으로 고품질 이미지 생성이 가능해졌지만 명시적인 공간 추론이 부족하여 비현실적인 객체 레이아웃이 발생했습니다. 반면, 로봇 공학의 전통적인 공간 계획 방법은 기하학적, 관계적 일관성을 강조하지만 시각적 장면에서 의미론적 풍부함을 포착하는 데 어려움을 겪습니다. 이러한 격차를 해소하기 위해 본 논문에서는 비전 언어 추론과 레이아웃 생성을 위한 구성 확산을 통합하는 에이전트 프레임워크인 LayoutAgent를 제안합니다. 대상 객체가 포함된 여러 입력 이미지가 주어지면 우리 방법은 먼저 시각적 언어 모델을 사용하여 분할, 객체 크기 추정, 장면 그래프 구성 및 프롬프트 재작성을 통해 입력을 전처리합니다. 그런 다음 공간 레이아웃을 위해 장면 그래프에 인코딩된 객체 관계를 존중하는 경계 상자를 합성하기 위해 로봇 공학에서 전통적으로 사용되는 방법인 구성 확산을 활용합니다. 결국 전경 조절 이미지 생성기는 설계된 프롬프트에 따라 객체를 계획된 레이아웃으로 렌더링하여 전체 장면을 구성합니다. 실험에 따르면 LayoutAgent는 레이아웃 일관성, 공간 현실성 및 미적 정렬 측면에서 다른 최첨단 레이아웃 생성 모델보다 성능이 뛰어납니다."
904,http://arxiv.org/abs/2509.20077 ,Queryable 3D Scene Representation: A Multi-Modal Framework for Semantic Reasoning and Robotic Task Planning,"Xun Li, Rodrigo Santa Cruz, Mingze Xi, Hu Zhang, Madhawa Perera, Ziwei Wang, Ahalya Ravendran, Brandon J. Matthews, Feng Xu, Matt Adcock, Dadong Wang, Jiajun Liu","로봇이 높은 수준의 인간 지시를 이해하고 복잡한 작업을 수행할 수 있도록 하기 위한 핵심 과제는 포괄적인 장면 이해, 즉 의미 있는 방식으로 3D 환경을 해석하고 상호 작용하는 것입니다. 이를 위해서는 정확한 기하학적 구조와 풍부하고 인간이 이해할 수 있는 의미를 융합한 스마트 지도가 필요합니다. 이 문제를 해결하기 위해 우리는 세 가지 보완적인 3D 표현을 통합하는 멀티미디어 데이터를 기반으로 구축된 새로운 프레임워크인 3D QSR(3D 쿼리 가능한 장면 표현)을 소개합니다. (1) 3D 일관성이 있는 새로운 뷰 렌더링 및 팬옵틱 재구성의 분할, (2) 3D 포인트 클라우드의 정확한 기하학, (3) 3D 장면 그래프를 통한 구조화되고 확장 가능한 구성. 객체 중심 설계를 기반으로 구축된 프레임워크는 대규모 비전 언어 모델과 통합되어 다중 모드 객체 임베딩을 연결하고 기하학적, 시각적, 의미론적 정보의 객체 수준 검색을 지원함으로써 의미론적 쿼리 가능성을 지원합니다. 그런 다음 검색된 데이터는 다운스트림 실행을 위해 로봇 작업 플래너에 로드됩니다. 우리는 추상적인 언어 지침에 따라 실내 공개 데이터 세트인 Replica를 사용하여 Unity에서 시뮬레이션된 로봇 작업 계획 시나리오를 통해 접근 방식을 평가합니다. 또한 이를 실제 실험실 환경의 디지털 복제본에 적용하여 비상 대응을 위한 QSR 지원 로봇 작업 계획을 테스트합니다. 결과는 장면 이해를 촉진하고 공간 및 의미 추론을 통합하여 복잡한 3D 환경에서 높은 수준의 인간 지침을 정밀한 로봇 작업 계획으로 효과적으로 변환하는 프레임워크의 능력을 보여줍니다."
903,http://arxiv.org/abs/2509.20401 ,SGAligner++: Cross-Modal Language-Aided 3D Scene Graph Alignment,"Binod Singh, Sayan Deb Sarkar, Iro Armeni","3D 장면 그래프를 정렬하는 것은 로봇 탐색 및 구체화된 인식의 여러 응용 분야에서 중요한 초기 단계입니다. 3D 장면 그래프 정렬의 현재 방법은 종종 단일 양식 포인트 클라우드 데이터에 의존하고 불완전하거나 잡음이 많은 입력으로 인해 어려움을 겪습니다. 3D 장면 그래프 정렬을 위한 크로스 모달 언어 지원 프레임워크인 SGAigner++를 소개합니다. 우리의 방법은 통합된 조인트 임베딩 공간을 학습하여 이질적인 양식에 걸쳐 부분적으로 겹치는 장면 관찰을 정렬하는 문제를 해결하고 낮은 중첩 조건 및 센서 노이즈에서도 정확한 정렬을 가능하게 합니다. SGAigner++는 가벼운 단일 모달 인코더와 주의 기반 융합을 사용하여 시각적 위치 파악, 3D 재구성 및 탐색과 같은 작업에 대한 장면 이해를 향상시키는 동시에 확장성을 보장하고 계산 오버헤드를 최소화합니다. 실제 데이터 세트에 대한 광범위한 평가를 통해 SGAigner++는 시끄러운 실제 재구성에서 최첨단 방법보다 최대 40% 뛰어난 성능을 발휘하는 동시에 모드 간 일반화를 가능하게 한다는 것을 보여줍니다."
902,http://arxiv.org/abs/2509.19579 ,Terra: Hierarchical Terrain-Aware 3D Scene Graph for Task-Agnostic Outdoor Mapping,"Chad R. Samuelson, Abigail Austin, Seth Knoop, Blake Romrell, Gabriel R. Slade, Timothy W. McLain, Joshua G. Mangelson","실외 지능형 자율 로봇 작동은 충분히 표현 가능한 환경 지도에 의존합니다. 고전적인 기하학적 매핑 방법은 필수적인 구조적 환경 정보를 유지하지만 높은 수준의 로봇 추론을 허용하는 의미론적 이해와 구성이 부족합니다. 3D 장면 그래프(3DSG)는 기하학적, 위상적, 의미론적 관계를 다단계 그래프 기반 맵에 통합하여 이러한 제한을 해결합니다. 실외 자율 운영은 일반적으로 작업 의존성 또는 로봇 플랫폼의 이동 가능성으로 인해 지형 정보에 의존합니다. 우리는 실내 3DSG 기술을 표준 실외 기하학적 매핑 및 지형 인식 추론과 결합하여 실외 환경을 위한 지형 인식 장소 노드와 계층적으로 구성된 영역을 생성하는 새로운 접근 방식을 제안합니다. 우리의 방법은 작업에 구애받지 않는 메트릭 의미론적 희소 맵을 생성하고 이 맵에서 다운스트림 계획 작업을 위해 3DSG를 구성하는 동시에 자율 로봇 작동을 위해 경량을 유지합니다. 우리의 철저한 평가는 3DSG 방법이 객체 검색에서 최첨단 카메라 기반 3DSG 방법과 동등하게 수행되고 메모리 효율성을 유지하면서 영역 분류에서 이를 능가한다는 것을 보여줍니다. 우리는 시뮬레이션과 실제 환경 모두에서 객체 검색 및 지역 모니터링과 같은 다양한 로봇 작업에서 그 효율성을 입증합니다."
901,http://arxiv.org/abs/2509.18592 ,VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation,"Neel P. Bhatt, Yunhao Yang, Rohan Siva, Pranay Samineni, Daniel Milan, Zhangyang Wang, Ufuk Topcu","눈에 보이지 않는 환경에서의 신속한 적응은 확장 가능한 실제 자율성을 위해 필수적이지만 기존 접근 방식은 일반화에 실패하는 철저한 탐색 또는 엄격한 탐색 정책에 의존합니다. 우리는 비전 언어 모델을 활용하여 상징적 장면 그래프를 효율적으로 구성하고 제로샷 신경 기호 탐색을 가능하게 하는 2단계 비전 언어 탐색 프레임워크인 VLN-Zero를 제시합니다. 탐색 단계에서 구조화된 프롬프트는 VLM 기반 검색을 유익하고 다양한 궤적으로 안내하여 간결한 장면 그래프 표현을 생성합니다. 배포 단계에서 신경 기호 계획자는 실행 가능한 계획을 생성하기 위해 장면 그래프와 환경 관찰을 추론하고, 캐시 지원 실행 모듈은 이전에 계산된 작업 위치 궤적을 재사용하여 적응을 가속화합니다. 제안된 프레임워크는 신속한 탐색, 상징적 추론 및 캐시 지원 실행을 결합하여 이전 비전 언어 탐색 방법의 계산 비효율성과 빈약한 일반화를 극복하여 보이지 않는 환경에서 강력하고 확장 가능한 의사 결정을 가능하게 합니다. VLN-Zero는 최첨단 제로샷 모델에 비해 2배 더 높은 성공률을 달성하고, 대부분의 미세 조정된 기준선을 능가하며, 다양한 환경에서 최첨단 모델에 비해 평균 55% 더 적은 VLM 호출로 절반의 시간 내에 목표 위치에 도달합니다. VLN-Zero에 대한 코드베이스, 데이터 세트 및 비디오는 https://vln-zero.github.io/에서 확인할 수 있습니다."
900,http://arxiv.org/abs/2509.16336 ,Neural Atlas Graphs for Dynamic Scene Decomposition and Editing,"Jan Philipp Schneider, Pratik Singh Bisht, Ilya Chugunov, Andreas Kolb, Michael Moeller, Felix Heide",동적 장면에 대한 편집 가능한 고해상도 장면 표현을 학습하는 것은 자율 주행에서 창의적 편집에 이르기까지 모든 영역에 걸쳐 적용되는 공개 문제입니다. 오늘날 가장 성공적인 접근 방식은 편집 가능성과 지원 장면 복잡성 사이의 절충안을 제시합니다. 신경 아틀라스는 동적 장면을 2D에서 편집 가능하지만 여러 객체가 가려지고 상호 작용할 때 분해되는 두 개의 변형 이미지 레이어(전경 및 배경)로 표현합니다. 이와 대조적으로 장면 그래프 모델은 자율 주행 데이터 세트의 마스크 및 경계 상자와 같은 주석이 달린 데이터를 사용하여 복잡한 3D 공간 관계를 캡처하지만 암시적 체적 노드 표현은 일관되게 뷰를 편집하기가 어렵습니다. 우리는 하이브리드 고해상도 장면 표현인 신경 아틀라스 그래프(NAG)를 제안합니다. 여기서 모든 그래프 노드는 뷰 종속 신경 아틀라스로서 2D 모양 편집과 장면 요소의 3D 순서 지정 및 위치 지정을 모두 용이하게 합니다. 테스트 시간에 맞춰 NAG는 Waymo Open Dataset에서 기존 방법에 비해 PSNR이 5dB 증가한 최첨단 정량적 결과를 달성하고 고해상도 및 시각적 품질로 환경 편집을 가능하게 하여 새로운 배경과 편집된 차량 외관으로 반사실적 운전 시나리오를 생성합니다. 우리는 이 방법이 운전 장면을 넘어 일반화하고 다양한 인간 및 동물 중심 장면 세트를 포함하는 DAVIS 비디오 데이터 세트의 최근 매트 및 비디오 편집 기준선과 PSNR의 7dB 이상을 유리하게 비교한다는 사실을 발견했습니다.   프로젝트 페이지: https://princeton-computational-imaging.github.io/nag/
899,http://arxiv.org/abs/2509.16053 ,Compose by Focus: Scene Graph-based Atomic Skills,"Han Qi, Changhe Chen, Heng Yang","일반 로봇의 핵심 요구 사항은 구성 일반화, 즉 원자 기술을 결합하여 복잡하고 장기적인 작업을 해결하는 능력입니다. 이전 작업은 주로 사전 학습된 기술을 순서대로 나열하는 플래너를 합성하는 데 중점을 두었지만 장면 구성에 의해 유도된 분포 변화에 따라 시력 운동 정책이 실패하는 경우가 많기 때문에 개별 기술 자체의 강력한 실행 자체는 여전히 어려운 과제로 남아 있습니다. 이 문제를 해결하기 위해 작업 관련 개체 및 관계에 초점을 맞춰 관련 없는 변화에 대한 민감도를 완화하는 장면 그래프 기반 표현을 도입합니다. 이 아이디어를 바탕으로 우리는 그래프 신경망과 확산 기반 모방 학습을 통합하고 ""집중된"" 장면 그래프 기술을 비전 언어 모델(VLM) 기반 작업 계획과 결합하는 장면 그래프 기술 학습 프레임워크를 개발합니다. 시뮬레이션 및 실제 조작 작업의 실험은 최첨단 기준보다 훨씬 더 높은 성공률을 보여 주었으며, 장거리 작업에서 향상된 견고성과 구성 일반화가 강조되었습니다."
898,http://arxiv.org/abs/2509.14949 ,Human Interaction for Collaborative Semantic SLAM using Extended Reality,"Laura Ribeiro, Muhammad Shaheer, Miguel Fernandez-Cortizas, Ali Tourani, Holger Voos, Jose Luis Sanchez-Lopez","Semantic SLAM(Simultaneous Localization and Mapping) 시스템은 구조적, 의미적 정보로 로봇 맵을 풍부하게 하여 로봇이 복잡한 환경에서 더욱 효과적으로 작동할 수 있도록 해줍니다. 그러나 이러한 시스템은 인간이 자연스럽게 적용하는 더 높은 수준의 공간 및 의미론적 지식을 완전히 활용할 수 없기 때문에 폐색, 불완전한 데이터 또는 모호한 기하학이 있는 실제 시나리오에서 어려움을 겪습니다. 실시간 협업을 위해 공유 확장 현실 환경을 사용하는 Human-in-the-Loop 의미론적 SLAM 프레임워크인 HICS-SLAM을 소개합니다. 이 시스템을 통해 인간 운영자는 로봇의 3D 장면 그래프와 직접 상호 작용하고 시각화할 수 있으며 높은 수준의 의미 개념(예: 방 또는 구조적 개체)을 매핑 프로세스에 추가할 수 있습니다. 우리는 이러한 인간 개입과 로봇 인식을 통합하여 향상된 상황 인식을 위한 확장 가능한 협업을 가능하게 하는 그래프 기반 의미 융합 방법론을 제안합니다. 실제 건설 현장 데이터세트에 대한 실험적 평가는 자동화된 기준선에 비해 공간 감지 정확도, 지도 정밀도 및 의미적 완전성이 향상되었음을 보여주며 접근 방식의 효율성과 향후 확장 가능성을 모두 보여줍니다."
897,http://arxiv.org/abs/2509.13733 ,FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with Hierarchical Multi-modal Scene Graph,"Xiaolin Zhou, Tingyang Xiao, Liu Liu, Yucheng Wang, Maiyue Chen, Xinrui Meng, Xinjie Wang, Wei Feng, Wei Sui, Zhizhong Su","VLN(시각 언어 탐색)은 실제 환경에서 구현된 에이전트를 배포하기 위한 광범위한 응용 프로그램을 갖춘 로봇 시스템의 근본적인 과제입니다. 최근의 발전에도 불구하고 기존 접근 방식은 장거리 공간 추론에 제한이 있으며, 특히 장거리 탐색 작업에서 성공률이 낮고 추론 대기 시간이 높은 경우가 많습니다. 이러한 한계를 해결하기 위해 우리는 HMSG(Hierarchical Multi-modal Scene Graph)와 FSR(Fast-to-Slow Navigation Reasoning)을 결합한 비전 언어 탐색 시스템인 FSR-VLN을 제안합니다. HMSG는 대략적인 공간 수준 위치 파악부터 세부적인 목표 보기 및 개체 식별에 이르기까지 점진적인 검색을 지원하는 다중 모드 지도 표현을 제공합니다. HMSG를 기반으로 구축된 FSR은 먼저 빠른 매칭을 수행하여 후보 룸, 뷰 및 객체를 효율적으로 선택한 다음 최종 목표 선택을 위해 VLM 기반 개선을 적용합니다. 우리는 휴머노이드 로봇이 수집한 4개의 포괄적인 실내 데이터 세트에서 FSR-VLN을 평가했으며, 다양한 범위의 객체 범주를 포괄하는 87개의 명령을 활용했습니다. FSR-VLN은 검색 성공률(RSR)로 측정된 모든 데이터 세트에서 최첨단(SOTA) 성능을 달성하는 동시에 빠른 직관이 실패하는 경우에만 느린 추론을 활성화하여 투어 비디오의 VLM 기반 방법에 비해 응답 시간을 82% 줄입니다. 또한 FSR-VLN을 Unitree-G1 휴머노이드 로봇의 음성 상호 작용, 계획 및 제어 모듈과 통합하여 자연어 상호 작용 및 실시간 탐색이 가능합니다."
896,http://arxiv.org/abs/2509.12901 ,MSGFusion: Multimodal Scene Graph-Guided Infrared and Visible Image Fusion,"Guihui Li, Bowei Dong, Kaizhi Dong, Jiayi Li, Haiyong Zheng","적외선 및 가시 이미지 융합은 복잡하고 가혹한 환경에서 이 두 가지 방식의 강력한 상호 보완성으로 인해 상당한 주목을 받았습니다. 딥러닝 기반 융합 방법은 특징 추출, 정렬, 융합 및 재구성 분야에서 눈에 띄는 발전을 이루었지만 여전히 질감 및 대비와 같은 낮은 수준의 시각적 단서에 크게 의존하고 이미지에 포함된 높은 수준의 의미 정보를 캡처하는 데 어려움을 겪습니다. 의미론적 지침의 소스로 텍스트를 통합하려는 최근 시도는 엔터티, 속성 및 관계를 명시적으로 모델링하지도 않고 공간적 위치를 제공하지도 않는 구조화되지 않은 설명에 의존하여 세분화된 융합 성능을 제한했습니다. 이러한 과제를 극복하기 위해 적외선 및 가시 이미지를 위한 다중 모드 장면 그래프 기반 융합 프레임워크인 MSGFusion을 소개합니다. 텍스트와 비전에서 파생된 구조화된 장면 그래프를 심층적으로 결합함으로써 MSGFusion은 엔터티, 속성 및 공간 관계를 명시적으로 표현한 다음 장면 그래프 표현, 계층적 집계 및 그래프 기반 융합을 위한 연속 모듈을 통해 높은 수준의 의미와 낮은 수준의 세부 정보를 동시에 개선합니다. 여러 공개 벤치마크에 대한 광범위한 실험을 통해 MSGFusion은 특히 세부 보존 및 구조적 명확성 측면에서 최첨단 접근 방식을 크게 능가하고 저조도 객체 감지, 의미론적 분할, 의료 이미지 융합과 같은 다운스트림 작업에서 우수한 의미론적 일관성 및 일반화 가능성을 제공하는 것으로 나타났습니다."
895,http://arxiv.org/abs/2509.11959 ,Learning to Generate 4D LiDAR Sequences,"Ao Liang, Youquan Liu, Yu Yang, Dongyue Lu, Linfeng Li, Lingdong Kong, Huaici Zhao, Wei Tsang Ooi","생성적 세계 모델은 고급 비디오 및 점유 기반 데이터 합성 기능을 갖추고 있지만 LiDAR 생성은 정확한 3D 인식에 대한 중요성에도 불구하고 여전히 미개발 상태로 남아 있습니다. 생성을 4D LiDAR 데이터로 확장하면 제어 가능성, 시간 안정성 및 평가에 문제가 발생합니다. 자유 형식 언어를 편집 가능한 LiDAR 시퀀스로 변환하는 통합 프레임워크인 LiDARCrafter를 소개합니다. 지침은 삼중 확산 모델이 개체 레이아웃, 궤적 및 모양으로 변환되는 자기 중심 장면 그래프로 구문 분석됩니다. 범위 이미지 확산 모델은 초기 스캔을 생성하고 자동 회귀 모듈은 이를 시간적으로 일관된 시퀀스로 확장합니다. 명시적인 레이아웃 디자인은 삽입이나 재배치와 같은 객체 수준 편집을 추가로 지원합니다. 공정한 평가를 위해 우리는 장면, 객체, 시퀀스 수준 지표를 포괄하는 벤치마크인 EvalSuite를 제공합니다. NuScenes에서 LiDARCrafter는 최첨단 충실도, 제어 가능성 및 시간적 일관성을 달성하여 LiDAR 기반 시뮬레이션 및 데이터 증강을 위한 기반을 제공합니다."
894,http://arxiv.org/abs/2509.11895 ,Integrating Prior Observations for Incremental 3D Scene Graph Prediction,"Marian Renz, Felix Igelbrink, Martin Atzmueller","3D 의미론적 장면 그래프(3DSSG)는 객체, 속성 및 관계를 명시적으로 모델링하여 환경의 간결한 구조적 표현을 제공합니다. 3DSSG는 로봇 공학 및 AI 구현에서 유망한 것으로 나타났지만, 기존의 많은 방법은 주로 센서 데이터에 의존하며 의미론적으로 풍부한 환경의 추가 정보를 통합하지 않습니다. 또한 대부분의 방법은 완전한 장면 재구성에 대한 액세스를 가정하여 실제 증분 설정에서의 적용 가능성을 제한합니다. 본 논문에서는 이전 관찰과 같은 추가 다중 모드 정보를 메시지 전달 프로세스에 직접 통합하는 증분 3DSSG 예측을 위한 새로운 이종 그래프 모델을 소개합니다. 여러 레이어를 활용하는 이 모델은 특수 모듈이나 전체 장면 재구성 없이 전역 및 로컬 장면 표현을 유연하게 통합합니다. 우리는 3DSSG 데이터 세트에 대한 접근 방식을 평가하여 의미론적 임베딩(예: CLIP) 및 사전 관찰과 같은 다중 모달 정보가 풍부한 GNN이 복잡한 실제 환경을 위한 확장 가능하고 일반화 가능한 솔루션을 제공한다는 것을 보여줍니다. 제시된 아키텍처의 전체 소스 코드는 https://github.com/m4renz/incremental-scene-graph-prediction에서 확인할 수 있습니다."
893,http://arxiv.org/abs/2509.11862 ,Bridging Vision Language Models and Symbolic Grounding for Video Question Answering,"Haodi Ma, Vyom Pathak, Daisy Zhe Wang","VQA(비디오 질문 응답) 모델은 비디오의 공간적, 시간적, 인과적 단서를 추론해야 합니다. 최근 VLM(비전 언어 모델)은 강력한 결과를 달성하지만 얕은 상관 관계에 의존하는 경우가 많아 시간 기반이 약하고 해석 가능성이 제한되는 경우가 많습니다. 우리는 VQA의 중간 접지 신호로 기호 장면 그래프(SG)를 연구합니다. SG는 VLM의 전체적인 추론을 보완하는 구조화된 개체 관계 표현을 제공합니다. 프롬프트 및 시각적 위치 파악을 통해 고정된 VLM을 장면 그래프 접지와 통합하는 모듈식 프레임워크인 SG-VLM을 소개합니다. 세 가지 벤치마크(NExT-QA, iVQA, ActivityNet-QA)와 여러 VLM(QwenVL, InternVL)에서 SG-VLM은 인과 및 시간적 추론을 개선하고 이전 기준을 능가하지만 강력한 VLM에 대한 이점은 제한적입니다. 이러한 연구 결과는 기호 접지의 가능성과 현재 한계를 모두 강조하고 비디오 이해에서 미래의 하이브리드 VLM 기호 접근 방식에 대한 지침을 제공합니다."
892,http://arxiv.org/abs/2509.11796 ,FineQuest: Adaptive Knowledge-Assisted Sports Video Understanding via Agent-of-Thoughts Reasoning,"Haodong Chen, Haojian Huang, XinXiang Yin, Dian Shao",LLM(대형 언어 모델)을 기반으로 하는 Video QA(비디오 질문 응답)는 일반적인 비디오 이해에 잠재력을 보여 주었지만 본질적으로 복잡한 스포츠 비디오 영역에 적용할 때 상당한 어려움에 직면합니다. 이 작업에서 우리는 인지 과학에서 영감을 받은 이중 모드 추론을 활용하는 최초의 훈련 없는 프레임워크인 FineQuest를 제안합니다. i) 간단한 스포츠 쿼리를 위한 반응 추론 및 ii) 보다 복잡한 쿼리를 위한 심의 추론. 범용 모델과 도메인별 스포츠 이해 간의 지식 격차를 해소하기 위해 FineQuest는 시각적 인스턴스와 도메인별 용어를 모두 인코딩하여 추론 정확도를 높이는 9개 스포츠에 걸친 다중 모드 스포츠 지식 장면 그래프인 SSGraph를 통합합니다. 또한 FineGym 및 FineDiving 데이터 세트에서 파생된 두 가지 새로운 스포츠 VideoQA 벤치마크인 Gym-QA 및 Diving-QA를 도입하여 다양하고 포괄적인 평가를 가능하게 합니다. FineQuest는 강력한 일반 VideoQA 기능을 유지하면서 이러한 벤치마크와 기존 SPORTU 데이터 세트에서 최첨단 성능을 달성합니다.
891,http://arxiv.org/abs/2509.09658 ,Measuring Epistemic Humility in Multimodal Large Language Models,"Bingkui Tong, Jiaer Xia, Sifeng Shang, Kaiyang Zhou","모델이 입력 이미지와 일치하지 않는 콘텐츠를 생성하는 MLLM(다중 언어 모델)의 환각은 시각적 질문 답변의 잘못된 정보부터 의사 결정의 안전하지 않은 오류까지 실제 응용 프로그램에 심각한 위험을 초래합니다. 기존 벤치마크는 주로 인식 정확도를 테스트합니다. 즉, 모델이 방해 요소 중에서 정답을 선택할 수 있는지 평가합니다. 이는 신뢰할 수 있는 AI의 똑같이 중요한 기능, 즉 제공된 옵션 중 어느 것도 올바르지 않은 경우를 인식하는 인식론적 겸손을 반영하는 동작을 간과합니다. 우리는 세 가지 환각 유형(객체, 관계, 속성)에 걸쳐 그럴듯하지만 잘못된 답변을 거부하는 MLLM의 능력을 평가하기 위해 설계된 새로운 환각 벤치마크인 HumbleBench를 제시합니다. Panoptic 장면 그래프 데이터 세트를 기반으로 구축된 우리는 세밀한 장면 그래프 주석을 활용하여 실측 엔터티 및 관계를 추출하고 GPT-4-Turbo가 객관식 질문을 생성하도록 유도한 후 엄격한 수동 필터링 프로세스를 수행합니다. 각 질문에는 ""해당 사항 없음"" 옵션이 포함되어 있어 모델이 올바른 시각적 정보를 인식할 뿐만 아니라 제공된 답변이 유효하지 않은 경우도 식별해야 합니다. 우리는 HumbleBench에서 범용 및 전문 추론 모델을 포함한 다양한 최첨단 MLLM을 평가하고 귀중한 결과와 통찰력을 커뮤니티와 공유합니다. 명시적인 잘못된 옵션 거부 기능을 통합함으로써 HumbleBench는 현재 평가 제품군의 주요 격차를 메워 안전이 중요한 설정에서 MLLM 신뢰성에 대한 보다 현실적인 측정값을 제공합니다. 우리의 코드와 데이터 세트는 공개적으로 공개되었으며 https://github.com/maifoundations/HumbleBench에서 액세스할 수 있습니다."
890,http://arxiv.org/abs/2509.09594 ,ObjectReact: Learning Object-Relative Control for Visual Navigation,"Sourav Garg, Dustin Craggs, Vineeth Bhat, Lachlan Mares, Stefan Podgorski, Madhava Krishna, Feras Dayoub, Ian Reid","단일 카메라와 위상 지도를 사용하는 시각적 탐색은 최근 추가 센서와 3D 지도가 필요한 방법에 대한 매력적인 대안이 되었습니다. 이는 일반적으로 주어진 현재 관찰 및 하위 목표 이미지 쌍에서 제어를 추정하는 ""이미지 상대적"" 접근 방식을 통해 달성됩니다. 그러나 이미지 수준의 세계 표현에는 이미지가 에이전트의 포즈 및 구현과 엄격하게 연결되어 있기 때문에 한계가 있습니다. 대조적으로, 지도의 속성인 객체는 구체화 및 궤적 불변의 세계 표현을 제공합니다. 이 작업에서 우리는 몇 가지 바람직한 특성을 나타내는 학습 ""객체 관련"" 제어의 새로운 패러다임을 제시합니다. a) 이전 경험을 엄격히 모방할 필요 없이 새로운 경로를 탐색할 수 있습니다. b) 제어 예측 문제를 이미지 일치 문제 해결에서 분리할 수 있으며, c) 훈련-테스트 및 매핑-실행 설정 모두에 걸친 변형에 대한 교차 구현 배포에서 높은 불변성을 달성할 수 있습니다. 우리는 더 많은 정보를 제공하는 객체 수준 전역 경로 계획 비용을 얻는 데 사용되는 ""상대"" 3D 장면 그래프 형태의 지형도 표현을 제안합니다. 우리는 명시적인 RGB 입력이 필요 없는 상위 수준의 ""WayObject Costmap"" 표현을 직접적으로 조정하여 ""ObjectReact""라는 로컬 컨트롤러를 훈련합니다. 우리는 센서 높이 변화와 기본 공간 이해 기능에 도전하는 여러 탐색 작업(예: 지도 궤적을 반대 방향으로 탐색) 전반에 걸쳐 이미지 상대적 제어에 대한 객체 상대적 제어를 학습하는 이점을 보여줍니다. 우리는 시뮬레이션 전용 정책이 실제 실내 환경에 잘 일반화될 수 있음을 보여줍니다. 코드 및 보충 자료는 프로젝트 페이지(https://object-react.github.io/)를 통해 액세스할 수 있습니다."
889,http://arxiv.org/abs/2509.07957 ,Graph-Fused Vision-Language-Action for Policy Reasoning in Multi-Arm Robotic Manipulation,"Shunlei Li, Longsen Gao, Jiuwen Cao, Yingbai Hu","인간의 비디오 시연에서 능숙한 로봇 기술을 획득하는 것은 주로 낮은 수준의 궤적 복제에 대한 기존의 의존으로 인해 여전히 중요한 과제로 남아 있으며, 이는 다양한 객체, 공간 레이아웃 및 조작기 구성에 걸쳐 일반화하는 데 종종 실패합니다. 이러한 제한 사항을 해결하기 위해 우리는 이중 팔 로봇 시스템이 RGB-D 인간 시연에서 직접 작업 수준 추론 및 실행을 수행할 수 있도록 하는 통합 프레임워크인 GF-VLA(Graph-Fused Vision-Language-Action)를 소개합니다. GF-VLA는 정보 이론적 접근 방식을 사용하여 작업 관련 단서를 추출하고 중요한 손-물체 및 개체-물체 상호 작용을 선택적으로 강조합니다. 이러한 단서는 시간적으로 정렬된 장면 그래프로 구성되며, 이후 언어 조절 변환기와 통합되어 계층적 동작 트리와 해석 가능한 데카르트 모션 프리미티브를 생성합니다. 양손 실행의 효율성을 높이기 위해 명시적인 기하학적 모델링 없이 그리퍼 할당을 자율적으로 결정하는 교차 팔 할당 전략을 제안합니다. 우리는 기호 구조 구성 및 공간 일반화와 관련된 4개의 이중 암 블록 어셈블리 벤치마크에서 GF-VLA를 검증합니다. 경험적 결과는 제안된 표현이 95% 이상의 그래프 정확도와 93% 이상의 하위 작업 분할을 달성하여 언어 동작 계획자가 강력하고 해석 가능한 작업 정책을 생성할 수 있음을 보여줍니다. 이중 팔 로봇에 배포하면 이러한 정책은 스태킹, 문자 형성 및 기하학적 재구성 작업 전반에 걸쳐 94%의 파악 신뢰성, 89%의 배치 정확도, 90%의 전체 작업 성공을 달성하여 다양한 공간 및 의미 변형에서 강력한 일반화 및 견고성을 입증합니다."
888,http://arxiv.org/abs/2509.06165 ,UNO: Unifying One-stage Video Scene Graph Generation via Object-Centric Visual Representation Learning,"Huy Le, Nhat Chung, Tung Kieu, Jingkang Yang, Ngan Le",VidSGG(비디오 장면 그래프 생성)는 객체를 감지하고 객체의 시간적 상호 작용을 구조화된 그래프로 모델링하여 동적 시각적 콘텐츠를 표현하는 것을 목표로 합니다. 이전 연구는 일반적으로 대략적인 박스 수준 또는 세밀한 팬옵틱 픽셀 수준 VidSGG를 대상으로 하며 종종 작업별 아키텍처와 다단계 교육 파이프라인이 필요합니다. 이 문서에서는 엔드투엔드 아키텍처 내에서 두 작업을 공동으로 처리하는 단일 단계 통합 프레임워크인 UNO(UNified Object-centric VidSGG)를 제시합니다. UNO는 작업별 수정을 최소화하고 매개변수 공유를 최대화하도록 설계되어 다양한 수준의 시각적 세분성 전반에 걸쳐 일반화가 가능합니다. UNO의 핵심은 시각적 기능을 개체 및 관계 슬롯으로 분해하는 확장된 슬롯 주의 메커니즘입니다. 강력한 시간 모델링을 보장하기 위해 명시적 추적 모듈에 의존하지 않고 프레임 전체에서 일관된 개체 표현을 적용하는 개체 시간 일관성 학습을 도입합니다. 또한 동적 삼중항 예측 모듈은 관계 슬롯을 해당 개체 쌍에 연결하여 시간이 지남에 따라 진화하는 상호 작용을 캡처합니다. 우리는 표준 박스 수준 및 픽셀 수준 VidSGG 벤치마크에서 UNO를 평가합니다. 결과는 UNO가 두 작업 모두에서 경쟁력 있는 성능을 달성할 뿐만 아니라 통합된 객체 중심 설계를 통해 향상된 효율성을 제공한다는 것을 보여줍니다. 코드는 https://github.com/Fsoft-AIC/UNO에서 확인할 수 있습니다.
887,http://arxiv.org/abs/2509.05661 ,Language-Driven Object-Oriented Two-Stage Method for Scene Graph Anticipation,"Xiaomeng Zhu, Changwei Wang, Haozhe Wang, Xinyu Liu, Fangzhen Lin","장면 그래프는 동적 장면에서 객체와 객체의 시공간 관계를 구조적으로 표현한 것입니다. SGA(장면 그래프 예측)에는 비디오 클립에서 미래 장면 그래프를 예측하여 지능형 감시 및 인간-기계 협업에 적용할 수 있는 기능이 포함됩니다. 최근 SGA 접근 방식은 시각적 증거를 활용하는 데 탁월한 반면, 장거리 예측은 근본적으로 시각적 특징에서만 추출하기 어려운 의미론적 사전 예측과 상식적 시간 규칙성에 의존합니다. 이러한 의미론적 역학을 명시적으로 모델링하기 위해 우리는 비디오 작업 시 모듈식 프런트엔드에서 처리되는 시각적 장면 그래프 감지를 사용하여 텍스트화된 장면 그래프 시퀀스에 대해 시간적 관계 추론을 수행하는 SGA의 언어적 공식인 언어적 장면 그래프 예측(LSGA)을 제안합니다. 이 공식을 바탕으로 우리는 객체 세트 역학을 예측하고 시간적 일관성 정규화를 통해 객체 중심 관계 궤적을 예측하는 언어 기반 프레임워크인 OOTSM(객체 지향 2단계 방법)을 도입하고 Action Genome 주석으로 구성된 전용 벤치마크에서 이를 평가합니다. 광범위한 실험을 통해 최대 30억 개의 매개변수를 갖춘 소형 미세 조정 언어 모델이 일치하는 텍스트 입력 및 컨텍스트 창에서 GPT-4o, GPT-4o-mini 및 DeepSeek-V3를 포함한 강력한 제로 및 원샷 API 기준보다 지속적으로 성능이 뛰어난 것으로 나타났습니다. 기성 시각적 장면 그래프 생성기와 결합하면 결과 다중 모드 시스템은 비디오 기반 SGA에서 상당한 개선을 달성하여 강력한 시각적 SGA 기준에 비해 장수평 mR@50을 최대 21.9\%까지 향상시킵니다."
886,http://arxiv.org/abs/2509.03516 ,"Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage, but Not Direct the Play?","Ouxiang Li, Yuan Wang, Xinting Hu, Huijuan Huang, Rui Chen, Jiarong Ou, Xin Tao, Pengfei Wan, Xiaojuan Qi, Fuli Feng","T2I(텍스트-이미지) 생성은 텍스트 프롬프트에서 이미지를 합성하는 것을 목표로 하며, 이는 표시되어야 하는 내용을 지정하고 추론할 수 있는 내용을 암시하므로 구성과 추론이라는 두 가지 핵심 기능에 해당합니다. 구성과 추론 모두에서 최근 T2I 모델의 발전에도 불구하고 기존 벤치마크의 평가는 여전히 제한적입니다. 두 기능 모두에 걸쳐 포괄적인 적용 범위를 제공하지 못할 뿐만 아니라 평가를 낮은 장면 밀도와 단순한 일대일 추론으로 크게 제한합니다. 이러한 제한 사항을 해결하기 위해 우리는 T2I 모델의 구성과 추론 기능을 모두 평가하는 포괄적이고 복잡한 벤치마크인 T2I-CoReBench를 제안합니다. 포괄성을 보장하기 위해 장면 그래프 요소(인스턴스, 속성 및 관계)를 중심으로 구성을 구성하고 추론의 철학적 프레임워크(연역적, 귀납적 및 귀납적)를 중심으로 추론을 구성하여 12차원 평가 분류 체계를 공식화합니다. 본질적인 실제 복잡성에 따라 복잡성을 높이기 위해 우리는 구성에 대한 구성 밀도를 높이고 추론을 위한 추론 강도를 높여 각 프롬프트를 선별합니다. 세밀하고 신뢰할 수 있는 평가를 용이하게 하기 위해 우리는 각 평가 프롬프트를 개별 예/아니요 질문을 지정하는 체크리스트와 결합하여 각 의도된 요소를 독립적으로 평가합니다. 통계에서 우리의 벤치마크는 1,080개의 도전적인 프롬프트와 약 13,500개의 체크리스트 질문으로 구성됩니다. 28개의 현재 T2I 모델에 대한 실험을 통해 구성 능력이 높은 구성 시나리오에서 여전히 제한적인 반면, 추론 능력은 심각한 병목 현상으로 인해 훨씬 ​​뒤쳐져 모든 모델이 프롬프트에서 암시적 요소를 추론하는 데 어려움을 겪고 있는 것으로 나타났습니다."
885,http://arxiv.org/abs/2509.01209 ,Measuring Image-Relation Alignment: Reference-Free Evaluation of VLMs and Synthetic Pre-training for Open-Vocabulary Scene Graph Generation,"Maëlic Neau, Zoe Falomir, Cédric Buche, Akihiro Sugimoto",SGG(장면 그래프 생성)는 이미지의 개체 간의 시각적 관계를 그래프 구조로 인코딩합니다. VLM(Vision-Language Models)의 발전 덕분에 광범위하고 다양한 관계를 학습하기 위해 모델의 기능을 평가하는 Open-Vocabulary SGG 작업이 최근 제안되었습니다. 그러나 SGG의 현재 벤치마크는 어휘가 매우 제한되어 있어 오픈 소스 모델을 비효율적으로 평가할 수 있습니다. 본 논문에서는 관계 예측을 위한 VLM의 개방형 어휘 기능을 공정하게 평가하기 위한 새로운 참조 없는 측정법을 제안합니다. Open-Vocabulary SGG의 또 다른 한계는 사전 훈련을 위해 품질이 좋지 않은 약한 감독 데이터에 의존한다는 것입니다. 또한 VLM의 지역별 신속한 튜닝을 통해 고품질 합성 데이터를 빠르게 생성할 수 있는 새로운 솔루션을 제안합니다. 실험 결과는 이 새로운 데이터 분할을 통한 사전 훈련이 Open-Voc SGG 모델의 일반화 기능에 도움이 될 수 있음을 보여줍니다.
884,http://arxiv.org/abs/2508.15258 ,Spatio-Temporal Mixed and Augmented Reality Experience Description for Interactive Playback,"Dooyoung Kim, Woontack Woo","우리는 사용자의 현재 물리적 공간에서 대화형 및 적응형 재생을 위해 과거 이벤트 표현을 표준화하는 새로운 프레임워크인 MAR-ED(시공간 혼합 및 증강 현실 경험 설명)를 제안합니다. 현재의 공간 미디어 기술은 주로 콘텐츠를 정적 자산으로 캡처하거나 재생하는 데 중점을 두고 시청자 환경과의 연결이 끊기거나 제한된 상호 작용을 제공하는 경우가 많지만, 경험의 기본 의미 및 상호 작용 구조를 설명하는 수단은 아직 충분히 탐구되지 않은 상태로 남아 있습니다. 우리는 1) 의미론적 장면 그래프 표현을 위한 이벤트 프리미티브, 2) 효율적이고 의미 있는 데이터 액세스를 위한 키프레임 프리미티브, 3) 기록된 MAR 경험의 사용자 중심 적응형 대화형 재생을 위한 재생 프리미티브의 세 가지 핵심 프리미티브를 기반으로 하는 MAR-ED라는 설명 프레임워크를 제안합니다. 제안된 MAR-ED 프레임워크의 3단계 프로세스에 대해 제안된 순서도는 기록된 경험을 재생 중에 고유한 적응형 MAR 경험으로 변환합니다. 여기서 시공간 구조는 새로운 환경에 동적으로 부합하고 해당 내러티브는 라이브 사용자 입력에 의해 변경될 수 있습니다. 이 프레임워크를 활용하여 개인의 디지털 추억과 녹화된 이벤트는 수동적인 2D/3D 비디오를 넘어 공간적으로 통합된 몰입형 그룹 경험으로 발전할 수 있으며, 복잡한 사용자별 적응형 렌더링 없이도 교육, 문화 유산 및 대화형 스토리텔링을 위한 새로운 패러다임을 열 수 있습니다."
883,http://arxiv.org/abs/2508.14502 ,SATURN: Autoregressive Image Generation Guided by Scene Graphs,"Thanh-Nhan Vo, Trong-Thuan Nguyen, Tam V. Nguyen, Minh-Triet Tran",최첨단 텍스트-이미지 모델은 사실적 렌더링에 탁월하지만 복잡한 프롬프트가 암시하는 레이아웃과 개체 관계를 캡처하는 데 종종 어려움을 겪습니다. 장면 그래프는 자연스러운 구조적 사전을 제공하지만 이전의 그래프 기반 접근 방식은 일반적으로 속도와 충실도 모두에서 최신 자동 회귀 아키텍처보다 뒤떨어지는 과도한 GAN 또는 확산 파이프라인에 의존했습니다. 장면 그래프를 돌출 순서 토큰 시퀀스로 변환하는 VAR-CLIP의 경량 확장인 SATURN(Structured Arrayment of Triplets for Unified Rendering Networks)을 소개합니다. 이를 통해 고정된 CLIP-VQ-VAE 백본은 VAR 변환기만 미세 조정하면서 그래프 구조를 해석할 수 있습니다. Visual Genome 데이터 세트에서 SATURN은 FID를 56.45%에서 21.62%로 줄이고 Inception Score를 16.03에서 24.78로 증가시켜 추가 모듈이나 다단계 교육 없이도 SG2IM 및 SGDiff와 같은 이전 방법보다 성능이 뛰어납니다. 질적 결과는 객체 수 충실도와 공간 관계 정확도의 개선을 더욱 확인하여 SATURN이 구조적 인식과 최첨단 자동 회귀 충실도를 효과적으로 결합한다는 것을 보여줍니다.
882,http://arxiv.org/abs/2508.12916 ,RoboRetriever: Single-Camera Robot Object Retrieval via Active and Interactive Perception with Dynamic Scene Graph,"Hecheng Wang, Jiankun Ren, Jia Yu, Lizhe Qi, Yunquan Sun","인간은 한 쌍의 눈만으로 시각적 추론, 적극적인 관점 조정 및 물리적 상호 작용을 결합하여 어수선하고 부분적으로 관찰 가능한 환경에서 물체를 쉽게 검색합니다. 대조적으로, 대부분의 기존 로봇 시스템은 완벽한 장면 가시성을 갖춘 신중하게 배치된 고정 또는 다중 카메라 설정에 의존하므로 적응성이 제한되고 높은 하드웨어 비용이 발생합니다. 우리는 손목에 장착된 \textbf{single} RGB-D 카메라와 자유 형식 자연어 명령만을 사용하여 작동하는 실제 객체 검색을 위한 새로운 프레임워크인 \textbf{RoboRetriever}를 제시합니다. RoboRetriever는 시간이 지남에 따라 객체 의미, 기하학 및 객체 간 관계를 인코딩하는 \textbf{동적 계층적 장면 그래프}를 구축하고 업데이트하기 위해 시각적 관찰을 기반으로 합니다. 감독자 모듈은 이 메모리와 작업 명령을 추론하여 대상 객체를 추론하고 \textbf{능동 인식}, \textbf{상호작용 인식} 및 \textbf{조작}을 결합한 통합 작업 모듈을 조정합니다. 작업 인식 장면 기반 활성 인식을 활성화하기 위해 대규모 추론 비전 언어 모델을 활용하여 의미론적 작업 목표 및 형상 장면 컨텍스트에 맞춰 6-DoF 카메라 포즈를 결정하는 새로운 시각적 프롬프트 체계를 도입합니다. 우리는 인간 개입이 포함된 시나리오를 포함하여 다양한 실제 객체 검색 작업에 대해 RoboRetriever를 평가하고 단 하나의 RGB-D 카메라를 사용하여 복잡한 장면에서 강력한 적응성과 견고성을 보여줍니다."
881,http://arxiv.org/abs/2508.11286 ,Scene Graph-Guided Proactive Replanning for Failure-Resilient Embodied Agent,"Che Rin Yu, Daewon Chae, Dabin Seo, Sangwon Lee, Hyeongwoo Im, Jinkyu Kim","인간이 일상적인 작업을 수행할 때 우리는 환경의 현재 상태에 따라 자연스럽게 행동을 조정합니다. 예를 들어, 서랍에 무언가를 넣으려고 했는데 닫혀 있는 것을 발견하면 먼저 서랍을 엽니다. 그러나 많은 자율 로봇에는 이러한 적응 인식이 부족합니다. 그들은 현장의 미묘하지만 중요한 변화를 간과할 수 있는 사전 계획된 작업을 따르는 경우가 많으며, 이로 인해 오래된 가정 하에 작업이 실행되고 결국 실패할 수 있습니다. 강력한 자율성을 위해서는 재계획이 중요하지만 대부분의 기존 방법은 복구가 비효율적이거나 실행 불가능할 수 있는 오류가 발생한 후에만 대응합니다. 사전 재계획을 통해 사전에 오류를 예방할 수 있지만 현재 솔루션은 수동으로 설계된 규칙과 광범위한 감독에 의존하는 경우가 많습니다. 이 작업에서는 현재 RGB-D 관찰로 구성된 장면 그래프를 성공적인 시연에서 추출된 참조 그래프와 비교하여 하위 작업 경계에서 오류를 감지하고 수정하는 사전 재계획 프레임워크를 제시합니다. 현재 장면이 참조 궤적과 정렬되지 않으면 경량 추론 모듈이 활성화되어 불일치를 진단하고 계획을 조정합니다. AI2-THOR 시뮬레이터의 실험은 우리의 접근 방식이 실행 실패가 발생하기 전에 의미론적 및 공간적 불일치를 감지하여 작업 성공과 견고성을 크게 향상시키는 것을 보여줍니다."
880,http://arxiv.org/abs/2508.07023 ,MV-CoRe: Multimodal Visual-Conceptual Reasoning for Complex Visual Question Answering,"Jingwei Peng, Jiehao Chen, Mateo Alejandro Rojas, Meilin Zhang","정교한 다중 모달 추론과 외부 지식 통합을 요구하는 복잡한 시각적 질문 응답(복잡한 VQA) 작업은 종종 높은 수준의 글로벌 기능에 대한 의존으로 인해 제한되는 기존 LVLM(대형 비전 언어 모델)에 심각한 과제를 제시합니다. 이를 해결하기 위해 우리는 다양한 시각적 정보와 언어적 정보의 심층 융합을 통해 복잡한 VQA 성능을 향상하도록 설계된 새로운 모델인 MV-CoRe(Multimodal Visual-Conceptual Reasoning)를 제안합니다. MV-CoRe는 사전 훈련된 VLM(Vision Large Model) 및 LLM(Language Large Model)의 글로벌 임베딩을 객체 감지 특성 및 장면 그래프 표현을 포함한 세분화된 의미 인식 시각적 기능과 꼼꼼하게 통합합니다. 그런 다음 혁신적인 Multimodal Fusion Transformer가 이러한 다양한 기능 세트를 처리하고 심층적으로 통합하여 풍부한 교차 모드 주의를 지원하고 복잡한 추론을 촉진합니다. VQAv2에 대한 교육을 마친 후 GQA, A-OKVQA 및 OKVQA를 포함한 까다로운 복잡한 VQA 벤치마크에 대해 MV-CoRe를 평가합니다. 우리의 실험 결과는 MV-CoRe가 확립된 LVLM 기준선을 지속적으로 능가하여 GQA에서 77.5%의 전체 정확도를 달성한다는 것을 보여줍니다. 절제 연구는 객체 및 장면 그래프 기능 모두의 중요한 기여를 확인하고 인간의 평가는 MV-CoRe의 뛰어난 사실적 정확성과 추론 깊이를 더욱 검증하여 심층적인 시각적 및 개념적 이해를 위한 강력한 기능을 강조합니다."
879,http://arxiv.org/abs/2508.06990 ,Imaginative World Modeling with Scene Graphs for Embodied Agent Navigation,"Yue Hu, Junzhe Wu, Ruihan Xu, Hang Liu, Avery Xi, Henry X. Liu, Ram Vasudevan, Maani Ghaffari","의미론적 탐색에는 에이전트가 보이지 않는 환경에서 지정된 대상을 향해 탐색해야 합니다. 조치를 취하기 전에 미래 장면을 예측하는 상상력이 풍부한 탐색 전략을 사용하면 에이전트가 대상을 더 빨리 찾을 수 있습니다. 이 아이디어에서 영감을 받아 우리는 상징적 세계 모델링을 활용하여 글로벌 환경 표현을 적극적으로 구축하는 새롭고 창의적인 탐색 프레임워크인 SGImagineNav를 제안합니다. SGImagineNav는 진화하는 계층적 장면 그래프를 유지하고 대규모 언어 모델을 사용하여 환경의 보이지 않는 부분을 예측하고 탐색합니다. 기존 방법은 과거 관찰에만 의존하는 반면, 이 상상력이 풍부한 장면 그래프는 보다 풍부한 의미론적 컨텍스트를 제공하여 에이전트가 사전에 대상 위치를 추정할 수 있도록 합니다. 이를 바탕으로 SGImagineNav는 약속할 때 의미론적 지름길을 활용하고 그렇지 않은 경우 추가 컨텍스트를 수집하기 위해 알려지지 않은 영역을 탐색하는 적응형 탐색 전략을 채택합니다. 이 전략은 알려진 환경을 지속적으로 확장하고 귀중한 의미 컨텍스트를 축적하여 궁극적으로 에이전트를 대상으로 안내합니다. SGImagineNav는 실제 시나리오와 시뮬레이션 벤치마크 모두에서 평가됩니다. SGImagineNav는 이전 방법보다 지속적으로 뛰어난 성능을 발휘하여 HM3D 및 HSSD에서 성공률을 65.4 및 66.8로 향상시키고 실제 환경에서 바닥 간 및 방 간 탐색을 시연하여 효율성과 일반화 가능성을 강조합니다."
878,http://arxiv.org/abs/2508.06283 ,Situationally-aware Path Planning Exploiting 3D Scene Graphs,"Saad Ejaz, Marco Giberna, Muhammad Shaheer, Jose Andres Millan-Romera, Ali Tourani, Paul Kremer, Holger Voos, Jose Luis Sanchez-Lopez",3D 장면 그래프는 미터법 정보와 의미 정보를 모두 통합하지만 경로 계획 효율성과 해석 가능성을 향상시키는 데 그 구조는 여전히 활용도가 낮습니다. 이 연구에서는 실내 3D 장면 그래프의 미터법-의미론적 구조를 활용하여 계획 효율성을 크게 향상시키는 상황 인식 경로 플래너인 S-Path를 제시합니다. S-Path는 2단계 프로세스를 따릅니다. 먼저 장면 그래프에서 파생된 의미 그래프에 대한 검색을 수행하여 사람이 이해할 수 있는 상위 수준 경로를 생성합니다. 이는 또한 계획을 위한 관련 영역을 식별하여 나중에 문제를 병렬로 해결할 수 있는 더 작고 독립적인 하위 문제로 분해할 수 있도록 합니다. 또한 실행 불가능한 경로가 있는 경우 이전에 해결된 하위 문제의 정보를 재사용하여 의미적 휴리스틱을 업데이트하고 재사용 우선 순위를 지정하여 향후 계획 시도의 효율성을 더욱 향상시키는 재계획 메커니즘을 도입합니다. 실제 환경과 시뮬레이션 환경에 대한 광범위한 실험을 통해 S-Path는 계획 시간을 평균 5.7배 단축하는 동시에 기존 샘플링 기반 플래너와 비슷한 경로 최적성을 유지하고 복잡한 시나리오에서 이를 능가하여 실내 3D 장면 그래프로 표현되는 환경에 대한 효율적이고 해석 가능한 경로 플래너가 되는 것으로 나타났습니다.
877,http://arxiv.org/abs/2508.06125 ,SC-Captioner: Improving Image Captioning with Self-Correction by Reinforcement Learning,"Lin Zhang, Xianfang Zeng, Kangcong Li, Gang Yu, Tao Chen","우리는 이미지 캡션 모델의 자체 수정 기능을 가능하게 하는 강화 학습 프레임워크인 SC-Captioner를 제안합니다. 우리의 중요한 기술은 정확한 캡션 수정을 장려하는 보상 기능 설계에 있습니다. 구체적으로 예측 캡션과 참조 캡션은 장면 그래프 구문 분석 알고리즘을 사용하여 개체, 속성 및 관계 집합으로 분해됩니다. 추가된 요소와 제거된 요소를 식별하기 위해 초기 캡션 세트와 자체 수정 캡션 세트 간의 세트 차이를 계산합니다. 이러한 요소는 참조 세트와 일치하여 정확한 개선을 위한 정확성 보너스와 잘못된 추가 및 제거에 대한 실수 처벌을 계산하여 최종 보상을 형성합니다. 이미지 캡션 품질 평가를 위해 우리는 불완전한 정밀도 평가와 비효율적인 관계 매칭 문제를 완화하는 CAPTURE에서 개선된 일련의 측정항목을 제안합니다. 또한 COCO 데이터 세트에서 6.5K의 다양한 이미지로 구성된 세분화된 주석이 달린 이미지 캡션 데이터 세트인 RefinedCaps를 수집합니다. 실험에 따르면 대규모 시각적 언어 모델에 SC-Captioner를 적용하면 다양한 시나리오에서 더 나은 이미지 캡션을 생성할 수 있으며 직접 선호도 최적화 교육 전략보다 훨씬 뛰어난 성능을 발휘할 수 있습니다."
876,http://arxiv.org/abs/2508.05342 ,Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control,"Shunlei Li, Longsen Gao, Jin Wang, Chang Che, Xi Xiao, Jiuwen Cao, Yingbai Hu, Hamid Reza Karimi","인간 비디오에서 로봇의 능숙한 기술을 가르치는 것은 개체 유형, 공간 레이아웃 및 조작기 구성 전반에 걸쳐 일반화하지 못하는 낮은 수준의 궤적 모방에 의존하기 때문에 여전히 어려운 일입니다. 우리는 양팔 로봇 시스템이 RGB 및 Depth 인간 시연에서 직접 작업 수준 추론 및 실행을 수행할 수 있도록 하는 프레임워크인 GF-VLA(Graph-Fused Vision-Language-Action)를 제안합니다. GF-VLA는 먼저 Shannon 정보 기반 단서를 추출하여 작업 관련성이 가장 높은 손과 물체를 식별한 다음 이러한 단서를 손-물체 및 개체-물체 상호 작용을 모두 캡처하는 시간적으로 정렬된 장면 그래프로 인코딩합니다. 이러한 그래프는 계층적 동작 트리와 해석 가능한 데카르트 동작 명령을 생성하는 언어 조절 변환기와 융합됩니다. 양손 설정에서 실행 효율성을 높이기 위해 명시적인 기하학적 추론 없이 최적의 그리퍼 할당을 추론하는 교차 선택 정책을 추가로 도입합니다. 우리는 상징적 형태 구성 및 공간 일반화와 관련된 네 가지 구조화된 이중 암 블록 조립 작업에 대해 GF-VLA를 평가합니다. 실험 결과에 따르면 정보 이론적 장면 표현은 95% 이상의 그래프 정확도와 93% 이상의 하위 작업 분할을 달성하여 LLM 플래너가 신뢰할 수 있고 사람이 읽을 수 있는 작업 정책을 생성하도록 지원합니다. 이중 팔 로봇에 의해 실행될 때 이러한 정책은 스태킹, 문자 작성 및 기하학적 재구성 시나리오 전반에 걸쳐 94%의 파악 성공, 89%의 배치 정확도, 90%의 전체 작업 성공을 거두며 다양한 공간 및 의미 변형에 걸쳐 강력한 일반화 및 견고성을 보여줍니다."
875,http://arxiv.org/abs/2508.04943 ,TRKT: Weakly Supervised Dynamic Scene Graph Generation with Temporal-enhanced Relation-aware Knowledge Transferring,"Zhu Xu, Ting Lei, Zhimin Li, Guan Wang, Qingchao Chen, Yuxin Peng, Yang liu",DSGG(Dynamic Scene Graph Generation)는 객체를 감지하고 그 관계를 예측하여 각 비디오 프레임에 대한 장면 그래프를 생성하는 것을 목표로 합니다. WS-DSGG(Weakly Supervised DSGG)는 훈련을 위해 비디오당 단일 프레임의 현지화되지 않은 장면 그래프를 사용하여 주석 작업량을 줄입니다. 기존 WS-DSGG 방법은 기성품 외부 개체 감지기에 의존하여 후속 DSGG 훈련을 위한 의사 레이블을 생성합니다. 그러나 정적 객체 중심 이미지에 대해 훈련된 감지기는 DSGG에 필요한 동적 관계 인식 시나리오에서 어려움을 겪으며 부정확한 위치 파악 및 낮은 신뢰도 제안으로 이어집니다. WS-DSGG의 외부 객체 감지기로 인한 문제를 해결하기 위해 우리는 관계 인식 동적 시나리오에서 감지를 향상시키기 위해 지식을 활용하는 TRKT(Temporal-enhanced Relation-aware Knowledge Transferring) 방법을 제안합니다. TRKT는 두 가지 주요 구성 요소를 기반으로 구축되었습니다. (1) 관계 인식 지식 마이닝: 먼저 개체 영역과 대화형 영역을 모두 강조하기 위해 범주별 주의 지도를 생성하는 개체 및 관계 클래스 디코더를 사용합니다. 그런 다음 인접한 프레임의 광학 흐름을 활용하여 주의 맵을 향상시켜 모션을 인식하고 모션 블러에 강인하게 만드는 프레임 간 주의 증강 전략을 제안합니다. 이 단계에서는 WS-DSGG에 대한 관계 및 동작 인식 지식 마이닝이 생성됩니다. (2) 카테고리별 주의 지도를 외부 감지에 통합하여 객체 위치 파악을 개선하고 객체 제안에 대한 신뢰도 점수를 높이는 듀얼 스트림 융합 모듈을 소개합니다. 광범위한 실험을 통해 TRKT가 Action Genome 데이터 세트에서 최첨단 성능을 달성했음을 보여줍니다. 우리 코드는 https://github.com/XZPKU/TRKT.git에서 사용할 수 있습니다.
874,http://arxiv.org/abs/2508.04678 ,Open Scene Graphs for Open-World Object-Goal Navigation,"Joel Loo, Zhanxin Wu, David Hsu","개방형 의미론적 탐색(예: 자연어로 지정된 대상 객체에 대한 새로운 환경 검색)을 위한 범용 로봇 시스템을 어떻게 구축할 수 있습니까? 이 문제를 해결하기 위해 우리는 오픈 월드 Object-Goal Navigation(ObjectNav)을 위한 기본 모델로 구성된 모듈식 시스템인 OSG Navigator를 소개합니다. 기초 모델은 세계에 대한 막대한 의미론적 지식을 제공하지만 공간 정보를 규모에 맞게 효과적으로 구성하고 유지하는 데 어려움을 겪습니다. OSG Navigator의 핵심은 OSG Navigator의 공간 메모리 역할을 하는 Open Scene Graph 표현입니다. 이는 각각 환경 클래스의 공통 구조를 설명하는 템플릿인 OSG 스키마를 사용하여 공간 정보를 계층적으로 구성합니다. OSG 스키마는 주어진 환경(예: ""집"" 또는 ""슈퍼마켓"")의 간단한 의미 레이블에서 자동으로 생성될 수 있습니다. 이를 통해 OSG Navigator는 제로샷을 새로운 환경 유형에 적응할 수 있습니다. 우리는 시뮬레이션과 실제 세계에서 Fetch와 Spot 로봇을 모두 사용하여 실험을 수행하여 OSG Navigator가 ObjectNav 벤치마크에서 최첨단 성능을 달성하고 다양한 목표, 환경 및 로봇 구현에 대해 제로샷을 일반화한다는 것을 보여주었습니다."
873,http://arxiv.org/abs/2508.06546 ,Statistical Confidence Rescoring for Robust 3D Scene Graph Generation from Multi-View Images,"Qi Xun Yeo, Yanyan Li, Gim Hee Lee","최신 3D 의미론적 장면 그래프 추정 방법은 실제 3D 주석을 활용하여 대상 개체, 조건자 및 관계를 정확하게 예측합니다. 제공된 3D 지상 실제 표현이 없는 경우 다중 뷰 RGB 이미지만 활용하여 이 작업을 해결합니다. 정확한 장면 그래프 추정을 위한 강력한 특징을 얻으려면 예측된 깊이 맵에서 잡음이 있는 재구성된 의사 점 기반 형상을 극복하고 다중 뷰 이미지 특징에 존재하는 배경 잡음의 양을 줄여야 합니다. 핵심은 정확한 의미 및 공간 정보와 인접 관계를 통해 노드 및 에지 기능을 풍부하게 하는 것입니다. 우리는 배경 특징을 필터링하기 위해 특징 집계를 안내하고 장면 그래프 추정의 견고성을 돕기 위해 이웃 노드 정보를 통합하는 새로운 방법을 설계하기 위해 의미론적 마스크를 얻습니다. 또한 훈련 요약 통계에서 계산된 명시적 통계 사전 정보를 활용하여 단일 홉 이웃을 기반으로 노드 및 에지 예측을 개선합니다. 우리의 실험은 우리의 방법이 순전히 다중 뷰 이미지를 초기 입력으로 사용하는 현재 방법보다 성능이 우수하다는 것을 보여줍니다. 우리 프로젝트 페이지는 https://qixun1.github.io/projects/SCRSSG에서 확인할 수 있습니다."
872,http://arxiv.org/abs/2508.03692 ,LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences,"Ao Liang, Youquan Liu, Yu Yang, Dongyue Lu, Linfeng Li, Lingdong Kong, Huaici Zhao, Wei Tsang Ooi","생성적 세계 모델은 자율 주행을 위한 필수 데이터 엔진이 되었지만 대부분의 기존 노력은 고유한 LiDAR 속성을 간과한 비디오 또는 점유 그리드에 중점을 둡니다. LiDAR 생성을 동적 4D 세계 모델링으로 확장하면 제어 가능성, 시간적 일관성 및 평가 표준화에 문제가 발생합니다. 이를 위해 우리는 4D LiDAR 생성 및 편집을 위한 통합 프레임워크인 LiDARCrafter를 제시합니다. 자유로운 형식의 자연어 입력이 주어지면 지침을 자아 중심 장면 그래프로 구문 분석하여 삼중 확산 네트워크를 조절하여 객체 구조, 모션 궤적 및 기하학을 생성합니다. 이러한 구조화된 조건은 다양하고 세밀한 장면 편집을 가능하게 합니다. 또한 자동 회귀 모듈은 부드러운 전환을 통해 시간적으로 일관된 4D LiDAR 시퀀스를 생성합니다. 표준화된 평가를 지원하기 위해 우리는 장면, 개체 및 시퀀스 수준 측면을 포괄하는 다양한 측정항목을 사용하여 포괄적인 벤치마크를 설정합니다. 이 벤치마크를 사용한 nuScenes 데이터 세트에 대한 실험은 LiDARCrafter가 모든 수준에서 충실도, 제어 가능성 및 시간적 일관성 측면에서 최첨단 성능을 달성하여 데이터 확대 및 시뮬레이션을 위한 길을 닦았다는 것을 보여줍니다. 코드와 벤치마크가 커뮤니티에 공개됩니다."
871,http://arxiv.org/abs/2508.03691 ,La La LiDAR: Large-Scale Layout Generation from LiDAR Data,"Youquan Liu, Lingdong Kong, Weidong Yang, Xin Li, Ao Liang, Runnan Chen, Ben Fei, Tongliang Liu","사실적인 LiDAR 장면의 제어 가능한 생성은 자율 주행 및 로봇 공학과 같은 응용 분야에 매우 중요합니다. 최근의 확산 기반 모델은 충실도가 높은 LiDAR 생성을 달성하지만 전경 개체 및 공간 관계에 대한 명시적인 제어가 부족하여 시나리오 시뮬레이션 및 안전 검증에 대한 유용성이 제한됩니다. 이러한 제한 사항을 해결하기 위해 우리는 구조화된 LiDAR 레이아웃 생성을 위한 관계 인식 상황 조건화와 함께 의미 강화 장면 그래프 확산을 도입한 새로운 레이아웃 기반 생성 프레임워크인 대규모 레이아웃 기반 LiDAR 생성 모델(""La La LiDAR"")을 제안하고 이어서 전체 장면 생성을 위한 전경 인식 제어 주입을 제안합니다. 이를 통해 공간적 및 의미론적 일관성을 보장하면서 개체 배치에 대한 사용자 정의 가능한 제어가 가능합니다. 구조화된 LiDAR 생성을 지원하기 위해 레이아웃 합성을 위한 새로운 평가 지표와 함께 두 개의 대규모 LiDAR 장면 그래프 데이터 세트인 Waymo-SG 및 nuScenes-SG를 도입합니다. 광범위한 실험을 통해 La La LiDAR가 LiDAR 생성 및 다운스트림 인식 작업 모두에서 최첨단 성능을 달성하여 제어 가능한 3D 장면 생성을 위한 새로운 벤치마크를 확립했음을 보여줍니다."
870,http://arxiv.org/abs/2507.21893 ,Aether Weaver: Multimodal Affective Narrative Co-Generation with Dynamic Scene Graphs,Saeed Ghorbani,"순차적 텍스트-시각 파이프라인의 한계를 극복하는 다중 모드 내러티브 공동 생성을 위한 새로운 통합 프레임워크인 Aether Weaver를 소개합니다. 우리 시스템은 긴밀하게 통합된 공동 생성 메커니즘을 통해 텍스트 내러티브, 동적 장면 그래프 표현, 시각적 장면 및 정서적 사운드스케이프를 동시에 합성합니다. 기본적으로 대형 언어 모델인 내레이터는 내러티브 텍스트와 다중 모드 프롬프트를 생성하는 반면, 디렉터는 동적 장면 그래프 관리자 역할을 하고 텍스트를 분석하여 스토리 세계의 구조화된 표현을 구축 및 유지하여 시각적 렌더링 및 후속 내러티브 생성을 위한 시공간적 및 관계적 일관성을 보장합니다. 또한 Narrative Arc Controller는 높은 수준의 스토리 구조를 안내하여 다중 모드의 정서적 일관성에 영향을 미치고 모든 양식에 걸쳐 일치하는 감정 표현을 보장하는 Affective Tone Mapper로 더욱 보완됩니다. 다양한 장르를 포괄하는 다양한 내러티브 프롬프트에 대한 질적 평가를 통해 Aether Weaver가 계단식 기본 접근 방식에 비해 내러티브 깊이, 시각적 충실도 및 감정적 공명을 크게 향상시킨다는 것을 보여줍니다. 이 통합 프레임워크는 신속한 창의적 프로토타입 제작과 몰입형 스토리텔링 경험을 위한 강력한 플랫폼을 제공합니다."
869,http://arxiv.org/abs/2507.20804 ,MMGraphRAG: Bridging Vision and Language with Interpretable Multimodal Knowledge Graphs,"Xueyao Wan, Hang Yu","RAG(Retrieval-Augmented Generation)는 외부 지식 기반에서 관련 정보를 검색하여 언어 모델 생성을 향상시킵니다. 그러나 기존 RAG 방법은 다중 모드 정보가 누락되는 문제에 직면해 있습니다. 멀티모달 RAG 방법은 이미지와 텍스트를 공유 임베딩 공간에 매핑하여 융합함으로써 이 문제를 해결하지만, 양식 간의 지식 구조와 논리적 체인을 포착하는 데 실패합니다. 또한 특정 작업에 대한 대규모 교육이 필요하므로 일반화 능력이 제한됩니다. 이러한 한계를 해결하기 위해 우리는 장면 그래프를 통해 시각적 콘텐츠를 정제하고 텍스트 기반 KG와 연계하여 MMKG(Multimodal Knowledge Graph)를 구성하는 MMGraphRAG를 제안합니다. 이는 스펙트럼 클러스터링을 사용하여 교차 모달 엔터티 연결을 달성하고 추론 경로를 따라 컨텍스트를 검색하여 생성 프로세스를 안내합니다. 실험 결과에 따르면 MMGraphRAG는 DocBench 및 MMLongBench 데이터 세트에서 최첨단 성능을 달성하여 강력한 도메인 적응성과 명확한 추론 경로를 보여줍니다."
868,http://arxiv.org/abs/2507.19993 ,FROSS: Faster-than-Real-Time Online 3D Semantic Scene Graph Generation from RGB-D Images,"Hao-Yu Hou, Chun-Yi Lee, Motoharu Sonogashira, Yasutomo Kawanishi","복잡한 3D 환경을 단순화되고 구조화된 표현으로 추상화하는 능력은 다양한 영역에서 매우 중요합니다. 3D 의미론적 장면 그래프(SSG)는 개체를 노드로, 개체의 상호 관계를 가장자리로 표현하여 높은 수준의 장면 이해를 촉진함으로써 이를 달성합니다. 그러나 3D SSG 생성을 위한 기존 방법은 실시간 오픈 월드 애플리케이션에 대한 적합성을 방해하는 높은 계산 요구 사항 및 비증분 처리를 포함하여 심각한 문제에 직면해 있습니다. 이 문제를 해결하기 위해 우리는 2D 장면 그래프를 3D 공간으로 직접 리프팅하고 객체를 3D 가우스 분포로 나타내는 온라인 및 실시간보다 빠른 3D SSG 생성을 위한 혁신적인 접근 방식인 FROSS(Faster-than-Real-Time Online 3D Semantic Scene Graph Generation)를 제안합니다. 이 프레임워크는 정확하고 계산 집약적인 포인트 클라우드 처리에 대한 의존성을 제거합니다. 또한 개체 간 관계 주석을 사용하여 Replica 데이터 세트를 확장하여 FROSS의 포괄적인 평가를 위한 ReplicaSSG 데이터 세트를 생성합니다. ReplicaSSG 및 3DSSG 데이터 세트에 대한 평가의 실험 결과는 FROSS가 이전 3D SSG 생성 방법보다 훨씬 빠르게 작동하면서 우수한 성능을 달성할 수 있음을 보여줍니다. 우리의 구현 및 데이터 세트는 https://github.com/Howardkhh/FROSS에서 공개적으로 제공됩니다."
867,http://arxiv.org/abs/2507.18562 ,GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation,"Jiafeng Xiong, Yuting Zhao","다중 모드 기계 번역(MMT)은 기계 번역에서 시각적 정보의 중요한 도움을 보여주었습니다. 그러나 기존 MMT 방법은 훈련된 다중 모달 도메인 내에서 추론에 국한되면서 엄격한 시각적-언어 정렬을 적용함으로써 모달리티 격차를 활용하는 데 어려움을 겪고 있습니다. 이 작업에서 우리는 양식별 정보를 보존 및 통합하기 위해 새로운 다중 모드 장면 그래프를 구성하고, 통합된 융합 공간에서 다중 모드 지식을 학습하고 이를 더 넓은 이미지 없는 변환 도메인으로 귀납적으로 일반화하기 위해 교차 모드 Graph Attention Network 어댑터를 사용하는 2단계 그래프 유도 유도 이미지 없는 MMT 프레임워크인 GIIFT를 소개합니다. 영어-프랑스어 및 영어-독일어 작업의 Multi30K 데이터 세트에 대한 실험 결과는 우리의 GIIFT가 추론 중 이미지 없이도 기존 접근 방식을 능가하고 최첨단을 달성한다는 것을 보여줍니다. WMT 벤치마크 결과는 이미지 없는 번역 기준에 비해 상당한 개선을 보여주며, 이는 이미지 없는 유도 추론에 대한 GIIFT의 강점을 보여줍니다."
866,http://arxiv.org/abs/2507.15782 ,Interleaved LLM and Motion Planning for Generalized Multi-Object Collection in Large Scene Graphs,"Ruochu Yang, Yu Zhou, Fumin Zhang, Mengxue Hou","가정용 로봇은 오랜 연구 주제였지만 특히 개방형 물체를 조작하고 대규모 환경을 효율적이고 정확하게 탐색하는 데 있어서 인간과 같은 지능이 여전히 부족합니다. 이 경계를 넓히기 위해 우리는 로봇이 여러 인간 명령의 긴 임무에서 여러 위치에 걸쳐 여러 개체를 선택하고 배치해야 하는 대형 장면 그래프의 일반화된 다중 개체 수집 문제를 고려합니다. 이 문제는 불확실성이 높은 광대한 작업 상태 공간에서 장기적인 계획이 필요하기 때문에 매우 어렵습니다. 이를 위해 우리는 새로운 인터리브 LLM과 동작 계획 알고리즘 Inter-LLM을 제안합니다. 다중 모드 작업 비용 유사성 함수를 설계함으로써 우리의 알고리즘은 기록을 반영하고 미래를 내다보며 계획을 최적화하고 품질과 효율성의 균형을 잘 맞출 수 있습니다. 시뮬레이션 실험에서는 최신 연구와 비교하여 우리의 알고리즘이 인간 명령 이행, 임무 성공률 극대화 및 임무 비용 최소화 측면에서 전체 임무 성능을 30% 향상시키는 것으로 나타났습니다."
865,http://arxiv.org/abs/2507.15541 ,Towards Holistic Surgical Scene Graph,"Jongmin Shin, Enki Cho, Ka Young Kim, Jung Yong Kim, Seong Tae Kim, Namkee Oh","수술 장면 이해는 수술 도구, 해부학적 구조 및 상호 작용과 같은 다양한 요소를 포함하는 수술 장면에 대한 시각적 이해가 필요한 컴퓨터 보조 중재 시스템에 매우 중요합니다. 수술 장면의 복잡한 정보를 효과적으로 표현하기 위해 그래프 기반 접근 방식을 통해 수술 엔터티와 그 관계를 구조적으로 모델링했습니다. 이전의 수술 장면 그래프 연구에서는 그래프를 사용하여 수술 장면을 표현하는 타당성을 입증했습니다. 그러나 도구-작업-대상의 다양한 조합 및 도구를 작동하는 손의 신원과 같은 수술 장면의 특정 측면은 그 중요성에도 불구하고 그래프 기반 표현에서 충분히 탐구되지 않은 상태로 남아 있습니다. 이러한 측면을 그래프 표현에 통합하기 위해 도구-작업-대상 조합 및 손 식별에 대한 주석이 포함된 Endoscapes-SG201 데이터 세트를 제안합니다. 또한 이러한 중요한 요소를 학습하고 표현하도록 설계된 그래프 기반 방법인 SSG-Com을 소개합니다. 안전성 평가에 대한 비판적 관점 및 삼중항 동작 인식과 같은 다운스트림 작업에 대한 실험을 통해 이러한 필수 장면 그래프 구성 요소를 통합하는 것의 중요성을 입증하고 수술 장면 이해에 대한 중요한 기여를 강조했습니다. 코드와 데이터 세트는 https://github.com/ailab-kyunghee/SSG-Com에서 확인할 수 있습니다."
864,http://arxiv.org/abs/2507.12846 ,Enter the Mind Palace: Reasoning and Planning for Long-term Active Embodied Question Answering,"Muhammad Fadhil Ginting, Dong-Ki Kim, Xiangyun Meng, Andrzej Reinke, Bandi Jai Krishna, Navid Kayhani, Oriana Peltzer, David D. Fan, Amirreza Shaban, Sung-Kyun Kim, Mykel J. Kochenderfer, Ali-akbar Agha-mohammadi, Shayegan Omidshafiei","로봇이 며칠, 몇 주, 심지어 몇 달에 걸쳐 장기간 작동할 수 있게 되면서 환경에 대한 지식을 축적하고 이 경험을 활용하여 인간을 보다 효과적으로 지원할 것으로 예상됩니다. 이 논문에서는 로봇이 과거 경험을 기억하고 환경을 적극적으로 탐색하여 복잡하고 시간에 기반한 질문에 답해야 하는 새로운 작업인 LA-EQA(Long-term Active Embodied Question Answering) 문제를 연구합니다. 일반적으로 현재 환경을 이해하거나 단일 과거 관찰을 회상하는 데 초점을 맞추는 기존 EQA 설정과 달리 LA-EQA는 에이전트에게 과거, 현재 및 가능한 미래 상태에 대해 추론하고 탐색할 시기, 메모리 참조 시기, 관찰 수집을 중지하고 최종 답변을 제공할 시기를 결정하도록 요구합니다. 대규모 모델을 기반으로 하는 표준 EQA 접근 방식은 제한된 컨텍스트 창, 영구 메모리 부재, 메모리 회상과 활성 탐색을 결합할 수 없는 문제로 인해 이 설정에서 어려움을 겪습니다. 이를 해결하기 위해 우리는 인지 과학의 마음 궁전 방법에서 영감을 받은 로봇을 위한 구조화된 메모리 시스템을 제안합니다. 우리의 방법은 장면 그래프 기반 세계 인스턴스로 에피소드 경험을 인코딩하여 대상 메모리 검색 및 안내 탐색을 가능하게 하는 추론 및 계획 알고리즘을 형성합니다. 탐색-회상 절충의 균형을 맞추기 위해 에이전트가 충분한 정보를 수집한 시기를 결정하는 정보 가치 기반 중지 기준을 도입합니다. 우리는 실제 실험을 통해 우리의 방법을 평가하고 인기 있는 시뮬레이션 환경과 실제 산업 현장을 포괄하는 새로운 벤치마크를 소개합니다. 우리의 접근 방식은 최첨단 기준선보다 훨씬 뛰어난 성능을 발휘하여 답변 정확도와 탐색 효율성 모두에서 상당한 이점을 얻었습니다."
863,http://arxiv.org/abs/2507.12123 ,Open-Vocabulary Indoor Object Grounding with 3D Hierarchical Scene Graph,"Sergey Linok, Gleb Naumov","우리는 OVIGo-3DHSG 방법인 3D 계층적 장면 그래프를 이용한 객체의 Open-Vocabulary 실내 접지를 제안합니다. OVIGo-3DHSG는 일련의 개방형 어휘 기반 모델 및 센서 데이터 처리를 활용하여 RGB-D 프레임 시퀀스에서 파생된 계층적 장면 그래프를 통해 광범위한 실내 환경을 나타냅니다. 계층적 표현은 바닥, 방, 위치 및 객체 간의 공간 관계를 명시적으로 모델링합니다. 다른 객체에 대한 공간 참조와 관련된 복잡한 쿼리를 효과적으로 처리하기 위해 계층적 장면 그래프를 다단계 추론을 위한 대형 언어 모델과 통합합니다. 이러한 통합은 레이어 간(예: 공간-객체) 및 레이어 내(예: 객체-객체) 연결을 활용하여 공간적 맥락 이해를 향상시킵니다. 우리는 Habitat Matterport 3D Semantic 다중 층 장면에서 계층적 표현의 의미론적 및 기하학적 정확성을 조사합니다. 우리의 접근 방식은 기존 방법에 비해 효율적인 장면 이해와 강력한 객체 접지를 보여줍니다. 전반적으로 OVIGo-3DHSG는 실내 환경에 대한 공간적 추론과 이해가 필요한 애플리케이션에 대한 강력한 잠재력을 보여줍니다. 관련 자료는 https://github.com/linukc/OVIGo-3DHSG에서 확인할 수 있습니다."
862,http://arxiv.org/abs/2507.11913 ,Scene Graph-Aided Probabilistic Semantic Communication for Image Transmission,"Chen Zhu, Siyun Liang, Zhouxiang Zhao, Jianrong Bao, Zhaohui Yang, Zhaoyang Zhang, Dusit Niyato","의미론적 의사소통은 원시 기호보다는 의미 전달을 강조합니다. 이는 네트워크 혼잡을 완화하고 전송 효율성을 향상시키는 유망한 솔루션을 제공합니다. 본 논문에서는 분산된 사용자들 사이에서 공유되는 의미 지식 기반으로 확률 그래프를 활용하는 무선 영상 통신 프레임워크를 제안한다. 높은 수준의 이미지 의미는 장면 그래프를 통해 표현되며, 학습된 조건 및 동시 발생 확률을 기반으로 예측 가능한 구성 요소를 제거하기 위해 2단계 압축 알고리즘이 고안되었습니다. 송신기에서 알고리즘은 중복 관계와 엔터티 쌍을 필터링하는 반면, 수신기에서는 의미론적 복구가 동일한 확률 그래프를 활용하여 누락된 정보를 재구성합니다. 추가 연구를 위해 이론적 성능 분석을 통해 다중 라운드 의미 압축 알고리즘도 제시했습니다. 시뮬레이션 결과는 의미론적 인식 체계가 우수한 전송 처리량과 만족스러운 의미론적 정렬을 달성하여 이미지 통신을 위해 높은 수준의 의미론을 활용하는 효율성을 검증한다는 것을 보여줍니다."
861,http://arxiv.org/abs/2507.11770 ,Generating Actionable Robot Knowledge Bases by Combining 3D Scene Graphs with Robot Ontologies,"Giang Nguyen, Mihai Pomarlan, Sascha Jongebloed, Nils Leusmann, Minh Nhat Vu, Michael Beetz","로봇 공학에서는 MJCF, URDF, SDF와 같이 장면 설명에 일반적으로 사용되는 데이터 형식의 다양성과 비호환성으로 인해 환경 데이터를 실행 가능한 지식에 효과적으로 통합하는 것이 여전히 중요한 과제로 남아 있습니다. 본 논문에서는 이러한 다양한 형식을 USD(Universal Scene Description) 형식으로 표준화하는 통합 장면 그래프 모델을 개발하여 이러한 문제를 해결하는 새로운 접근 방식을 제시합니다. 이러한 표준화는 의미론적 보고를 통해 이러한 장면 그래프와 로봇 온톨로지의 통합을 용이하게 하여 복잡한 환경 데이터를 인지 로봇 제어에 필수적인 실행 가능한 지식으로 변환할 수 있도록 합니다. 우리는 절차적 3D 환경을 USD 형식으로 변환하여 접근 방식을 평가했습니다. 그런 다음 의미론적으로 주석을 달고 지식 그래프로 변환하여 역량 질문에 효과적으로 답변함으로써 실시간 로봇 의사 결정에 대한 유용성을 입증했습니다. 또한 의미론적 매핑 프로세스를 지원하는 웹 기반 시각화 도구를 개발하여 사용자에게 3D 환경을 관리할 수 있는 직관적인 인터페이스를 제공했습니다."
860,http://arxiv.org/abs/2507.09200 ,THYME: Temporal Hierarchical-Cyclic Interactivity Modeling for Video Scene Graphs in Aerial Footage,"Trong-Thuan Nguyen, Pha Nguyen, Jackson Cothren, Alper Yilmaz, Minh-Triet Tran, Khoa Luu","자율 주행, 감시, 스포츠 분석과 같은 애플리케이션에서 비디오가 급속히 확산됨에 따라 동적 장면 이해를 위한 강력한 방법이 필요합니다. 정적 장면 그래프 생성의 발전과 비디오 장면 그래프 생성의 초기 시도에도 불구하고 이전 방법은 세분화된 표현으로 인해 종종 어려움을 겪으며 세밀한 공간 세부 정보와 장거리 시간 종속성을 동시에 캡처하지 못합니다. 이러한 제한 사항을 해결하기 위해 THYME(Temporal Hierarchical Cyclic Scene Graph) 접근 방식을 도입합니다. 이 접근 방식은 계층적 특징 집계와 순환 시간적 개선을 시너지 효과적으로 통합하여 이러한 제한 사항을 해결합니다. 특히 THYME은 다중 규모 공간 컨텍스트를 효과적으로 모델링하고 프레임 전체에 걸쳐 시간적 일관성을 강화하여 보다 정확하고 일관된 장면 그래프를 생성합니다. 또한 기존 데이터 세트의 제약을 극복하고 동적 장면 그래프 생성을 위한 포괄적인 벤치마크를 제공하는 5가지 유형의 상호 작용이 강화된 새로운 항공 비디오 데이터 세트인 AeroEye-v1.0을 제시합니다. 경험적으로, ASPIRe 및 AeroEye-v1.0에 대한 광범위한 실험은 제안된 THYME 접근 방식이 최첨단 방법보다 성능이 뛰어나 지상 및 항공 시나리오에서 향상된 장면 이해를 제공한다는 것을 보여줍니다."
859,http://arxiv.org/abs/2507.05798 ,SPADE: Spatial-Aware Denoising Network for Open-vocabulary Panoptic Scene Graph Generation with Long- and Local-range Context Reasoning,"Xin Hu, Ke Qin, Guiduo Duan, Ming Li, Yuan-Fang Li, Tao He",PSG(Panoptic Scene Graph Generation)는 인스턴스 분할과 관계 이해를 통합하여 복잡한 장면에서 픽셀 수준의 구조적 관계를 캡처합니다. 사전 훈련된 VLM(비전 언어 모델)을 활용하는 최근 접근 방식은 개방형 어휘 설정에서 성능을 크게 향상시켰지만 일반적으로 객체 상대 위치를 구별하는 어려움과 같은 공간 관계 추론에서 VLM의 고유한 한계를 무시하여 차선의 관계 예측을 초래합니다. 입력 이미지의 공간 구조를 보존하는 노이즈 제거 확산 모델의 반전 프로세스에 영감을 받아 개방형 어휘 PSG에 대한 새로운 접근 방식인 SPADE(SPatial-Aware Denoising-nEtwork) 프레임워크를 제안합니다. SPADE는 (1) UNet 적응을 위한 반전 유도 보정과 (2) 공간 인식 컨텍스트 추론의 두 가지 주요 단계로 구성됩니다. 첫 번째 단계에서는 가벼운 LoRA 기반 미세 조정 전략을 통해 반전 중에 파생된 교차 주의 맵을 사용하여 일반적인 사전 훈련된 교사 확산 모델을 PSG 관련 노이즈 제거 네트워크로 보정합니다. 두 번째 단계에서는 로컬 및 장거리 상황 정보를 모두 캡처하여 고품질 관계 쿼리 생성을 촉진하는 공간 인식 관계 그래프 변환기를 개발합니다. 벤치마크 PSG 및 Visual Genome 데이터세트에 대한 광범위한 실험은 SPADE가 폐쇄형 및 개방형 시나리오 모두에서 특히 공간 관계 예측에서 최첨단 방법보다 성능이 우수하다는 것을 보여줍니다.
858,http://arxiv.org/abs/2507.13362 ,Enhancing Spatial Reasoning in Vision-Language Models via Chain-of-Thought Prompting and Reinforcement Learning,"Binbin Ji, Siddharth Agrawal, Qiance Tang, Yvonne Wu","이 연구에서는 CoT(사고 사슬) 프롬프트 및 강화 학습을 통해 VLM(비전 언어 모델)의 공간 추론 기능을 조사합니다. 우리는 다양한 프롬프트 전략의 영향을 평가하는 것부터 시작하여 모델이 대답하기 전에 추론 단계를 생성하는 간단한 CoT 형식이 도움이 되지 않을 뿐만 아니라 모델의 원래 성능에 해를 끼칠 수도 있다는 것을 발견했습니다. 이와 대조적으로 장면 그래프(SceneGraph CoT)를 기반으로 하는 구조화된 다단계 프롬프트는 공간 추론 정확도를 크게 향상시킵니다. 또한 공간 추론 능력을 향상시키기 위해 SAT 데이터 세트에서 GRPO(Group Relative Policy Optimization)를 사용하여 모델을 미세 조정하고 CVBench에서 성능을 평가합니다. SFT(감독된 미세 조정)와 비교하여 GRPO는 Pass@1 평가에서 더 높은 정확도를 달성하고 OOD(배포 외) 조건에서 뛰어난 견고성을 보여줍니다. 특히, 우리는 SFT가 표면 수준의 언어 패턴에 과적합되어 테스트 시간 구문이 변경될 때(예: ""가까운""에서 ""더 먼""으로) 성능이 저하될 수 있음을 발견했습니다. 반면 GRPO는 이러한 변화에도 보다 안정적으로 일반화하고 안정적인 성능을 유지합니다. 우리의 연구 결과는 강화 학습과 구조화된 프롬프트가 현대 VLM의 공간 추론 능력과 일반화 동작을 어떻게 개선하는지에 대한 통찰력을 제공합니다. 모든 코드는 https://github.com/Yvonne511/spatial-vlm-investigator에서 오픈 소스입니다."
857,http://arxiv.org/abs/2507.02861 ,LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans,"Zhening Huang, Xiaoyang Wu, Fangcheng Zhong, Hengshuang Zhao, Matthias Nießner, Joan Lasenby","우리는 실내 환경의 RGB-D 스캔을 작고 사실적인 대화형 3D 가상 복제본으로 변환하는 새로운 파이프라인인 LiteReality를 제안합니다. LiteReality는 시각적으로 현실과 유사한 장면을 재구성할 뿐만 아니라 개체 개별성, 관절, 고품질 물리 기반 렌더링 자료, 물리 기반 상호 작용과 같은 그래픽 파이프라인에 필수적인 주요 기능도 지원합니다. LiteReality의 핵심은 먼저 장면 이해를 수행하고 구조화된 장면 그래프를 사용하여 결과를 일관된 3D 레이아웃과 개체로 구문 분석하는 것입니다. 그런 다음 선별된 자산 데이터베이스에서 시각적으로 가장 유사한 3D 아티스트 제작 모델을 검색하여 장면을 재구성합니다. 다음으로, Material Painting 모듈은 공간적으로 다양한 고품질 재료를 복구하여 사실감을 향상시킵니다. 마지막으로 재구성된 장면은 대화형 동작을 가능하게 하기 위해 기본 물리적 속성을 갖춘 시뮬레이션 엔진에 통합됩니다. 결과 장면은 작고 편집 가능하며 표준 그래픽 파이프라인과 완벽하게 호환되므로 AR/VR, 게임, 로봇 공학 및 디지털 트윈 애플리케이션에 적합합니다. 또한 LiteReality는 Scan2CAD 벤치마크에서 최첨단 유사성 성능을 달성하는 훈련이 필요 없는 객체 검색 모듈과 심각한 정렬 불량, 폐색 및 조명 불량 하에서도 모든 스타일의 이미지에서 3D 자산으로 외관을 전송할 수 있는 강력한 재료 페인팅 모듈을 도입합니다. 실제 스캔과 공개 데이터세트 모두에서 LiteReality의 효율성을 입증합니다. 프로젝트 페이지: https://litereality.github.io; 영상: https://www.youtube.com/watch?v=ecK9m3LXg2c"
856,http://arxiv.org/abs/2507.02029 ,RoboBrain 2.0 Technical Report," BAAI RoboBrain Team, Mingyu Cao, Huajie Tan, Yuheng Ji, Xiansheng Chen, Minglan Lin, Zhiyu Li, Zhou Cao, Pengwei Wang, Enshen Zhou, Yi Han, Yingbo Tang, Xiangqi Xu, Wei Guo, Yaoxu Lyu, Yijie Xu, Jiayu Shi, Mengfei Du, Cheng Chi, Mengdi Zhao, Xiaoshuai Hao, Junkai Zhao, Xiaojie Zhang, Shanyu Rong, Huaihai Lyu, Zhengliang Cai, Yankai Fu, Ning Chen, Bolun Zhang, Lingfeng Zhang, Shuyi Zhang, Dong Liu, Xi Feng, Songjing Wang, Xiaodan Liu, Yance Jiao, Mengsi Lyu, Zhuo Chen, Chenrui He, Yulong Ao, Xue Sun, Zheqi He, Jingshu Zheng, Xi Yang, Donghai Shi, Kunchang Xie, Bochao Zhang, Shaokai Nie, Chunlei Men, Yonghua Lin, Zhongyuan Wang, Tiejun Huang, Shanghang Zhang","물리적 환경에서 복잡한 구현 작업에 대한 인식, 추론 및 계획을 통합하도록 설계된 최신 세대의 구현 비전 언어 기반 모델인 RoboBrain 2.0을 소개합니다. 경량 7B 모델과 풀스케일 32B 모델의 두 가지 변형으로 제공되며 비전 인코더와 언어 모델을 갖춘 이기종 아키텍처를 특징으로 합니다. RoboBrain 2.0은 작은 크기에도 불구하고 광범위한 구체화된 추론 작업에서 강력한 성능을 달성합니다. 공간적 및 시간적 벤치마크 모두에서 32B 변형은 이전 오픈 소스 및 독점 모델을 능가하는 최고의 결과를 달성했습니다. 특히 공간 이해(예: 어포던스 예측, 공간 참조, 궤적 예측) 및 시간적 의사 결정(예: 폐쇄 루프 상호 작용, 다중 에이전트 장거리 계획 및 장면 그래프 업데이트)을 포함한 주요 실제 구현 AI 기능을 지원합니다. 이 보고서는 모델 아키텍처, 데이터 구성, 다단계 교육 전략, 인프라 및 실제 적용에 대해 자세히 설명합니다. RoboBrain 2.0이 구현된 AI 연구를 발전시키고 일반 구현 에이전트를 구축하기 위한 실질적인 단계가 되기를 바랍니다. 코드, 체크포인트, 벤치마크는 https://superrobobrain.github.io에서 확인할 수 있습니다."
855,http://arxiv.org/abs/2506.22593 ,Pixels-to-Graph: Real-time Integration of Building Information Models and Scene Graphs for Semantic-Geometric Human-Robot Understanding,"Antonello Longo, Chanyoung Chung, Matteo Palieri, Sung-Kyun Kim, Ali Agha, Cataldo Guaragnella, Shehryar Khattak","자율 로봇은 위험도가 높고 위험한 응용 분야에서 작업자를 위한 지원 플랫폼으로서 점점 더 중요한 역할을 하고 있습니다. 어려운 작업을 수행하려면 효율적인 인간-로봇 협력과 이해가 필요합니다. 일반적으로 로봇 계획은 3D 기하학적 정보를 활용하는 반면, 인간 운영자는 BIM(건물 정보 모델)을 나타내는 하향식 2D 지도와 같이 환경을 간략하게 표현하는 높은 수준에 익숙합니다. 3D 장면 그래프는 사람이 읽을 수 있는 2D BIM과 로봇 3D 지도 사이의 격차를 해소하는 강력한 도구로 등장했습니다. 이 작업에서는 리소스가 제한된 로봇 플랫폼에서 알려지지 않은 환경을 자율적으로 탐색하기 위해 이미지 픽셀과 LiDAR 맵에서 실시간으로 구조화된 장면 그래프를 생성하는 새로운 경량 방법인 Pixels-to-Graph(Pix2G)를 소개합니다. 온보드 컴퓨팅 제약 조건을 충족하기 위해 프레임워크는 CPU에서만 모든 작업을 수행하도록 설계되었습니다. 방법 출력은 노이즈가 제거된 2D 하향식 환경 맵과 구조 분할된 3D 포인트 클라우드이며, 이는 객체 수준에서 건물 수준까지 정보를 추상화하는 다층 그래프를 사용하여 원활하게 연결됩니다. 제안된 방법은 NASA JPL NeBula-Spot 다리 로봇을 사용하여 수행된 실제 실험에서 정량적 및 정성적으로 평가되어 어수선한 차고 및 도시 사무실과 같은 환경을 실시간으로 자율적으로 탐색하고 매핑합니다."
854,http://arxiv.org/abs/2506.21839 ,GenEscape: Hierarchical Multi-Agent Generation of Escape Room Puzzles,"Mengyi Shan, Brian Curless, Ira Kemelmacher-Shlizerman, Steve Seitz","우리는 시각적으로 매력적이고, 논리적으로 견고하며, 지적으로 자극적인 탈출실 퍼즐 이미지를 생성하여 텍스트-이미지 모델에 도전합니다. 기본 이미지 모델이 공간 관계 및 어포던스 추론으로 어려움을 겪는 동안, 우리는 이 작업을 기능적 디자인, 상징적 장면 그래프 추론, 레이아웃 합성 및 로컬 이미지 편집과 같은 구조화된 단계로 분해하는 계층적 다중 에이전트 프레임워크를 제안합니다. 전문 에이전트는 반복적인 피드백을 통해 협력하여 장면이 시각적으로 일관되고 기능적으로 해결 가능하도록 보장합니다. 실험에 따르면 에이전트 협업은 시각적 품질을 유지하면서 해결 가능성, 지름길 회피 및 어포던스 명확성 측면에서 출력 품질을 향상시키는 것으로 나타났습니다."
853,http://arxiv.org/abs/2506.21813 ,CAT-SG: A Large Dynamic Scene Graph Dataset for Fine-Grained Understanding of Cataract Surgery,"Felix Holm, Gözde Ünver, Ghazal Ghazaei, Nassir Navab","백내장 수술의 복잡한 워크플로우를 이해하려면 수술 도구, 해부학적 구조 및 절차 기술 간의 복잡한 상호 작용을 모델링해야 합니다. 기존 데이터 세트는 주로 도구 감지 또는 단계 분할과 같은 수술 분석의 고립된 측면을 다루지만 시간이 지남에 따라 엔터티 간의 의미 관계를 캡처하는 포괄적인 표현이 부족합니다. 이 백서에서는 도구-조직 상호 작용, 절차적 변형 및 시간적 종속성에 대한 구조화된 주석을 제공하는 최초의 백내장 수술 장면 그래프(CAT-SG) 데이터 세트를 소개합니다. CAT-SG는 상세한 의미론적 관계를 통합함으로써 수술 워크플로우에 대한 전체적인 관점을 제공하여 수술 단계와 기술을 보다 정확하게 인식할 수 있습니다. 또한 구조화된 수술 표현을 생성하는 데 있어 현재 방법을 능가하는 새로운 장면 그래프 생성 모델인 CatSGG를 제시합니다. CAT-SG 데이터 세트는 AI 기반 수술 훈련, 실시간 의사 결정 지원 및 워크플로 분석을 향상하여 임상 실습에서 보다 지능적이고 상황 인식 시스템을 위한 기반을 마련하도록 설계되었습니다."
852,http://arxiv.org/abs/2506.21357 ,CoPa-SG: Dense Scene Graphs with Parametric and Proto-Relations,"Julian Lorenz, Mrunmai Phatak, Robin Schön, Katja Ludwig, Nico Hörmann, Annemarie Friedrich, Rainer Lienhart",2D 장면 그래프는 장면 이해를 위한 구조적이고 설명 가능한 프레임워크를 제공합니다. 그러나 현재 작업에서는 정확한 장면 그래프 데이터가 부족하여 여전히 어려움을 겪고 있습니다. 이러한 데이터 병목 현상을 극복하기 위해 우리는 매우 정확한 지상 진실과 모든 객체 간의 철저한 관계 주석을 갖춘 합성 장면 그래프 데이터 세트인 CoPa-SG를 제시합니다. 또한 장면 그래프의 두 가지 새로운 기본 개념인 파라메트릭 관계와 프로토 관계를 소개합니다. 전자는 각도나 거리와 같은 추가 매개변수로 관계를 강화하여 기존 방식보다 훨씬 더 세밀한 표현을 제공합니다. 후자는 장면 그래프의 가상 관계를 인코딩하고 새 객체가 장면에 배치될 경우 관계가 어떻게 형성되는지 설명합니다. CoPa-SG를 사용하여 다양한 장면 그래프 생성 모델의 성능을 비교합니다. 우리는 새로운 관계 유형을 다운스트림 애플리케이션에 통합하여 계획 및 추론 기능을 향상시키는 방법을 보여줍니다.
851,http://arxiv.org/abs/2506.20394 ,SPARK: Graph-Based Online Semantic Integration System for Robot Task Planning,"Mimo Shirasaka, Yuya Ikeda, Tatsuya Matsushima, Yutaka Matsuo, Yusuke Iwasawa","범용 서비스 로봇에서는 작업 수행 중에 다양한 수단을 통해 획득한 정보를 온라인으로 업데이트하는 능력이 중요합니다. 이 정보에는 기하학적 및 의미론적 데이터가 포함됩니다. SLAM은 2D 지도나 3D 포인트 클라우드의 기하학적 업데이트를 처리하지만 의미 정보의 온라인 업데이트는 아직 탐색되지 않은 상태로 남아 있습니다. 우리는 유틸리티와 확장성 때문에 온라인 장면 그래프 표현이 문제라고 생각합니다. 오프라인 장면 그래프 표현에 관한 이전 연구를 바탕으로 본 연구에서는 의미 정보의 온라인 그래프 표현을 연구합니다. SPARK: 공간 인식 및 로봇 지식 통합을 소개합니다. 이 프레임워크는 환경에 내장된 단서에서 의미 정보를 추출하고 이에 따라 장면 그래프를 업데이트하며, 이는 후속 작업 계획에 사용됩니다. 우리는 공간 관계의 그래프 표현이 동적 환경에서 작업을 수행하고 제스처와 같은 비전통적인 공간 신호에 적응하는 로봇 시스템의 능력을 향상시킨다는 것을 보여줍니다."
850,http://arxiv.org/abs/2506.19683 ,Semantic Scene Graph for Ultrasound Image Explanation and Scanning Guidance,"Xuesong Li, Dianye Huang, Yameng Zhang, Nassir Navab, Zhongliang Jiang",의료용 초음파 영상을 이해하는 것은 영상 및 획득 매개변수의 차이로 인한 상당한 시각적 가변성으로 인해 오랜 과제로 남아 있습니다. LLM(대형 언어 모델)의 최근 발전은 충분한 생리학적 지식을 갖춘 임상의를 대상으로 전문 용어가 풍부한 요약을 자동으로 생성하는 데 사용되었습니다. 그럼에도 불구하고 현장 진료 환경 등 비전문가 사용자 사이에서 개선된 초음파 해석성과 기본 스캐닝 안내에 대한 수요 증가는 아직 조사되지 않았습니다. 본 연구에서는 영상 내용을 일상적으로 설명하고 초음파 스캐닝에 대한 지침을 제공하기 위해 초음파 영상용 장면 그래프(SG)를 먼저 도입한다. 초음파 SG는 먼저 변환기 기반 1단계 방법을 사용하여 계산되므로 명시적인 개체 감지가 필요하지 않습니다. 일반에 대한 파악 가능한 이미지 설명을 생성하기 위해 사용자 쿼리는 LLM을 통해 추상 SG 표현을 더욱 구체화하는 데 사용됩니다. 또한 예측된 SG는 현재 영상 보기 내에서 누락된 해부학적 구조에 대한 초음파 스캐닝을 안내하여 일반 사용자가 보다 표준화되고 완전한 해부학적 탐색을 달성할 수 있도록 지원하는 잠재력을 탐구합니다. 이 SG 기반 이미지 설명 및 스캐닝 지침의 효과는 5명의 지원자의 경동맥과 갑상선을 포함한 왼쪽 및 오른쪽 목 부위의 이미지에서 검증되었습니다. 결과는 일반인의 해석 가능성과 유용성을 향상시켜 초음파를 최대한 민주화할 수 있는 방법의 잠재력을 보여줍니다.
849,http://arxiv.org/abs/2506.19639 ,HOIverse: A Synthetic Scene Graph Dataset With Human Object Interactions,"Mrunmai Vivek Phatak, Julian Lorenz, Nico Hörmann, Jörg Hähner, Rainer Lienhart","인간과 로봇 에이전트가 환경에 공존하는 경우 에이전트가 탐색 및 계획과 같은 다양한 다운스트림 작업을 수행하려면 장면 이해가 중요합니다. 따라서 에이전트는 인간이 수행하는 작업을 위치화하고 식별할 수 있어야 합니다. 현재 연구에는 인간도 장면의 일부인 실내 환경에서 장면 이해를 수행하기 위한 신뢰할 수 있는 데이터 세트가 부족합니다. 장면 그래프를 사용하면 장면이나 이미지의 구조화된 표현을 생성하여 시각적 장면 이해를 수행할 수 있습니다. 이를 해결하기 위해 우리는 장면 그래프와 인간-객체 상호 작용의 교차점에 HOIverse를 제시합니다. 이는 인간과 주변 객체 사이의 정확하고 조밀한 관계 지상 진실과 해당 RGB 이미지, 분할 마스크, 깊이 이미지 및 인간 키포인트로 구성됩니다. 우리는 다양한 객체 쌍과 인간-객체 쌍 간의 파라메트릭 관계를 계산하여 정확하고 모호하지 않은 관계 정의를 얻습니다. 또한, 파라메트릭 관계 및 인간-객체 상호 작용을 예측하기 위해 최첨단 장면 그래프 생성 모델에 대한 데이터 세트를 벤치마킹합니다. 이 데이터 세트를 통해 우리는 사람과 관련된 장면 이해 분야의 연구를 가속화하는 것을 목표로 합니다."
848,http://arxiv.org/abs/2506.18825 ,SViP: Sequencing Bimanual Visuomotor Policies with Object-Centric Motion Primitives,"Yizhou Chen, Hang Xu, Dongjie Yu, Zeqing Zhang, Yi Ren, Jia Pan","특히 정책 훈련을 위해 고차원 시각적 입력을 활용할 때 모방 학습(IL)은 복잡한 양손 조작 작업에서 직관적이고 효과적인 것으로 입증되었습니다. 그럼에도 불구하고 시력 운동 정책의 일반화 기능은 여전히 ​​제한적이며, 특히 소규모 데모 데이터 세트를 사용할 수 있는 경우 더욱 그렇습니다. 시력 운동 정책에 누적된 오류는 장거리 작업을 완료하는 능력을 크게 방해합니다. 이러한 제한 사항을 해결하기 위해 우리는 시각 운동 정책을 작업 및 동작 계획(TAMP)에 원활하게 통합하는 프레임워크인 SViP를 제안합니다. SViP는 시맨틱 장면 그래프 모니터를 사용하여 인간의 시연을 이중 수동 작업과 단일 수동 작업으로 분할합니다. 주요 장면 그래프의 연속 결정 변수는 전환 조건 생성기를 훈련하는 데 사용됩니다. 이 생성기는 분포를 벗어난 관찰이 발생하는 경우에도 안정적인 성능을 보장하는 매개변수화된 스크립트 기본 요소를 생성합니다. 20개의 실제 데모를 사용하여 SViP를 사용하면 객체 자세 추정기를 요구하지 않고도 분포를 벗어난 초기 조건에 걸쳐 시력 운동 정책을 일반화할 수 있음을 보여줍니다. 이전에 볼 수 없었던 작업의 경우 SViP는 TAMP 형식주의의 제약 모델링을 활용하여 목표를 달성하기 위한 효과적인 솔루션을 자동으로 찾습니다. 실제 실험에서 SViP는 최첨단 생성적 IL 방법보다 성능이 뛰어나므로 더 복잡한 작업에 더 폭넓게 적용할 수 있음을 나타냅니다. 프로젝트 웹사이트: https://sites.google.com/view/svip-bimanual"
847,http://arxiv.org/abs/2506.17590 ,DRAMA-X: A Fine-grained Intent Prediction and Risk Reasoning Benchmark For Driving,"Mihir Godbole, Xiangbo Gao, Zhengzhong Tu","보행자나 자전거 운전자와 같은 취약한 도로 사용자(VRU)의 단기 동작을 이해하는 것은 안전한 자율 주행을 위해 매우 중요하며, 특히 모호하거나 위험도가 높은 행동이 있는 도시 시나리오에서는 더욱 그렇습니다. VLM(비전 언어 모델)은 개방형 어휘 인식을 가능하게 했지만 세부적인 의도 추론에 대한 유틸리티는 아직 충분히 연구되지 않았습니다. 특히, 안전이 중요한 상황에서 다중 클래스 의도 예측을 평가하는 기존 벤치마크는 없습니다. 이러한 격차를 해결하기 위해 자동화된 주석 파이프라인을 통해 DRAMA 데이터 세트로 구성된 세분화된 벤치마크인 DRAMA-X를 소개합니다. DRAMA-X에는 객체 경계 상자, 9가지 클래스 방향 의도 분류, 이진 위험 점수, 전문가가 생성한 자아 차량에 대한 작업 제안 및 설명 모션 요약으로 레이블이 지정된 5,686개의 사고 발생 가능성이 있는 프레임이 포함되어 있습니다. 이러한 주석을 사용하면 객체 감지, 의도 예측, 위험 평가, 작업 제안 등 자율적 의사 결정의 중심이 되는 네 가지 상호 연관된 작업을 체계적으로 평가할 수 있습니다. 참조 기준으로 우리는 자아 차량의 추론 파이프라인을 반영하는 훈련이 필요 없는 경량 프레임워크인 SGG-Intent를 제안합니다. VLM 지원 감지기를 사용하여 시각적 입력에서 장면 그래프를 순차적으로 생성하고, 의도를 추론하고, 위험을 평가하고, 대규모 언어 모델로 구동되는 구성 추론 단계를 사용하여 작업을 권장합니다. 우리는 네 가지 DRAMA-X 작업 전체의 성능을 비교하여 다양한 최신 VLM을 평가합니다. 우리의 실험은 장면 그래프 기반 추론이 특히 상황별 단서가 명시적으로 모델링될 때 의도 예측 및 위험 평가를 향상시킨다는 것을 보여줍니다."
846,http://arxiv.org/abs/2506.15828 ,Context Matters! Relaxing Goals with LLMs for Feasible 3D Scene Planning,"Emanuele Musumeci, Michele Brienza, Francesco Argenziano, Abdel Hakim Drid, Vincenzo Suriani, Daniele Nardi, Domenico D. Bloisi","구현된 에이전트는 실제적이고 복잡한 3D 환경에서 안정적으로 계획하고 행동해야 합니다. 고전적인 계획(예: PDDL)은 구조와 보장을 제공하지만 실제로는 시끄러운 인식과 잘못된 술어 접지로 인해 실패합니다. 반면, LLM(대형 언어 모델) 기반 계획자는 상식적 추론을 활용하지만 실현 불가능하거나 안전하지 않은 조치를 자주 제안합니다. 두 가지 접근 방식을 결합한 최근 연구에 이어 계층적 목표 완화를 수행하기 위해 LLM과 고전적 계획을 융합하는 프레임워크인 ContextMatters를 소개합니다. LLM은 기호를 장면에 고정하는 데 도움을 주고, 대상에 도달할 수 없는 경우 제약 조건을 점진적으로 완화하여 목표를 에이전트 환경의 컨텍스트에 맞게 조정하는 기능적으로 동등한 목표를 제안합니다. 3D 장면 그래프에서 작동하는 이 메커니즘은 명목상 실행 불가능한 많은 작업을 다루기 쉬운 계획으로 바꾸고 전체 완료가 불가능할 때 상황 인식 부분 성취를 가능하게 합니다. 우리의 실험 결과는 최첨단 LLM+PDDL 기준에 비해 성공률이 52.45% 향상되어 우리 접근 방식의 효율성을 보여줍니다. 또한 TIAGo 로봇에 배포하여 실제 시나리오에서 ContextMatter의 실행을 검증합니다. 코드, 데이터 세트 및 보충 자료는 커뮤니티(https://lab-rococo-sapienza.github.io/context-matters/)에서 확인할 수 있습니다."
845,http://arxiv.org/abs/2506.15583 ,DiscoSG: Towards Discourse-Level Text Scene Graph Parsing through Iterative Graph Refinement,"Shaoqing Lin, Chong Teng, Fei Li, Donghong Ji, Lizhen Qu, Zhuang Li","VLM(비전 언어 모델)은 담론 수준의 다중 문장 시각적 설명, 단일 문장 캡션-그래프 매핑을 위해 구축된 까다로운 텍스트 장면 그래프 파서를 생성합니다. 현재 접근 방식은 일반적으로 담화 입력에 대한 문장 수준 구문 분석 출력을 병합합니다. 종종 문장 간 상호 참조와 같은 현상이 누락되어 그래프가 단편화되고 다운스트림 VLM 작업 성능이 저하됩니다. 우리는 새로운 작업인 Discourse 수준 텍스트 장면 그래프 구문 분석(DiscoSG)을 도입하고 전문가 주석이 달린 400개의 데이터 세트와 8,430개의 합성된 다중 문장 캡션-그래프 쌍으로 구성된 데이터 세트인 DiscoSG-DS를 출시합니다. 각 캡션은 평균 9개의 문장으로 이루어져 있으며, 각 그래프에는 기존 데이터 세트보다 최소 3배 더 많은 트리플이 포함되어 있습니다.   DiscoSG-DS에서 GPT-4o를 미세 조정하면 최상의 문장 병합 기준보다 40% 이상 더 높은 SPICE 지표가 생성됩니다. 그러나 높은 추론 비용과 라이선스로 인해 오픈 소스 사용이 제한됩니다. 더 작고 미세 조정된 오픈 소스 모델(예: Flan-T5)은 단순한 그래프에서는 성능이 좋지만 밀도가 높고 복잡한 그래프에서는 성능이 저하됩니다. 이러한 격차를 해소하기 위해 우리는 시드 그래프 초안을 작성하고 새로운 학습된 그래프 편집 모델로 이를 반복적으로 개선하는 경량 오픈 소스 파서인 DiscoSG-Refiner를 소개합니다. 이를 통해 기준보다 30% 더 높은 SPICE를 달성하는 동시에 GPT-4o보다 86배 빠른 추론을 제공합니다. 단순한 그래프부터 조밀한 그래프까지 일반화하여 담화 수준 캡션 평가 및 환각 감지를 포함한 다운스트림 VLM 작업을 지속적으로 개선하고 대체 오픈 소스 파서보다 성능이 뛰어납니다. 코드와 데이터는 https://github.com/ShaoqLin/DiscoSG에서 확인할 수 있습니다."
844,http://arxiv.org/abs/2506.15402 ,MCOO-SLAM: A Multi-Camera Omnidirectional Object SLAM System,"Miaoxin Pan, Jinnan Li, Yaowen Zhang, Yi Yang, Yufeng Yue","객체 수준 SLAM은 구조화되고 의미론적으로 의미 있는 환경 표현을 제공하여 더 해석하기 쉽고 높은 수준의 로봇 작업에 적합합니다. 그러나 대부분의 기존 접근 방식은 RGB-D 센서 또는 단안 뷰에 의존하며, 이는 특히 대규모 또는 실외 환경에서 좁은 시야, 폐색 감도 및 제한된 깊이 인식으로 인해 어려움을 겪습니다. 이러한 제한으로 인해 시스템이 제한된 관점에서 개체의 부분적인 보기만 관찰하도록 제한되는 경우가 많아 부정확한 개체 모델링과 신뢰할 수 없는 데이터 연결이 발생합니다. 이 연구에서는 복잡한 실외 시나리오에서 견고하고 일관되며 의미적으로 풍부한 매핑을 달성하기 위해 서라운드 뷰 카메라 구성을 완전히 활용하는 새로운 다중 카메라 전방향 개체 SLAM 시스템인 MCOO-SLAM을 제안합니다. 우리의 접근 방식은 개방형 어휘 의미론으로 강화된 포인트 기능과 객체 수준 랜드마크를 통합합니다. 의미-기하학적-시간적 융합 전략이 도입되어 여러 뷰에 걸친 강력한 객체 연관을 통해 일관성이 향상되고 정확한 객체 모델링이 이루어지며, 전방향 루프 폐쇄 모듈은 장면 수준 설명자를 사용하여 시점 불변 장소 인식이 가능하도록 설계되었습니다. 또한 구성된 맵은 다운스트림 추론 작업을 지원하기 위해 계층적 3D 장면 그래프로 추상화됩니다. 실제에서의 광범위한 실험은 MCOO-SLAM이 폐색, 포즈 변형 및 환경 복잡성에 대한 향상된 견고성을 통해 정확한 위치 파악 및 확장 가능한 개체 수준 매핑을 달성한다는 것을 보여줍니다."
843,http://arxiv.org/abs/2506.14178 ,TACS-Graphs: Traversability-Aware Consistent Scene Graphs for Ground Robot Localization and Mapping,"Jeewon Kim, Minho Oh, Hyun Myung","장면 그래프는 로봇을 위한 강력한 도구로 등장하여 고급 작업 계획을 위한 공간적, 의미적 관계의 구조화된 표현을 제공합니다. 잠재력에도 불구하고 기존의 3D 실내 장면 그래프는 특히 구조적으로 복잡한 환경에서 공간 레이어를 과소 분할하거나 과도하게 분할하는 등 심각한 한계에 직면해 있습니다. 과소 분할은 통과할 수 없는 영역을 방의 일부(주로 열린 공간)로 잘못 분류하는 반면, 과도 분할은 복잡한 환경에서 단일 방을 겹치는 세그먼트로 분할합니다. 이러한 문제는 기하학적 근접성에만 의존하고 횡단 가능한 공간의 구조적 제약을 무시하고 장면 그래프 내에서 일관되지 않은 공간 레이어를 생성하는 순진한 복셀 기반 지도 표현에서 비롯됩니다. 우리가 아는 한, 이 작업은 분할 불일치를 과제로 해결하고 지상 로봇 횡단 기능과 공간 분할 기능을 통합하는 새로운 프레임워크인 TACS-Graphs(횡단성 인식 일관성 장면 그래프)를 사용하여 이를 해결한 최초의 작업입니다. 공간 경계를 정의하는 핵심 요소로 횡단 가능성을 활용함으로써 제안된 방법은 의미상으로 더 의미 있고 위상적으로 일관된 분할을 달성하여 복잡한 환경에서 복셀 기반 장면 그래프 접근 방식의 부정확성을 효과적으로 완화합니다. 또한 향상된 분할 일관성은 제안된 일관된 장면 그래프 활용 루프 폐쇄 감지(CoSG-LCD)에서 루프 폐쇄 감지 효율성을 향상시켜 더 높은 포즈 추정 정확도를 제공합니다. 실험 결과는 제안된 접근 방식이 장면 그래프 일관성 및 포즈 그래프 최적화 성능 측면에서 최신 방법보다 성능이 우수하다는 것을 확인했습니다."
842,http://arxiv.org/abs/2506.13629 ,FreeQ-Graph: Free-form Querying with Semantic Consistent Scene Graph for 3D Scene Understanding,"Chenlu Zhan, Yufei Zhang, Gaoang Wang, Hongwei Wang","자유 형식 언어를 통해 복잡한 3D 장면에서 의미론적 쿼리를 수행하는 것은 중요한 과제입니다. 기존 3D 장면 이해 방법은 대규모 훈련 데이터와 CLIP을 사용하여 텍스트 쿼리를 3D 의미론적 특징과 일치시킵니다. 그러나 훈련 데이터의 사전 정의된 어휘에 대한 의존도는 자유 형식 의미론적 쿼리를 방해합니다. 게다가 최근의 고급 방법은 장면 이해를 위해 LLM에 의존하지만 포괄적인 3D 장면 수준 정보가 부족하고 LLM 생성 출력의 잠재적인 불일치를 간과하는 경우가 많습니다. 본 논문에서는 3차원 장면 이해를 위한 의미론적 일관성 장면 그래프를 통해 Free-form Querying이 가능한 FreeQ-Graph를 제안한다. 핵심 아이디어는 사전 정의된 어휘 없이 완전하고 정확한 3D 장면 그래프에서 자유 형식 쿼리를 인코딩하고 이를 3가지 주요 단계를 통해 달성되는 3D 일관된 의미 레이블과 정렬하는 것입니다. 훈련 데이터나 사전 정의된 사전 정의가 전혀 없는 LLM 및 LVLM 안내를 통해 자유 형식 객체와 그 관계를 매핑하는 완전하고 정확한 3D 장면 그래프를 구성하는 것부터 시작합니다. 가장 중요한 것은 병합된 슈퍼포인트의 3D 의미 체계 정렬 기능을 활용하여 그래프 노드를 정확한 의미 체계 레이블과 정렬하고 3D 의미 체계 일관성을 향상한다는 것입니다. 자유 형식 의미론적 쿼리를 활성화하기 위해 장면 수준 및 개체 수준 정보를 결합하여 복잡한 추론을 수행하는 LLM 기반 추론 알고리즘을 설계합니다. 우리는 3D 의미론적 접지, 분할 및 복잡한 쿼리 작업에 대한 광범위한 실험을 수행하는 동시에 그래프 생성의 정확성을 검증했습니다. 6개 데이터 세트에 대한 실험에서는 우리 모델이 복잡한 자유 형식 의미 쿼리와 복잡한 관계형 추론 모두에서 탁월한 것으로 나타났습니다."
841,http://arxiv.org/abs/2506.13149 ,Cognitive Synergy Architecture: SEGO for Human-Centric Collaborative Robots,Jaehong Oh,"본 논문에서는 기하학적 인식, 의미론적 추론, 설명 생성을 인간 중심의 협동 로봇을 위한 통합 프레임워크로 통합하도록 설계된 인지 매핑 아키텍처인 SEGO(Semantic Graph Ontology)를 제시합니다. SEGO는 환경의 공간적 구성뿐만 아니라 감지된 객체 간의 의미론적 관계와 존재론적 일관성을 나타내는 동적 인지 장면 그래프를 구성합니다. 이 아키텍처는 SLAM 기반 위치 파악, 딥 러닝 기반 개체 감지 및 추적, 온톨로지 기반 추론을 원활하게 결합하여 의미론적으로 일관된 실시간 매핑을 지원합니다."
840,http://arxiv.org/abs/2506.12525 ,A Spatial Relationship Aware Dataset for Robotics,"Peng Wang, Minh Huy Pham, Zhihao Guo, Wei Zhou","실제 환경에서 로봇 작업을 계획하려면 객체 인식뿐만 아니라 객체 간의 공간 관계에 대한 미묘한 이해도 필요합니다. 우리는 로봇이 획득한 거의 1,000개의 실내 이미지로 구성된 공간 관계 인식 데이터 세트를 제시하며, 객체 속성, 위치 및 상세한 공간 관계에 대한 주석이 추가되었습니다. Boston Dynamics Spot 로봇을 사용하여 캡처하고 맞춤형 주석 도구로 라벨을 붙인 데이터 세트는 유사하거나 동일한 객체와 복잡한 공간 배열로 구성된 복잡한 시나리오를 반영합니다. 우리는 이 데이터 세트에서 6개의 최첨단 장면 그래프 생성 모델을 벤치마킹하여 추론 속도와 관계 정확도를 분석합니다. 우리의 결과는 모델 성능의 상당한 차이를 강조하고 ChatGPT 4o와 같은 기본 모델에 명시적인 공간 관계를 통합하면 로봇 공학에 대한 실행 가능하고 공간 인식 계획을 생성하는 능력이 크게 향상된다는 것을 보여줍니다. 데이터 세트 및 주석 도구는 https://github.com/PengPaulWang/SpatialAwareRobotDataset에서 공개적으로 제공되어 로봇 공학의 공간 추론에 대한 추가 연구를 지원합니다."
839,http://arxiv.org/abs/2506.10966 ,GENMANIP: LLM-driven Simulation for Generalizable Instruction-Following Manipulation,"Ning Gao, Yilun Chen, Shuai Yang, Xinyi Chen, Yang Tian, Hao Li, Haifeng Huang, Hanqing Wang, Tai Wang, Jiangmiao Pang","실제 환경에서의 로봇 조작은 특히 강력한 일반화와 관련하여 여전히 어려운 과제입니다. 기존 시뮬레이션 플랫폼은 정책이 다양한 지침과 시나리오에 어떻게 적응하는지 탐색하기 위한 충분한 지원이 부족합니다. 따라서 적응성이 중요하지만 공정한 비교에서 여전히 과소 탐구되는 LLM과 같은 지침을 따르는 기초 모델에 대한 관심 증가에 뒤처져 있습니다. 이러한 격차를 해소하기 위해 정책 일반화 연구에 맞춤화된 현실적인 테이블탑 시뮬레이션 플랫폼인 GenManip을 소개합니다. LLM 기반 작업 지향 장면 그래프를 통한 자동 파이프라인을 통해 10K 주석이 달린 3D 개체 자산을 사용하여 대규모의 다양한 작업을 합성합니다. 일반화를 체계적으로 평가하기 위해 Human-In-The-Loop 수정을 통해 개선된 200개 시나리오의 벤치마크인 GenManip-Bench를 제시합니다. 우리는 두 가지 정책 유형을 평가합니다. (1) 인식, 추론 및 계획을 위한 기반 모델을 통합하는 모듈식 조작 시스템과 (2) 확장 가능한 데이터 수집을 통해 훈련된 엔드투엔드 정책입니다. 결과는 데이터 확장이 엔드투엔드 방법에 도움이 되는 반면, 기초 모델로 강화된 모듈식 시스템은 다양한 시나리오에서 보다 효과적으로 일반화된다는 것을 보여줍니다. 우리는 이 플랫폼이 현실적인 상황에서 정책 일반화를 발전시키기 위한 중요한 통찰력을 촉진할 것으로 기대합니다. 프로젝트 페이지: https://genmanip.axi404.top/."
838,http://arxiv.org/abs/2506.08553 ,From Pixels to Graphs: using Scene and Knowledge Graphs for HD-EPIC VQA Challenge,"Agnese Taluzzi, Davide Gesualdi, Riccardo Santambrogio, Chiara Plizzari, Francesca Palermo, Simone Mentasti, Matteo Matteucci","이 보고서는 HD-EPIC VQA Challenge 2025를 위해 개발된 접근 방식인 SceneNet과 KnowledgeNet을 소개합니다. SceneNet은 다중 모달 대형 언어 모델(MLLM)로 생성된 장면 그래프를 활용하여 세분화된 개체 상호 작용, 공간 관계 및 시간적으로 기반이 되는 이벤트를 캡처합니다. 동시에 KnowledgeNet은 ConceptNet의 외부 상식 지식을 통합하여 엔터티 간의 높은 수준의 의미 연결을 도입하여 직접적으로 관찰할 수 있는 시각적 증거 이상의 추론을 가능하게 합니다. 각 방법은 HD-EPIC 벤치마크의 7개 범주에서 뚜렷한 강점을 보여 주며 프레임워크 내에서 이러한 방법을 조합하면 문제에 대한 전체 정확도가 44.21%로 나타나 복잡한 자기 중심적 VQA 작업에 대한 효율성이 강조됩니다."
837,http://arxiv.org/abs/2506.08189 ,Open World Scene Graph Generation using Vision Language Models,"Amartya Dutta, Kazi Sajeed Mehrab, Medha Sawhney, Abhilash Neog, Mridul Khurana, Sepideh Fatemi, Aanish Pradhan, M. Maruf, Ismini Lourentzou, Arka Daw, Anuj Karpatne","SGG(Scene-Graph Generation)는 이미지의 개체를 인식하고 해당 개체의 두드러진 쌍 관계를 추출하려고 합니다. 대부분의 방법은 다양한 상호 작용을 학습하기 위해 데이터 세트별 감독에 의존하여 새로운 객체 및/또는 관계가 포함된 개방형 환경에서의 유용성을 제한합니다. 대규모 VLM(Vision Language Model)을 활용하는 방법이라도 일반적으로 벤치마크별 미세 조정이 필요합니다. 사전 훈련된 VLM 지식을 직접 활용하여 추가 학습 없이 장면 그래프를 생성하는 훈련이 필요 없고 효율적이며 모델에 구애받지 않는 프레임워크인 Open-World SGG를 소개합니다. SGG를 제로샷 구조적 추론 문제로 캐스팅하는 우리의 방법은 다중 모달 프롬프트, 임베딩 정렬 및 경량 쌍 구체화 전략을 결합하여 보이지 않는 객체 어휘 및 관계 세트에 대한 추론을 가능하게 합니다. 이 설정을 평가하기 위해 우리는 개체 및 관계 측면에서 SGG 관련 데이터가 관찰되지 않은 경우 성능을 측정하는 Open-World 평가 프로토콜을 공식화합니다. Visual Genome, Open Images V6 및 PSG(Panoptic Scene Graph) 데이터세트에 대한 실험은 사전 훈련된 VLM이 작업 수준 훈련 없이 관계형 이해를 수행할 수 있는 능력을 보여줍니다."
836,http://arxiv.org/abs/2506.07643 ,Synthetic Visual Genome,"Jae Sung Park, Zixian Ma, Linjie Li, Chenhao Zheng, Cheng-Yu Hsieh, Ximing Lu, Khyathi Chandu, Quan Kong, Norimasa Kobori, Ali Farhadi, Yejin Choi, Ranjay Krishna","시각적 관계(공간적, 기능적, 상호작용적, 사회적 등)에 대한 추론은 인간 인지의 기본 구성 요소로 간주됩니다. 그러나 다중 모드 언어 모델(MLM)의 시각적 이해력이 크게 발전했음에도 불구하고 관계와 세대에 대한 정확한 추론은 여전히 ​​어려운 과제로 남아 있습니다. 대규모로 고품질의 조밀한 장면 그래프를 구성할 수 있는 조밀하게 주석이 달린 관계로 조정된 MLM 명령어인 ROBIN을 소개합니다. ROBIN을 훈련시키기 위해 우리는 교사 MLM과 신중하게 설계된 필터링 프로세스를 사용하여 기존 장면 그래프에서 선택한 객체의 누락된 관계를 완성하여 합성 장면 그래프 데이터 세트인 SVG를 선별하여 고품질을 보장합니다. 모든 이미지에 대해 더 정확하고 풍부한 장면 그래프를 대규모로 생성하기 위해 GPT-4o가 가능성이 낮은 관계를 제거하거나 관련 관계를 제안하여 ROBIN의 예측 장면 그래프를 더욱 구체화하는 자체 증류 프레임워크인 SG-EDIT를 도입합니다. 전체적으로 우리 데이터 세트에는 146K 이미지와 260만 개의 개체에 대한 560만 개의 관계가 포함되어 있습니다. 결과에 따르면 ROBIN-3B 모델은 300만 개 미만의 인스턴스로 훈련되었음에도 불구하고 관계 이해 벤치마크에서 3억 개가 넘는 인스턴스로 훈련된 유사한 크기의 모델보다 성능이 뛰어나며 최대 13B 매개변수까지 더 큰 모델을 능가합니다. 특히, 표현이해력 참조 부문에서는 88.9점으로 기존 최고점인 87.4점을 뛰어넘는 최고 수준의 성능을 달성했다. 우리의 결과는 정제된 장면 그래프 데이터에 대한 훈련이 다양한 시각적 추론 작업에서 높은 성능을 유지하는 데 중요하다는 것을 시사합니다."
835,http://arxiv.org/abs/2506.07454 ,Language-Grounded Hierarchical Planning and Execution with Multi-Robot 3D Scene Graphs,"Jared Strader, Aaron Ray, Jacob Arkin, Mason B. Peterson, Yun Chang, Nathan Hughes, Christopher Bradley, Yi Xuan Jia, Carlos Nieto-Granda, Rajat Talak, Chuchu Fan, Luca Carlone, Jonathan P. How, Nicholas Roy","본 논문에서는 3D 장면 그래프를 기반으로 매핑, 위치 파악, 작업 및 동작 계획(TAMP)을 통합하여 자연어로 표현된 복잡한 명령을 실행하는 다중 로봇 시스템을 소개합니다. 우리 시스템은 다중 로봇 3D 장면 그래프 융합에 활용되는 개방형 객체 기반 지도를 통합한 공유 3D 장면 그래프를 구축합니다. 이 표현은 실시간, 뷰 불변 재위치화(객체 기반 지도를 통해) 및 계획(3D 장면 그래프를 통해)을 지원하므로 로봇 팀이 주변 환경을 추론하고 복잡한 작업을 실행할 수 있습니다. 또한 공유 3D 장면 그래프 및 로봇 기능의 컨텍스트를 활용하여 LLM(대형 언어 모델)을 사용하여 운영자 의도를 PDDL(계획 도메인 정의 언어) 목표로 변환하는 계획 접근 방식을 소개합니다. 우리는 대규모 실외 환경의 실제 작업에 대한 시스템 성능에 대한 실험적 평가를 제공합니다. 보충 영상은 https://youtu.be/8xbGGOLfLAY에서 보실 수 있습니다."
834,http://arxiv.org/abs/2506.06804 ,IRS: Instance-Level 3D Scene Graphs via Room Prior Guided LiDAR-Camera Fusion,"Hongming Chen, Yiyang Lin, Ziliang Li, Biyu Ye, Yuying Zhang, Ximin Lyu","실내 장면 이해는 탐색 및 조작과 같은 다운스트림 작업에 직접적인 영향을 미치면서 로봇 공학의 근본적인 과제로 남아 있습니다. 기존 접근 방식은 폐쇄형 인식 또는 루프 폐쇄에 의존하는 경우가 많아 개방형 환경에서의 적응성이 제한됩니다. VFM(시각적 기반 모델)의 출현으로 개방형 어휘 인식 및 자연어 쿼리가 가능해졌으며 3D 장면 그래프 구성의 새로운 가능성이 열렸습니다.   본 논문에서는 LiDAR-카메라 융합을 통해 인스턴스 수준의 3D 장면 그래프 구축을 위한 강력하고 효율적인 프레임워크를 제안합니다. LiDAR의 넓은 시야각(FOV)과 장거리 감지 기능을 활용하여 공간 수준의 기하학적 사전확률을 빠르게 획득합니다. 의미 추출의 정확성과 일관성을 향상시키기 위해 다중 레벨 VFM이 사용됩니다. 인스턴스 융합 중에 룸 기반 분할을 통해 병렬 처리가 가능하며, 기하학적 및 의미론적 단서의 통합으로 융합 정확성과 견고성이 크게 향상됩니다. 최첨단 방법과 비교하여 우리의 접근 방식은 높은 의미 정확도를 유지하면서 구성 속도를 최대 10배까지 향상시킵니다.   시뮬레이션 환경과 실제 환경 모두에서 광범위한 실험을 통해 우리 접근 방식의 효율성이 검증되었습니다. 우리는 언어 기반 의미 탐색 작업을 통해 실제적인 가치를 보여주며 실제 로봇 응용 프로그램에 대한 잠재력을 강조합니다."
833,http://arxiv.org/abs/2506.06562 ,Towards Terrain-Aware Task-Driven 3D Scene Graph Generation in Outdoor Environments,"Chad R Samuelson, Timothy W McLain, Joshua G Mangelson","높은 수준의 자율 작동은 환경에 대해 충분히 표현력 있는 모델을 구성하는 로봇의 능력에 따라 달라집니다. 포인트 클라우드 및 점유 그리드와 같은 전통적인 3차원(3D) 장면 표현은 상세한 기하학적 정보를 제공하지만 상위 수준 추론에 필요한 구조화된 의미 체계 구성이 부족합니다. 3D 장면 그래프(3DSG)는 기하학적, 위상적, 의미론적 관계를 다중 레벨 그래프 기반 표현에 통합하여 이러한 제한을 해결합니다. 3DSG는 객체와 공간 레이아웃의 계층적 추상화를 캡처함으로써 로봇이 구조화된 방식으로 환경을 추론할 수 있도록 하여 상황 인식 의사 결정 및 적응형 계획을 향상시킵니다. 가장 최근의 작업은 실내 3DSG에 중점을 두었지만 이 문서에서는 실외 환경에서의 구성과 유용성을 조사합니다. 우리는 대규모 실외 설정을 위한 작업 독립적 메트릭 의미 포인트 클라우드를 생성하는 방법을 제시하고 실외 적용성을 위해 기존 실내 3DSG 생성 기술에 대한 수정을 제안합니다. 우리의 예비 정성적 결과는 실외 3DSG의 타당성을 보여주고 실제 현장 로봇 응용 분야에서 향후 배포 가능성을 강조합니다."
832,http://arxiv.org/abs/2506.06560 ,Semantics-aware Predictive Inspection Path Planning,"Mihir Dharmadhikari, Kostas Alexis","본 논문에서는 ""의미 인식 예측 계획""(SPP)이라는 새로운 의미 인식 검사 경로 계획 패러다임을 제시합니다. 선박 내부의 평형수 탱크와 같은 특정 객체 또는 구조(""의미론""이라고 함)의 검사가 필요한 산업 환경에서는 관심 있는 의미론의 구조화되고 반복적인 공간 배열이 나타나는 경우가 많습니다. 이에 동기를 부여하여 우리는 먼저 의미론적 장면 그래프 표현에서 정확하거나 부정확한 의미론의 공간적으로 반복되는 패턴을 식별하고 이러한 패턴을 사용하여 환경의 보이지 않는 부분에서 그래프의 진화에 대해 예측하는 알고리즘을 제공합니다. 또한 이러한 예측을 활용한 평형수 탱크 검사에 맞춘 두 가지 검사 경로 계획 전략을 제안합니다. 새로운 예측 계획 패러다임의 성능을 평가하기 위해 시뮬레이션과 실험 평가가 모두 수행됩니다. 먼저, 이 방법을 관련 최첨단 기술과 비교하는 시뮬레이션 연구를 수행하고 불완전한 패턴을 처리하는 능력을 보여주는 현재 테스트를 진행합니다. 둘째, 실제 선박 두 척의 밸러스트 탱크 내부에서 작동하는 충돌 방지 공중 로봇에 우리 방법을 배포합니다. 시뮬레이션과 현장 실험의 결과는 동일하거나 더 나은 의미론적 표면 적용 범위를 유지하면서 검사 시간 측면에서 최첨단 기술에 비해 크게 개선되었음을 보여줍니다. 방법의 다양한 부분과 현장 배포를 설명하는 일련의 비디오는 https://tinyurl.com/spp-videos에서 볼 수 있습니다. 이 작업의 코드는 https://github.com/ntnu-arl/predictive_planning_ros에서 확인할 수 있습니다."
831,http://arxiv.org/abs/2506.05787 ,EASG-Bench: Video Q&A Benchmark with Egocentric Action Scene Graphs,"Ivan Rodin, Tz-Ying Wu, Kyle Min, Sharath Nittur Sridhar, Antonino Furnari, Subarna Tripathi, Giovanni Maria Farinella","배우, 행동, 객체 사이의 복잡한 관계를 포착하는 시공간 기반 동적 장면 그래프로부터 질문 답변 쌍이 생성되는 자기 중심적 비디오에 대한 질문 답변 벤치마크인 EASG-Bench를 소개합니다. 우리는 체계적인 평가 프레임워크를 제안하고 이 벤치마크에서 여러 언어 전용 모델과 비디오 LLM(대형 언어 모델)을 평가합니다. 우리는 특히 시간 순서에 초점을 맞춘 질문에서 언어 전용 LLM과 비디오 LLM의 성능 격차를 관찰하여 긴 맥락 비디오 이해 분야의 연구 격차를 식별합니다. 연구 결과의 재현성을 높이고 추가 연구를 촉진하기 위해 벤치마크 및 관련 코드를 다음 GitHub 페이지에서 사용할 수 있습니다: https://github.com/fpv-iplab/EASG-bench."
830,http://arxiv.org/abs/2506.05651 ,"Hallucinate, Ground, Repeat: A Framework for Generalized Visual Relationship Detection","Shanmukha Vellamcheti, Sanjoy Kundu, Sathyanarayanan N. Aakur","객체 간의 관계를 이해하는 것은 구체화된 AI, 보조 시스템 및 장면 이해의 애플리케이션을 통해 시각적 지능의 핵심입니다. 그러나 대부분의 VRD(시각적 관계 감지) 모델은 고정된 조건자 세트에 의존하므로 일반화를 새로운 상호 작용으로 제한합니다. 주요 과제는 외부 지식을 바탕으로 가정된 의미론적으로 그럴듯하지만 주석이 없는 관계를 시각적으로 근거로 삼을 수 없다는 것입니다. 이 작업에서는 LLM(대형 언어 모델)을 구조화된 관계형 사전 모델로 활용하는 반복적인 시각적 기반 프레임워크를 소개합니다. 기대 최대화(EM)에서 영감을 받은 우리의 방법은 LLM(기대)을 사용하여 감지된 객체로부터 후보 장면 그래프를 생성하는 것과 이러한 가설을 지각 증거(최대화)와 정렬하기 위한 시각적 모델 교육을 번갈아 가며 수행합니다. 이 프로세스는 주석이 달린 데이터 이상의 관계형 이해를 부트스트랩하고 보이지 않는 조건에 대한 일반화를 가능하게 합니다. 또한 21개의 고정 조건을 갖춘 Visual Genome의 오픈 월드 VRD에 대한 새로운 벤치마크를 도입하고 표시, 표시되지 않음, 혼합의 세 가지 설정에서 평가합니다. 우리 모델은 LLM 전용, 퓨샷 및 편견 없는 기준선보다 성능이 뛰어나 이 세 가지 세트의 예측 분류에서 평균 재현율(mR@50) 15.9, 13.1, 11.7을 달성합니다. 이러한 결과는 확장 가능한 오픈 월드 시각적 이해를 위한 기반 LLM 사전 지식의 가능성을 강조합니다."
829,http://arxiv.org/abs/2506.04505 ,"SGN-CIRL: Scene Graph-based Navigation with Curriculum, Imitation, and Reinforcement Learning","Nikita Oskolkov, Huzhenyu Zhang, Dmitry Makarov, Dmitry Yudin, Aleksandr Panov","3D 장면 그래프는 객체 간 공간 관계를 모델링하여 에이전트가 부분적으로 관찰 가능한 환경에서 효율적으로 탐색하고 대상 객체의 위치를 ​​예측할 수 있도록 합니다. 본 논문에서는 개방형 어휘 3D 장면 그래프의 학습 가능한 표현을 갖춘 맵리스 강화 학습 기반 로봇 탐색을 위한 SGN-CIRL(3D Scene Graph-Based Reinforcement Learning Navigation)이라는 독창적인 프레임워크를 제안합니다. 강화 학습 기반 알고리즘의 훈련을 가속화하고 안정화하기 위해 프레임워크는 모방 학습과 커리큘럼 학습도 사용합니다. 첫 번째는 에이전트가 데모를 통해 학습할 수 있도록 하는 반면, 두 번째는 간단한 시나리오에서 고급 시나리오까지 작업 복잡성을 점진적으로 증가시켜 교육 프로세스를 구성합니다. Isaac Sim 환경에서 수행된 수치 실험에서는 강화 학습을 위해 3D 장면 그래프를 사용하면 어려운 탐색 상황에서 성공률이 크게 높아지는 것으로 나타났습니다. 코드는 오픈 소스이며 https://github.com/Xisonik/Aloha\_graph에서 사용할 수 있습니다."
828,http://arxiv.org/abs/2506.03135 ,OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models,"Mengdi Jia, Zekun Qi, Shaochen Zhang, Wenyao Zhang, Xinqiang Yu, Jiawei He, He Wang, Li Yi","공간 추론은 인지 심리학의 핵심 측면이며 현재 비전 언어 모델(VLM)의 병목 현상으로 남아 있습니다. 광범위한 연구는 왼쪽과 오른쪽, 가까운 곳과 먼 곳을 구별하고 객체 계산과 같은 기본 공간 관계에 대한 VLM의 이해를 평가하거나 향상시키는 것을 목표로 했지만 이러한 작업은 공간 추론의 가장 기본적인 계층만을 다루며 최신 추론 모델에서는 대체로 포화 상태에 접근하고 있습니다. 본 연구에서는 인지 심리학에 기반을 둔 공간 추론에 대한 포괄적이고 도전적인 벤치마크인 OmniSpatial을 소개합니다. OmniSpatial은 동적 추론, 복잡한 공간 논리, 공간 상호 작용, 관점 수용이라는 네 가지 주요 범주를 50개의 세분화된 하위 범주로 다룹니다. 세심한 수동 주석을 통해 우리는 8.4K개 이상의 질문-답변 쌍을 구성합니다. 광범위한 실험을 통해 오픈 소스 및 폐쇄 소스 VLM 모두 포괄적인 공간 추론에 상당한 한계가 있음이 나타났습니다. 또한 공간 추론을 강화하기 위해 PointGraph(명시적인 장면 그래프 큐)와 SpatialCoT(새로운 관점의 사고 사슬)라는 두 가지 전략을 탐구합니다."
827,http://arxiv.org/abs/2506.03082 ,SG2VID: Scene Graphs Enable Fine-Grained Control for Video Synthesis,"Ssharvien Kumar Sivakumar, Yannik Frisch, Ghazal Ghazaei, Anirban Mukhopadhyay","수술 시뮬레이션은 초보 외과 의사를 교육하고 학습 곡선을 가속화하며 수술 중 오류를 줄이는 데 중추적인 역할을 합니다. 그러나 기존의 시뮬레이션 도구는 필요한 사실적 표현과 인체 해부학의 다양성을 제공하는 데 부족합니다. 이에 대응하여 현재의 방법은 생성 모델 기반 시뮬레이터로 전환되고 있습니다. 그러나 이러한 접근 방식은 정밀한 인간 제어 측면을 무시하면서 정확한 합성을 위해 점점 더 복잡해지는 컨디셔닝을 사용하는 데 주로 중점을 둡니다. 이러한 격차를 해소하기 위해 정확한 비디오 합성과 세밀한 인간 제어를 위해 장면 그래프를 활용하는 최초의 확산 기반 비디오 모델인 SG2VID를 소개합니다. 우리는 백내장 및 담낭절제술 수술을 특징으로 하는 세 가지 공개 데이터 세트에서 SG2VID의 기능을 시연합니다. SG2VID는 질적, 양적 측면에서 이전 방법보다 성능이 뛰어나지만 정밀한 합성이 가능하여 도구와 해부학적 구조의 크기 및 움직임, 새로운 도구의 도입, 전체 장면 레이아웃에 대한 정확한 제어를 제공합니다. 우리는 SG2VID가 생성적 증강에 어떻게 사용될 수 있는지 질적으로 동기를 부여하고, 합성 비디오로 훈련 세트를 확장할 때 다운스트림 위상 검출 작업을 개선하는 능력을 보여주는 실험을 제시합니다. 마지막으로 인간의 제어를 유지하는 SG2VID의 능력을 보여주기 위해 장면 그래프와 상호 작용하여 심각하지만 드문 수술 중 불규칙성을 묘사하는 새로운 비디오 샘플을 생성합니다."
826,http://arxiv.org/abs/2506.02781 ,FreeScene: Mixed Graph Diffusion for 3D Scene Synthesis from Free Prompts,"Tongyuan Bai, Wangyuanfan Bai, Dong Chen, Tieru Wu, Manyi Li, Rui Ma","제어 가능성은 3D 실내 장면 합성의 실제 적용에 중요한 역할을 합니다. 기존 작업에서는 편리하지만 세밀한 장면 사용자 정의가 부족한 대략적인 언어 기반 제어를 허용하거나, 더 나은 제어 가능성을 제공하지만 번거로운 그래프 디자인 프로세스에 상당한 지식이 필요한 그래프 기반 제어를 사용합니다. 이러한 과제를 해결하기 위해 우리는 실내 장면 합성을 편리하고 효과적으로 제어할 수 있는 사용자 친화적인 프레임워크인 FreeScene을 제시합니다. 특히 FreeScene은 텍스트 설명 및/또는 참조 이미지를 포함한 자유로운 형식의 사용자 입력을 지원하므로 사용자가 다양한 디자인 의도를 표현할 수 있습니다. 사용자 입력은 VLM 기반 그래프 디자이너에 의해 적절하게 분석되고 그래프 표현에 통합됩니다. 그런 다음 그래프 인식 노이즈 제거를 수행하여 장면 생성을 향상시키는 혼합 그래프 확산 변환기인 MG-DiT를 제안합니다. MG-DiT는 그래프 구조를 보존하는 데 탁월할 뿐만 아니라 단일 모델 내에서 텍스트-장면, 그래프-장면 및 재배열을 포함하되 이에 국한되지 않는 다양한 작업에 대한 광범위한 적용 가능성을 제공합니다. 광범위한 실험을 통해 FreeScene은 텍스트 기반 및 그래프 기반 장면 합성을 통합하는 효율적이고 사용자 친화적인 솔루션을 제공하며 다양한 애플리케이션에서 생성 품질 및 제어 가능성 측면에서 최첨단 방법을 능가하는 것으로 나타났습니다."
825,http://arxiv.org/abs/2506.01487 ,FDSG: Forecasting Dynamic Scene Graphs,"Yi Yang, Yuren Cong, Hao Cheng, Bodo Rosenhahn, Michael Ying Yang","동적 장면 그래프 생성은 개체 관계와 시간적 진화를 모델링하여 장면 그래프 생성을 이미지에서 비디오로 확장합니다. 그러나 기존 방법은 시간 역학을 명시적으로 모델링하지 않고 관찰된 프레임에서 장면 그래프를 생성하거나 정적 엔터티 레이블 및 위치를 가정하면서 관계만 예측합니다. 이러한 제한은 엔터티와 관계 역학의 효과적인 추정을 방해하여 비디오 장면 이해를 제한합니다. 우리는 관찰되지 않은 프레임에 대해 미래 엔터티 레이블, 경계 상자 및 관계를 예측하는 동시에 관찰된 프레임에 대한 장면 그래프를 생성하는 새로운 프레임워크인 예측 동적 장면 그래프(FDSG)를 제안합니다. 우리의 장면 그래프 예측 모듈은 쿼리 분해와 신경 확률론적 미분 방정식을 활용하여 엔터티 및 관계 역학을 모델링합니다. 시간 집계 모듈은 교차 관심을 통해 예측 정보와 관찰 정보를 통합하여 예측을 더욱 구체화합니다. FDSG를 벤치마킹하기 위해 완전한 미래 장면 그래프 예측을 위한 새로운 작업인 장면 그래프 예측을 소개합니다. Action Genome에 대한 실험은 FDSG가 동적 장면 그래프 생성, 장면 그래프 예측 및 장면 그래프 예측에 대한 최첨단 방법보다 성능이 우수하다는 것을 보여줍니다. 코드는 출판 시 공개됩니다."
824,http://arxiv.org/abs/2506.01174 ,GraphPad: Inference-Time 3D Scene Graph Updates for Embodied Question Answering,"Muhammad Qasim Ali, Saeejith Nair, Alexander Wong, Yuchen Cui, Yuhao Chen","구조화된 장면 표현은 구현된 에이전트의 핵심 구성 요소로, 원시 감각 스트림을 읽기 가능하고 모듈식이며 검색 가능한 형식으로 통합하는 데 도움이 됩니다. 높은 계산 오버헤드로 인해 많은 접근 방식에서는 작업에 앞서 그러한 표현을 구축합니다. 그러나 작업 사양이 변경되면 주요 개체, 공간 관계 및 세부 사항을 놓칠 수 있으므로 이러한 정적 접근 방식은 부적절해집니다. 에이전트가 API 호출을 통해 작업 요구 사항에 맞게 조정할 수 있는 수정 가능한 구조적 메모리인 GraphPad를 소개합니다. 이는 환경을 나타내는 변경 가능한 장면 그래프, 프레임별 콘텐츠를 색인화하는 탐색 로그, 작업별 메모를 위한 스크래치 패드로 구성됩니다. GraphPad는 완전하고 최신 상태를 유지하며 장면과 해당 작업에 대한 상담원의 즉각적인 이해와 일치하는 동적 작업 공간 역할을 합니다. OpenEQA 벤치마크에서 GraphPad는 5배 적은 입력 프레임으로 작동하면서 동일한 비전 언어 모델을 사용하는 이미지 전용 기준에 비해 +3.0% 증가한 55.3%를 달성했습니다. 이러한 결과는 3D 메모리의 온라인 언어 기반 개선을 허용하면 추가 교육이나 데이터 수집 없이도 더 유익한 표현을 얻을 수 있음을 보여줍니다."
823,http://arxiv.org/abs/2506.00083 ,Hi-Dyna Graph: Hierarchical Dynamic Scene Graph for Robotic Autonomy in Human-Centric Environments,"Jiawei Hou, Xiangyang Xue, Taiping Zeng","인간 중심 현장에서 서비스 로봇의 자율 운영은 변화하는 환경에 대한 이해와 상황 인식 의사결정의 필요성으로 인해 여전히 어려운 과제로 남아 있습니다. 토폴로지 맵과 같은 기존 접근 방식은 효율적인 공간 사전 예측을 제공하지만 일시적 개체 관계를 모델링하는 데 실패하는 반면, 조밀한 신경 표현(예: NeRF)은 엄청난 계산 비용을 발생시킵니다. 계층적 장면 표현과 비디오 장면 그래프 생성 작업에서 영감을 받아 우리는 구현된 로봇 자율성을 위해 지속적인 글로벌 레이아웃과 지역화된 동적 의미론을 통합하는 계층적 동적 장면 그래프 아키텍처인 Hi-Dyna Graph를 제안합니다. 우리의 프레임워크는 실내 규모의 연결성과 대형 정적 개체(예: 가구)를 인코딩하는 포즈를 취한 RGB-D 입력에서 글로벌 토폴로지 그래프를 구성하는 반면, 환경 및 자기 중심 카메라는 개체 위치 관계 및 인간-개체 상호 작용 패턴으로 동적 하위 그래프를 채웁니다. 하이브리드 아키텍처는 의미론적 및 공간적 제약을 사용하여 이러한 하위 그래프를 글로벌 토폴로지에 고정함으로써 수행되므로 환경이 발전함에 따라 원활한 업데이트가 가능합니다. 통합 그래프를 해석하고, 잠재 작업 트리거를 추론하고, 로봇 어포던스를 기반으로 실행 가능한 지침을 생성하기 위해 LLM(대형 언어 모델)으로 구동되는 에이전트가 사용됩니다. Hi-Dyna Grap의 우수한 장면 표현 효율성을 입증하기 위해 복잡한 실험을 수행합니다. 실제 배포는 모바일 조작기를 통해 시스템의 실용성을 검증합니다. 로봇 공학은 카페테리아 보조원으로서 역동적인 장면에서 추가 교육이나 복잡한 보상 없이 복잡한 작업을 자율적으로 완료합니다. 비디오 데모 및 자세한 내용은 https://anonymous.4open.science/r/Hi-Dyna-Graph-B326을 참조하세요."
822,http://arxiv.org/abs/2505.24287 ,EgoExOR: An Ego-Exo-Centric Operating Room Dataset for Surgical Activity Understanding,"Ege Özsoy, Arda Mamur, Felix Tristram, Chantal Pellegrini, Magdalena Wysocki, Benjamin Busam, Nassir Navab","수술실(OR)은 빠르게 진행되고 폐색이 많은 환경에서 외과의사, 간호사 및 장비 간의 정확한 조정을 요구하므로 안전성과 효율성을 향상시키기 위한 고급 인식 모델이 필요합니다. 기존 데이터세트는 부분적인 자기 중심적 관점이나 희박한 외심적 다중 관점 컨텍스트를 제공하지만 두 가지의 포괄적인 조합을 탐색하지는 않습니다. 1인칭과 3인칭 관점을 융합하는 최초의 OR 데이터세트이자 관련 벤치마크인 EgoExOR을 소개합니다. 94분(15FPS에서 84,553프레임)의 두 가지 에뮬레이트된 척추 수술인 초음파 유도 바늘 삽입 및 최소 침습 척추 수술에 걸쳐 EgoExOR은 웨어러블 안경의 자기 중심 데이터(RGB, 시선, 손 추적, 오디오), 외심 RGB 및 RGB-D 카메라의 깊이, 초음파 이미지를 통합합니다. 36개의 엔터티와 22개의 관계(568,235개 트리플렛)를 포괄하는 상세한 장면 그래프 주석을 통해 임상 상호 작용의 강력한 모델링을 가능하게 하고 동작 인식 및 인간 중심 인식과 같은 작업을 지원합니다. 우리는 두 가지 최신 모델의 수술 장면 그래프 생성 성능을 평가하고 EgoExOR의 다중 모드 및 다중 관점 신호를 명시적으로 활용하는 새로운 기준을 제공합니다. 이 새로운 데이터세트와 벤치마크는 OR 인식을 위한 새로운 기반을 마련하여 차세대 임상 인식을 위한 풍부한 다중 모드 리소스를 제공합니다."
821,http://arxiv.org/abs/2505.23451 ,A Reverse Causal Framework to Mitigate Spurious Correlations for Debiasing Scene Graph Generation,"Shuzhou Sun, Li Liu, Tianpeng Liu, Shuaifeng Zhi, Ming-Ming Cheng, Janne Heikkilä, Yongxiang Liu","기존의 2단계 장면 그래프 생성(SGG) 프레임워크는 일반적으로 관계 특징을 추출하는 감지기와 이러한 관계를 분류하는 분류자를 통합합니다. 따라서 훈련 패러다임은 인과 사슬 구조를 따르며, 여기서 탐지기의 입력은 분류기의 입력을 결정하고 이는 최종 예측에 영향을 미칩니다. 그러나 이러한 인과 사슬 구조는 탐지기의 입력과 최종 예측 사이에 잘못된 상관 관계를 생성할 수 있습니다. 즉, 특정 관계의 예측이 다른 관계의 영향을 받을 수 있습니다. 이 영향은 적어도 두 가지 관찰 가능한 편향을 유발할 수 있습니다. 꼬리 관계는 머리 관계로 예측되고 전경 관계는 배경 관계로 예측됩니다. 특히 후자의 편견은 문헌에서 거의 논의되지 않습니다. 이 문제를 해결하기 위해 우리는 인과 사슬 구조를 역인과 구조로 재구성할 것을 제안합니다. 여기서 분류기의 입력은 혼란 요인으로 처리되고 탐지기의 입력과 최종 예측은 모두 인과 변수로 간주됩니다. 구체적으로 우리는 재구성된 인과 패러다임을 SGG(RcSGG)에 대한 역인과 프레임워크라고 부릅니다. RcSGG는 초기에 제안된 ARE(Active Reverse Estimation)를 사용하여 교란변수에 개입하여 역인과성, 즉 최종 예측에서 분류기 입력까지의 인과성을 추정합니다. 그리고 관계 정보를 고려하여 역인과성 추정을 더욱 강화하기 위해 MIS(Maximum Information Sampling)를 제안한다. 이론적으로 RcSGG는 SGG 프레임워크에 내재된 가짜 상관관계를 완화하여 결과적으로 유도된 편향을 제거할 수 있습니다. 인기 있는 벤치마크와 다양한 SGG 프레임워크에 대한 포괄적인 실험은 최첨단 평균 재현율을 보여줍니다."
820,http://arxiv.org/abs/2505.21955 ,Towards Comprehensive Scene Understanding: Integrating First and Third-Person Views for LVLMs,"Insu Lee, Wooje Park, Jaeyun Jang, Minyoung Noh, Kyuhong Shim, Byonghyo Shim","대형 비전 언어 모델(LVLM)은 머리 장착 카메라로 캡처한 1인칭(자기 중심) 뷰가 주요 입력 역할을 하는 가상 및 증강 현실과 같은 대화형 애플리케이션에 점점 더 많이 배포되고 있습니다. 이 보기는 사용자 주의와 손-객체 상호 작용에 대한 세밀한 단서를 제공하지만, 좁은 시야와 전역적 맥락 부족으로 공간적 또는 맥락적으로 까다로운 쿼리에 실패하는 경우가 많습니다. 이 문제를 해결하기 위해 우리는 3인칭(외심) 보기로 자기중심적 입력을 강화하여 LVLM에 대한 전역 장면 레이아웃 및 개체 가시성과 같은 보완적인 정보를 제공하는 프레임워크를 도입합니다. 우리는 동기화된 ego-exo 이미지 쌍을 기반으로 한 4K 고품질 질문-답변 쌍을 갖춘 최초의 멀티뷰 질문 답변 벤치마크인 E3VQA를 제시합니다. 또한 우리는 세 가지 보완적인 관점에서 장면 그래프를 통합하여 통일된 장면 표현을 구성하는 훈련이 필요 없는 프롬프트 기술인 M3CoT를 제안합니다. M3CoT를 사용하면 LVLM이 뷰 전반에 걸쳐 보다 효과적으로 추론할 수 있어 최근 CoT 기준에 비해 일관된 성능 향상(GPT-4o의 경우 4.84%, Gemini 2.0 Flash의 경우 5.94%)을 얻을 수 있습니다. 우리의 광범위한 평가는 다중 뷰 추론에서 LVLM의 주요 강점과 한계를 밝히고 자기중심적 입력과 외중심적 입력을 모두 활용하는 가치를 강조합니다. 데이터 세트와 소스 코드는 https://github.com/Leeinsu1/Towards-Comprehensive-Scene-Understanding에서 확인할 수 있습니다."
819,http://arxiv.org/abs/2505.21322 ,Assured Autonomy with Neuro-Symbolic Perception,"R. Spencer Hallyburton, Miroslav Pajic",사이버 물리 시스템(CPS)에 배포된 많은 최첨단 AI 모델은 매우 정확하지만 단순히 패턴 일치에 불과합니다. ~ 제한된 보안 보장으로 인해 안전이 중요하고 경쟁이 치열한 영역에서 신뢰성에 대한 우려가 있습니다. 보장된 AI를 발전시키기 위해 우리는 낮은 수준의 특징과 높은 수준의 맥락에 대해 추론하는 인간의 능력에서 영감을 받아 데이터 기반 인식 모델에 상징적 구조를 부여하는 패러다임 전환을 옹호합니다. 우리는 인식을 위한 신경 기호 패러다임(NeuSPaPer)을 제안하고 공동 객체 감지 및 장면 그래프 생성(SGG)이 어떻게 깊은 장면 이해를 제공하는지 설명합니다. 오프라인 지식 추출을 위한 기본 모델과 실시간 배포를 위한 특수 SGG 알고리즘을 기반으로 구조화된 관계형 그래프를 활용하여 자율적으로 상황 인식의 무결성을 보장하는 프레임워크를 설계합니다. 물리 기반 시뮬레이터와 실제 데이터 세트를 사용하여 SGG가 어떻게 낮은 수준의 센서 인식과 높은 수준의 추론 사이의 격차를 해소하고 탄력적인 상황 인식 AI의 기반을 구축하고 CPS에서 신뢰할 수 있는 자율성을 향상시키는 방법을 보여줍니다.
818,http://arxiv.org/abs/2505.20106 ,From Data to Modeling: Fully Open-vocabulary Scene Graph Generation,"Zuyao Chen, Jinlin Wu, Zhen Lei, Chang Wen Chen","우리는 기존 폐쇄형 모델의 한계를 극복하는 완전 개방형 어휘 장면 그래프 생성을 위한 새로운 변환기 기반 프레임워크인 OvSGTR을 제시합니다. 기존 방법은 객체 인식과 관계 인식을 고정된 어휘로 제한하여 새로운 개념이 자주 등장하는 실제 시나리오에 대한 적용 가능성을 방해합니다. 대조적으로, 우리의 접근 방식은 사전 정의된 범주를 넘어서 객체(노드)와 객체의 상호 관계(에지)를 공동으로 예측합니다. OvSGTR은 고정된 이미지 백본과 텍스트 인코더를 갖춘 DETR과 유사한 아키텍처를 활용하여 고품질의 시각적 및 의미적 특징을 추출한 다음 엔드투엔드 장면 그래프 예측을 위해 변환기 디코더를 통해 융합합니다. 복잡한 시각적 관계에 대한 모델의 이해를 강화하기 위해 약한 지도 방식으로 장면 그래프 주석을 합성하는 관계 인식 사전 훈련 전략을 제안합니다. 구체적으로 우리는 최소한의 수동 주석으로 전송 가능한 감독 신호를 생성하기 위해 장면 파서 기반, LLM 기반, 멀티모달 LLM 기반의 세 가지 파이프라인을 조사합니다. 또한, 우리는 지식 증류 전략과 결합된 시각적 개념 유지 메커니즘을 통합하여 모델이 미세 조정 중에 풍부한 의미론적 단서를 유지하도록 보장함으로써 개방형 어휘 설정에서 치명적인 망각의 일반적인 문제를 해결합니다. VG150 벤치마크에 대한 광범위한 실험에서는 OvSGTR이 폐쇄 세트, 개방형 어휘 개체 감지 기반, 관계 기반 및 완전 개방형 어휘 시나리오를 포함한 여러 설정에서 최첨단 성능을 달성한다는 것을 보여줍니다. 우리의 결과는 보다 일반화되고 안정적인 시각적 이해를 향한 장면 그래프 생성을 발전시키기 위한 대규모 관계 인식 사전 훈련 및 변환기 아키텍처의 가능성을 강조합니다."
817,http://arxiv.org/abs/2505.19510 ,LLM Meets Scene Graph: Can Large Language Models Understand and Generate Scene Graphs? A Benchmark and Empirical Study,"Dongil Yang, Minjin Kim, Sunghwan Kim, Beong-woo Kwak, Minjun Park, Jinseok Hong, Woontack Woo, Jinyoung Yeo","LLM(대형 언어 모델)의 놀라운 추론 및 일반화 기능은 구현된 AI, 로봇 공학 및 기타 실제 작업에서 응용 프로그램을 확장할 수 있는 길을 열었습니다. 이러한 애플리케이션을 효과적으로 지원하려면 다중 모드 환경에서 공간적, 시간적 이해를 기반으로 하는 것이 필수적입니다. 이를 위해 최근 작품에서는 장면의 개체, 속성 및 관계를 인코딩하는 구조적 표현인 장면 그래프를 활용했습니다. 그러나 장면 그래프를 활용하는 LLM의 능력에 대한 포괄적인 평가는 여전히 제한적입니다. 이 작업에서는 (1) 장면 그래프를 이해하고 (2) 텍스트 내러티브에서 장면 그래프를 생성하는 LLM의 능력을 체계적으로 평가하기 위해 설계된 벤치마크인 TSG(텍스트 장면 그래프) 벤치를 소개합니다. TSG Bench를 사용하여 11개의 LLM을 평가하고 모델이 장면 그래프 이해에서는 잘 수행되지만 특히 복잡한 내러티브의 경우 장면 그래프 생성에 어려움을 겪는다는 사실을 밝힙니다. 우리의 분석에 따르면 이러한 모델은 복잡한 내러티브에서 개별 장면을 효과적으로 분해하지 못하여 장면 그래프를 생성할 때 병목 현상이 발생하는 것으로 나타났습니다. 이러한 발견은 장면 그래프 생성에 있어 개선된 방법론의 필요성을 강조하고 향후 연구에 귀중한 통찰력을 제공합니다. 벤치마크 데모는 https://tsg-bench.netlify.app에서 확인할 수 있습니다. 또한, 우리의 코드와 평가 데이터는 https://github.com/docworlds/tsg-bench에서 공개적으로 제공됩니다."
816,http://arxiv.org/abs/2505.19507 ,Multimodal Machine Translation with Visual Scene Graph Pruning,"Chenyu Lu, Shiliang Sun, Jing Zhao, Nan Zhang, Tengfei Song, Hao Yang",다중 모드 기계 번역(MMT)은 시각적 정보를 통합하여 번역 작업에서 언어적 다의어성과 모호함으로 인한 문제를 해결하려고 합니다. 현재 MMT 연구의 주요 병목 현상은 시각적 데이터의 효과적인 활용입니다. 이전 접근 방식은 전역 또는 지역 수준의 이미지 특징을 추출하고 다중 모드 정보 융합을 위한 주의 또는 게이팅 메커니즘을 사용하는 데 중점을 두었습니다. 그러나 이러한 방법들은 MMT의 시각 정보 중복성 문제를 적절하게 해결하지 못했으며 효과적인 해결책을 제시하지도 못했습니다. 본 논문에서는 언어 장면 그래프 정보를 활용하여 시각적 장면 그래프에서 중복 노드 정리를 안내함으로써 다운스트림 번역 작업의 노이즈를 줄이는 시각적 장면 그래프 정리(PSG)를 사용한 다중 모드 기계 번역이라는 새로운 접근 방식을 소개합니다. 최첨단 방법과 절제 연구를 통한 광범위한 비교 실험을 통해 PSG 모델의 효율성을 입증합니다. 우리의 결과는 또한 MMT 분야를 발전시키는 데 있어 시각적 정보 가지치기의 유망한 잠재력을 강조합니다.
815,http://arxiv.org/abs/2505.19098 ,SPADE: Towards Scalable Path Planning Architecture on Actionable Multi-Domain 3D Scene Graphs,"Vignesh Kottayam Viswanathan, Akash Patel, Mario Alberto Valdes Saucedo, Sumeet Satpute, Christoforos Kanellakis, George Nikolakopoulos",본 연구에서는 3D 장면 그래프를 사용하여 동적 환경에서 자율 탐색을 위해 설계된 경로 계획 프레임워크인 SPADE를 소개합니다. SPADE는 계층적 경로 계획과 로컬 기하학적 인식을 결합하여 동적 장면에서 충돌 없는 움직임을 가능하게 합니다. 프레임워크는 계획 문제를 두 가지로 나눕니다. (a) 희박한 추상 글로벌 레이어 계획 해결 및 (b) 로컬 기하학적 장면 탐색에 맞춰 밀도가 높은 하위 로컬 레이어에 걸친 반복 경로 개선입니다. 밀집된 다중 작업 도메인 장면 그래프에서 실행 가능한 경로를 효율적으로 추출하기 위해 프레임워크는 경로 계획 전에 횡단 가능한 가장자리에 대한 정보를 바탕으로 샘플링을 시행합니다. 이는 경로 계획과 관련되지 않은 불필요한 정보를 제거하고 그래프에 대한 전반적인 계획 복잡성을 줄입니다. 기존 접근 방식은 계층적 경로 평가 프로세스와 기하학적 경로 평가 프로세스를 분리하여 장면 그래프에 대한 경로 계획 문제를 해결합니다. 특히 이로 인해 원래 경로를 차단하는 경로 장애물이 발생할 때 전체 장면 그래프에 대한 비효율적인 재계획이 발생합니다. 이와 대조적으로 SPADE는 로컬 기하학적 장면 탐색과 결합된 로컬 레이어 계획을 우선시하여 횡단 가능한 경로 계산의 효율성을 유지하면서 동적 장면을 탐색할 수 있습니다. 우리는 4족 보행 로봇에 대한 광범위한 시뮬레이션 실험과 실제 배포를 통해 SPADE를 검증하여 복잡하고 역동적인 시나리오를 처리하는 데 있어 SPADE의 효율성을 입증합니다.
814,http://arxiv.org/abs/2505.16237 ,Align-GRAG: Anchor and Rationale Guided Dual Alignment for Graph Retrieval-Augmented Generation,"Derong Xu, Pengyue Jia, Xiaopeng Li, Yingyi Zhang, Maolin Wang, Qidong Liu, Xiangyu Zhao, Yichao Wang, Huifeng Guo, Ruiming Tang, Enhong Chen, Tong Xu","강력한 능력에도 불구하고 LLM(대형 언어 모델)은 여전히 ​​환각과 오래된 지식에 대한 의존으로 인해 어려움을 겪고 있어 지식 집약적인 작업에 대한 우려가 커지고 있습니다. 그래프 기반 검색 증강 생성(GRAG)은 관계형 증거를 활용하여 그래프를 검색하여 지식으로 LLM을 강화하지만 두 가지 문제에 직면합니다. 이웃 확장으로 인해 도입된 구조 결합 관련 없는 지식과 그래프 임베딩과 LLM 의미 체계 간의 구조 추론 불일치입니다. 우리는 이러한 과제를 해결하기 위해 기준점과 이론적 근거에 따른 개선 프레임워크인 우리 모델을 제안합니다. 이는 중요한 노드를 식별하고 잡음이 있는 증거를 제거하는 \textbf{(1) 노드 수준 정렬}과 대조 학습을 통해 그래프와 언어 의미 공간을 연결하는 \textbf{(2) 그래프 수준 정렬}에 대한 중간 감독을 제공하는 앵커 및 이론적 근거 체인을 추출하도록 LLM에 요청합니다. 상식 추론, 장면 그래프 이해 및 지식 그래프 추론에 대한 광범위한 실험은 18가지 강력한 기준에 대한 일관된 이득을 보여주며, 그래프 기반 생성 개선을 위한 우리 모델의 효율성을 검증합니다. 코드는 https://anonymous.4open.science/r/Align-GRAG-F3D8/에서 찾을 수 있습니다."
813,http://arxiv.org/abs/2505.15867 ,SCENIR: Visual Semantic Clarity through Unsupervised Scene Graph Retrieval,"Nikolaos Chaidos, Angeliki Dimitriou, Maria Lymperaiou, Giorgos Stamou","이미지 간 검색에서 컨벌루션 및 변환기 기반 아키텍처가 우세함에도 불구하고 이러한 모델은 색상과 같은 낮은 수준의 시각적 기능으로 인해 발생하는 편향이 발생하기 쉽습니다. 의미론적 이해가 부족하다는 점을 주요 한계로 인식하여 표면적인 이미지 특성보다 의미론적 내용을 강조하는 새로운 장면 그래프 기반 검색 프레임워크를 제안합니다. 장면 그래프 검색에 대한 이전 접근 방식은 주로 지도형 그래프 신경망(GNN)에 의존하며, 여기에는 이미지 캡션에서 구동되는 실제 그래프 쌍이 필요합니다. 그러나 가변 텍스트 인코딩으로 인한 캡션 기반 감독의 불일치로 인해 검색 신뢰성이 저하됩니다. 이러한 문제를 해결하기 위해 우리는 레이블이 지정된 훈련 데이터에 대한 의존성을 제거하는 Graph Autoencoder 기반 비지도 검색 프레임워크인 SCENIR을 제시합니다. 우리 모델은 메트릭과 런타임 효율성 전반에 걸쳐 탁월한 성능을 보여 기존의 비전 기반, 다중 모드 및 감독 GNN 접근 방식을 능가합니다. 우리는 장면 그래프 유사성에 대한 결정적이고 강력한 실측 측정값으로 GED(Graph Edit Distance)를 옹호하며, 이미지 간 검색 평가에서 처음으로 일관되지 않은 캡션 기반 대안을 대체합니다. 마지막으로, 자동화된 장면 그래프 생성을 통해 주석이 없는 데이터 세트에 이를 적용하는 동시에 반사실 이미지 검색의 최첨단 기술을 발전시키는 데 크게 기여함으로써 방법의 일반화 가능성을 검증합니다."
812,http://arxiv.org/abs/2505.09118 ,Seeing Beyond the Scene: Enhancing Vision-Language Models with Interactional Reasoning,"Dayong Liang, Changmeng Zheng, Zhiyuan Wen, Yi Cai, Xiao-Yong Wei, Qing Li","기존 장면 그래프는 주로 공간 관계에 중점을 두고 시각적 장면의 복잡한 상호 작용을 추론하는 VLM(시각 언어 모델) 기능을 제한합니다. 이 논문은 두 가지 주요 과제를 다룹니다. (1) 기존 감지-구성 방법은 초점이 맞지 않고 상황에 맞지 않는 관계 세트를 생성하며 (2) 기존 접근 방식은 상호 작용 추론을 새로운 장면에 일반화하기 위한 지속적인 메모리를 형성하지 못합니다. 우리는 세 가지 보완 구성 요소를 통해 VLM의 상호 작용 추론을 향상시키는 프레임워크인 ISGR(Interaction-Augmented Scene Graph Reasoning)을 제안합니다. 첫째, 듀얼 스트림 그래프 생성자는 SAM 기반 공간 관계 추출과 상호 작용 인식 캡션을 결합하여 공간 기반을 갖춘 기능적으로 두드러진 장면 그래프를 생성합니다. 둘째, 대상 기능에 대한 VLM의 잠재 지식을 활성화하기 위해 대상 상호 작용 쿼리를 사용하여 수동 인식을 개체가 함께 작동하는 방식에 대한 능동적 추론으로 변환합니다. 마지막으로, 일시적 패턴을 장기 추론 휴리스틱으로 변환하는 특수한 상호 작용 중심 보상 기능을 갖춘 외로운 기억 강화 학습 전략을 소개합니다. 광범위한 실험을 통해 우리의 접근 방식이 상호 작용이 많은 추론 벤치마크에서 기본 방법보다 훨씬 뛰어난 성능을 발휘하며 특히 복잡한 장면 이해 작업에서 강력한 개선이 이루어졌음을 보여줍니다. 소스 코드는 https://github.com/open_upon_acceptance에서 액세스할 수 있습니다."
811,http://arxiv.org/abs/2505.07815 ,"Imagine, Verify, Execute: Memory-guided Agentic Exploration with Vision-Language Models","Seungjae Lee, Daniel Ekpo, Haowen Liu, Furong Huang, Abhinav Shrivastava, Jia-Bin Huang","탐색은 범용 로봇 학습에 필수적이며, 특히 촘촘한 보상, 명시적인 목표 또는 작업별 감독이 부족한 개방형 환경에서는 더욱 그렇습니다. 객체, 공간 관계 및 잠재적 결과에 대한 의미론적 추론을 갖춘 비전 언어 모델(VLM)은 높은 수준의 탐색적 동작을 생성하기 위한 강력한 기반을 제공합니다. 그러나 출력이 접지되지 않은 경우가 많기 때문에 상상한 전환이 물리적으로 실현 가능한지 또는 유익한지 여부를 판단하기가 어렵습니다. 상상력과 실행 사이의 격차를 해소하기 위해 인간의 호기심에서 영감을 얻은 에이전트 탐색 프레임워크인 IVE(Imagine, Verify, Execute)를 제시합니다. 인간의 탐험은 종종 새로운 장면 구성을 발견하고 환경에 대한 이해를 심화시키려는 욕구에 의해 주도됩니다. 마찬가지로 IVE는 VLM을 활용하여 RGB-D 관찰을 의미론적 장면 그래프로 추상화하고, 새로운 장면을 상상하고, 물리적 타당성을 예측하고, 액션 도구를 통해 실행 가능한 기술 시퀀스를 생성합니다. 우리는 시뮬레이션된 환경과 실제 테이블탑 환경 모두에서 IVE를 평가합니다. 결과는 방문 상태의 엔트로피가 4.1~7.8배 증가한 것으로 입증된 것처럼 IVE가 RL 기준선보다 더 다양하고 의미 있는 탐색을 가능하게 한다는 것을 보여줍니다. 더욱이 수집된 경험은 다운스트림 학습을 지원하여 인간이 수집한 시연에 대해 교육받은 성능과 거의 일치하거나 그 이상의 정책을 생성합니다."
810,http://arxiv.org/abs/2505.04058 ,LSVG: Language-Guided Scene Graphs with 2D-Assisted Multi-Modal Encoding for 3D Visual Grounding,"Feng Xiao, Hongbin Xu, Guocan Zhao, Wenxiong Kang","3D 시각적 접지는 3D 장면에서 자연어로 표현되는 고유한 대상을 국지화하는 것을 목표로 합니다. 3D와 언어 양식 사이의 상당한 차이로 인해 설명된 공간 관계를 통해 여러 유사한 개체를 구별하는 것이 주목할만한 과제입니다. 현재 방법은 참조 객체의 모델링을 무시하고 대상 중심 학습 메커니즘을 통해 복잡한 장면에서 교차 모드 이해를 달성하려고 시도합니다. 우리는 관계 인식을 향상시키기 위해 참조 객체 식별을 통해 언어 유도 장면 그래프를 구성하는 새로운 3D 시각적 접지 프레임워크를 제안합니다. 프레임워크에는 다중 모드 3D 인코딩을 향상하고 감독하기 위해 사전 훈련된 2D 의미론을 활용하는 이중 분기 시각적 인코더가 통합되어 있습니다. 또한, 우리는 교차 모달 상호 작용에서 관계 지향 정보 융합을 촉진하기 위해 그래프 주의를 사용합니다. 학습된 객체 표현과 장면 그래프 구조를 통해 3D 시각적 콘텐츠와 텍스트 설명 간의 효과적인 정렬이 가능합니다. 인기 있는 벤치마크에 대한 실험 결과는 특히 여러 유사한 방해 요소의 문제를 처리하는 데 있어서 최첨단 방법에 비해 우수한 성능을 보여줍니다."
809,http://arxiv.org/abs/2505.03581 ,DyGEnc: Encoding a Sequence of Textual Scene Graphs to Reason and Answer Questions in Dynamic Scenes,"Sergey Linok, Vadim Semenov, Anastasia Trunova, Oleg Bulichev, Dmitry Yudin","역동적인 환경에서 발생하는 사건의 분석은 인간과 상호 작용할 수 있는 지능형 에이전트 및 로봇의 개발에 근본적인 과제를 제기합니다. 현재 접근 방식은 주로 시각적 모델을 활용합니다. 그러나 이러한 방법은 해석 가능한 공간-시간 개체 표현이 부족하여 이미지에서 암시적으로 정보를 캡처하는 경우가 많습니다. 이 문제를 해결하기 위해 동적 그래프를 인코딩하는 새로운 방법인 DyGEnc를 소개합니다. 이 방법은 압축된 시공간 구조 관찰 표현과 대규모 언어 모델의 인지 기능을 통합합니다. 이 통합의 목적은 일련의 텍스트 장면 그래프를 기반으로 고급 질문 답변을 가능하게 하는 것입니다. STAR 및 AGQA 데이터 세트에 대한 확장된 평가에 따르면 DyGEnc는 인간과 사물의 상호 작용 이력에 관한 질문을 처리하는 데 있어 기존 시각적 방법보다 15~25% 더 뛰어난 성능을 나타냅니다. 또한, 제안된 방법은 바퀴 달린 조작기 플랫폼을 사용하여 수행된 로봇 실험의 결과에 의해 입증된 것처럼 명시적인 텍스트 장면 그래프를 추출하기 위한 기본 모델을 활용하여 원시 입력 이미지를 처리하도록 원활하게 확장될 수 있습니다. 우리는 이러한 발견이 장거리 추론을 위한 강력하고 압축된 그래프 기반 로봇 메모리를 구현하는 데 기여할 수 있기를 바랍니다. 코드는 github.com/linukc/DyGEnc에서 확인할 수 있습니다."
808,http://arxiv.org/abs/2505.03128 ,HCOA*: Hierarchical Class-ordered A* for Navigation in Semantic Environments,"Evangelos Psomiadis, Panagiotis Tsiotras","이 논문은 혼합된 기하학적/의미론적 3D 환경에서 로봇 탐색 문제를 다룹니다. 환경의 계층적 표현을 고려할 때 목표는 작업별 안전 제약 조건을 충족하고 계산 비용을 최소화하면서 시작 위치에서 목표까지 이동하는 것입니다. 혼합 기하학적/의미론적 그래프에서 효율적이고 안전한 경로 계획을 위해 환경의 계층 구조를 활용하는 알고리즘인 계층적 클래스 순서 A*(HCOA*)를 소개합니다. 우리는 의미 클래스에 대한 전체 순서를 사용하고 알고리즘에 대한 이론적 성능 보장을 증명합니다. 우리는 최하위 계층의 의미론을 기반으로 상위 계층 노드를 분류하는 세 가지 접근 방식인 그래프 신경망(Graph Neural Network) 방법, k-Nearest Neighbors 방법, Majority-Class 방법을 제안합니다. 우리는 두 개의 3D 장면 그래프에 대한 시뮬레이션을 통해 HCOA*를 평가하고 이를 최신 기술과 비교하고 각 분류 접근 방식의 성능을 평가합니다. 결과에 따르면 HCOA*는 탐색 계산 시간을 최대 50%까지 줄이는 동시에 다양한 시나리오에서 거의 최적의 성능을 유지하는 것으로 나타났습니다."
807,http://arxiv.org/abs/2505.03035 ,MORE: Mobile Manipulation Rearrangement Through Grounded Language Reasoning,"Mohammad Mohammadi, Daniel Honerkamp, Martin Büchner, Matteo Cassinelli, Tim Welschehold, Fabien Despinoy, Igor Gilitschenski, Abhinav Valada","자율적 장거리 모바일 조작에는 장면 역학, 미개척 영역 및 오류 복구를 포함한 다양한 과제가 포함됩니다. 최근 연구에서는 장면 수준의 로봇 추론 및 계획을 위한 기초 모델을 활용했습니다. 그러나 이러한 방법은 개체 수가 많고 대규모 환경을 처리할 때 성능이 저하됩니다. 이러한 한계를 해결하기 위해 우리는 재배열 작업에 대한 제로 샷 모바일 조작 계획을 해결하기 위해 언어 모델의 기능을 향상시키는 새로운 접근 방식인 MORE를 제안합니다. MORE는 장면 그래프를 활용하여 환경을 표현하고 인스턴스 차별화를 통합하며 객체 및 영역 인스턴스의 작업 관련 하위 그래프를 추출하는 활성 필터링 체계를 도입합니다. 이러한 단계는 제한된 계획 문제를 생성하여 환각을 효과적으로 완화하고 신뢰성을 향상시킵니다. 또한 실내 및 실외 환경 모두에 걸쳐 계획을 수립할 수 있는 몇 가지 향상된 기능을 소개합니다. 우리는 BEHAVIOR-1K 벤치마크에서 81개의 다양한 재배열 작업에 대해 MORE를 평가합니다. 이는 벤치마크의 상당 부분을 성공적으로 해결하는 첫 번째 접근 방식이 되어 최근의 기초 모델 기반 접근 방식을 능가합니다. 또한 일상적인 활동을 모방하여 여러 가지 복잡한 실제 작업에서 접근 방식의 기능을 보여줍니다. 우리는 https://more-model.cs.uni-freiburg.de에서 코드를 공개적으로 제공합니다."
806,http://arxiv.org/abs/2505.02405 ,Estimating Commonsense Scene Composition on Belief Scene Graphs,"Mario A. V. Saucedo, Vignesh Kottayam Viswanathan, Christoforos Kanellakis, George Nikolakopoulos","본 작업은 보이지 않는 사물의 공간적 분포를 추정하여 Belief Scene Graph를 확장하는 데 중점을 두고 상식적인 장면 구성의 개념을 확립합니다. 구체적으로, 상식적인 장면 구성 능력은 장면의 관련 개체 간의 공간적 관계에 대한 이해를 의미하며, 이 문서에서는 의미 개체 클래스의 가능한 모든 위치에 대한 공동 확률 분포로 모델링됩니다. 제안된 프레임워크에는 확률 분포 학습을 위한 CECI(상관 정보) 모델의 두 가지 변형이 포함되어 있습니다. (i) Graph Convolutional Network를 기반으로 한 기본 접근 방식과 (ii) LLM(Large Language Models)을 기반으로 공간 온톨로지를 통합하는 신경 기호 확장입니다. 또한 이 문서에서는 이러한 작업에 대한 데이터 세트 생성 프로세스에 대한 자세한 설명을 제공합니다. 마지막으로 프레임워크는 실제 실내 환경뿐만 아니라 시뮬레이션된 데이터에 대한 여러 실행을 통해 검증되었으며, 다양한 방 유형에 걸쳐 장면을 공간적으로 해석하는 능력을 입증했습니다."
805,http://arxiv.org/abs/2505.00831 ,"SmallPlan: Leverage Small Language Models for Sequential Path Planning with Simulation-Powered, LLM-Guided Distillation","Quang P. M. Pham, Khoi T. N. Nguyen, Nhi H. Doan, Cuong A. Pham, Qinbo Sun, Weimin Qi, Kentaro Inui, Dezhen Song",특히 대규모의 복잡한 환경 내에서 로봇 공학의 효율적인 경로 계획은 여전히 ​​중요한 장애물로 남아 있습니다. LLM(대형 언어 모델)은 강력한 추론 기능을 제공하지만 높은 계산 비용과 제한된 적응성으로 인해 에지 장치의 실시간 배포가 방해됩니다. 우리는 높은 수준의 경로 계획 작업을 위해 경량 소형 언어 모델(SLM)을 교육하기 위해 LLM을 교사 모델로 활용하는 새로운 프레임워크인 SmallPlan을 제시합니다. SmallPlan에서 SLM은 전체 크기의 3D 장면을 간결하게 표현하는 장면 그래프를 탐색할 수 있는 최적의 동작 시퀀스를 제공합니다. SLM은 LLM 기반 감독 미세 조정(SFT) 및 강화 학습(RL)을 통해 시뮬레이션 기반의 인터리브 방식으로 교육됩니다. 이 전략을 통해 SLM은 탐색 작업을 성공적으로 완료할 수 있을 뿐만 아니라 거리 이동과 같은 중요한 요소를 인식하여 보다 효율적인 경로 계획을 제공할 수 있습니다. 실험을 통해 우리는 미세 조정된 SLM이 환각 및 과적합을 겪지 않고 순차 경로 계획에서 GPT-4o와 같은 더 큰 모델과 경쟁적으로 수행된다는 것을 보여줍니다. SmallPlan은 리소스 효율적이므로 에지 장치 배포 및 실용적인 자율 로봇 공학 발전에 매우 적합합니다. 우리의 소스 코드는 https://github.com/quangpham2006/SmallPlan에서 확인할 수 있습니다.
804,http://arxiv.org/abs/2504.21360 ,ImaginateAR: AI-Assisted In-Situ Authoring in Augmented Reality,"Jaewook Lee, Filippo Aleotti, Diego Mazala, Guillermo Garcia-Hernando, Sara Vicente, Oliver James Johnston, Isabel Kraus-Liang, Jakub Powierza, Donghoon Shin, Jon E. Froehlich, Gabriel Brostow, Jessica Van Brummelen","증강 현실(AR)은 물리적 세계에 뿌리를 둔 새로운 방식의 플레이, 스토리텔링, 아이디어 탐색을 가능하게 하지만, 개인화된 AR 콘텐츠를 저작하는 것은 비전문가에게는 여전히 어렵고 전문적인 도구와 시간이 필요한 경우가 많습니다. 이전 시스템에서는 AI 기반 XR 디자인을 탐색했지만 일반적으로 수동으로 정의된 VR 환경과 고정 자산 라이브러리에 의존하여 창의적인 유연성과 실제 관련성을 제한했습니다. 오프라인 장면 이해, 빠른 3D 자산 생성 및 LLM을 결합하여 사용자가 자연어 상호 작용을 통해 야외 장면을 만들 수 있도록 하는 AI 지원 AR 저작을 위한 최초의 모바일 도구인 ImagineAR을 소개합니다. 예를 들어, ""캠프파이어를 즐기고 있는 용""(P7)이라고 말하면 시스템이 관련 자산을 생성하고 배열한 후 수동으로 다듬을 수 있습니다. 우리의 기술 평가에 따르면 우리의 맞춤형 파이프라인은 이전 방법보다 더 정확한 실외 장면 그래프를 생성하고 3D 메시를 더 빠르게 생성하는 것으로 나타났습니다. 세 부분으로 구성된 사용자 연구(N=20)에서는 AI에서 선호하는 역할, 사용자가 자유 형식으로 생성하는 방법, 미래 AR 저작 도구에 대한 설계 의미가 밝혀졌습니다. ImaginateAR은 누구나 자신의 상상을 말로 표현함으로써 어디서나 AR 경험을 만들 수 있도록 지원하는 방향으로 나아가고 있습니다."
803,http://arxiv.org/abs/2504.20091 ,VideoMultiAgents: A Multi-Agent Framework for Video Question Answering,"Noriyuki Kugo, Xiang Li, Zixin Li, Ashish Gupta, Arpandeep Khatua, Nidhish Jain, Chaitanya Patel, Yuta Kyuragi, Yasunori Ishii, Masamoto Tanabiki, Kazuki Kozuka, Ehsan Adeli","VQA(비디오 질문 응답)는 본질적으로 다중 모달 추론에 의존하여 시각적, 시간적, 언어적 단서를 통합하여 비디오 콘텐츠에 대한 더 깊은 이해를 얻습니다. 그러나 기존의 많은 방법은 프레임 수준 캡션을 단일 모델에 공급하는 데 의존하므로 시간적 및 대화형 컨텍스트를 적절하게 캡처하기가 어렵습니다. 이러한 제한 사항을 해결하기 위해 비전, 장면 그래프 분석 및 텍스트 처리를 위한 전문 에이전트를 통합하는 프레임워크인 VideoMultiAgents를 소개합니다. 독립적으로 작동하는 에이전트의 보완적인 다중 모달 추론을 활용하여 비디오 이해도를 향상시킵니다. 또한 우리의 접근 방식은 질문 기반 캡션 생성으로 보완됩니다. 이 캡션은 주어진 쿼리와 직접 관련된 개체, 작업 및 시간적 전환을 강조하는 캡션을 생성하여 답변 정확도를 향상시킵니다. 실험 결과는 우리 방법이 Intent-QA(이전 SOTA에 비해 79.0%, +6.2%), EgoSchema 하위 집합(75.4%, +3.4%) 및 NExT-QA(79.6%, +0.4%)에서 최첨단 성능을 달성한다는 것을 보여줍니다. 소스 코드는 https://github.com/PanasonicConnect/VideoMultiAgents에서 확인할 수 있습니다."
802,http://arxiv.org/abs/2504.16782 ,Graph2Nav: 3D Object-Relation Graph Generation to Robot Navigation,"Tixiao Shan, Abhinav Rajvanshi, Niluthpol Mithun, Han-Pang Chiu",현실 세계에서의 자율주행을 위한 실시간 3차원 객체관계 그래프 생성 프레임워크인 Graph2Nav를 제안합니다. 우리의 프레임워크는 실내 및 실외 장면 모두에 적용할 수 있는 3D 레이어 장면 그래프에서 3D 객체와 객체 간의 풍부한 의미론적 관계 세트를 모두 생성하고 활용합니다. 3D 의미론적 매핑 기술을 통해 최첨단 2D 파노라마 장면 그래프를 3D 세계로 활용하고 발전시켜 객체 간의 3D 의미론적 관계를 생성하는 방법을 학습합니다. 이 접근 방식은 3D 데이터에서 직접 3D 장면 그래프를 학습할 때 이전의 학습 데이터 제약을 피합니다. 우리는 3D 장면 그래프에서 3D 개체를 찾고 개체 관계에 라벨을 지정하는 정확성을 검증하기 위한 실험을 수행합니다. 또한 대규모 언어 모델을 기반으로 한 최첨단 플래너인 SayNav와의 통합을 통해 Graph2Nav가 무인 지상 로봇에서 실제 환경의 객체 검색 작업에 미치는 영향을 평가합니다. 우리의 결과는 장면 그래프에서 객체 관계를 모델링하면 이러한 탐색 작업에서 검색 효율성이 향상된다는 것을 보여줍니다.
801,http://arxiv.org/abs/2504.15785 ,WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents,"Siyu Zhou, Tianyi Zhou, Yijun Yang, Guodong Long, Deheng Ye, Jing Jiang, Chengqi Zhang","LLM(대형 언어 모델)에서 정확한 세계 모델을 구축할 수 있습니까? 월드 모델이 LLM 에이전트에게 어떤 이점을 줄 수 있나요? LLM에 대한 사전 지식과 지정된 환경의 역학 사이의 격차는 일반적으로 세계 모델로서 LLM의 성능에 병목 현상을 발생시킵니다. 격차를 해소하기 위해 우리는 LLM을 보완하는 환경의 상징적 지식을 배우는 훈련이 필요 없는 ""세계 정렬""을 제안합니다. 상징적 지식은 탐색 궤적에서 LLM에 의해 추출되고 LLM 에이전트의 정책을 규제하기 위해 실행 가능한 코드로 인코딩되는 작업 규칙, 지식 그래프 및 장면 그래프를 포함합니다. 우리는 MPC(모델 예측 제어) 프레임워크를 통해 RL이 없는 모델 기반 에이전트 ""WALL-E 2.0""을 제안합니다. 즉석에서 비용이 많이 드는 최적화가 필요한 기존 MPC와 달리 신경 기호 세계 모델과 상호 작용하여 향후 단계 작업의 효율적인 예측 최적화 프로그램으로 LLM 에이전트를 채택합니다. LLM 에이전트의 강력한 경험적 접근 방식을 통해 MPC에서 효율적인 계획을 세우는 동시에 계획된 작업의 품질도 정렬된 세계 모델의 정확한 예측을 통해 보장됩니다. 이들은 함께 새로운 환경에서 학습 효율성을 상당히 향상시킵니다. 화성(Minecraft와 같은) 및 ALFWorld(실내 환경 구현)의 오픈 월드 챌린지에서 WALL-E 2.0은 기존 방법을 크게 능가합니다. 예를 들어 화성의 기준선을 16.1%-51.6%의 성공률과 최소 61.7%의 점수로 능가합니다. ALFWorld에서는 단 4번의 반복만으로 98%의 성공률이라는 신기록을 달성했습니다."
800,http://arxiv.org/abs/2504.15049 ,ScanEdit: Hierarchically-Guided Functional 3D Scan Editing,"Mohamed el amine Boudjoghra, Ivan Laptev, Angela Dai",3D 캡처 기술의 빠른 속도와 그에 따른 풍부한 3D 데이터로 인해 효과적인 3D 장면 편집은 다양한 그래픽 응용 프로그램에 필수적입니다. 이 작업에서는 복잡한 실제 3D 스캔의 기능 편집을 위한 지침 중심 방법인 ScanEdit을 소개합니다. 크고 상호 의존적인 객체 세트를 모델링하기 위해 우리는 계층적으로 안내되는 접근 방식을 제안합니다. 객체 인스턴스로 분해된 3D 스캔이 주어지면 먼저 효과적이고 다루기 쉬운 편집이 가능하도록 계층적 장면 그래프 표현을 구성합니다. 그런 다음 LLM(대형 언어 모델)의 추론 기능을 활용하고 고급 언어 지침을 장면 그래프에 계층적으로 적용되는 실행 가능한 명령으로 변환합니다. 마지막으로 ScanEdit는 LLM 기반 지침을 명시적인 물리적 제약 조건과 통합하고 객체 배열이 물리학과 상식을 모두 준수하는 현실적인 장면을 생성합니다. 광범위한 실험 평가에서 ScanEdit은 최첨단 기술을 능가하며 다양한 실제 장면 및 입력 명령에 대해 탁월한 결과를 보여줍니다.
799,http://arxiv.org/abs/2504.14440 ,SG-Reg: Generalizable and Efficient Scene Graph Registration,"Chuhao Liu, Zhijian Qiao, Jieqi Shi, Ke Wang, Peize Liu, Shaojie Shen","이 문서에서는 자율 에이전트가 원격 에이전트 또는 이전 맵에 대해 맵을 등록해야 할 때 필수적인 기능인 두 개의 엄격한 의미론적 장면 그래프를 등록하는 문제를 다룹니다. 고전적인 의미 기반 등록의 손으로 만든 설명자 또는 학습 기반 장면 그래프 등록의 실제 주석 의존성은 실제 환경에서의 적용을 방해합니다. 문제를 해결하기 위해 우리는 의미론적 노드의 다양한 양식(개방형 의미론적 특징, 공간 인식이 있는 로컬 토폴로지, 모양 특징)을 인코딩하는 장면 그래프 네트워크를 설계합니다. 이러한 양식은 융합되어 컴팩트한 의미 노드 기능을 생성합니다. 그런 다음 일치 레이어는 대략적인 방식에서 미세한 방식으로 대응 항목을 검색합니다. 백엔드에서는 대응에 따른 변환을 결정하기 위해 강력한 포즈 추정기를 사용합니다. 우리는 희박하고 계층적인 장면 표현을 유지 관리합니다. 우리의 접근 방식은 다중 에이전트 작업에서 더 적은 GPU 리소스와 더 적은 통신 대역폭을 요구합니다. 또한, 의미론적 장면 그래프를 재구성하기 위해 비전 기반 모델과 의미론적 매핑 모듈을 사용하여 새로운 데이터 생성 접근 방식을 설계합니다. 이는 데이터를 생성하기 위해 실제 의미 주석에 의존하는 이전 작업과 크게 다릅니다. 우리는 두 에이전트 SLAM 벤치마크에서 방법을 검증합니다. 등록 성공률 측면에서 수작업 기준을 훨씬 능가합니다. 시각적 루프 폐쇄 네트워크와 비교하여 우리의 방법은 각 쿼리 프레임에 대해 52KB의 통신 대역폭만 필요로 하면서 약간 더 높은 등록 리콜을 달성합니다. 코드는 다음에서 확인할 수 있습니다: \href{http://github.com/HKUST-Aerial-Robotics/SG-Reg}{http://github.com/HKUST-Aerial-Robotics/SG-Reg}."
798,http://arxiv.org/abs/2504.13617 ,Compile Scene Graphs with Reinforcement Learning,"Zuyao Chen, Jinlin Wu, Zhen Lei, Marc Pollefeys, Chang Wen Chen","다음 토큰 예측은 LLM(대형 언어 모델) 교육의 기본 원칙이며, RL(강화 학습)은 추론 성능을 더욱 향상시킵니다. 언어, 이미지, 비디오 및 기타 형식을 모델링하는 효과적인 방법으로서 장면 그래프와 같은 구조화된 시각적 표현의 엔드투엔드 추출을 위한 LLM의 사용은 여전히 ​​과소 탐구되고 있습니다. 이를 위해서는 모델이 토큰별로 텍스트 토큰을 생성하는 대신 개체 집합과 관계 삼중항을 정확하게 생성해야 합니다. 이를 달성하기 위해 처음에는 장면 그래프 데이터세트에서 SFT(감독 미세 조정)를 통해 훈련한 후 강화 학습을 사용하여 개선하여 엔드투엔드 방식으로 장면 그래프를 생성하는 기능을 향상시키는 다중 모드 LLM(M-LLM)인 R1-SGG를 소개합니다. SFT는 기존의 신속한 응답 패러다임을 따르는 반면, RL은 효과적인 보상 신호 설계가 필요합니다. 우리는 객체 및 관계 수준에서 예측과 실제 사이의 의미론적 및 공간적 정렬을 평가하는 세 가지 회상 기반 변형(Hard Recall, Hard Recall+Relax 및 Soft Recall)을 포함하여 그래프 중심 보상 세트를 설계합니다. 형식 일관성 보상은 출력이 예상되는 구조 스키마를 따르도록 보장합니다. VG150 및 PSG 벤치마크에 대한 광범위한 실험에서 R1-SGG는 실패율을 크게 줄이고 Recall 및 Mean Recall에서 강력한 성능을 달성하여 기존 SGG 모델과 기존 다중 모드 언어 모델을 능가하는 것으로 나타났습니다. 우리 코드는 https://github.com/gpt4vision/R1-SGG에서 확인할 수 있습니다."
797,http://arxiv.org/abs/2504.12606 ,Robo-SGG: Exploiting Layout-Oriented Normalization and Restitution Can Improve Robust Scene Graph Generation,"Changsheng Lv, Zijian Fu, Mengshi Qi","본 논문에서는 강력한 장면 그래프 생성(SGG)을 위한 플러그 앤 플레이 모듈인 Robo-SGG를 제안합니다. 표준 SGG와 달리 강력한 장면 그래프 생성은 다양한 범위의 손상된 이미지에 대해 추론을 수행하는 것을 목표로 하며, 핵심 과제는 깨끗한 이미지와 손상된 이미지 간의 도메인 이동입니다. 기존 SGG 방법은 변화된 시각적 특징(예: 손상 간섭 또는 폐색)으로 인해 성능이 저하되는 문제를 겪고 있습니다. 강력한 시각적 기능을 얻기 위해 도메인 이동에 강력한 이미지의 전역 구조를 나타내는 레이아웃 정보를 활용하여 손상 시 SGG 방법의 견고성을 향상시킵니다. 특히 우리는 제안된 레이아웃 지향 복원을 통해 도메인별 변형을 완화하고 견고한 구조적 특징(즉, 개체 간의 위치 및 의미 관계)을 복구하기 위해 인스턴스 정규화(IN)를 사용합니다. 또한 손상된 이미지에서 게이팅 메커니즘을 통해 레이아웃과 시각적 기능을 적응적으로 융합하는 LEE(레이아웃 포함 인코더)를 도입하여 객체와 조건자에 대한 위치 및 의미 표현의 견고성을 향상시킵니다. 제안된 Robo-SGG 모듈은 모든 기본 SGG 모델에 쉽게 통합될 수 있는 플러그 앤 플레이 구성 요소로 설계되었습니다. 광범위한 실험을 통해 제안된 Robo-SGG에 최첨단 방법을 통합함으로써 VG-C 벤치마크에서 PredCls, SGCls 및 SGDet 작업에 대해 mR@50에서 각각 6.3%, 11.1% 및 8.0%의 상대적 개선을 달성하고 부패 장면 그래프 생성 벤치마크(VG-C 및 GQA-C)에서 새로운 최첨단 성능을 달성하는 것으로 나타났습니다. 소스 코드와 모델을 공개하겠습니다."
796,http://arxiv.org/abs/2504.12100 ,Generalized Visual Relation Detection with Diffusion Models,"Kaifeng Gao, Siqi Chen, Hanwang Zhang, Jun Xiao, Yueting Zhuang, Qianru Sun","VRD(시각적 관계 감지)는 이미지에 있는 개체 쌍 간의 관계(또는 상호 작용)를 식별하는 것을 목표로 합니다. 최근 VRD 모델은 인상적인 성능을 달성했지만 모두 사전 정의된 관계 범주로 제한되어 있으며 시각적 관계의 의미적 모호성 특성을 고려하지 않습니다. 객체와 달리 시각적 관계의 모습은 항상 미묘하며 다양한 관점에서 여러 술어로 설명될 수 있습니다. 예를 들어 '라이드'는 스포츠 관점과 공간 위치 관점에서 각각 '레이스'와 '앉아'로 묘사될 수 있습니다. 이를 위해 우리는 시각적 관계를 연속 임베딩으로 모델링하고 Diff-VRD라고 하는 조건부 생성 방식으로 일반화된 VRD를 달성하기 위한 확산 모델을 설계하는 것을 제안합니다. 우리는 잠재 공간에서 확산 과정을 모델링하고 임베딩 시퀀스로 이미지에서 가능한 모든 관계를 생성합니다. 생성 중에 주체-객체 쌍의 시각적 및 텍스트 임베딩은 조건부 신호 역할을 하며 교차 주의를 통해 주입됩니다. 생성 후에는 의미론적 유사성을 고려하여 주체-객체 쌍에 관계 단어를 할당하는 후속 매칭 단계를 설계합니다. 확산 기반 생성 프로세스의 이점을 활용하는 Diff-VRD는 사전 정의된 데이터세트 카테고리 라벨을 넘어서는 시각적 관계를 생성할 수 있습니다. 이 일반화된 VRD 작업을 올바르게 평가하기 위해 두 가지 평가 지표, 즉 텍스트-이미지 검색과 이미지 캡션에서 영감을 얻은 SPICE PR 곡선을 소개합니다. 인간-객체 상호 작용(HOI) 감지와 장면 그래프 생성(SGG) 벤치마크에 대한 광범위한 실험은 Diff-VRD의 우수성과 효율성을 입증합니다."
795,http://arxiv.org/abs/2504.10798 ,AdapCsiNet: Environment-Adaptive CSI Feedback via Scene Graph-Aided Deep Learning,"Jiayi Liu, Jiajia Guo, Yiming Cui, Chao-Kai Wen, Shi Jin","다중 안테나 무선 통신 시스템의 잠재력을 최대한 활용하려면 정확한 채널 상태 정보(CSI)가 중요합니다. 딥 러닝(DL) 기반 CSI 피드백 방법은 피드백 오버헤드를 줄이는 데 유망한 것으로 나타났지만, 데이터 기반 특성으로 인해 다양한 전파 환경에서의 일반화 기능은 여전히 ​​제한적입니다. 온라인 교육을 기반으로 하는 기존 솔루션은 적응성을 향상시키지만 데이터 수집 및 계산 리소스 측면에서 상당한 오버헤드를 초래합니다. 본 연구에서는 온라인 교육이 필요 없는 환경 적응형 DL 기반 CSI 피드백 프레임워크인 AdapCsiNet을 제안합니다. 장면 그래프로 표시되는 환경 정보를 하이퍼네트워크 기반 CSI 재구성 프로세스에 통합함으로써 AdapCsiNet은 다양한 채널 조건에 동적으로 적응합니다. 기본 재구성 성능과 효과적인 환경 인식 적응을 보장하기 위해 2단계 훈련 전략이 도입되었습니다. 시뮬레이션 결과는 AdapCsiNet이 CSI 재구성 정확도에서 최대 46.4% 향상을 달성하고 추가적인 런타임 오버헤드 없이 온라인 학습 방법의 성능과 일치함을 보여줍니다."
794,http://arxiv.org/abs/2504.09587 ,GeoNav: Empowering MLLMs with Explicit Geospatial Reasoning Abilities for Language-Goal Aerial Navigation,"Haotian Xu, Yue Hu, Chen Gao, Zhengqiu Zhu, Yong Zhao, Yong Li, Quanjun Yin","언어 목표 항공 항법은 UAV가 텍스트 사양을 기반으로 도시 블록과 같은 복잡한 환경에서 목표물을 위치화해야 하는 구현된 AI의 중요한 과제입니다. 종종 실내 탐색에서 채택된 기존 방법은 제한된 시야, 객체 간의 의미 모호성 및 구조화된 공간 추론 부족으로 인해 확장하는 데 어려움을 겪습니다. 본 연구에서는 장거리 탐색을 가능하게 하는 지리공간 인식 다중 모드 에이전트인 GeoNav를 제안합니다. GeoNav는 랜드마크 탐색, 대상 검색, 인간의 대략적인 공간 전략을 모방한 정밀한 위치 파악 등 3단계로 작동합니다. 이러한 추론을 지원하기 위해 두 가지 유형의 공간 메모리를 동적으로 구축합니다. 첫 번째는 랜드마크 지역으로의 빠른 탐색을 위해 사전 텍스트 지리 지식과 구체화된 시각적 단서를 하향식 주석 형식으로 융합하는 전역적이지만 도식적인 인지 지도입니다. 두 번째는 블록, 랜드마크 및 개체 간의 계층적 공간 관계를 나타내는 로컬이지만 섬세한 장면 그래프로, 이는 명확한 대상 위치 파악에 사용됩니다. 이러한 구조적 표현 외에도 GeoNav는 공간적으로 인식되는 다중 모드 사고 연쇄 프롬프트 메커니즘을 사용하여 여러 단계에 걸쳐 효율적이고 해석 가능한 의사 결정을 통해 다중 모드 대형 언어 모델을 활성화합니다. CityNav 도시 내비게이션 벤치마크에서 GeoNav는 현재의 최첨단 성공률을 최대 12.53%까지 능가하고 어려운 수준의 작업에서도 내비게이션 효율성을 크게 향상시킵니다. 절제 연구는 각 모듈의 중요성을 강조하며 지리 공간적 표현과 대략적인 추론이 UAV 탐색을 어떻게 향상시키는지 보여줍니다."
793,http://arxiv.org/abs/2504.08307 ,DSM: Constructing a Diverse Semantic Map for 3D Visual Grounding,"Qinghongbing Xie, Zijian Liang, Fuhao Li, Long Zeng","효과적인 장면 표현은 표현의 시각적 접지 능력에 매우 중요하지만 3D 시각적 접지를 위한 기존 방법은 종종 제한됩니다. 기하학적이고 시각적인 단서에만 초점을 맞추거나 전통적인 3D 장면 그래프처럼 복잡한 추론에 필요한 다차원 속성이 부족합니다. 이러한 격차를 해소하기 위해 우리는 외관, 물리적 특성 및 어포던스를 포함한 VLM 파생 의미의 스펙트럼으로 강력한 기하학적 모델을 풍부하게 하는 새로운 장면 표현 프레임워크인 DSM(다양한 의미 맵) 프레임워크를 소개합니다. DSM은 먼저 시간적 슬라이딩 윈도우 내에서 다중 뷰 관찰을 융합하여 지속적이고 포괄적인 세계 모델을 생성함으로써 온라인으로 구성됩니다. 이러한 기반을 바탕으로 우리는 기반을 자유 형식 VLM 쿼리에서 의미가 풍부한 맵을 통해 구조화된 추론 프로세스로 전환하여 정확도와 해석 가능성을 크게 향상시키는 새로운 패러다임인 DSM-Grounding을 제안합니다. 광범위한 평가를 통해 우리 접근 방식의 우수성이 입증되었습니다. ScanRefer 벤치마크에서 DSM-Grounding은 IoU@0.5의 최첨단 59.06% 전체 정확도를 달성하여 다른 정확도를 10% 능가합니다. 의미론적 세분화에서 DSM은 67.93% F-mIoU를 달성하여 권한 있는 기준을 포함한 모든 기준을 능가합니다. 또한 복잡한 탐색 및 파악 작업을 위한 물리적 로봇에 대한 성공적인 배포는 실제 시나리오에서 프레임워크의 실용적인 유용성을 확인합니다."
792,http://arxiv.org/abs/2504.07867 ,SAMJAM: Zero-Shot Video Scene Graph Generation for Egocentric Kitchen Videos,"Joshua Li, Fernando Jose Pena Cantu, Emily Yu, Alexander Wong, Yuchen Cui, Yuhao Chen",비디오 장면 그래프 생성(VidSGG)은 역동적인 주방 환경을 이해하는 데 중요한 주제입니다. VidSGG의 현재 모델은 장면 그래프를 생성하기 위해 광범위한 교육이 필요합니다. 최근 VLM(Vision Language Model) 및 VFM(Vision Foundation Model)은 다양한 작업에서 인상적인 제로샷 기능을 보여주었습니다. 그러나 Gemini와 같은 VLM은 VidSGG의 역학 문제로 인해 프레임 전체에서 안정적인 개체 ID를 유지하지 못합니다. 이러한 한계를 극복하기 위해 우리는 SAM2의 시간 추적과 Gemini의 의미 이해를 결합한 제로샷 파이프라인인 SAMJAM을 제안합니다. SAM2는 또한 보다 정확한 경계 상자를 생성하여 Gemini의 객체 접지를 개선합니다. 우리의 방법에서는 먼저 Gemini에게 프레임 수준 장면 그래프를 생성하도록 요청합니다. 그런 다음 일치 알고리즘을 사용하여 장면 그래프의 각 개체를 SAM2 생성 또는 SAM2 전파 마스크로 매핑하여 동적 환경에서 시간적으로 일관된 장면 그래프를 생성합니다. 마지막으로 다음 각 프레임에서 이 프로세스를 다시 반복합니다. 우리는 SAMJAM이 EPIC-KITCHENS 및 EPIC-KITCHENS-100 데이터세트의 평균 재현율에서 Gemini보다 8.33% 더 뛰어난 성능을 보인다는 것을 경험적으로 보여줍니다.
791,http://arxiv.org/abs/2504.06661 ,Domain-Conditioned Scene Graphs for State-Grounded Task Planning,"Jonas Herzog, Jiangpin Liu, Yue Wang",최근 로봇 작업 계획 프레임워크에는 GPT-4o와 같은 대규모 다중 모드 모델(LMM)이 통합되어 있습니다. 이러한 모델의 접지 문제를 해결하기 위해 파이프라인을 인식 상태 접지와 후속 상태 기반 계획으로 분할하는 것이 제안되었습니다. 이 작업에서 볼 수 있듯이 LMM 기반 접근 방식의 상태 접지 기능은 세부적이고 구조화된 도메인별 장면 이해의 약점으로 인해 여전히 제한됩니다. 이러한 단점을 해결하기 위해 우리는 장면 표현으로 도메인 조건 장면 그래프를 특징으로 하는 보다 구조화된 상태 접지 프레임워크를 개발합니다. 우리는 그러한 표현이 PDDL(Planning Domain Definition Language)과 같은 계획 언어의 상징적 상태에 직접 매핑될 수 있으므로 실제로 실행 가능하다는 것을 보여줍니다. 우리는 도메인 관련 객체 감지 위에 도메인 특정 조건자를 분류하는 경량 비전 언어 접근 방식을 사용하여 도메인 조건 장면 그래프 생성이 구현되는 상태 접지 프레임워크의 인스턴스화를 제공합니다. 세 가지 영역에 걸쳐 평가된 우리의 접근 방식은 LMM 기반 접근 방식에 비해 훨씬 더 높은 상태 반올림 정확도와 작업 계획 성공률을 달성합니다.
790,http://arxiv.org/abs/2504.06553 ,ASHiTA: Automatic Scene-grounded HIerarchical Task Analysis,"Yun Chang, Leonor Fermoselle, Duy Ta, Bernadette Bucher, Luca Carlone, Jiuguang Wang","장면 재구성 및 이해에 대한 최근 작업이 자연어를 물리적 3D 환경에 접지하는 데 진전을 이루었지만 추상적이고 높은 수준의 지침을 3D 장면에 접지하는 것은 여전히 ​​​​어려운 일입니다. 높은 수준의 명령은 장면에서 의미 체계 요소를 명시적으로 호출하지 않을 수 있으며, 높은 수준의 작업을 보다 구체적인 하위 작업 집합으로 나누는 프로세스(계층적 작업 분석이라는 프로세스)도 환경에 따라 다릅니다. 본 연구에서는 상위 수준 작업을 기반 하위 작업으로 분할하여 3D 장면 그래프 기반 작업 계층 구조를 생성하는 최초의 프레임워크인 ASHiTA를 제안합니다. ASHiTA는 LLM 지원 계층적 작업 분석을 대체하여 작업 분석을 생성하고 작업 중심 3D 장면 그래프 구성을 통해 환경에 대한 적절한 표현을 생성합니다. 우리의 실험에 따르면 ASHiTA는 높은 수준의 작업을 환경에 따른 하위 작업으로 분류하는 데 있어 LLM 기준보다 훨씬 더 나은 성능을 발휘하며 추가로 최첨단 방법에 필적하는 기반 성능을 달성할 수 있습니다."
789,http://arxiv.org/abs/2504.05463 ,REVEAL: Relation-based Video Representation Learning for Video-Question-Answering,"Sofian Chaybouti, Walid Bousselham, Moritz Wolter, Hilde Kuehne","Video-Question-Answering(VideoQA)은 시간이 지남에 따라 복잡한 시각적 관계 변화를 캡처하는 것으로 구성되며, 고급 비디오 언어 모델(VLM)에서도 문제로 남아 있습니다. 즉, 해당 모델에 대한 합리적인 크기의 입력으로 시각적 콘텐츠를 표현해야 하기 때문입니다. 이 문제를 해결하기 위해 우리는 시각적 관계 정보를 구조화되고 분해된 표현으로 인코딩하여 캡처하도록 설계된 프레임워크인 REVEAL(Relation-based Video rEpresentAtion Learning)을 제안합니다. 특히 시공간 장면 그래프에서 영감을 받아 언어 임베딩을 통해 시간이 지남에 따라 (\textit{subject-predicate-object}) 형식의 관계 삼중항 세트로 비디오 시퀀스를 인코딩할 것을 제안합니다. 이를 위해 비디오 캡션에서 명시적인 관계를 추출하고 Q-Former 아키텍처와 함께 MM-NCE(다대다 노이즈 대비 추정)를 도입하여 순서가 지정되지 않은 비디오 파생 쿼리 세트를 해당 텍스트 기반 관계 설명과 정렬합니다. 추론 시 결과 Q-former는 VideoQA용 VLM에 대한 입력 역할을 할 수 있는 효율적인 토큰 표현을 생성합니다.   우리는 NeXT-QA, Intent-QA, STAR, VLEP 및 TVQA의 5가지 까다로운 벤치마크에서 제안된 프레임워크를 평가합니다. 이는 결과 쿼리 기반 비디오 표현이 전역 정렬 기반 CLS 또는 패치 토큰 표현보다 성능이 뛰어나고 특히 시간적 추론 및 관계 이해가 필요한 작업에서 최첨단 모델에 비해 경쟁력 있는 결과를 달성할 수 있음을 보여줍니다. 코드와 모델은 공개적으로 공개됩니다."
788,http://arxiv.org/abs/2504.03164 ,NuScenes-SpatialQA: A Spatial Understanding and Reasoning Benchmark for Vision-Language Models in Autonomous Driving,"Kexin Tian, Jingrui Mao, Yunlong Zhang, Jiwan Jiang, Yang Zhou, Zhengzhong Tu","VLM(Vision-Language Model)의 최근 발전은 자율 주행 작업에 대한 강력한 잠재력을 보여주었습니다. 그러나 자율주행의 핵심인 공간 이해와 추론 능력에는 여전히 상당한 한계가 존재합니다. 특히, 기존 벤치마크 중 어느 것도 운전 시나리오에서 VLM의 공간 추론 기능을 체계적으로 평가하지 않습니다. 이러한 격차를 메우기 위해 우리는 자율 주행에서 VLM의 공간 이해 및 추론 기능을 평가하기 위해 특별히 설계된 최초의 대규모 실측 기반 QA(질문 답변) 벤치마크인 NuScenes-SpatialQA를 제안합니다. NuScenes 데이터세트를 기반으로 구축된 벤치마크는 자동화된 3D 장면 그래프 생성 파이프라인과 QA 생성 파이프라인을 통해 구성됩니다. 벤치마크는 공간 이해와 다차원 추론 모두에서 VLM의 성능을 체계적으로 평가합니다. 이 벤치마크를 사용하여 우리는 일반 및 공간 강화 모델을 포함한 다양한 VLM에 대한 광범위한 실험을 수행하여 자율 주행에서 공간 기능에 대한 최초의 포괄적인 평가를 제공합니다. 놀랍게도 실험 결과는 공간 강화 VLM이 정성적 QA에서는 성능이 우수하지만 정량적 QA에서는 경쟁력을 보여주지 못하는 것으로 나타났습니다. 일반적으로 VLM은 공간 이해 및 추론에 있어 여전히 상당한 어려움에 직면해 있습니다."
787,http://arxiv.org/abs/2504.01089 ,HomeEmergency -- Using Audio to Find and Respond to Emergencies in the Home,"James F. Mullen, Dhruva Kumar, Xuewei Qi, Rajasimman Madhivanan, Arnie Sen, Dinesh Manocha, Richard Kim","미국에서만 가정사고로 인한 사망이 연간 128,000건을 초과합니다. 우리의 작업은 가정 내 긴급 상황에 대응하여 부상과 사망을 예방하는 가정용 로봇을 구현하는 것을 목표로 합니다. ThreeDWorld 시뮬레이터를 기반으로 한 새로운 가정 비상 데이터 세트를 소개합니다. 데이터 세트의 각 시나리오는 긴급 상황일 수도 있고 아닐 수도 있는 즉각적이거나 주기적인 소리로 시작됩니다. 에이전트는 응급 상황이 있는지 확인하기 위해 시뮬레이터의 오디오 신호 및 이미지와 함께 사전 관찰을 사용하여 멀티룸 홈 장면을 탐색해야 합니다.   새로운 데이터 세트 외에도 잠재적인 가정 비상 상황을 파악하고 식별하기 위한 모듈식 접근 방식을 제시합니다. 우리의 접근 방식을 뒷받침하는 것은 새로운 확률적 동적 장면 그래프(P-DSG)입니다. 여기서 핵심 통찰력은 에이전트에 해당하는 그래프 노드가 확률적 우위로 표현될 수 있다는 것입니다. 베이지안 추론을 사용하여 개선된 이 에지는 장면에서 에이전트의 효율적이고 효과적인 위치 파악을 가능하게 합니다. 또한 다중 모드 비전 언어 모델(VLM)을 접근 방식의 구성 요소로 활용하여 객체 특성(예: 인화성)을 결정하고 긴급 상황을 식별합니다. 우리는 소비자 로봇에서 실제 버전의 작업을 완료하는 방법을 시연하여 작업과 방법의 이전 가능성을 보여줍니다. 우리의 데이터 세트는 이 논문이 출판되면 대중에게 공개될 것입니다."
786,http://arxiv.org/abs/2504.00844 ,PRISM-0: A Predicate-Rich Scene Graph Generation Framework for Zero-Shot Open-Vocabulary Tasks,"Abdelrahman Elskhawy, Mengze Li, Nassir Navab, Benjamin Busam","SGG(장면 그래프 생성)에서는 시각적 입력에서 객체 노드 및 연결 조건으로 구조화된 표현이 추출되어 다양한 다운스트림 작업에 대한 이미지 기반 추론이 가능합니다. 완전히 감독되는 SGG는 꾸준히 개선되었지만 제한된 선별 데이터와 롱테일 조건자 분포로 인해 훈련 ​​편향으로 인해 조건자 다양성이 떨어지고 다운스트림 성능이 저하되었습니다. 우리는 광범위한 술어를 포착하기 위해 상향식 파이프라인에서 기초 모델을 활용하는 제로 샷 개방형 어휘 SGG 프레임워크인 PRISM-0을 제시합니다. 감지된 객체 쌍은 필터링되고 VLM(Vision-Language Model)을 통해 설명되며 LLM(Large Language Model)으로 처리되어 세밀하고 대략적인 조건자를 생성한 다음 VQA(Visual Question Answering) 모델로 검증됩니다. PRISM-0 모듈식, 데이터세트 독립적 설계는 Visual Genome과 같은 기존 SGG 데이터세트를 강화하고 다양하고 편견 없는 그래프를 생성합니다. 완전히 제로샷 설정에서 작동하는 동안 PRISM-0은 SGG 벤치마크에서 최첨단 약한 지도 모델과 심지어 문장-그래프 검색과 같은 작업에서 최첨단 지도 방법과 동등한 성능을 달성합니다."
785,http://arxiv.org/abs/2504.00775 ,Visual Environment-Interactive Planning for Embodied Complex-Question Answering,"Ning Lan, Baoshan Ou, Xuemei Xie, Guangming Shi","본 연구는 구체화된 복합 질문 응답 작업에 중점을 두고 있습니다. 이는 구체화된 로봇이 복잡한 구조와 추상적인 의미론을 통해 인간의 질문을 이해해야 함을 의미합니다. 이 작업의 핵심은 시각적 환경에 대한 인식을 바탕으로 적절한 계획을 세우는 것입니다. 기존 방법은 일회성 방식, 즉 1단계 계획으로 계획을 생성하는 경우가 많습니다. 이러한 접근 방식은 환경에 대한 충분한 이해 없이 대규모 모델에 의존합니다. 본 논문에서는 다단계 계획을 고려하여 순차적으로 계획을 수립하는 프레임워크를 제안합니다. 복잡한 질문을 해결할 수 있는 프레임워크의 능력을 보장하기 위해 우리는 질문 본질에 대한 계층적 시각적 인식과 연쇄 표현이 반복적인 상호 작용을 달성할 수 있는 구조화된 의미 공간을 만듭니다. 이 공간을 통해 순차적인 작업 계획이 가능해집니다. 프레임워크 내에서 먼저 시각적 계층적 장면 그래프를 기반으로 인간의 자연어를 분석하여 질문의 의도를 명확히 할 수 있습니다. 그런 다음 현재 단계에 대한 계획을 세우기 위해 외부 규칙을 통합하여 대형 모델에 대한 의존도를 약화시킵니다. 모든 계획은 시각적 인식의 피드백을 기반으로 생성되며 답변을 얻을 때까지 여러 차례의 상호 작용을 통해 생성됩니다. 이 접근 방식을 통해 지속적인 피드백과 조정이 가능해 로봇이 행동 전략을 최적화할 수 있습니다. 프레임워크를 테스트하기 위해 우리는 더 복잡한 질문이 포함된 새로운 데이터세트를 제공합니다. 실험 결과는 우리의 접근 방식이 복잡한 작업에서 훌륭하고 안정적으로 수행된다는 것을 보여줍니다. 또한 실제 시나리오에서 우리 접근 방식의 타당성이 확립되어 실제 적용 가능성이 있음을 나타냅니다."
784,http://arxiv.org/abs/2503.23679 ,The Devil is in the Distributions: Explicit Modeling of Scene Content is Key in Zero-Shot Video Captioning,"Mingkai Tian, Guorong Li, Yuankai Qi, Amin Beheshti, Javen Qinfeng Shi, Anton van den Hengel, Qingming Huang","제로샷 비디오 캡션을 사용하려면 모델이 훈련을 위해 사람이 주석을 추가한 비디오-텍스트 쌍 없이 고품질 캡션을 생성해야 합니다. 문제에 대한 최첨단 접근 방식은 CLIP을 활용하여 시각적 관련 텍스트 프롬프트를 추출하여 캡션 생성 시 언어 모델을 안내합니다. 이러한 방법은 장면의 한 가지 주요 측면에 초점을 맞추고 나머지 시각적 입력을 무시하는 캡션을 작성하는 경향이 있습니다. 이 문제를 해결하고 보다 정확하고 완전한 캡션을 생성하기 위해 우리는 제로샷 비디오 캡션을 위한 새롭고 진보적인 다중 세분성 텍스트 프롬프트 전략을 제안합니다. 우리의 접근 방식은 명사구, 명사구의 장면 그래프 및 전체 문장을 포함하는 세 가지 별개의 메모리 뱅크를 구성합니다. 또한 문제의 특정 주제를 둘러싼 자연어 분포를 모델링하는 카테고리 인식 검색 메커니즘을 소개합니다. 광범위한 실험을 통해 기존 최첨단 기술과 비교하여 MSR-VTT, MSVD 및 VATEX 벤치마크의 주요 지표 CIDEr 측면에서 우리 방법의 효율성이 5.7%, 16.2%, 3.4% 향상된 것으로 나타났습니다."
783,http://arxiv.org/abs/2503.19199 ,Open-Vocabulary Functional 3D Scene Graphs for Real-World Indoor Spaces,"Chenyangguang Zhang, Alexandros Delitzas, Fangjinhua Wang, Ruida Zhang, Xiangyang Ji, Marc Pollefeys, Francis Engelmann","포즈를 취한 RGB-D 이미지로부터 실제 실내 환경에 대한 기능적 3D 장면 그래프를 예측하는 작업을 소개합니다. 객체의 공간적 관계에 초점을 맞춘 기존 3D 장면 그래프와 달리 기능적 3D 장면 그래프는 객체, 대화형 요소 및 기능적 관계를 캡처합니다. 훈련 데이터가 부족하기 때문에 VLM(시각 언어 모델) 및 LLM(대형 언어 모델)을 포함한 기초 모델을 활용하여 기능적 지식을 인코딩합니다. 우리는 기능적인 3D 장면 그래프로 주석이 달린 확장된 SceneFun3D 데이터세트와 새로 수집된 데이터세트인 FunGraph3D에 대한 접근 방식을 평가합니다. 우리의 방법은 Open3DSG 및 ConceptGraph를 포함한 적응된 기준선보다 훨씬 뛰어난 성능을 발휘하여 복잡한 장면 기능을 모델링하는 데 있어 효율성을 입증합니다. 또한 기능적인 3D 장면 그래프를 사용한 3D 질문 응답 및 로봇 조작과 같은 다운스트림 애플리케이션을 시연합니다. https://openfungraph.github.io에서 프로젝트 페이지를 참조하세요."
782,http://arxiv.org/abs/2503.18988 ,SG-Tailor: Inter-Object Commonsense Relationship Reasoning for Scene Graph Manipulation,"Haoliang Shang, Hanyu Wu, Guangyao Zhai, Boyang Sun, Fangjinhua Wang, Federico Tombari, Marc Pollefeys",장면 그래프는 객체 간의 복잡한 관계를 포착하여 콘텐츠 생성 및 조작을 위한 강력한 사전 역할을 합니다. 그러나 노드를 추가하든 가장자리를 수정하든 장면 그래프를 합리적으로 조작하는 것은 여전히 ​​어렵고 손대지 않은 작업입니다. 그래프에 노드를 추가하거나 노드와 다른 모든 노드의 관계를 추론하는 등의 작업은 계산상 다루기 어렵습니다. 단 하나의 에지 수정이라도 그래프 내의 복잡한 상호 의존성으로 인해 충돌을 유발할 수 있기 때문입니다. 이러한 문제를 해결하기 위해 두 노드 간의 충돌 없는 관계를 예측하는 자동 회귀 모델인 SG-Tailor를 소개합니다. SG-Tailor는 새로 추가된 노드에 대한 상식적인 에지 생성을 포함하여 개체 간 관계를 추론할 뿐만 아니라 에지 수정으로 인해 발생하는 충돌을 해결하여 다운스트림 작업에 대한 일관되고 조작된 그래프를 생성합니다. 노드 추가의 경우 모델은 그래프에서 대상 노드와 다른 노드를 쿼리하여 적절한 관계를 예측합니다. 가장자리 수정의 경우 SG-Tailor은 Cut-And-Stitch 전략을 사용하여 충돌을 해결하고 그래프를 전체적으로 조정합니다. 광범위한 실험을 통해 SG-Tailor는 경쟁 방법보다 훨씬 뛰어난 성능을 발휘하며 장면 생성 및 로봇 조작 작업을 위한 플러그인 모듈로 원활하게 통합될 수 있음을 보여줍니다.
781,http://arxiv.org/abs/2503.17862 ,A Causal Adjustment Module for Debiasing Scene Graph Generation,"Li Liu, Shuzhou Sun, Shuaifeng Zhi, Fan Shi, Zhen Liu, Janne Heikkilä, Yongxiang Liu","최근 SGG(장면 그래프 생성)에 대한 편향성 제거 방법이 인상적인 성능을 보여주었지만, 이러한 노력은 종종 모델 편향을 관계의 롱테일 분포에만 돌리고, 편향된 개체 및 개체 쌍 분포에서 비롯되는 더 깊은 원인을 간과하는 경우가 많습니다. 본 논문에서는 관찰된 편향된 분포 사이의 인과관계를 모델링하기 위해 인과 추론 기술을 사용합니다. 우리의 통찰력은 복잡한 분포 사이에서 관찰할 수 없는 인과 효과를 포착하는 인과 추론 능력에 있으며, 이는 모델 편향의 근원을 추적하는 데 중요합니다. 구체적으로, 객체, 객체 쌍, 관계 간의 인과관계를 모델링하는 것 외에도 인과성을 보완하기 위해 매개변인, 즉 동시 발생 분포를 통합하는 MCCM(Mediator-based Causal Chain Model)을 소개합니다. 이에 따라 우리는 MCCM의 변수를 입력으로 사용하여 편향된 모델 예측을 수정하기 위한 일련의 조정 요인을 생성하여 모델링된 인과 구조를 추정하는 인과 조정 모듈(CAModule)을 제안합니다. 또한, 우리의 방법은 제로샷 관계의 구성을 가능하게 하여 그러한 관계를 인식하는 모델의 능력을 향상시킵니다. 다양한 SGG 백본과 인기 있는 벤치마크에서 수행된 실험에서는 CAModule이 최첨단 평균 재현율을 달성했으며 까다로운 제로샷 재현율 측정 항목에서도 상당한 개선이 관찰되었음을 보여줍니다."
780,http://arxiv.org/abs/2503.17224 ,Neuro-Symbolic Scene Graph Conditioning for Synthetic Image Dataset Generation,"Giacomo Savazzi, Eugenio Lomurno, Cristian Sbrolli, Agnese Chiatti, Matteo Matteucci","기계 학습 모델의 규모와 복잡성이 증가함에 따라 충분한 교육 데이터를 얻는 것이 취득 비용, 개인 정보 보호 제약 및 전문 도메인의 데이터 부족으로 인해 심각한 병목 현상이 되었습니다. 합성 데이터 생성이 유망한 대안으로 떠올랐지만, 특히 작업 복잡성이 증가함에 따라 실제 데이터로 훈련된 모델에 비해 눈에 띄는 성능 격차가 남아 있습니다. 동시에, 신경망의 학습 강점과 상징적 추론의 구조화된 표현을 결합한 신경 상징적 방법은 다양한 인지 작업 전반에 걸쳐 상당한 잠재력을 보여주었습니다. 이 문서에서는 특히 장면 그래프 생성 모델의 성능 향상에 중점을 두고 합성 이미지 데이터 세트 생성을 위한 신경 기호 조건화의 유용성을 살펴봅니다. 이 연구에서는 장면 그래프 형태의 구조화된 기호 표현이 관계 제약 조건의 명시적인 인코딩을 통해 합성 데이터 품질을 향상시킬 수 있는지 여부를 조사합니다. 결과는 Neuro-Symbolic 컨디셔닝이 데이터 세트 확대에 사용될 때 표준 Recall 메트릭에서 최대 +2.59%, No Graph Constraint Recall 메트릭에서 최대 +2.83%의 상당한 개선을 가져온다는 것을 보여줍니다. 이러한 연구 결과는 Neuro-Symbolic 접근 방식과 생성 접근 방식을 병합하면 실제 데이터와 결합할 때 모델 성능을 향상시키는 보완적인 구조 정보가 포함된 합성 데이터를 생성하여 복잡한 시각적 추론 작업에서도 데이터 부족 한계를 극복할 수 있는 새로운 접근 방식을 제공한다는 사실을 입증했습니다."
779,http://arxiv.org/abs/2503.16867 ,ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question Generation and Answering,"Kaisi Guan, Zhengfeng Lai, Yuchong Sun, Peng Zhang, Wei Liu, Kieran Liu, Meng Cao, Ruihua Song","텍스트 프롬프트와 생성된 비디오 간의 의미 체계 정렬을 정확하게 평가하는 것은 T2V(텍스트-비디오) 생성에서 여전히 어려운 과제입니다. CLIPScore와 같은 기존 텍스트-비디오 정렬 측정항목은 세밀한 정렬 세부 정보 없이 대략적인 점수만 생성하므로 사람의 선호도에 맞춰 정렬하지 못합니다. 이러한 한계를 해결하기 위해 우리는 세밀한 질문 생성 및 답변을 통한 텍스트-비디오 정렬의 새로운 평가 방법인 ETVA를 제안합니다. 첫째, 다중 에이전트 시스템은 프롬프트를 의미론적 장면 그래프로 구문 분석하여 원자적 질문을 생성합니다. 그런 다음 질문 답변을 위한 지식 증강 다단계 추론 프레임워크를 설계합니다. 여기서 보조 LLM은 먼저 관련 상식 지식(예: 물리 법칙)을 검색한 다음 비디오 LLM이 다단계 추론 메커니즘을 통해 생성된 질문에 답변합니다. 광범위한 실험에 따르면 ETVA는 Spearman의 상관 계수인 58.47을 달성했으며, 이는 31.0에 불과한 기존 지표보다 인간 판단과 훨씬 더 높은 상관 관계를 보여줍니다. 또한 우리는 10개 범주에 걸쳐 2,000개의 다양한 프롬프트와 12,000개의 원자 질문을 특징으로 하는 텍스트-비디오 정렬 평가를 위해 특별히 설계된 포괄적인 벤치마크를 구축합니다. 기존 15개 텍스트-비디오 모델에 대한 체계적인 평가를 통해 주요 기능과 한계를 식별하여 차세대 T2V 세대의 기반을 마련합니다."
778,http://arxiv.org/abs/2503.17406 ,IRef-VLA: A Benchmark for Interactive Referential Grounding with Imperfect Language in 3D Scenes,"Haochen Zhang, Nader Zantout, Pujith Kachana, Ji Zhang, Wenshan Wang","최근 대규모 언어 모델, 비전 언어 모델 및 기타 일반 기반 모델이 등장하면서 자연어 입력이 주어지는 다양한 환경에서 작동할 수 있는 다중 모드, 다중 작업 로봇 공학의 잠재력이 커지고 있습니다. 이러한 응용 프로그램 중 하나는 자연어 지침을 사용하는 실내 탐색입니다. 그러나 최근의 진전에도 불구하고 이 문제는 3차원 공간 추론과 의미론적 이해가 필요하기 때문에 여전히 어려운 문제입니다. 또한 사용된 언어가 불완전하거나 장면과 잘못 정렬되어 작업이 더욱 복잡해질 수 있습니다. 이 문제를 해결하기 위해 우리는 불완전한 참조가 있는 3D 장면의 대화형 참조 비전 및 언어 기반 동작을 위한 벤치마크 데이터 세트 IRef-VLA를 선별합니다. IRef-VLA는 참조 접지 작업을 위한 최대 규모의 실제 데이터 세트로, 기존 데이터 세트에서 11.5K개 이상의 스캔된 3D 공간, 경험적으로 생성된 의미 관계 760만 개, 참조 문 470만 개로 구성됩니다. 우리의 데이터 세트에는 의미 개체 및 방 주석, 장면 그래프, 탐색 가능한 여유 공간 주석도 포함되어 있으며 언어에 불완전하거나 모호한 부분이 있는 설명으로 보강됩니다. 우리는 성능 기준을 얻기 위해 최첨단 모델을 평가하여 데이터 세트의 일반화 가능성을 검증하고 장면 그래프 지식을 사용하여 성능 한계와 대안 생성을 입증하기 위한 그래프 검색 기준을 개발합니다. 이 벤치마크를 통해 우리는 강력한 대화형 탐색 시스템 개발에 도움이 되는 3D 장면 이해를 위한 리소스를 제공하는 것을 목표로 합니다. 데이터세트와 모든 소스 코드는 https://github.com/HaochenZ11/IRef-VLA에 공개되어 있습니다."
777,http://arxiv.org/abs/2503.15846 ,What can Off-the-Shelves Large Multi-Modal Models do for Dynamic Scene Graph Generation?,"Xuanming Cui, Jaiminkumar Ashokbhai Bhoi, Chionh Wei Peng, Adriel Kuek, Ser Nam Lim","비디오용 DSGG(Dynamic Scene Graph Generation)는 컴퓨터 비전에서 어려운 작업입니다. 기존 접근 방식은 정교한 아키텍처 설계에 초점을 맞추고 평가 중에 리콜만 사용하는 경우가 많지만, 예측된 장면 그래프를 자세히 살펴보고 기존 DSGG 방법에서 심각한 정밀도-리콜 절충, 삼중항 중요성에 대한 인식 부족, 부적절한 평가 프로토콜이라는 세 가지 중요한 문제를 발견했습니다. 반면, 최근 LMM(Large Multimodal Model)의 발전은 비디오 이해에 있어 뛰어난 기능을 보여 주었지만 DSGG와 같은 세분화된 프레임별 이해 작업에서는 테스트되지 않았습니다. 이 작업에서는 DSGG를 수행하기 위한 비디오 LMM에 대한 최초의 체계적인 분석을 수행합니다. 정교한 아키텍처 설계에 의존하지 않고 간단한 디코더 전용 구조를 가진 LMM이 앞서 언급한 문제를 효과적으로 극복하는 동시에 미세 조정(5-10% 훈련 데이터)이 거의 필요하지 않은 최첨단 장면 그래프 생성기로 전환될 수 있음을 보여줍니다."
776,http://arxiv.org/abs/2503.15761 ,GraPLUS: Graph-based Placement Using Semantics for Image Composition,"Mir Mohammad Khaleghi, Mehran Safayani, Abdolreza Mirzaei","우리는 장면 그래프와 대규모 언어 모델을 활용하는 이미지에서 그럴듯한 객체 배치를 위한 새로운 프레임워크인 GraPLUS(Graph-based Placement Using Semantics)를 제시합니다. 우리의 접근 방식은 그래프 구조의 장면 표현과 의미론적 이해를 고유하게 결합하여 상황에 맞는 개체 위치를 결정합니다. 프레임워크는 GPT-2를 사용하여 범주형 노드 및 에지 레이블을 정의 특성과 일반적인 공간 컨텍스트를 모두 캡처하는 풍부한 의미 체계 임베딩으로 변환하여 객체 관계 및 배치 패턴에 대한 미묘한 이해를 가능하게 합니다. GraPLUS는 OPA 데이터 세트에서 92.1%의 배치 정확도와 28.83의 FID 점수를 달성하여 경쟁력 있는 시각적 품질을 유지하면서 최첨단 방법보다 8.1% 더 뛰어난 성능을 발휘합니다. 19명의 참가자가 평가한 964개의 샘플을 포함하는 인간 평가 연구에서 52.1%의 사례에서 우리의 방법이 선호되었으며 이는 이전 접근 방식보다 훨씬 뛰어났습니다. 프레임워크의 주요 혁신에는 (i) 다른 도메인의 지식을 전달하는 사전 훈련된 장면 그래프 모델 활용, (ii) 구조화된 관계를 통해 장면 의미론을 처리하는 에지 인식 그래프 신경망, (iii) 향상된 장면 기능과 범주형 임베딩을 정렬하는 교차 모달 주의 메커니즘, (iv) 의미론적 일관성 제약 조건을 통합한 다목적 훈련 전략이 포함됩니다."
775,http://arxiv.org/abs/2503.15202 ,"A Unified Framework for Real-Time Failure Handling in Robotics Using Vision-Language Models, Reactive Planner and Behavior Trees","Faseeh Ahmad, Hashim Ismail, Jonathan Styrud, Maj Stenmark, Volker Krueger","로봇 시스템은 예상치 못한 장애물, 센서 오류 또는 환경 변화로 인해 실행 실패에 직면하는 경우가 많습니다. 기존의 장애 복구 방법은 미리 정의된 전략이나 사람의 개입에 의존하므로 적응성이 떨어집니다. 이 백서에서는 VLM(Vision-Language Model), 반응 플래너 및 BT(Behavior Tree)를 결합하여 실시간 오류 처리를 가능하게 하는 통합 오류 복구 프레임워크를 제시합니다. 우리의 접근 방식에는 실행 전에 잠재적인 실패를 확인하는 실행 전 검증과 기존 BT 조건을 확인하고 누락된 전제 조건을 추가하며 필요한 경우 새로운 기술을 생성하여 실행 중에 실패를 감지하고 수정하는 대응적 실패 처리가 포함됩니다. 프레임워크는 구조화된 환경 인식을 위해 장면 그래프를 사용하고 지속적인 모니터링을 위해 실행 기록을 사용하여 상황 인식 및 적응형 오류 처리를 지원합니다. 우리는 AI2-THOR 시뮬레이터뿐만 아니라 페그 삽입, 객체 정렬, 서랍 배치와 같은 작업에 대해 ABB YuMi 로봇을 사용한 실제 실험을 통해 프레임워크를 평가합니다. 사전 실행 방법과 반응 방법을 별도로 사용하는 것에 비해 우리의 접근 방식은 더 높은 작업 성공률과 더 큰 적응성을 달성합니다. 절제 연구는 로봇 공학의 효과적인 오류 복구를 위해 VLM 기반 추론, 구조화된 장면 표현 및 실행 기록 추적의 중요성을 강조합니다."
774,http://arxiv.org/abs/2503.15091 ,Intelligent Spatial Perception by Building Hierarchical 3D Scene Graphs for Indoor Scenarios with the Help of LLMs,"Yao Cheng, Zhe Han, Fengyang Jiang, Huaizhen Wang, Fengyu Zhou, Qingshan Yin, Lei Wei","이 문서는 실내 시나리오를 위한 계층적 3D 장면 그래프(3DSG)를 구성하기 위해 LLM(대형 언어 모델)의 기능을 활용하는 새로운 시스템을 도입함으로써 공간 환경에 대한 보다 전체적인 이해를 위한 고급 지능형 로봇 내비게이션의 높은 수요를 다루고 있습니다. 제안된 프레임워크는 풍부한 메트릭 의미 정보를 갖춘 기본 레이어, 시각적 설명자뿐만 아니라 객체 노드의 정확한 포인트 클라우드 표현을 특징으로 하는 객체 레이어, 방, 바닥 및 건물 노드의 상위 레이어로 구성된 3DSG를 구성합니다. LLM의 혁신적인 적용 덕분에 객체 노드뿐만 아니라 룸 노드와 같은 상위 레이어의 노드에도 지능적이고 정확한 방식으로 주석이 추가됩니다. 룸 노드 주석의 정확성과 신뢰성을 향상시키기 위해 LLM을 사용한 룸 분류를 위한 폴링 메커니즘이 제안되었습니다. 철저한 수치 실험은 의미론적 설명을 기하학적 데이터와 통합하여 상황 인식 탐색 및 작업 계획을 위한 도구인 환경의 정확하고 포괄적인 표현을 생성하는 시스템의 능력을 보여줍니다."
773,http://arxiv.org/abs/2503.15019 ,Learning 4D Panoptic Scene Graph Generation from Rich 2D Visual Scene,"Shengqiong Wu, Hao Fei, Jingkang Yang, Xiangtai Li, Juncheng Li, Hanwang Zhang, Tat-seng Chua","최근에 등장한 4D Panoptic Scene Graph(4D-PSG)는 동적 4D 시각적 실제 세계를 포괄적으로 모델링하기 위한 고급 표현을 제공합니다. 불행하게도 현재의 선구적인 4D-PSG 연구는 주로 심각한 데이터 부족 문제와 그에 따른 어휘 부족 문제로 어려움을 겪을 수 있습니다. 또한 벤치마크 생성 방법의 파이프라인 특성으로 인해 성능이 최적화되지 않을 수 있습니다. 이러한 문제를 해결하기 위해 이 문서에서는 풍부한 2D 시각적 장면 주석을 활용하여 4D 장면 학습을 향상시키는 4D-PSG 생성을 위한 새로운 프레임워크를 조사합니다. 먼저, 4D-PSG의 엔드투엔드 생성을 위해 3D 마스크 디코더와 통합된 4D-LLM(4D Large Language Model)을 소개합니다. 연결된 SG 추론 메커니즘은 LLM의 개방형 어휘 기능을 활용하여 정확하고 포괄적인 개체 및 관계 레이블을 반복적으로 추론하도록 추가로 설계되었습니다. 가장 중요한 것은 공간적 시간적 장면 초월 전략이 풍부한 2D SG 주석에서 차원 불변 기능을 4D 장면으로 효과적으로 전송하여 4D-PSG의 데이터 부족을 효과적으로 보상하는 2D에서 4D 시각적 장면 전송 학습 프레임워크를 제안한다는 것입니다. 벤치마크 데이터에 대한 광범위한 실험을 통해 우리는 기준 모델보다 훨씬 뛰어난 성능을 보여 우리 방법의 효율성을 강조했습니다."
772,http://arxiv.org/abs/2503.15005 ,Universal Scene Graph Generation,"Shengqiong Wu, Hao Fei, Tat-Seng Chua","장면 그래프(SG) 표현은 장면 의미론을 깔끔하고 효율적으로 설명할 수 있으며, 이는 SG 생성에 대한 지속적인 집중 연구를 주도해 왔습니다. 현실 세계에서는 이미지, 텍스트, 비디오, 3D 데이터 등 다양한 유형이 서로 다른 특성을 표현하는 여러 양식이 공존하는 경우가 많습니다. 불행하게도 현재 SG 연구는 주로 단일 양식 장면 모델링에 국한되어 전체적인 장면 의미론을 묘사하는 데 있어 다양한 양식 SG 표현의 보완적인 강점을 완전히 활용하지 못합니다. 이를 위해 우리는 양식 불변 장면과 양식 특정 장면을 포괄하는 양식 입력의 주어진 조합으로부터 포괄적인 의미론적 장면을 완전히 특성화할 수 있는 새로운 표현인 Universal SG(USG)를 소개합니다. 또한 우리는 크로스 모달 개체 정렬 및 도메인 외부 문제의 두 가지 주요 병목 현상을 효과적으로 해결하는 틈새 타겟팅 USG 파서인 USG-Par를 맞춤화했습니다. 우리는 엔드-투-엔드 USG 생성을 위한 모듈식 아키텍처로 USG-Par을 설계합니다. 여기서 우리는 교차 모달 객체 정렬을 위한 모달리티 격차를 완화하기 위한 객체 연관자를 고안합니다. 또한 다중 모달 객체와 관계를 텍스트 SG와 정렬하여 도메인 불균형을 완화하기 위한 텍스트 중심 장면 대조 학습 메커니즘을 제안합니다. 광범위한 실험을 통해 우리는 USG가 독립형 SG보다 장면 의미론을 표현하는 데 더 강력한 기능을 제공하고 USG-Par가 더 높은 효율성과 성능을 달성한다는 것을 입증했습니다."
771,http://arxiv.org/abs/2503.14607 ,Can Large Vision Language Models Read Maps Like a Human?,"Shuo Xing, Zezhou Sun, Shuangyu Xie, Kaiyuan Chen, Yanjia Huang, Yuping Wang, Jiachen Li, Dezhen Song, Zhengzhong Tu",본 문서에서는 복잡한 경로 찾기 시나리오에서 선별된 사람이 읽을 수 있는 픽셀 기반 지도 기반 실외 탐색을 위해 특별히 설계된 최초의 데이터세트인 MapBench를 소개합니다. MapBench는 100개의 다양한 지도에서 문제를 찾아내는 1600픽셀 이상의 공간 지도 경로로 구성됩니다. MapBench에서 LVLM은 지도 이미지와 시작 및 끝 랜드마크가 포함된 쿼리를 바탕으로 언어 기반 탐색 지침을 생성합니다. 각 맵에 대해 MapBench는 MSSG(Map Space Scene Graph)를 인덱싱 데이터 구조로 제공하여 자연어 간 변환과 LVLM 생성 결과 평가를 수행합니다. 우리는 MapBench가 제로 샷 프롬프트와 맵 탐색을 순차적 인지 프로세스로 분해하는 CoT(사상 사슬) 증강 추론 프레임워크 모두에서 최첨단 LVLM에 크게 도전하고 있음을 보여줍니다. 오픈 소스 및 폐쇄 소스 LVLM에 대한 우리의 평가는 MapBench가 제기하는 실질적인 어려움을 강조하며 공간 추론 및 구조화된 의사 결정 기능에 중요한 한계가 있음을 드러냅니다. 우리는 https://github.com/taco-group/MapBench에서 모든 코드와 데이터 세트를 공개합니다.
770,http://arxiv.org/abs/2503.13957 ,DIFFVSGG: Diffusion-Driven Online Video Scene Graph Generation,"Mu Chen, Liulei Li, Wenguan Wang, Yi Yang","VSGG(비디오 장면 그래프 생성)를 위한 최고의 솔루션은 일반적으로 오프라인 파이프라인을 채택합니다. 유망한 성능을 보여주었음에도 불구하고 실시간 비디오 스트림을 처리할 수 없고 대용량 GPU 메모리를 소비하지 못합니다. 더욱이 이러한 접근 방식은 시간적 추론이 부족하여 시간적 맥락에 대해 프레임 수준 예측을 집계하는 것뿐입니다. 이에 대한 응답으로 이 작업을 반복적인 장면 그래프 업데이트 문제로 구성하는 온라인 VSGG 솔루션인 DIFFVSGG를 소개합니다. 잠재 기능 임베딩의 노이즈 제거를 통해 이미지를 생성하는 LDM(잠재 확산 모델)에서 영감을 얻어 하나의 공유 기능 임베딩을 사용하여 객체 분류 디코딩, 경계 상자 회귀 및 그래프 생성 세 가지 작업을 통합합니다. 그런 다음 객체 쌍의 통합 기능을 포함하는 임베딩이 주어지면 LDM 내에서 단계별 노이즈 제거를 수행하여 객체 간의 관계를 명확하게 나타내는 깨끗한 임베딩을 제공합니다. 그런 다음 이 임베딩은 객체 분류, 장면 그래프 생성 등을 위한 작업별 헤드에 대한 입력 역할을 합니다. DIFFVSGG는 후속 프레임에 대한 예측이 과거 프레임의 결과를 LDM의 조건부 입력으로 활용하여 현재 프레임에 대한 역확산 프로세스를 안내하는 지속적인 시간적 추론을 더욱 용이하게 합니다. Action Genome의 세 가지 설정에 대한 광범위한 실험은 DIFFVSGG의 우수성을 입증합니다."
769,http://arxiv.org/abs/2503.13947 ,Conformal Prediction and MLLM aided Uncertainty Quantification in Scene Graph Generation,"Sayak Nag, Udita Ghosh, Calvin-Khang Ta, Sarosij Bose, Jiachen Li, Amit K Roy Chowdhury",SGG(장면 그래프 생성)는 객체와 객체의 쌍별 관계를 식별하여 시각적 장면을 표현하고 이미지 콘텐츠에 대한 구조화된 이해를 제공하는 것을 목표로 합니다. 그러나 긴 꼬리 클래스 분포 및 예측 변동성과 같은 본질적인 문제로 인해 SGG의 실제 실행 가능성을 위해서는 불확실성 정량화가 필요합니다. 본 논문에서는 생성된 장면 그래프에 대해 잘 보정된 예측 세트를 구성하여 예측 불확실성을 정량화하기 위해 기존 SGG 방법에 적응하는 새로운 CP(등각 예측) 기반 프레임워크를 소개합니다. 이러한 장면 그래프 예측 세트는 통계적으로 엄격한 적용 범위 보장을 달성하도록 설계되었습니다. 또한 이러한 예측 세트에 가장 실질적으로 해석 가능한 장면 그래프가 포함되도록 하기 위해 이러한 예측 세트 내에서 시각적으로나 의미적으로 가장 그럴듯한 장면 그래프를 선택하기 위한 효과적인 MLLM 기반 후처리 전략을 설계합니다. 우리는 제안된 접근 방식이 이미지에서 다양한 장면 그래프를 생성하고 SGG 방법의 신뢰성을 평가하며 전반적인 SGG 성능을 향상시킬 수 있음을 보여줍니다.
768,http://arxiv.org/abs/2503.12974 ,Exploring 3D Reasoning-Driven Planning: From Implicit Human Intentions to Route-Aware Activity Planning,"Xueying Jiang, Wenhao Li, Xiaoqin Zhang, Ling Shao, Shijian Lu","3D 작업 계획은 최근 다중 모드 학습의 발전으로 인해 인간-로봇 상호 작용 및 AI 구현 분야에서 점점 더 많은 관심을 받고 있습니다. 그러나 대부분의 기존 연구는 두 가지 공통적인 과제에 직면해 있습니다. 1) 암시적인 사용자 의도에 대한 추론이 거의 없이 명시적인 지침에 크게 의존합니다. 2) 로봇 이동에 대한 단계 간 경로 계획을 소홀히 함. 우리는 암시적 지침에서 의도한 활동을 추론하고 이를 장면 분할의 세밀한 3D 개체 모양 및 위치에 따라 계획을 수립하고 단계 간 경로를 사용하여 단계로 분해하는 새로운 3D 작업인 3D 추론 중심 계획을 제안하여 위의 과제를 해결합니다. 우리는 두 가지 관점에서 새로운 3D 작업을 다루고 있습니다. 먼저, 다단계 작업 계획, 단계 간 경로 계획 및 세분화된 분할을 위한 풍부한 암시적 지침과 자세한 주석을 통해 다양한 3D 장면을 포괄하는 대규모 벤치마크인 ReasonPlan3D를 구축합니다. 둘째, 여러 단계에 걸쳐 상황적 일관성을 갖춘 점진적인 계획 생성을 도입하는 새로운 프레임워크와 중요한 개체 및 해당 공간 관계를 캡처하기 위해 동적으로 업데이트되는 장면 그래프를 설계합니다. 광범위한 실험은 암시적인 인간 지시로부터 활동을 추론하고, 정확한 단계별 작업 계획을 생성하고, 다단계 이동을 위한 경로 계획을 원활하게 통합하는 데 있어 벤치마크 및 프레임워크의 효율성을 보여줍니다. 데이터 세트와 코드가 공개됩니다."
767,http://arxiv.org/abs/2503.12552 ,MTGS: Multi-Traversal Gaussian Splatting,"Tianyu Li, Yihang Qiu, Zhenhua Wu, Carl Lindström, Peng Su, Matthias Nießner, Hongyang Li","일상 통근이나 자율 주행 차량을 통해 일반적으로 수집되는 다중 횡단 데이터는 도로 블록 내에서 장면 재구성을 위한 다양한 관점을 제공합니다. 이 데이터는 자율 차량 시뮬레이터와 같은 응용 분야에 중요한 고품질의 새로운 뷰 합성에 대한 상당한 잠재력을 제공합니다. 그러나 다중 순회 데이터의 본질적인 문제로 인해 외관의 변화와 동적 개체의 존재를 포함하여 차선의 재구성 품질이 발생하는 경우가 많습니다. 이러한 문제를 해결하기 위해 우리는 동적 요소와 외관 변화를 별도로 처리하면서 공유된 정적 형상을 모델링하여 임의로 수집된 다중 횡단 데이터로부터 고품질 주행 장면을 재구성하는 새로운 접근 방식인 MTGS(Multi-Traversal Gaussian Splatting)를 제안합니다. 우리의 방법은 학습 가능한 구면 고조파 계수 잔차가 있는 색상 보정 노드로 보완된 공유 정적 노드와 순회별 동적 노드가 있는 다중 순회 동적 장면 그래프를 사용합니다. 이 접근 방식은 충실도가 높은 새로운 뷰 합성을 가능하게 하며 모든 관점을 탐색할 수 있는 유연성을 제공합니다. 다중 주행 데이터를 활용하여 대규모 주행 데이터 세트인 nuPlan에 대한 광범위한 실험을 수행합니다. 우리의 결과는 MTGS가 단일 횡단 기준선에 비해 LPIPS를 23.5%, 형상 정확도를 46.3% 향상시키는 것으로 나타났습니다. 코드와 데이터는 대중에게 공개됩니다."
766,http://arxiv.org/abs/2503.14524 ,Salient Temporal Encoding for Dynamic Scene Graph Generation,Zhihao Zhu,"구조화된 시공간 장면 그래프를 사용하여 동적 장면을 표현하는 것은 새롭고 특히 어려운 작업입니다. 이 작업을 수행하려면 공간적 관계 외에도 개체 간의 시간적 상호 작용을 배우는 것이 중요합니다. 현재 벤치마크 데이터 세트에는 명시적으로 주석이 달린 시간 관계가 없기 때문에 대부분의 기존 공간-시간 장면 그래프 생성 방법은 프레임 전체의 모든 개체 간에 조밀하고 추상적인 시간 연결을 구축합니다. 그러나 모든 시간적 연결이 의미 있는 시간적 역학을 인코딩하는 것은 아닙니다. 우리는 시간적 관련 개체 쌍 사이에서만 시간적 연결을 선택적으로 구축하고 장면 그래프에서 시간적 관계를 명시적인 가장자리로 나타내는 새로운 시공간적 장면 그래프 생성 방법을 제안합니다. 결과적으로 희박하고 명시적인 시간적 표현을 통해 장면 그래프 감지에서 강력한 장면 그래프 생성 기준을 최대 $4.4\%$까지 향상시킬 수 있습니다. 또한 우리의 접근 방식을 활용하여 다운스트림 비전 작업을 개선할 수 있음을 보여줍니다. 특히, 행동 인식에 대한 우리의 접근 방식을 적용하면 최신 기술에 비해 mAP에서 0.6\% 이득을 나타냅니다."
765,http://arxiv.org/abs/2503.12034 ,Real-Time Manipulation Action Recognition with a Factorized Graph Sequence Encoder,"Enes Erdogan, Eren Erdal Aksoy, Sanem Sariel","인간의 조작 행위를 실시간으로 인식하는 것은 안전하고 효과적인 인간-로봇 상호 작용 및 협업을 위해 필수적입니다. 문제는 실시간 실행이 가능할 정도로 가볍고 일반화가 가능한 모델을 개발하는 것입니다. 문헌의 일부 기존 방법은 실시간으로 실행될 수 있지만 시간적 확장성 문제로 어려움을 겪습니다. 즉, 장기간 조작에 효과적으로 적응하지 못합니다. 이 문제를 해결하기 위해 일반화 가능한 장면 그래프 표현을 활용하여 우리는 인수분해 인코더 아키텍처 덕분에 실시간으로 실행될 뿐만 아니라 시간적 차원에서 효과적으로 확장되는 새로운 Factorized Graph Sequence Encoder 네트워크를 제안합니다. 또한 그래프 수준 임베딩을 보다 집중적으로 추출하기 위한 간단한 풀링 작업인 Hand Pooling 작업을 소개합니다. 우리 모델은 KIT Bimanual Action(Bimacs) 데이터 세트와 Collaborative Action(CoAx) 데이터 세트에서 F1-매크로 점수가 각각 14.3% 및 5.6% 향상되어 이전의 최첨단 실시간 접근 방식보다 성능이 뛰어납니다. 또한 우리는 네트워크 설계 선택을 검증하기 위해 광범위한 절제 연구를 수행합니다. 마지막으로 우리 모델을 Bimacs 데이터세트의 구조적으로 유사한 RGB 기반 모델과 비교하고 객체 중심 조작 데이터세트에 대한 우리 모델과 대조하여 이 모델의 한계를 보여줍니다."
764,http://arxiv.org/abs/2503.11958 ,"CHOrD: Generation of Collision-Free, House-Scale, and Organized Digital Twins for 3D Indoor Scenes with Controllable Floor Plans and Optimal Layouts","Chong Su, Yingbin Fu, Zheyuan Hu, Jing Yang, Param Hanji, Shaojun Wang, Xuan Zhao, Cengiz Öztireli, Fangcheng Zhong","집 규모의 충돌 없는 계층 구조의 실내 디지털 트윈을 생성하도록 설계된 확장 가능한 3D 실내 장면 합성을 위한 새로운 프레임워크인 CHOrD를 소개합니다. 장면 레이아웃을 장면 그래프나 객체 목록으로 직접 합성하는 기존 방법과 달리 CHOrD는 2D 이미지 기반 중간 레이아웃 표현을 통합하여 생성 중에 배포 외(OOD) 시나리오로 성공적으로 캡처하여 충돌 아티팩트를 효과적으로 방지할 수 있습니다. 또한 기존 방법과 달리 CHOrD는 다중 모드 제어를 사용하여 복잡한 평면도를 준수하는 장면 레이아웃을 생성할 수 있으므로 방 구조의 기하학적 및 의미적 변화에 강력하고 일관된 집 전체 레이아웃을 생성할 수 있습니다. 또한, 가정용품 및 객실 구성에 대한 적용 범위가 확장되고 데이터 품질이 크게 향상된 새로운 데이터 세트를 제안합니다. CHOrD는 3D-FRONT와 제안된 데이터 세트 모두에서 최첨단 성능을 보여 임의의 평면도 변화에 적응할 수 있는 사실적이고 공간적으로 일관된 실내 장면 합성을 제공합니다."
763,http://arxiv.org/abs/2503.11113 ,Vipera: Towards systematic auditing of generative text-to-image models at scale,"Yanwei Huang, Wesley Hanwen Deng, Sijia Xiao, Motahhare Eslami, Jason I. Hong, Adam Perer","생성적 텍스트-이미지(T2I) 모델은 편견, 범죄, 잘못된 정보와 같은 위험과 관련된 것으로 알려져 있습니다. 현재의 AI 감사 방법은 확장성과 완전성 측면에서 문제에 직면해 있으며, 감사자가 구조적이고 효과적인 방식으로 감사 공간을 탐색할 수 있도록 하는 것은 훨씬 더 어렵습니다. Vipera는 장면 그래프를 포함한 여러 시각적 단서를 사용하여 이미지 수집 감지를 촉진하고 감사자가 감사 기준을 탐색하고 계층적으로 구성하도록 영감을 줍니다. 또한 LLM 기반 제안을 활용하여 탐구되지 않은 감사 방향을 쉽게 탐색할 수 있습니다. 관찰 사용자 연구는 감사자가 다양한 기준을 사용하면서 분석을 구성하는 데 도움이 되는 Vipera의 효율성을 보여줍니다."
762,http://arxiv.org/abs/2503.11089 ,EmbodiedVSR: Dynamic Scene Graph-Guided Chain-of-Thought Reasoning for Visual Spatial Tasks,"Yi Zhang, Qiang Zhang, Xiaozhu Ju, Zhaoyang Liu, Jilei Mao, Jingkai Sun, Jintao Wu, Shixiong Gao, Shihan Cai, Zhiyuan Qin, Linkai Liang, Jiaxu Wang, Yiqun Duan, Jiahang Cao, Renjing Xu, Jian Tang","다중 모드 대형 언어 모델(MLLM)은 구현된 지능 분야에서 획기적인 발전을 이루었지만 여전히 복잡한 장거리 작업에 대한 공간 추론에서 상당한 어려움에 직면해 있습니다. 이러한 격차를 해결하기 위해 우리는 구현된 에이전트에 대한 공간 이해를 향상시키기 위해 동적 장면 그래프 기반 CoT(사고 사슬) 추론을 통합하는 새로운 프레임워크인 EmbodiedVSR(Embodied Visual Spatial Reasoning)을 제안합니다. 동적 장면 그래프를 통해 구조화된 지식 표현을 명시적으로 구성함으로써 우리의 방법은 작업별 미세 조정 없이 제로샷 공간 추론을 가능하게 합니다. 이 접근 방식은 복잡한 공간 관계를 풀 뿐만 아니라 추론 단계를 실행 가능한 환경 역학에 맞춰 조정합니다. 성능을 엄격하게 평가하기 위해 세밀한 공간 주석과 적응형 작업 난이도 수준을 갖춘 실제 구현 시나리오를 포함하는 포괄적인 데이터 세트인 eSpatial-Benchmark를 소개합니다. 실험에 따르면 우리 프레임워크는 특히 반복적인 환경 상호 작용이 필요한 장거리 작업에서 정확성과 추론 일관성 측면에서 기존 MLLM 기반 방법보다 훨씬 뛰어난 성능을 보여줍니다. 결과는 구조화되고 설명 가능한 추론 메커니즘을 갖춘 경우 구체화된 지능에 대한 MLLM의 미개척 잠재력을 보여주며, 실제 공간 애플리케이션에서 보다 안정적인 배포를 위한 길을 열어줍니다. 코드와 데이터세트는 곧 공개될 예정입니다."
761,http://arxiv.org/abs/2503.10986 ,Image-Goal Navigation Using Refined Feature Guidance and Scene Graph Enhancement,"Zhicheng Feng, Xieyuanli Chen, Chenghao Shi, Lun Luo, Zhichao Chen, Yun-Hui Liu, Huimin Lu","본 논문에서는 RFSG라는 새로운 이미지 목표 탐색 접근 방식을 소개합니다. 우리의 초점은 제한된 이미지 데이터 내에서 목표, 관찰 및 환경 간의 세밀한 연결을 활용하는 동시에 탐색 아키텍처를 간단하고 가볍게 유지하는 데 있습니다. 이를 위해 우리는 네트워크가 다차원 특징의 중요성을 학습하여 목표 특징과 관찰 특징을 융합할 수 있도록 하는 공간 채널 주의 메커니즘을 제안합니다. 또한, 특징 표현 기능을 더욱 향상시키기 위해 자체 증류 메커니즘이 통합되었습니다. 탐색 작업은 보다 효율적인 탐색을 위해 주변 환경 정보가 필요하다는 점을 고려하여 이미지와 객체 수준 모두에서 특징 연관을 설정하고 주변 장면 정보를 효과적으로 인코딩하는 이미지 장면 그래프를 제안합니다. Gibson 및 HM3D 데이터 세트에 대해 크로스씬 성능 검증을 수행했으며, 제안된 방법은 RTX3080에서 초당 최대 53.5 프레임의 속도로 주류 방법 중 최첨단 결과를 달성했습니다. 이는 실제 시나리오에서 엔드투엔드 이미지 목표 탐색을 실현하는 데 기여합니다. 우리 방법의 구현과 모델은 https://github.com/nubot-nudt/RFSG에서 공개되었습니다."
760,http://arxiv.org/abs/2503.10630 ,UniGoal: Towards Universal Zero-shot Goal-oriented Navigation,"Hang Yin, Xiuwei Xu, Lingqing Zhao, Ziwei Wang, Jie Zhou, Jiwen Lu","본 논문에서는 범용 제로샷 목표 지향 탐색을 위한 일반적인 프레임워크를 제안합니다. 기존 제로샷 방법은 특정 작업을 위한 대규모 언어 모델(LLM)을 기반으로 추론 프레임워크를 구축하는데, 이는 전체 파이프라인에서 많이 다르며 다양한 목표 유형에 걸쳐 일반화하는 데 실패합니다. 보편적인 제로샷 탐색을 목표로 객체 카테고리, 인스턴스 이미지 및 텍스트 설명을 포함한 다양한 목표를 통합하기 위한 균일한 그래프 표현을 제안합니다. 또한 에이전트 관찰을 온라인으로 유지되는 장면 그래프로 변환합니다. 이러한 일관된 장면 및 목표 표현을 통해 순수 텍스트에 비해 대부분의 구조적 정보를 보존하고 명시적인 그래프 기반 추론을 위해 LLM을 활용할 수 있습니다. 구체적으로, 매 순간의 장면 그래프와 목표 그래프 간의 그래프 매칭을 수행하고, 매칭 상태에 따라 장기적인 탐색 목표를 생성하기 위한 다양한 전략을 제안합니다. 에이전트는 0이 일치할 때 먼저 목표의 하위 그래프를 반복적으로 검색합니다. 부분 일치를 통해 에이전트는 좌표 투영과 앵커 쌍 정렬을 활용하여 목표 위치를 추론합니다. 마지막으로 완벽한 매칭을 위해 장면 그래프 보정 및 목표 검증이 적용됩니다. 또한 단계 간 강력한 전환을 가능하게 하는 블랙리스트 메커니즘을 제시합니다. 여러 벤치마크에 대한 광범위한 실험을 통해 UniGoal은 단일 모델을 사용하여 연구된 세 가지 탐색 작업에서 최첨단 제로샷 성능을 달성했으며 작업별 제로샷 방법 및 감독된 범용 방법보다 뛰어난 성능을 발휘하는 것으로 나타났습니다."
759,http://arxiv.org/abs/2503.10331 ,OSMa-Bench: Evaluating Open Semantic Mapping Under Varying Lighting Conditions,"Maxim Popov, Regina Kurkova, Mikhail Iumanov, Jaafar Mahmoud, Sergey Kolyubin","OSM(Open Semantic Mapping)은 의미론적 분할과 SLAM 기술을 결합한 로봇 인식의 핵심 기술입니다. 이 문서에서는 OSMa-Bench(Open Semantic Mapping Benchmark)라고 불리는 OSM 솔루션을 평가하기 위한 동적으로 구성 가능하고 고도로 자동화된 LLM/LVLM 기반 파이프라인을 소개합니다. 이 연구는 실내 환경에서 중요한 과제인 다양한 실내 조명 조건에서 최첨단 의미 매핑 알고리즘을 평가하는 데 중점을 둡니다. 우리는 시뮬레이션된 RGB-D 시퀀스와 지상 실측 3D 재구성을 갖춘 새로운 데이터세트를 도입하여 다양한 조명 조건에서 매핑 성능에 대한 엄격한 분석을 용이하게 합니다. ConceptGraphs, BBQ, OpenScene 등 주요 모델에 대한 실험을 통해 객체 인식 및 분할의 의미 충실도를 평가합니다. 또한 의미 구조를 해석하는 모델의 능력을 분석하기 위해 장면 그래프 평가 방법을 소개합니다. 결과는 이러한 모델의 견고성에 대한 통찰력을 제공하여 탄력적이고 적응 가능한 로봇 시스템을 개발하기 위한 향후 연구 방향을 형성합니다. 프로젝트 페이지는 https://be2rlab.github.io/OSMa-Bench/에서 확인할 수 있습니다."
758,http://arxiv.org/abs/2503.09173 ,Long-Term Planning Around Humans in Domestic Environments with 3D Scene Graphs,"Ermanno Bartoli, Dennis Rotondi, Kai O. Arras, Iolanda Leite","국내 환경에서 작동하는 로봇에 대한 장기 계획은 인간, 사물, 공간 간의 상호 작용으로 인해 독특한 과제를 안고 있습니다. 최근 궤도 계획의 발전으로 비전 언어 모델(VLM)을 활용하여 실제 환경에서 작동하는 로봇에 대한 상황별 정보를 추출했습니다. 이러한 방법은 만족스러운 성능을 달성하지만 인간 활동을 명시적으로 모델링하지는 않습니다. 이러한 활동은 주변 물체에 영향을 미치고 공간적 제약을 재구성합니다. 이 논문은 풍부한 3D 장면 그래프(3DSG) 표현을 통해 인간 선호도, 활동 및 공간적 맥락을 통합하는 궤적 계획에 대한 새로운 접근 방식을 제시합니다. 활동 기반 관계를 통합함으로써 우리의 방법은 인간 행동의 공간적 영향을 포착하여 상황에 더욱 민감한 궤적 적응을 이끌어냅니다. 예비 결과는 우리의 접근 방식이 인간 활동의 영향을 받는 공간에 비용을 효과적으로 할당하여 로봇 궤적이 상황에 맞게 적절하고 진행 중인 환경에 민감하게 유지된다는 것을 보여줍니다. 작업 효율성과 사회적 적절성 사이의 이러한 균형은 가정 환경에서 상황 인식 인간-로봇 상호 작용을 향상시킵니다. 향후 작업에는 전체 계획 파이프라인을 구현하고 사용자 연구를 수행하여 궤도 수용성을 평가하는 것이 포함됩니다."
757,http://arxiv.org/abs/2503.08474 ,Collaborative Dynamic 3D Scene Graphs for Open-Vocabulary Urban Scene Understanding,"Tim Steinke, Martin Büchner, Niclas Vödisch, Abhinav Valada","매핑과 장면 표현은 모바일 로봇의 안정적인 계획과 탐색의 기본입니다. 복셀 그리드를 사용하는 순수 기하학적 맵은 일반적인 탐색을 허용하지만, 동적 대규모 환경에 맞게 확장되는 최신 공간 및 의미론적으로 풍부한 표현을 얻는 것은 여전히 ​​어려운 일입니다. 본 연구에서는 다중 에이전트 협업을 통해 도시 주행 장면의 계층적 분해를 생성하는 개방형 어휘 동적 3D 장면 그래프 엔진인 CURB-OSG를 제시합니다. 알 수 없는 초기 자세를 가진 여러 인식 에이전트의 카메라와 LiDAR 관찰을 융합함으로써 우리의 접근 방식은 단일 에이전트에 비해 더 정확한 지도를 생성하는 동시에 장면의 통합된 개방형 어휘 의미 체계 계층을 구성합니다. 실제 에이전트 포즈에 의존하거나 순전히 시뮬레이션으로 평가되는 이전 방법과 달리 CURB-OSG는 이러한 제약을 완화합니다. 우리는 Oxford Radar RobotCar 데이터 세트의 여러 세션에서 얻은 실제 다중 에이전트 센서 데이터에 대해 CURB-OSG의 기능을 평가합니다. 우리는 다중 에이전트 협업을 통해 향상된 매핑 및 개체 예측 정확도를 입증하고 제안된 접근 방식의 환경 분할 기능을 평가합니다. 추가 연구를 촉진하기 위해 https://ov-curb.cs.uni-freiburg.de에서 코드와 보충 자료를 공개합니다."
756,http://arxiv.org/abs/2503.07909 ,FunGraph: Functionality Aware 3D Scene Graphs for Language-Prompted Scene Interaction,"Dennis Rotondi, Fabio Scaparro, Hermann Blum, Kai O. Arras","3D 장면 그래프의 개념은 환경에 대한 강력한 의미론적 및 계층적 표현으로 점차 인식되고 있습니다. 현재 접근 방식은 대략적인 개체 수준 해상도에서 이 문제를 해결하는 경우가 많습니다. 이와 대조적으로, 우리의 목표는 기능적 상호작용 요소의 위치와 이러한 요소의 사용 방법을 모두 식별하여 로봇이 환경과 직접 상호작용할 수 있도록 하는 표현을 개발하는 것입니다. 이를 달성하기 위해 어포던스 관련 부분에 중점을 두고 보다 미세한 해상도로 객체를 감지하고 저장하는 데 중점을 둡니다. 주요 과제는 인스턴스 수준 감지 이상으로 확장되는 데이터의 부족과 로봇 센서를 사용하여 상세한 개체 특징을 캡처하는 데 내재된 어려움에 있습니다. 우리는 현재 사용 가능한 3D 리소스를 활용하여 2D 데이터를 생성하고 탐지기를 교육한 다음 표준 3D 장면 그래프 생성 파이프라인을 확장하는 데 사용됩니다. 실험을 통해 우리는 우리의 접근 방식이 최첨단 3D 모델에 필적하는 기능적 요소 분할을 달성하고 우리의 증강을 통해 현재 솔루션보다 더 높은 정확도로 작업 중심 어포던스 접지를 가능하게 함을 보여줍니다. https://fungraph.github.io에서 프로젝트 페이지를 참조하세요."
755,http://arxiv.org/abs/2503.07152 ,Controllable 3D Outdoor Scene Generation via Scene Graphs,"Yuheng Liu, Xinke Li, Yuning Zhang, Lu Qi, Xin Li, Wenping Wang, Chongshou Li, Xueting Li, Ming-Hsuan Yang","3차원 장면 생성은 자율 주행, 게임 및 메타버스에 걸친 응용 분야를 포함한 컴퓨터 비전에서 매우 중요합니다. 현재 방법은 사용자 제어가 부족하거나 부정확하고 직관적이지 않은 조건에 의존합니다. 본 연구에서는 야외 3D 장면을 생성하기 위해 접근 가능하고 사용자 친화적인 제어 형식인 장면 그래프를 사용하는 방법을 제안합니다. 우리는 희소 장면 그래프를 조밀한 BEV(Bird's Eye View) 임베딩 맵으로 변환하는 대화형 시스템을 개발합니다. 이는 조건부 확산 모델을 안내하여 장면 그래프 설명과 일치하는 3D 장면을 생성합니다. 추론 중에 사용자는 장면 그래프를 쉽게 생성하거나 수정하여 대규모 야외 장면을 생성할 수 있습니다. 우리는 BEV 임베딩 및 확산 모델을 훈련하기 위해 쌍을 이루는 장면 그래프와 3D 의미론적 장면을 갖춘 대규모 데이터 세트를 생성합니다. 실험 결과는 우리의 접근 방식이 입력 장면 그래프와 밀접하게 일치하는 고품질 3D 도시 장면을 일관되게 생성한다는 것을 보여줍니다. 우리가 아는 한, 이는 장면 그래프를 기반으로 3D 실외 장면을 생성하는 첫 번째 접근 방식입니다."
754,http://arxiv.org/abs/2503.06820 ,Towards Fine-Grained Video Question Answering,"Wei Dai, Alan Luo, Zane Durante, Debadutta Dash, Arnold Milstein, Kevin Schulman, Ehsan Adeli, Li Fei-Fei","빠르게 발전하는 비디오 이해 영역에서 Video QA(비디오 질문 응답)는 여전히 핵심입니다. 그러나 기존 데이터 세트는 시간적, 공간적 세분성에 차이가 있어 결과적으로 기존 VideoQA 방법의 기능이 제한됩니다. 본 논문에서는 시간적 위치 파악, 공간적 관계 추론, 엔터티 중심 쿼리를 강조하여 이러한 단점을 해결하도록 설계된 MOMA-QA(Multi-Object Multi-Actor Question Answering) 데이터 세트를 소개합니다. MOMA-QA는 지상 실제 장면 그래프와 시간 간격 주석을 통해 세밀한 비디오 이해를 위한 모델을 개발하는 데 이상적입니다. 또한 장면 그래프 예측기, 효율적인 프레임 검색기, 시간적 위치 파악 및 세밀한 관계 이해를 위해 사전 훈련된 대규모 언어 모델을 통합한 새로운 비디오 언어 모델인 SGVLM을 제시합니다. MOMA-QA 및 기타 공개 데이터 세트에 대한 평가는 우리 모델의 탁월한 성능을 입증하여 VideoQA에 대한 새로운 벤치마크를 설정했습니다."
753,http://arxiv.org/abs/2503.06182 ,FORESCENE: FOREcasting human activity via latent SCENE graphs diffusion,"Antonio Alliegro, Francesca Pistilli, Tatiana Tommasi, Giuseppe Averta",인간 행동의 높은 가변성으로 인해 일상 활동에서 인간과 환경의 상호 작용을 예측하는 것은 어렵습니다. 비디오를 통해 직접 예측하는 것도 가능하지만 상호 작용에 기여하지 않는 관련 없는 개체나 배경 소음과 같은 교란 요소로 인해 제한됩니다. 유망한 대안은 장면 그래프(SG)를 사용하여 관련 요소만 추적하는 것입니다. 그러나 미래 SG를 예측하기 위한 현재 방법은 심각한 문제에 직면하고 있으며 시간이 지남에 따라 고정된 개체와 같은 비현실적인 가정에 의존하는 경우가 많아 상호 작용하는 개체가 나타나거나 사라질 수 있는 장기 활동에 대한 적용 가능성이 제한됩니다. 본 논문에서는 시간이 지남에 따라 객체와 관계 진화를 모두 예측하는 장면 그래프 예측(SGA)을 위한 새로운 프레임워크인 FORESCENE을 소개합니다. FORESCENE은 맞춤형 그래프 자동 인코더를 사용하여 관찰된 비디오 세그먼트를 잠재 표현으로 인코딩하고 LDM(잠재 확산 모델)을 사용하여 미래 SG를 예측합니다. 우리의 접근 방식을 사용하면 그래프의 내용이나 구조에 대한 가정을 하지 않고도 상호 작용 역학을 지속적으로 예측할 수 있습니다. 우리는 훨씬 더 복잡한 작업을 해결하면서 기존 SGA 방법보다 뛰어난 성능을 발휘하는 Action Genome 데이터 세트에서 FORESCENE을 평가합니다.
752,http://arxiv.org/abs/2503.04034 ,GaussianGraph: 3D Gaussian-based Scene Graph Generation for Open-world Scene Understanding,"Xihan Wang, Dianyi Yang, Yu Gao, Yufeng Yue, Yi Yang, Mengyin Fu","3D GS(3D Gaussian Splatting)의 최근 발전으로 의미론적 장면 이해가 크게 향상되어 자연어 쿼리를 통해 장면 내 객체의 위치를 ​​파악할 수 있습니다. 그러나 기존 방법은 주로 압축된 CLIP 기능을 3D 가우시안에 삽입하는 데 중점을 두고 있어 객체 ​​분할 정확도가 낮고 공간 추론 기능이 부족합니다. 이러한 한계를 해결하기 위해 우리는 적응형 의미 클러스터링과 장면 그래프 생성을 통합하여 3DGS 기반 장면 이해를 향상시키는 새로운 프레임워크인 GaussianGraph를 제안합니다. 장면 규모와 특징 분포에 동적으로 적응하여 특징 압축을 피하고 분할 정확도를 크게 향상시키는 ""Control-Follow"" 클러스터링 전략을 소개합니다. 또한 2D 기반 모델에서 추출된 객체 속성과 공간 관계를 통합하여 장면 표현을 강화합니다. 공간 관계의 부정확성을 해결하기 위해 공간 일관성 검증을 통해 믿기 어려운 관계를 필터링하여 신뢰할 수 있는 장면 그래프 구성을 보장하는 3D 보정 모듈을 제안합니다. 세 가지 데이터 세트에 대한 광범위한 실험을 통해 GaussianGraph는 의미론적 분할 및 객체 기반 작업 모두에서 최첨단 방법보다 성능이 뛰어나 복잡한 장면 이해 및 상호 작용을 위한 강력한 솔루션을 제공한다는 사실이 입증되었습니다."
751,http://arxiv.org/abs/2503.03412 ,REACT: Real-time Efficient Attribute Clustering and Transfer for Updatable 3D Scene Graph,"Phuoc Nguyen, Francesco Verdoja, Ville Kyrki",현대의 자율 로봇은 정교한 작업을 수행하기 위해 높은 수준의 지도 표현이 필요합니다. 최근에는 효율적인 메모리 사용과 풍부한 기능 표현을 결합한 3D 장면 그래프(3DSG)가 기존 그리드 맵에 대한 유망한 대안으로 등장했습니다. 그러나 이를 적용하려는 대부분의 노력은 정적 세계에 국한되었습니다. 이 작업에서는 실시간 속성 클러스터링 및 전송을 효율적으로 수행하여 3DSG에서 객체 노드를 재지역화하는 프레임워크인 REACT를 소개합니다. REACT는 삼중 손실에 대해 훈련된 임베딩 모델을 사용하여 객체 인스턴스를 비교하고 인스턴스 클러스터링 및 매칭을 촉진하는 새로운 방법을 사용합니다. 실험 결과는 REACT가 계산 효율성을 유지하면서 객체를 재배치할 수 있음을 보여줍니다. REACT 프레임워크의 소스 코드는 오픈 소스 프로젝트로 제공되어 재사용 및 업데이트 가능한 3DSG의 추가 발전을 촉진합니다.
750,http://arxiv.org/abs/2503.03340 ,EnigmaToM: Improve LLMs' Theory-of-Mind Reasoning Capabilities with Neural Knowledge Base of Entity States,"Hainiu Xu, Siya Qi, Jiazheng Li, Yuxiang Zhou, Jinhua Du, Caroline Catmur, Yulan He","다른 사람의 인식과 정신 상태를 추론하는 능력인 ToM(Theory of Mind)은 인간 상호 작용의 기본이지만 LLM(대형 언어 모델)에서는 여전히 어려운 과제입니다. 기존 ToM 추론 방법은 지각적 관점 수용을 통한 추론에 대한 가능성을 보여 주지만 기성 LLM에 과도하게 의존하여 효율성을 줄이고 고차 ToM 추론에 대한 적용 가능성을 제한하는 경우가 많습니다. 이러한 문제를 해결하기 위해 우리는 (1) 정확한 관점 인식을 촉진하는 심리학에서 영감을 받은 반복 마스킹 메커니즘과 (2) 핵심 엔터티 정보를 도출하는 지식 주입을 위해 엔터티 상태의 신경 지식 기반(Enigma)을 통합하여 ToM 추론을 향상시키는 새로운 신경 기호 프레임워크인 EnigmaToM을 제시합니다. Enigma는 다양한 ToM 주문 전반에 걸쳐 믿음 추적을 위한 공간 장면 그래프를 구축하고 세분화된 엔터티 상태 세부 정보로 이벤트를 강화하기 위해 엔터티 상태에 대한 구조화된 지식을 생성합니다. ToMi, HiToM 및 FANToM 벤치마크에 대한 실험 결과에 따르면 EnigmaToM은 다양한 크기의 LLM 전반에 걸쳐 ToM 추론을 크게 향상시켰으며 특히 고차 추론 시나리오에서 탁월한 성능을 보였습니다."
749,http://arxiv.org/abs/2503.02579 ,MM-OR: A Large Multimodal Operating Room Dataset for Semantic Understanding of High-Intensity Surgical Environments,"Ege Özsoy, Chantal Pellegrini, Tobias Czempiel, Felix Tristram, Kun Yuan, David Bani-Harouni, Ulrich Eck, Benjamin Busam, Matthias Keicher, Nassir Navab","수술실(OR)은 수술 지원, 상황 인식 및 환자 안전을 향상시키기 위해 의료진, 도구 및 장비 간의 상호 작용에 대한 정확한 이해가 필요한 복잡하고 위험한 환경입니다. 현재 데이터세트는 규모와 사실성이 부족하고 수술실 장면의 다중 모드 특성을 포착하지 못하여 수술실 모델링의 진행이 제한됩니다. 이를 위해 현실적이고 대규모의 다중 모드 시공간 OR 데이터 세트이자 다중 모드 장면 그래프 생성을 가능하게 하는 최초의 데이터 세트인 MM-OR을 소개합니다. MM-OR은 RGB-D 데이터, 세부 보기, 오디오, 음성 기록, 로봇 로그 및 추적 데이터를 포함하는 포괄적인 OR 장면을 캡처하고 파노라마 분할, 의미론적 장면 그래프 및 다운스트림 작업 레이블로 주석을 달았습니다. 또한 장면 그래프 생성을 위한 최초의 다중 모드 대형 비전 언어 모델인 MM2SG를 제안하고 광범위한 실험을 통해 다중 모드 입력을 효과적으로 활용하는 능력을 입증합니다. MM-OR과 MM2SG는 함께 전체적 OR 이해를 위한 새로운 벤치마크를 구축하고 복잡하고 위험성이 높은 환경에서 다중 모드 장면 분석을 향한 길을 열어줍니다. 우리의 코드와 데이터는 https://github.com/egeozsoy/MM-OR에서 확인할 수 있습니다."
748,http://arxiv.org/abs/2503.02050 ,DYNEMO-SLAM: Dynamic Entity and Motion-Aware 3D Scene Graph SLAM,"Marco Giberna, Muhammad Shaheer, Miguel Fernandez-Cortizas, Jose Andres Millan-Romera, Jose Luis Sanchez-Lopez, Holger Voos","동적 환경에서 작동하는 로봇은 움직이는 에이전트와 옮겨진 물체로 인해 심각한 문제에 직면합니다. 기존 SLAM 시스템은 일반적으로 정적 세계를 가정하거나 동적 세계를 이상값으로 처리하여 지도 일관성을 유지하기 위해 해당 정보를 삭제합니다. 결과적으로 동적 엔터티를 지속적인 랜드마크로 활용할 수 없고 시간이 지남에 따라 모션을 모델링 및 활용하지 못하므로 신뢰할 수 있는 정적 기능이 거의 없는 매우 복잡한 환경에서 성능이 빠르게 저하됩니다. 본 논문은 SLAM 백엔드에서 동적 엔터티의 포즈를 모델링하고 추정하는 문제를 해결하는 새로운 3D 장면 그래프 기반 SLAM 프레임워크를 제시합니다. 우리의 프레임워크는 통합 그래프 공식 내에서 로봇 궤적, 동적 엔터티 포즈 및 주변 환경 구조를 공동으로 최적화하기 위해 의미론적 모션 사전 및 동적 엔터티 인식 제약 조건을 통합합니다. 이와 동시에 동적 키프레임 선택 정책과 의미론적 루프 폐쇄 사전 필터링 단계를 통해 시스템은 장면 변화에 지속적으로 적응하고 일관되지 않은 관찰을 필터링함으로써 매우 동적인 환경에서 강력하고 효과적인 상태를 유지할 수 있습니다. 시뮬레이션 및 실제 실험 결과는 사용된 기본 방법에 비해 ATE가 49.97% 감소한 것으로 나타났으며, 실시간 성능을 유지하면서 복잡한 시나리오에서 향상된 견고성과 풍부한 장면 표현을 위해 동적 엔터티를 통합하고 포즈를 추정하는 효과를 보여줍니다."
747,http://arxiv.org/abs/2503.01783 ,vS-Graphs: Tightly Coupling Visual SLAM and 3D Scene Graphs Exploiting Hierarchical Scene Understanding,"Ali Tourani, Saad Ejaz, Hriday Bavle, Miguel Fernandez-Cortizas, David Morilla-Cabello, Jose Luis Sanchez-Lopez, Holger Voos","현재 VSLAM(Visual Simultaneous Localization and Mapping) 시스템은 의미가 풍부하고 쉽게 해석할 수 있는 지도를 만드는 데 어려움을 겪는 경우가 많습니다. 의미론적 장면 지식을 통합하면 매핑된 객체 간의 맥락적 연관성을 통해 더욱 풍부한 지도를 구축하는 데 도움이 되지만 장면 그래프와 같은 구조화된 형식으로 표현하는 것은 널리 다루어지지 않아 지도 이해가 복잡해지고 확장성이 제한됩니다. 본 논문에서는 비전 기반 장면 이해와 지도 재구성 및 이해 가능한 그래프 기반 표현을 통합하는 새로운 실시간 VSLAM 프레임워크인 vS-Graphs를 소개합니다. 프레임워크는 감지된 건물 구성요소(예: 벽 및 지상 표면)에서 구조적 요소(예: 방 및 바닥)를 추론하고 이를 최적화 가능한 3D 장면 그래프에 통합합니다. 이 솔루션은 재구성된 지도의 의미론적 풍부함, 이해도 및 위치 파악 정확도를 향상시킵니다. 표준 벤치마크 및 실제 데이터 세트에 대한 광범위한 실험을 통해 vS-Graphs는 최첨단 VSLAM 방법에 비해 테스트된 모든 데이터 세트에서 평균 15.22%의 정확도 향상을 달성하는 것으로 나타났습니다. 또한 제안된 프레임워크는 시각적 특징만을 사용하여 정밀한 LiDAR 기반 프레임워크와 유사한 환경 기반 의미 엔터티 탐지 정확도를 달성합니다. 코드는 https://github.com/snt-arg/visual_sgraphs에서 공개적으로 제공되며 적극적으로 개선되고 있습니다. 또한, https://snt-arg.github.io/vsgraphs-results/에서 더 많은 미디어와 평가 결과를 담은 웹페이지를 확인할 수 있습니다."
746,http://arxiv.org/abs/2503.00548 ,Unbiased Video Scene Graph Generation via Visual and Semantic Dual Debiasing,"Yanjun Li, Zhaoyang Li, Honghui Chen, Lizhi Xu","VidSGG(비디오 장면 그래프 생성)는 비디오 프레임을 순차적으로 분석하고 시각적, 의미적 정보를 통합하여 개체 간의 동적 관계를 캡처하는 것을 목표로 합니다. 그러나 VidSGG는 예측을 왜곡하는 심각한 편향으로 인해 어려움을 겪고 있습니다. 이러한 편견을 완화하기 위해 우리는 편견 없는 VidSGG를 위한 VISA(VIsual and Semantic Awareness) 프레임워크를 제안합니다. VISA는 객체 표현을 향상시키는 동시에 삼중항 관계에서 파생된 포괄적인 의미 정보와 객체 특징을 반복적으로 통합함으로써 의미 편향을 줄이는 메모리 강화 시간적 통합을 통해 시각적 편향을 해결합니다. 이러한 시각적 의미론 이중 편향성 제거 접근 방식을 통해 복잡한 장면 역학을 보다 편견 없이 표현할 수 있습니다. 광범위한 실험을 통해 VISA가 기존의 편견 없는 VidSGG 접근 방식보다 훨씬 뛰어난 성능을 보이는 방법의 효율성이 입증되었습니다(예: Semi Constraint 하의 SGCLS 작업에 대해 mR@20 및 mR@50이 +13.1% 향상됨)."
745,http://arxiv.org/abs/2502.18735 ,QueryAdapter: Rapid Adaptation of Vision-Language Models in Response to Natural Language Queries,"Nicolas Harvey Chapman, Feras Dayoub, Will Browne, Christopher Lehnert","VLM(시각 언어 모델)을 교육하는 데 사용되는 대규모 인터넷 데이터와 로봇이 수집한 원시 이미지 스트림 사이에는 도메인 이동이 존재합니다. 기존 적응 전략은 닫힌 클래스 세트의 정의를 요구하는데, 이는 다양한 자연어 쿼리에 응답해야 하는 로봇에게는 비실용적입니다. 이에 대한 응답으로 QueryAdapter를 제시합니다. 자연어 쿼리에 응답하여 사전 훈련된 VLM을 신속하게 적용하기 위한 새로운 프레임워크입니다. QueryAdapter는 이전 배포 중에 수집된 레이블이 없는 데이터를 활용하여 VLM 기능을 쿼리와 관련된 의미 클래스에 맞춥니다. 학습 가능한 프롬프트 토큰을 최적화하고 훈련할 객체를 적극적으로 선택함으로써 적응된 모델을 단 몇 분 만에 생성할 수 있습니다. 또한 적응을 위해 실제 데이터를 사용할 때 쿼리와 관련 없는 개체를 어떻게 처리해야 하는지 살펴봅니다. 결과적으로 우리는 객체 캡션을 네거티브 클래스 레이블로 사용하여 적응 중에 더 나은 보정된 신뢰도 점수를 생성하는 데 도움을 줄 것을 제안합니다. ScanNet++에 대한 광범위한 실험에서는 QueryAdapter가 최첨단 비지도 VLM 어댑터 및 3D 장면 그래프 방법에 비해 객체 검색 성능을 크게 향상시키는 것으로 나타났습니다. 또한 이 접근 방식은 Ego4D와 같은 추상 어포던스 쿼리 및 기타 데이터 세트에 대한 강력한 일반화를 보여줍니다."
744,http://arxiv.org/abs/2502.18044 ,S-Graphs 2.0 -- A Hierarchical-Semantic Optimization and Loop Closure for SLAM,"Hriday Bavle, Jose Luis Sanchez-Lopez, Muhammad Shaheer, Javier Civera, Holger Voos","3D 장면 그래프의 계층 구조는 인간이 만든 환경의 일반적인 패턴에 적합하므로 표현 목적에 대한 높은 관련성을 보여줍니다. 그러나 추가적으로 이러한 계층적 표현의 의미론적 및 기하학적 정보를 활용하여 지도 요소와 로봇 포즈의 최적화 및 관리 속도를 높일 수 있습니다.   이러한 방향에서 우리는 효율적인 데이터 관리 및 최적화를 위해 실내 장면의 계층 구조를 활용하는 Situational Graphs 2.0(S-Graphs 2.0) 작업을 선보입니다. 우리의 알고리즘은 환경을 키프레임, 벽, 방, 바닥의 네 가지 레이어로 나타내는 상황 그래프를 구성하는 것으로 시작됩니다. 첫 번째 참신함은 계단을 식별하고 바닥 수준 의미 관계를 기본 레이어에 할당할 수 있는 바닥 감지 모듈을 포함하는 프런트 엔드에 있습니다. 바닥 수준 의미론을 사용하면 건물의 서로 다른 층 간의 앨리어싱으로 인해 일반적으로 나타나는 거짓 긍정 폐쇄를 효과적으로 거부하는 바닥 기반 루프 폐쇄 전략을 제안할 수 있습니다. 두 번째 참신함은 최적화에서 표현 계층 구조를 활용하는 것입니다. 우리의 제안은 다음으로 구성됩니다: (1) 최근 키프레임 창과 4개 표현 레이어에 걸쳐 연결된 구성 요소에 대한 로컬 최적화, (2) 루프 폐쇄 중 현재 층 내의 키프레임과 해당 연결에만 초점을 맞추는 바닥 수준 전역 최적화, (3) 공간 내 관찰을 공유하는 중복 키프레임을 소외시켜 계산 공간을 줄이는 공간 수준 로컬 최적화. 우리는 다양한 실제 다층 환경에서 알고리즘을 광범위하게 검증합니다. 우리의 접근 방식은 대규모 다층 환경에서 최첨단 정확도 메트릭을 보여주며, 경쟁 기준보다 평균 최대 10배 더 빠르게 계층적 표현을 추정합니다."
743,http://arxiv.org/abs/2502.16427 ,Fine-Grained Captioning of Long Videos through Scene Graph Consolidation,"Sanghyeok Chu, Seonguk Seo, Bohyung Han","최근 비전 언어 모델의 발전으로 이미지와 짧은 비디오 클립에 대한 캡션 생성이 눈에 띄게 발전했습니다. 그러나 이러한 모델은 제한된 시간적 수용 필드의 제약을 받아 긴 비디오에 대해 일관되고 포괄적인 캡션을 생성하기가 어렵습니다. 비디오 세그먼트 전체에 걸쳐 정보를 집계하기 위해 여러 가지 방법이 제안되었지만 감독된 미세 조정에 의존하거나 상당한 계산 오버헤드가 발생하는 경우가 많습니다. 이러한 문제를 해결하기 위해 그래프 통합을 기반으로 하는 긴 비디오 캡션을 위한 새로운 프레임워크를 소개합니다. 우리의 접근 방식은 먼저 기성 시각적 캡션 모델을 사용하여 개별 프레임 또는 짧은 비디오 간격에 해당하는 세그먼트 수준 캡션을 생성합니다. 그런 다음 이러한 캡션은 개별 장면 그래프로 구문 분석되고, 이후 비디오 전반에 걸쳐 전체적인 맥락과 세밀한 세부 정보를 모두 유지하는 통합 그래프 표현으로 통합됩니다. 그런 다음 경량 그래프-텍스트 디코더가 최종 비디오 수준 캡션을 생성합니다. 이 프레임워크는 긴 비디오 데이터 세트에 대한 추가 미세 조정 없이 기존 모델의 시간 이해 기능을 효과적으로 확장합니다. 실험 결과에 따르면 우리의 방법은 기존 LLM 기반 통합 접근 방식보다 훨씬 뛰어나며 계산 비용을 크게 줄이면서 강력한 제로샷 성능을 달성하는 것으로 나타났습니다."
742,http://arxiv.org/abs/2502.15370 ,Weakly Supervised Video Scene Graph Generation via Natural Language Supervision,"Kibum Kim, Kanghoon Yoon, Yeonjun In, Jaehyeong Jeon, Jinyoung Moon, Donghyun Kim, Chanyoung Park","기존 비디오 장면 그래프 생성(VidSGG) 연구는 비디오의 모든 프레임에 주석을 달아야 하는 완전 감독 방식으로 훈련되므로 이미지 장면 그래프 생성(ImgSGG)에 비해 주석 비용이 높습니다. VidSGG의 주석 비용은 이미지 캡션을 사용하는 ImgSGG(WS-ImgSGG)에 일반적으로 사용되는 약한 감독 방식을 채택하여 완화될 수 있지만 이러한 순진한 채택을 방해하는 두 가지 주요 이유가 있습니다. 1) 비디오 캡션 내의 시간성, 즉 이미지 캡션과 달리 비디오 캡션에는 시간 관련 세부 정보를 나타내는 시간적 마커(예: 이전, 동안, 이후)가 포함되며 2) 작업 기간의 가변성이 있습니다. 즉, 이미지 캡션의 인간 동작과 달리 비디오 캡션의 인간 동작은 다양한 기간에 걸쳐 전개됩니다. 이러한 문제를 해결하기 위해 우리는 VidSGG 모델 교육을 위해 쉽게 사용할 수 있는 비디오 캡션만 활용하는 자연어 기반 비디오 장면 그래프 생성(NL-VSGG) 프레임워크를 제안합니다. NL-VSGG는 시간성 인식 캡션 분할(TCS) 모듈과 동작 지속 시간 가변성 인식 캡션 프레임 정렬(ADV) 모듈의 두 가지 핵심 모듈로 구성됩니다. 구체적으로 TCS는 LLM(Large Language Model)을 기반으로 시간적 순서에 따라 비디오 캡션을 여러 문장으로 분할하고, ADV는 동작 지속 시간의 가변성을 고려하여 분할된 각 문장을 적절한 프레임에 정렬합니다. 우리의 접근 방식은 단순히 Action Genome 데이터세트의 VidSGG에 WS-ImgSGG 파이프라인을 적용하는 것에 비해 성능이 크게 향상됩니다. 약한 감독으로 비디오 캡션을 활용하는 추가 이점으로 우리는 NL-VSGG로 훈련된 VidSGG 모델이 훈련 데이터에 포함되지 않은 더 넓은 범위의 동작 클래스를 예측할 수 있음을 보여 프레임워크를 현실적으로 실용적으로 만듭니다."
741,http://arxiv.org/abs/2502.15309 ,DynamicGSG: Dynamic 3D Gaussian Scene Graphs for Environment Adaptation,"Luzhou Ge, Xiangyu Zhu, Zhuo Yang, Xuesong Li","실제 시나리오에서는 인간이나 에이전트 활동으로 인한 환경 변화로 인해 로봇이 다양한 장기 작업을 수행하는 것이 매우 어려워집니다. 최근 작품은 일반적으로 환경 변화에 따라 메모리의 환경 표현을 업데이트할 수 없고 환경을 세밀하게 재구성하지 못하기 때문에 동적 환경을 효과적으로 이해하고 적응하는 데 어려움을 겪습니다. 이러한 문제를 해결하기 위해 우리는 가우시안 스플래팅을 활용하는 동적, 충실도, 개방형 어휘 장면 그래프 구성 시스템인 DynamicGSG를 제안합니다. DynamicGSG는 고급 비전 언어 모델을 사용하여 환경 내 개체 간의 공간적, 의미적 관계를 표현하는 계층적 장면 그래프를 구축하고, 가우시안 맵을 최적화하면서 가우스 인스턴스 그룹화를 감독하도록 설계된 공동 기능 손실을 활용하고, 장기적인 환경 적응을 위해 실제 환경 변화에 따라 가우스 장면 그래프를 로컬로 업데이트합니다. 실험 및 절제 연구는 의미론적 분할, 언어 기반 개체 검색 및 재구성 품질 측면에서 제안된 방법의 성능과 효율성을 보여줍니다. 또한 실제 실험실 환경에서 시스템의 동적 업데이트 기능을 검증합니다. 소스 코드와 보충 실험 자료는 ~\href{https://github.com/GeLuzhou/Dynamic-GSG}{https://github.com/GeLuzhou/Dynamic-GSG}에서 공개됩니다."
740,http://arxiv.org/abs/2502.14113 ,Object-centric Binding in Contrastive Language-Image Pretraining,"Rim Assouel, Pietro Astolfi, Florian Bordes, Michal Drozdzal, Adriana Romero-Soriano","VLM(비전 언어 모델)의 최근 발전은 시각적 정보를 해당 텍스트 설명과 연관시키는 방법을 학습하는 CLIP과 같은 대조 모델에 의해 주도되었습니다. 그러나 이러한 모델은 여러 객체와 공간 관계가 포함된 복잡한 구성 장면을 이해하는 데 한계가 있습니다. 이러한 과제를 해결하기 위해 우리는 하드 네거티브 증강 설계에 의존하는 일반적으로 사용되는 전략과 다른 새로운 접근 방식을 제안합니다. 대신, 우리의 작업은 사전 훈련된 CLIP 유사 모델에 귀납적 편향을 통합하여 추가 하드 네거티브를 사용하지 않고 구성 이해를 향상시키는 데 중점을 둡니다. 이를 위해 텍스트 설명에서 파생된 장면 그래프를 슬롯 구조의 이미지 표현과 연결하여 두 양식 간의 구조화된 유사성 평가를 용이하게 하는 바인딩 모듈을 소개합니다. 또한 관계를 텍스트 조건에 따른 시각적 제약으로 활용하여 개체 간의 복잡한 상호 작용과 개체의 상황별 관계를 보다 효과적으로 포착합니다. 우리의 결과 모델은 다중 객체 구성 이해에서 CLIP 기반 모델의 성능을 향상시킬 뿐만 아니라 복잡한 장면의 보다 정확하고 샘플 효율적인 이미지-텍스트 매칭을 향한 길을 열어줍니다."
739,http://arxiv.org/abs/2502.10127 ,Leveraging V2X for Collaborative HD Maps Construction Using Scene Graph Generation,"Gamal Elghazaly, Raphael Frank","HD(고해상도) 지도는 자율주행차 내비게이션에서 중요한 역할을 하며 정확성과 안전성 향상을 위해 온보드 인식 센서를 보완합니다. 기존 HD 지도 생성은 전용 매핑 차량에 의존하는데, 이는 비용이 많이 들고 실시간 인프라 변경 사항을 포착하지 못합니다. 본 논문에서는 V2X 통신과 장면 그래프 생성을 활용하여 HD 맵의 국부적인 기하학적 레이어를 공동으로 구성하는 새로운 프레임워크인 HDMapLaneNet을 제시합니다. 전방 카메라 영상에서 차선 중심선을 추출해 그래프로 표현하고, 글로벌 집계용 데이터를 V2X를 통해 클라우드로 전송하는 방식이다. nuScenes 데이터 세트의 예비 결과는 최첨단 방법에 비해 우수한 연관 예측 성능을 보여줍니다."
738,http://arxiv.org/abs/2502.07945 ,SurGrID: Controllable Surgical Simulation via Scene Graph to Image Diffusion,"Yannik Frisch, Ssharvien Kumar Sivakumar, Çağhan Köksal, Elsa Böhm, Felix Wagner, Adrian Gericke, Ghazal Ghazaei, Anirban Mukhopadhyay","수술 시뮬레이션은 기존 수술 훈련에 유망한 추가 기능을 제공합니다. 그러나 사용 가능한 시뮬레이션 도구에는 사실성이 부족하고 하드코딩된 동작에 의존합니다. 노이즈 제거 확산 모델은 충실도가 높은 이미지 합성을 위한 유망한 대안이지만 기존의 최첨단 조절 방법은 생성된 장면에 대한 정확한 제어 또는 상호 작용을 제공하는 데 부족합니다.   장면 그래프를 활용하여 제어 가능한 수술 장면 합성을 가능하게 하는 이미지 확산 모델의 장면 그래프인 SurGrID를 소개합니다. 이러한 그래프는 수술 장면 구성 요소의 공간 및 의미 정보를 인코딩한 다음 로컬 및 글로벌 정보를 명시적으로 캡처하는 새로운 사전 학습 단계를 사용하여 중간 표현으로 변환됩니다.   우리가 제안하는 방법은 생성된 이미지의 충실도와 그래프 입력과의 일관성을 최신 기술보다 향상시킵니다. 또한 임상 전문가가 참여한 사용자 평가 연구를 통해 시뮬레이션의 현실성과 제어 가능성을 입증했습니다.   장면 그래프는 수술 장면을 시뮬레이션하기 위한 Denoising Diffusion Model의 정확한 대화형 조정에 효과적으로 사용될 수 있으며, 생성된 콘텐츠에 대한 높은 충실도와 대화형 제어를 가능하게 합니다."
737,http://arxiv.org/abs/2502.05874 ,MMGDreamer: Mixed-Modality Graph for Geometry-Controllable 3D Indoor Scene Generation,"Zhifei Yang, Keyang Lu, Chao Zhang, Jiaxing Qi, Hanqi Jiang, Ruifei Ma, Shenglin Yin, Yifan Xu, Mingzhe Xing, Zhen Xiao, Jieyi Long, Guangyao Zhai","제어 가능한 3D 장면 생성은 생성된 장면이 기하학 측면에서 높은 수준의 사실성과 제어 가능성을 보여야 하는 가상 현실 및 인테리어 디자인에 광범위하게 적용됩니다. 장면 그래프는 이러한 애플리케이션을 용이하게 하는 적절한 데이터 표현을 제공합니다. 그러나 현재의 그래프 기반 장면 생성 방법은 텍스트 기반 입력으로 제한되어 있으며 유연한 사용자 입력에 대한 적응성이 부족하여 객체 형상을 정확하게 제어하는 ​​기능을 방해합니다. 이 문제를 해결하기 위해 우리는 새로운 혼합 모드 그래프, 시각적 향상 모듈 및 관계 예측기를 통합하는 장면 생성을 위한 이중 분기 확산 모델인 MMGDreamer를 제안합니다. 혼합 양식 그래프를 사용하면 개체 노드가 노드 간 선택적 관계를 통해 텍스트 및 시각적 양식을 통합할 수 있습니다. 유연한 사용자 입력에 대한 적응성을 향상시키고 생성된 장면에서 객체의 기하학적 구조를 세밀하게 제어할 수 있습니다. 시각적 향상 모듈은 텍스트 임베딩을 사용하여 시각적 표현을 구성하여 텍스트 전용 노드의 시각적 충실도를 강화합니다. 또한, 우리의 관계 예측기는 노드 표현을 활용하여 노드 간의 관계가 없는 것을 추론하여 보다 일관된 장면 레이아웃을 생성합니다. 광범위한 실험 결과에 따르면 MMGDreamer는 객체 형상에 대한 탁월한 제어 기능을 보여주며 최첨단 장면 생성 성능을 달성합니다. 프로젝트 페이지: https://yangzhifeio.github.io/project/MMGDreamer."
736,http://arxiv.org/abs/2502.03856 ,Taking A Closer Look at Interacting Objects: Interaction-Aware Open Vocabulary Scene Graph Generation,"Lin Li, Chuhan Zhang, Dong Zhang, Chong Sun, Chen Li, Long Chen","오늘날의 개방형 어휘 장면 그래프 생성(OVSGG)은 미리 정의된 범주를 넘어 새로운 객체와 관계를 인식하고 사전 훈련된 대규모 모델의 지식을 활용하여 기존 SGG를 확장합니다. 대부분의 기존 방법은 2단계 파이프라인을 채택합니다. 이미지 캡션을 사용한 약한 지도 사전 학습과 완전히 주석이 달린 장면 그래프의 SFT(지도 미세 조정)입니다. 그럼에도 불구하고 상호 작용하는 개체에 대한 명시적인 모델링을 생략하고 모든 개체를 동일하게 처리하므로 일치하지 않는 관계 쌍이 발생합니다. 이를 위해 우리는 상호작용 인식 OVSGG 프레임워크 INOVA를 제안합니다. 사전 훈련 중에 INOVA는 상호작용하는 객체와 비상호작용하는 객체를 구별하기 위해 상호작용 인식 목표 생성 전략을 사용합니다. SFT에서 INOVA는 이분 그래프 매칭 중에 상호 작용하는 개체의 우선 순위를 지정하기 위해 상호 작용 기반 쿼리 선택 전략을 고안했습니다. 게다가, INOVA는 상호작용하는 객체 쌍을 배경에서 멀리 밀어내어 견고성을 강화하는 상호작용 일관성 지식 증류 기능을 갖추고 있습니다. 두 가지 벤치마크(VG 및 GQA)에 대한 광범위한 실험에서는 INOVA가 최첨단 성능을 달성하여 실제 응용 프로그램에 대한 상호 작용 인식 메커니즘의 잠재력을 보여줍니다."
735,http://arxiv.org/abs/2502.06819 ,Functional 3D Scene Synthesis through Human-Scene Optimization,"Yao Wei, Matteo Toso, Pietro Morerio, Michael Ying Yang, Alessio Del Bue","본 논문은 장면에 대한 텍스트 설명만으로 3D 실내 환경을 출력하는 새로운 생성적 접근 방식을 제시합니다. 현재 방법은 종종 장면 합성을 단순한 레이아웃 예측 작업으로 처리하여 생성된 환경의 실제 유용성을 제한적으로 고려하여 객체가 겹치거나 장면이 과도하게 구조화되는 방으로 이어집니다. 대신 우리의 접근 방식은 간단하지만 효과적인 원칙을 기반으로 합니다. 즉, 장면 합성을 조정하여 사람이 사용할 수 있는 공간을 생성합니다. 이 원리는 장면을 구성하는 객체와 상호 작용하는 3D 인간을 합성하여 구현됩니다. 인간 중심의 장면 생성이 가능하다면 방 레이아웃은 기능적이며 보다 일관된 3D 구조로 이어집니다. 이를 위해 추론, 3D 조립 및 최적화로 구성된 기능적 3D 장면 합성을 위한 새로운 방법을 제안합니다. 우리는 그래프 확산 네트워크를 통해 장면 그래프를 생성하여 텍스트 기반 3D 합성을 추론 과정으로 간주합니다. 객체 기능적 동시 발생을 고려하여 인간-객체 상호 작용 및 회피를 더 잘 수용하고 인간 인식 3D 장면 최적화를 달성하도록 새로운 전략이 설계되었습니다. 우리는 일관된 3D 장면 합성 결과를 생성하는 방법의 효율성을 검증하기 위해 정성적 및 정량적 실험을 모두 수행합니다."
734,http://arxiv.org/abs/2502.03450 ,Schema-Guided Scene-Graph Reasoning based on Multi-Agent Large Language Model System,"Yiye Chen, Harpreet Sawhney, Nicholas Gydé, Yanan Jian, Jack Saunders, Patricio Vela, Ben Lundell","장면 그래프는 LLM(대형 언어 모델)을 사용한 기반 공간 추론을 위한 구조화되고 직렬화 가능한 환경 표현으로 등장했습니다. 이 작업에서는 다중 에이전트 LLM을 기반으로 하는 반복적인 스키마 기반 장면 그래프 추론 프레임워크인 SG^2를 제안합니다. 에이전트는 추상 작업 계획 및 그래프 정보 쿼리 생성을 위한 (1) Reasoner 모듈과 쿼리에 따른 코드 작성을 기반으로 해당 그래프 정보를 추출하는 (2) Retriever 모듈의 두 가지 모듈로 그룹화됩니다. 두 모듈이 반복적으로 협력하여 그래프 정보에 대한 순차적 추론과 적응형 주의를 지원합니다. 두 모듈 모두에 표시되는 장면 그래프 스키마는 추론 및 검색 프로세스를 모두 간소화할 뿐만 아니라 두 모듈 간의 협력을 안내하는 역할도 합니다. 이렇게 하면 LLM에 전체 그래프 데이터를 표시할 필요가 없어 관련 없는 정보로 인한 환각 가능성이 줄어듭니다. 여러 시뮬레이션 환경에서의 실험을 통해 우리 프레임워크는 숫자 Q&A 및 계획 작업에서 기존 LLM 기반 접근 방식과 기본 단일 에이전트, 도구 기반 검색 중 이유 전략을 능가한다는 것을 보여줍니다."
733,http://arxiv.org/abs/2502.01949 ,LAYOUTDREAMER: Physics-guided Layout for Text-to-3D Compositional Scene Generation,"Yang Zhou, Zongjin He, Qixuan Li, Chao Wang","최근 텍스트 기반 3D 장면 생성 분야가 큰 주목을 받고 있습니다. 실제적인 현실감과 높은 제어 가능성에 부합하는 고품질 생성은 실용적인 3D 장면 애플리케이션에 매우 중요합니다. 그러나 기존 방법은 (i) 텍스트에 설명된 여러 개체 간의 복잡한 관계를 캡처하는 데 어려움이 있고, (ii) 물리적으로 그럴듯한 장면 레이아웃을 생성할 수 없으며, (iii) 구성 장면에서 제어 가능성 및 확장성이 부족하다는 근본적인 한계에 직면해 있습니다. 본 논문에서는 3DGS(3D Gaussian Splatting)를 활용하여 텍스트를 기반으로 하는 고품질의 물리적으로 일관된 구성 장면 생성을 촉진하는 프레임워크인 LayoutDreamer를 소개합니다. 구체적으로, 텍스트 프롬프트가 주어지면 이를 방향성 장면 그래프로 변환하고 초기 구성 3D 가우시안의 밀도와 레이아웃을 적응적으로 조정합니다. 그 후, 엔터티 수준의 생성 품질을 보장하기 위해 훈련 초점을 기반으로 동적 카메라 조정이 이루어집니다. 마지막으로 장면 그래프에서 방향성 종속성을 추출하여 물리적 및 레이아웃 에너지를 조정하여 사실성과 유연성을 모두 보장합니다. 포괄적인 실험을 통해 LayoutDreamer가 다른 구성 장면 생성 품질 및 의미 체계 정렬 방법보다 우수한 것으로 나타났습니다. 특히, T3Bench의 다중 객체 생성 지표에서 최첨단(SOTA) 성능을 달성합니다."
732,http://arxiv.org/abs/2502.00708 ,PhiP-G: Physics-Guided Text-to-3D Compositional Scene Generation,"Qixuan Li, Chao Wang, Zongjin He, Yan Peng","텍스트-3D 자산 생성은 2D 확산 이전 감독하에 상당한 최적화를 달성했습니다. 그러나 구성적인 장면을 다룰 때 기존 방법은 다음과 같은 몇 가지 문제에 직면합니다. 1). 합성 장면 레이아웃이 물리적 법칙을 준수하는지 확인하지 못했습니다. 2). 복잡한 장면 설명에 설명된 자산과 관계를 정확하게 캡처하는 데 어려움이 있습니다. 3). LLM(대형 언어 모델)을 활용하는 레이아웃 접근 방식 중 자율 자산 생성 기능이 제한되어 있습니다. 이러한 타협을 피하기 위해 우리는 생성 기술과 세계 모델 기반 레이아웃 지침을 완벽하게 통합하는 구성 장면 생성을 위한 새로운 프레임워크인 PhiP-G를 제안합니다. PhiP-G는 LLM 기반 에이전트를 활용하여 복잡한 장면 설명을 분석하여 장면 그래프를 생성하고, 대상 자산 생성을 위해 다중 모드 2D 생성 에이전트와 3D ​​가우스 생성 방법을 통합합니다. 레이아웃 단계에서 PhiP-G는 접착 기능을 갖춘 물리적 풀과 시각적 감독 에이전트를 사용하여 레이아웃 예측 및 계획을 위한 세계 모델을 형성합니다. 광범위한 실험을 통해 PhiP-G가 구성 장면의 생성 품질과 물리적 합리성을 크게 향상시키는 것으로 나타났습니다. 특히, PhiP-G는 CLIP 점수에서 최첨단(SOTA) 성능을 달성하고, T$^3$Bench로 측정된 발전 품질의 선도적인 방법과 동등성을 달성하며 효율성을 24배 향상시킵니다."
731,http://arxiv.org/abs/2501.14914 ,Light3R-SfM: Towards Feed-forward Structure-from-Motion,"Sven Elflein, Qunjie Zhou, Sérgio Agostinho, Laura Leal-Taixé","우리는 제한되지 않은 이미지 컬렉션에서 효율적인 대규모 SfM(Structure-from-Motion)을 위한 피드포워드, 엔드투엔드 학습 가능 프레임워크인 Light3R-SfM을 제시합니다. 정확한 3D 재구성을 달성하기 위해 비용이 많이 드는 매칭 및 전역 최적화에 의존하는 기존 SfM 솔루션과 달리 Light3R-SfM은 새로운 잠재 전역 정렬 모듈을 통해 이러한 제한을 해결합니다. 이 모듈은 기존 전역 최적화를 학습 가능한 주의 메커니즘으로 대체하여 강력하고 정확한 카메라 자세 추정을 위해 이미지 전반에 걸쳐 다중 뷰 제약 조건을 효과적으로 캡처합니다. Light3R-SfM은 검색 점수 기반 최단 경로 트리를 통해 희소 장면 그래프를 구성하여 순진한 접근 방식에 비해 메모리 사용량과 계산 오버헤드를 획기적으로 줄입니다. 광범위한 실험을 통해 Light3R-SfM이 런타임을 크게 줄이면서 경쟁력 있는 정확성을 달성하여 런타임 제약이 있는 실제 응용 프로그램의 3D 재구성 작업에 이상적이라는 것이 입증되었습니다. 이 작업은 데이터 중심의 피드포워드 SfM 접근 방식을 개척하여 실제 환경에서 확장 가능하고 정확하며 효율적인 3D 재구성을 향한 길을 열었습니다."
730,http://arxiv.org/abs/2501.14520 ,Scene Understanding Enabled Semantic Communication with Open Channel Coding,"Zhe Xiang, Fei Yu, Quan Deng, Yuandi Li, Zhiguo Wan","통신 시스템이 기호 전송에서 의미 있는 정보 전달로 전환됨에 따라 6세대(6G) 네트워크는 의미론적 통신을 강조합니다. 이 접근 방식은 높은 수준의 의미 정보에 우선 순위를 두어 텍스트, 음성, 이미지와 같은 형식 전반에서 견고성을 향상하고 중복성을 줄입니다. 그러나 전통적인 의미론적 의사소통은 정적 코딩 전략, 잘못된 일반화, 적응성을 방해하는 작업별 지식 기반에 대한 의존 등의 한계에 직면해 있습니다. 이러한 과제를 극복하기 위해 우리는 장면 이해, LLM(대형 언어 모델) 및 개방형 채널 코딩을 결합한 \textbf{OpenSC}라는 새로운 시스템을 제안합니다. 기존 시스템은 고정된 도메인별 지식 기반에 의존하므로 일반화 능력이 제한됩니다. 당사의 개방형 채널 코딩 접근 방식은 공유되고 공개적으로 사용 가능한 지식을 활용하여 유연하고 적응형 인코딩을 가능하게 합니다. 이 동적 시스템은 정적 작업별 데이터에 대한 의존도를 줄여 다양한 작업과 환경에 대한 적응성을 향상시킵니다. 또한 구조화된 의미론적 인코딩을 위해 장면 그래프를 사용하고 개체 관계 및 컨텍스트를 캡처하여 VQA(시각적 질문 응답)와 같은 작업을 개선합니다. 우리의 접근 방식은 핵심 의미 요소를 선택적으로 인코딩하여 중복성을 최소화하고 전송 효율성을 향상시킵니다. 실험 결과는 의미론적 이해와 효율성 모두에서 상당한 개선을 보여 6G 네트워크에서 적응형, 일반화 가능한 의미론적 통신의 잠재력을 향상시켰습니다."
729,http://arxiv.org/abs/2501.14832 ,Resource Allocation Driven by Large Models in Future Semantic-Aware Networks,"Haijun Zhang, Jiaxin Ni, Zijun Wu, Xiangnan Liu, V. C. M. Leung","대형 모델은 미래의 네트워크 지능형 애플리케이션의 인기를 높이는 핵심 요소로 등장했습니다. 그러나 지능형 애플리케이션으로 인해 발생하는 데이터 트래픽의 급증은 미래 네트워크의 자원 활용도와 에너지 소비에 압박을 가하고 있습니다. 효율적인 콘텐츠 이해 기능을 통해 의미론적 통신은 지능형 애플리케이션에서 데이터 전송을 줄이는 데 상당한 잠재력을 가지고 있습니다. 이 기사에서는 의미 인식 네트워크에서 대규모 모델에 의해 구동되는 리소스 할당을 조사합니다. 구체적으로, 효율적인 데이터 전송을 달성하기 위해 장면 그래프 모델과 다중 모드 사전 학습 모델을 기반으로 하는 의미 인식 통신 네트워크 아키텍처가 설계되었습니다. 제안된 네트워크 아키텍처를 기반으로 자원 활용 효율성을 더욱 향상시키기 위한 의미 인식 네트워크의 지능형 자원 할당 기법을 제안한다. 자원 할당 기법에서는 의미론적 전송 품질을 평가 지표로 채택하고 무선 채널 페이딩이 의미론적 전송에 미치는 영향을 분석한다. 다중 사용자에 대한 의미 전송 품질을 극대화하기 위해 의미 인식 네트워크의 전력 할당 문제를 해결하기 위한 확산 모델 기반 의사 결정 체계가 설계되었습니다. 시뮬레이션 결과는 제안된 대규모 모델 기반 네트워크 아키텍처와 자원 할당 기법이 고품질 의미 전달을 달성함을 보여줍니다."
728,http://arxiv.org/abs/2501.09733 ,ComplexVAD: Detecting Interaction Anomalies in Video,"Furkan Mumcu, Michael J. Jones, Yasin Yilmaz, Anoop Cherian","기존 비디오 이상 탐지 데이터 세트는 객체 간 상호 작용으로 인해 발생하는 복잡한 이상 현상을 표현하는 데 적합하지 않습니다. 이전 비디오 이상 탐지 데이터 세트에 복잡한 이상 현상이 없었기 때문에 단순한 이상 현상에 초점을 옮겨 연구에 영향을 미쳤습니다. 이 문제를 해결하기 위해 새로운 대규모 데이터세트인 ComplexVAD를 도입했습니다. 또한, 시공간 속성을 갖는 장면 그래프를 이용하여 객체 간 상호 작용을 모델링함으로써 복잡한 이상 현상을 탐지하는 새로운 방법을 제안합니다. 제안된 방법과 다른 두 가지 최첨단 비디오 이상 탐지 방법을 사용하여 ComplexVAD에 대한 기준 점수를 얻고 새로운 방법이 기존 작업보다 성능이 우수하다는 것을 입증합니다."
727,http://arxiv.org/abs/2501.09041 ,Generative Visual Commonsense Answering and Explaining with Generative Scene Graph Constructing,"Fan Yuan, Xiaoyuan Fang, Rong Quan, Jing Li, Wei Bi, Xiaogang Xu, Piji Li",고도의 시각적 장면 이해를 추구하는 어려운 과제 중 하나로 꼽히는 시각상식추론(Visual Commonsense Reasoning)은 AI 시스템의 추론 능력을 진단하는 데 활용됐다. 그러나 신뢰할 수 있는 추론을 위해서는 장면의 세부 사항을 잘 파악해야 합니다. 기존 작업은 장면 내에 존재하는 실제 개체 관계 정보를 효과적으로 활용하지 못하고 대신 훈련 메모리의 지식에 지나치게 의존합니다. 이러한 관찰을 바탕으로 우리는 먼저 이미지 패치와 LLM을 활용하여 위치가 없는 장면 그래프를 구성한 다음 장면 그래프의 정보를 기반으로 답변하고 설명하는 \textit{\textbf{G2}}라는 새로운 장면 그래프 강화 시각적 상식 추론 생성 방법을 제안합니다. 또한 학습 중에 귀중한 장면 그래프 정보를 흡수하기 위한 자동 장면 그래프 필터링 및 선택 전략을 제안합니다. 장면 그래프 구성 작업과 시각적 상식 답변 및 설명 작업 및 데이터 세트에 대해 광범위한 실험을 수행합니다. 실험 결과와 절제 분석은 제안된 프레임워크의 효율성을 보여줍니다.
726,http://arxiv.org/abs/2501.08575 ,GOTPR: General Outdoor Text-based Place Recognition Using Scene Graph Retrieval with OpenStreetMap,"Donghwi Jung, Keonwoo Kim, Seong-Woo Kim",우리는 GPS 신호가 수신되지 않는 실외 환경을 위해 설계된 강력한 장소 인식 방법인 GOTPR을 제안합니다. 용량이 크고 저장이 어려운 포인트 클라우드 맵을 사용하는 기존 방식과 달리 GOTPR은 텍스트 설명과 지도에서 생성된 장면 그래프를 장소 인식에 활용합니다. 이 방법은 포인트 클라우드를 컴팩트한 데이터 구조로 대체하여 확장성을 향상시켜 로봇이 광범위한 지도 데이터를 효율적으로 저장하고 활용할 수 있도록 해줍니다. 또한 GOTPR은 전역 공간 정보를 제공하는 공개적으로 사용 가능한 OpenStreetMap 데이터를 사용하여 맞춤형 지도 생성의 필요성을 제거합니다. KITTI360Pose 데이터 세트와 해당 OpenStreetMap 데이터를 사용하여 기존 포인트 클라우드 기반 장소 인식 방법과 비교하여 성능을 평가했습니다. 결과는 GOTPR이 스토리지 요구 사항을 크게 줄이면서 비슷한 정확도를 달성한다는 것을 보여줍니다. 도시 규모의 테스트에서는 몇 초 안에 처리가 완료되어 실제 로봇 응용 분야에 매우 실용적입니다. 자세한 내용은 https://donghwijung.github.io/GOTPR_page/에서 확인할 수 있습니다.
725,http://arxiv.org/abs/2501.07214 ,TimeLogic: A Temporal Logic Benchmark for Video QA,"Sirnam Swetha, Hilde Kuehne, Mubarak Shah","인간 인지의 핵심 측면인 시간적 논리적 이해는 복잡한 순차적 사건과 비디오 내 시간적 관계를 포착하는 데 중추적인 역할을 합니다. 이 기능은 일관된 답변을 제공하기 위해 텍스트 데이터와 함께 시간이 지남에 따라 시각적 데이터를 처리하는 것이 목표인 Video QA(비디오 질문 응답)와 같은 작업에서 특히 중요합니다. 그러나 현재 VideoQA 벤치마크에서는 시간적 논리에 주석을 다는 문제로 인해 이 중요한 기술을 평가하는 데 거의 중점을 두지 않습니다. 비전 언어 모델의 발전에도 불구하고 시간적 논리적 추론 능력을 평가하는 것은 여전히 ​​어려운 일입니다. 주로 형식적이고 복잡한 시간적 추론을 요구하는 QA 쌍이 부족하기 때문입니다. 이러한 격차를 해소하기 위해 우리는 시간적 논리적 이해를 평가하도록 특별히 설계된 QA 쌍을 자동으로 생성하는 TimeLogic QA(TLQA) 프레임워크를 도입했습니다. 이를 위해 TLQA는 논리 이론에서 파생된 시간 연산자와 함께 기존 비디오 데이터세트의 시간 주석을 활용하여 이벤트 시퀀스와 시간 관계에 대한 이해를 테스트하는 질문을 구성합니다. TLQA 프레임워크는 일반적이고 확장 가능하며, 시간적 동작 분할 주석이 있는 기존 비디오 동작 데이터세트 또는 시간적 장면 그래프 주석이 있는 비디오 데이터세트를 모두 활용하여 시간적 논리적 질문을 자동으로 생성할 수 있습니다. STAR, Breakfast, AGQA, CrossTask라는 4개의 데이터 세트를 활용하고 각 카테고리에 대해 2,000개 및 10,000개의 QA 쌍을 포함하는 소형(TLQA-S) 및 대형(TLQA-L)의 두 가지 VideoQA 데이터 세트 변형을 생성하여 데이터 세트당 총 32,000개 및 160,000개의 쌍을 생성합니다. 우리는 TLQA를 사용하여 시간적 논리적 이해 기능을 벤치마킹하는 최첨단 VideoQA 모델에 대한 포괄적인 평가를 수행합니다. 우리는 시간적 복잡성이 다양한 16가지 범주의 시간적 논리에 대한 VideoQA 모델의 시간적 추론 성능을 평가합니다."
724,http://arxiv.org/abs/2501.05687 ,UniQ: Unified Decoder with Task-specific Queries for Efficient Scene Graph Generation,"Xinyao Liao, Wei Wei, Dangyang Chen, Yuanyuan Fu","장면 그래프 생성(SGG)은 주어진 이미지 내에서 개체 개체를 식별하고 개체 관계를 추론하는 것을 목표로 하는 장면 이해 작업입니다. 대형 객체 검출기(예: Faster R-CNN)를 기반으로 하는 일반적인 2단계 방법과 달리 1단계 방법은 고정 크기의 학습 가능한 쿼리 세트를 통합하여 관계형 삼중항 <주어, 술어, 객체>를 공동으로 추론합니다. 이 패러다임은 매개변수와 계산 오버헤드가 크게 감소하면서 강력한 성능을 보여줍니다. 그러나 1단계 방법의 문제점은 약한 얽힘 문제에서 비롯됩니다. 여기서 관계에 관련된 엔터티는 세 쌍 내에서 공유되는 결합된 특징과 분리된 시각적 특징을 모두 필요로 합니다. 이전 방법은 결합된 삼중 특징 모델링을 위해 단일 디코더를 채택하거나 별도의 시각적 특징 추출을 위해 다중 디코더를 채택했지만 두 가지를 모두 고려하지 못했습니다. 본 논문에서는 작업별 쿼리가 주제, 개체 및 조건자에 대해 각각 분리된 시각적 특징을 생성하고 통합 디코더를 사용하여 관계형 삼중항 내에서 결합된 기능 모델링을 가능하게 하는 작업별 쿼리 아키텍처를 갖춘 통합 디코더인 UniQ를 소개합니다. Visual Genome 데이터 세트에 대한 실험 결과는 UniQ가 1단계 방법과 2단계 방법 모두에서 우수한 성능을 가지고 있음을 보여줍니다."
723,http://arxiv.org/abs/2501.04303 ,Graph-Based Multimodal Contrastive Learning for Chart Question Answering,"Yue Dai, Soyeon Caren Han, Wei Liu","차트 질문 답변(ChartQA)은 차트 요소의 이질적인 구성과 차트 요소가 인코딩하는 미묘한 데이터 패턴으로 인해 어려움을 겪고 있습니다. 이 작업은 차트 구성 요소와 기본 구조 간의 관계를 명시적으로 모델링하는 새로운 공동 다중 모드 장면 그래프 프레임워크를 소개합니다. 프레임워크는 시각적 그래프와 텍스트 그래프를 모두 통합하여 구조적 및 의미론적 특성을 포착하는 한편, 그래프 대조 학습 전략은 양식 전반에 걸쳐 노드 표현을 정렬하여 소프트 프롬프트로 변환기 디코더에 원활하게 통합할 수 있도록 합니다. 또한 환각을 완화하여 제로 시나리오에서 다중 모드 MLLM(다중 언어 모델)을 향상시키기 위해 맞춤형 CoT(사고 사슬) 프롬프트 세트가 제안되었습니다. ChartQA, OpenCQA 및 ChartX를 포함한 벤치마크에 대한 광범위한 평가는 상당한 성능 개선을 보여주고 제안된 접근 방식의 효율성을 검증합니다."
722,http://arxiv.org/abs/2501.04279 ,OpenIN: Open-Vocabulary Instance-Oriented Navigation in Dynamic Domestic Environments,"Yujie Tang, Meiling Wang, Yinan Deng, Zibo Zheng, Jingchuan Deng, Yufeng Yue","일상적인 가정 환경에서 컵과 같이 자주 사용되는 물건은 위치가 고정되지 않고 동일한 카테고리 내에서 여러 인스턴스가 있는 경우가 많으며 운반자도 자주 변경됩니다. 결과적으로 로봇이 특정 인스턴스를 효율적으로 탐색하는 것이 어려워집니다. 이 과제를 해결하려면 로봇은 장면 변화와 계획을 지속적으로 캡처하고 업데이트해야 합니다. 그러나 현재의 객체 탐색 접근 방식은 주로 의미 수준에 초점을 맞추고 있으며 장면 표현을 동적으로 업데이트하는 기능이 부족합니다. 대조적으로, 이 백서는 자주 사용되는 객체와 정적 캐리어 간의 관계를 포착합니다. 개방형 어휘 CRSG(Carrier-Relationship Scene Graph)를 구성하고 로봇 탐색 중 운반 상태를 업데이트하여 장면의 동적 변화를 반영합니다. CRSG를 기반으로 탐색 프로세스를 Markov 결정 프로세스로 모델링하는 인스턴스 탐색 전략을 추가로 제안합니다. 각 단계에서 결정은 대형 언어 모델의 상식적 지식과 시각적 언어 기능 유사성을 바탕으로 이루어집니다. 우리는 Habitat 시뮬레이터에서 자주 사용되는 일상 항목에 대한 일련의 긴 순서 탐색 작업을 설계했습니다. 결과는 CRSG를 업데이트함으로써 로봇이 이동된 목표를 효율적으로 탐색할 수 있음을 보여줍니다. 또한 실제 로봇에 알고리즘을 배포하고 실제 효율성을 검증했습니다. 프로젝트 페이지는 https://OpenIN-nav.github.io에서 찾을 수 있습니다."
721,http://arxiv.org/abs/2412.20927 ,Enhanced Multimodal RAG-LLM for Accurate Visual Question Answering,"Junxiao Xue, Quan Deng, Fei Yu, Yanhao Wang, Jun Wang, Yuehua Li","GPT-4o, Gemini, LLaVA 및 Flamingo와 같은 다중 모드 대형 언어 모델(MLLM)은 시각적 양식과 텍스트 양식을 통합하는 데 상당한 진전을 이루었으며 시각적 질문 답변(VQA), 이미지 캡션 작성 및 콘텐츠 검색과 같은 작업에서 탁월한 성능을 발휘했습니다. 이미지에 대한 일관되고 상황에 맞는 설명을 생성할 수 있습니다. 그러나 특히 겹치거나 작은 물체가 있는 복잡한 장면에서 물체를 정확하게 식별 및 계산하고 공간 위치를 결정하는 데 여전히 어려움을 겪고 있습니다. 이러한 한계를 해결하기 위해 우리는 구조화된 장면 그래프를 도입하여 객체 인식, 관계 식별 및 이미지 내 공간 이해를 향상시키는 다중 모드 검색 증강 생성(RAG)을 기반으로 하는 새로운 프레임워크를 제안합니다. 우리의 프레임워크는 정밀한 시각적 설명이 필요한 작업을 처리할 수 있는 MLLM의 용량을 향상시키며, 특히 조감도나 조밀한 개체 배열이 있는 장면과 같이 관점이 까다로운 시나리오에서 더욱 그렇습니다. 마지막으로 1인칭 시각적 이해에 초점을 맞춘 VG-150 데이터세트와 항공 이미지가 포함된 AUG 데이터세트에 대한 광범위한 실험을 수행합니다. 결과는 우리의 접근 방식이 VQA 작업에서 기존 MLLM보다 지속적으로 뛰어난 성능을 발휘한다는 것을 보여줍니다. 이는 다양한 공간적 맥락에서 객체를 인식, 위치 파악 및 수량화하는 데 탁월하며 보다 정확한 시각적 설명을 제공합니다."
720,http://arxiv.org/abs/2412.20473 ,Toward Scene Graph and Layout Guided Complex 3D Scene Generation,"Yu-Hsiang Huang, Wei Wang, Sheng-Yu Huang, Yu-Chiang Frank Wang","객체 중심 텍스트-3D 생성의 최근 발전은 인상적인 결과를 보여주었습니다. 그러나 복잡한 3D 장면을 생성하는 것은 객체 간의 복잡한 관계로 인해 여전히 어려운 과제로 남아 있습니다. 더욱이 기존 방법은 주로 SDS(Score Distillation Sampling)를 기반으로 하며, 이는 특정 상호 작용으로 다중 개체를 조작하는 기능을 제한합니다. 이러한 중요하지만 아직 탐구되지 않은 문제를 해결하기 위해 우리는 장면 그래프 및 레이아웃 안내 3D 장면 생성(GraLa3D)의 새로운 프레임워크를 제시합니다. 복잡한 3D 장면을 설명하는 텍스트 프롬프트가 주어지면 GraLa3D는 LLM을 활용하여 레이아웃 경계 상자 정보가 포함된 장면 그래프 표현을 사용하여 장면을 모델링합니다. GraLa3D는 단일 객체 노드와 복합 슈퍼 노드를 사용하여 장면 그래프를 고유하게 구성합니다. 바람직한 레이아웃 내에서 3D 생성을 제한하는 것 외에도 슈퍼 노드에 있는 개체 간의 상호 작용을 모델링하는 동시에 해당 노드 내의 개체 전체에서 모양 누출을 완화하는 데 주요 기여가 있습니다. 우리의 실험에서는 GraLa3D가 위의 한계를 극복하고 텍스트 프롬프트와 밀접하게 일치하는 복잡한 3D 장면을 생성한다는 것을 확인했습니다."
719,http://arxiv.org/abs/2412.19582 ,An Actionable Hierarchical Scene Representation Enhancing Autonomous Inspection Missions in Unknown Environments,"Vignesh Kottayam Viswanathan, Mario Alberto Valdes Saucedo, Sumeet Gajanan Satpute, Christoforos Kanellakis, George Nikolakopoulos",이 기사에서는 다중 모달 임무 계획기인 FLIE: First-Look 기반 검사 및 탐색 계획기와 완전히 통합된 새로운 실행 가능한 계층적 장면 그래프인 계층형 의미 그래프(LSG)를 소개합니다. 이 작업의 참신함은 직관적이고 다중 해상도 장면 표현을 유지하는 작업을 해결하는 동시에 알려지지 않은 환경에서 선험적으로 알려지지 않은 관심 대상에 대한 지속적인 검사 임무 동안 계획 및 장면 이해를 위한 다루기 쉬운 기반을 제공하는 것을 목표로 하는 것에서 비롯됩니다. 제안된 LSG 방식은 통합 FLIE 플래너의 기능에 기초한 추상 개념을 사용하여 여러 추상화 계층에서 로컬로 중첩된 계층적 그래프로 구성됩니다. 또한 LSG는 계층적 표현 내에서 원하는 의미 체계 요소의 추출 및 지역화를 제공하는 실시간 의미 체계 분할 모델을 캡슐화합니다. 이는 검사 플래너의 기능을 확장하여 LSG를 활용하여 관심 있는 특정 의미 체계를 검사하기 위한 정보에 입각한 결정을 내릴 수 있습니다. 우리는 또한 알려지지 않은 환경에서 인간 운영자의 상황 인식을 개선하여 검사 임무를 확장할 수 있는 LSG의 계층적 및 의미론적 경로 계획 기능을 강조합니다. 제안된 방식의 타당성은 시뮬레이션에서 제안된 아키텍처에 대한 광범위한 평가와 도시 야외 환경 설정에서 Boston Dynamics Spot 4족 로봇에 대한 실험적 현장 배포를 통해 입증되었습니다.
718,http://arxiv.org/abs/2412.19571 ,xFLIE: Leveraging Actionable Hierarchical Scene Representations for Autonomous Semantic-Aware Inspection Missions,"Vignesh Kottayam Viswanathan, Mario A. V. Saucedo, Sumeet Gajanan Satpute, Christoforos Kanellakis, George Nikolakopoulos","우리는 의미 인식 검사 임무 동안 계층적 3D 장면 그래프 표현의 점진적인 구축 및 활용을 목표로 하는 새로운 아키텍처를 제시합니다. 특히 이전에 볼 수 없었던 환경에 분산된 대상에 대한 검사 계획은 추론, 탐색 및 장면 이해 중에 장면의 의미 구조를 활용할 수 있는 기회를 제공합니다. 이에 동기를 부여하여 우리는 증분 방식으로 구성되고 실시간 계획 요구를 지원하는 추상화 계층으로 구성되는 계층적 검사 장면 그래프인 3D Layered Semantic Graph(3DLSG)를 제안합니다. 의미 인식 검사 작업을 해결하기 위해 3DLSG와 검사 플래너를 긴밀하게 연결하는 xFLIE(Enhanced First-Look Inspect Explore)라는 임무 프레임워크가 제안되었습니다. 우리는 시뮬레이션과 실험 시험을 통해 성능을 평가하고 3DLSG 모델에 대한 목표 선택, 경로 계획 및 의미 탐색 작업을 평가합니다. 제시된 시나리오는 도시 규모의 분산에서부터 시뮬레이션된 세계의 단일 인프라 대상 및 후속 4족 로봇에 탑재된 실외 및 지하 환경 배포에 이르기까지 다양합니다. 제안된 방법은 임무 목표를 달성하기 위해 3DLSG 표현에 대한 점진적인 구성 및 계획을 성공적으로 보여줍니다. 또한 프레임워크는 검사 임무가 끝날 때 구조화된 인터페이스를 통한 성공적인 의미 탐색 작업을 보여줍니다. 마지막으로, 우리는 다양한 환경 규모에 걸쳐 기존의 체적 맵 기반 방법에 비해 경로 계획 시간이 여러 단계 감소하여 제안된 접근 방식의 계획 효율성과 확장성을 보여줍니다."
717,http://arxiv.org/abs/2412.19021 ,Relation-aware Hierarchical Prompt for Open-vocabulary Scene Graph Generation,"Tao Liu, Rongjie Li, Chongyu Wang, Xuming He",개방형 어휘 장면 그래프 생성(OV-SGG)은 시각적 관계 표현을 개방형 텍스트 표현과 정렬하여 폐쇄 집합 가정의 한계를 극복합니다. 이를 통해 새로운 시각적 관계를 식별할 수 있어 다양한 관계가 있는 실제 시나리오에 적용할 수 있습니다. 그러나 기존 OV-SGG 방법은 고정된 텍스트 표현으로 인해 제한되어 이미지-텍스트 정렬의 다양성과 정확성이 제한됩니다. 이러한 과제를 해결하기 위해 우리는 주체-객체 및 지역별 관계 정보를 통합하여 텍스트 표현을 향상시키는 RAHP(Relation-Aware Hierarchical Prompting) 프레임워크를 제안합니다. 우리의 접근 방식은 개체 클러스터링을 활용하여 관계 삼중 범주의 복잡성을 해결하고 주체-객체 정보의 효과적인 통합을 가능하게 합니다. 또한 우리는 LLM(대형 언어 모델)을 활용하여 상세한 지역 인식 프롬프트를 생성하고 세밀한 시각적 상호 작용을 캡처하며 시각적 양식과 텍스트 양식 간의 정렬을 개선합니다. RAHP는 또한 VLM(Vision-Language Models) 내에 동적 선택 메커니즘을 도입하여 시각적 콘텐츠를 기반으로 관련 텍스트 프롬프트를 적응적으로 선택하고 관련 없는 프롬프트에서 발생하는 노이즈를 줄입니다. Visual Genome 및 Open Images v6 데이터 세트에 대한 광범위한 실험은 우리의 프레임워크가 지속적으로 최첨단 성능을 달성하고 개방형 어휘 장면 그래프 생성 문제를 해결하는 데 있어 효율성을 입증한다는 것을 보여줍니다. 코드는 https://github.com/Leon022/RAHP에서 확인할 수 있습니다.
716,http://arxiv.org/abs/2412.18450 ,3DGraphLLM: Combining Semantic Graphs and Large Language Models for 3D Scene Understanding,"Tatiana Zemskova, Dmitry Yudin","3D 장면 그래프는 존재하는 객체와 객체 간의 의미 관계를 모두 캡처하여 컴팩트한 장면 모델을 나타내므로 로봇 응용 분야에 유망한 구조입니다. 사용자와 효과적으로 상호 작용하려면 내장된 지능형 에이전트가 주변 3D 환경에 대한 광범위한 자연어 쿼리에 응답할 수 있어야 합니다. LLM(대형 언어 모델)은 자연어 이해 및 추론 능력으로 인해 사용자-로봇 상호 작용에 유용한 솔루션입니다. 장면 표현을 학습하는 최근 방법에 따르면 이러한 표현을 3D 세계에 적용하면 LLM 응답의 품질이 크게 향상될 수 있습니다. 그러나 기존 방법은 일반적으로 객체 좌표와 같은 기하학적 정보에만 의존하고 객체 간의 풍부한 의미 관계를 간과합니다. 본 연구에서는 의미론적 관계를 명시적으로 통합하는 3D 장면 그래프의 학습 가능한 표현을 구성하는 방법인 3DGraphLLM을 제안합니다. 이 표현은 3D 비전 언어 작업을 수행하기 위해 LLM에 대한 입력으로 사용됩니다. 인기 있는 ScanRefer, Multi3DRefer, ScanQA, Sqa3D 및 Scan2cap 데이터세트에 대한 실험에서 우리는 우리의 접근 방식이 객체 간의 의미론적 관계를 활용하지 않는 기준선보다 성능이 우수하다는 것을 입증했습니다. 코드는 https://github.com/CognitiveAISystems/3DGraphLLM에서 공개적으로 제공됩니다."
715,http://arxiv.org/abs/2412.18381 ,MR-COGraphs: Communication-efficient Multi-Robot Open-vocabulary Mapping System via 3D Scene Graphs,"Qiuyi Gu, Zhaocheng Ye, Jincheng Yu, Jiahao Tang, Tinghao Yi, Yuhan Dong, Jian Wang, Jinqiang Cui, Xinlei Chen, Yu Wang",알려지지 않은 환경에서의 협업 인식은 다중 로봇 시스템에 매우 중요합니다. 기초 모델의 출현으로 로봇은 이제 기하학적 정보를 인식할 수 있을 뿐만 아니라 개방형 어휘 장면 이해도 달성할 수 있습니다. 그러나 개방형 어휘 쿼리를 지원하는 기존 지도 표현에는 대용량 데이터가 포함되는 경우가 많아 통신이 제한된 환경에서 다중 로봇 전송에 병목 현상이 발생합니다. 이 문제를 해결하기 위해 우리는 COGraph라는 그래프 구조의 3D 표현을 구성하는 방법을 개발합니다. 여기서 노드는 의미론적 특징을 가진 객체를 나타내고 가장자리는 공간적 인접 관계를 캡처합니다. 전송하기 전에 COGraph의 기능 크기를 압축하기 위해 데이터 기반 기능 인코더가 적용됩니다. 다른 로봇으로부터 COGraph를 수신하면 디코더를 사용하여 각 노드의 의미적 특징을 복구합니다. 또한 장소 인식 및 번역 추정을 위한 특징 기반 접근 방식을 제안하여 지역 COGraph를 통합된 글로벌 지도로 병합할 수 있습니다. 우리는 두 가지 현실적인 데이터 세트와 실제 환경에서 프레임워크를 검증합니다. 결과는 개방형 어휘 맵 구성을 위한 기존 기준과 비교하여 우리 프레임워크가 매핑 및 쿼리 성능을 손상 없이 유지하면서 데이터 볼륨을 80% 이상 줄인다는 것을 보여줍니다. 자세한 내용은 당사 웹사이트(https://github.com/efc-robot/MR-COGraphs)를 참조하세요.
714,http://arxiv.org/abs/2412.15199 ,LiDAR-RT: Gaussian-based Ray Tracing for Dynamic LiDAR Re-simulation,"Chenxu Zhou, Lvchang Fu, Sida Peng, Yunzhi Yan, Zhanhua Zhang, Yong Chen, Jiazhi Xia, Xiaowei Zhou","이 문서에서는 동적 주행 시나리오에서 실시간 LiDAR 재시뮬레이션 문제를 다루고 있습니다. 최근 접근 방식에서는 LiDAR 센서의 물리적 모델링과 결합된 신경 방사 필드를 활용하여 충실도가 높은 재시뮬레이션 결과를 얻습니다. 불행하게도 이러한 방법은 대규모 장면에서 높은 계산 요구로 인해 한계에 직면하고 실시간 LiDAR 렌더링을 수행할 수 없습니다. 이러한 제약을 극복하기 위해 우리는 운전 장면에 대한 실시간, 물리적으로 정확한 LiDAR 재시뮬레이션을 지원하는 새로운 프레임워크인 LiDAR-RT를 제안합니다. 우리의 주요 기여는 가우스 프리미티브와 하드웨어 가속 광선 추적 기술을 통합하는 효율적이고 효과적인 렌더링 파이프라인을 개발하는 것입니다. 구체적으로, 우리는 학습 가능한 매개변수가 있는 가우스 프리미티브를 사용하여 LiDAR 센서의 물리적 특성을 모델링하고 장면 그래프를 통합하여 장면 역학을 처리합니다. 이 장면 표현을 기반으로 프레임워크는 먼저 BVH(Bounding Volume Hierarchy)를 구성한 다음 각 픽셀에 대해 광선을 투사하고 미분 가능한 렌더링 알고리즘을 통해 새로운 LiDAR 보기를 생성합니다. 중요한 것은 우리 프레임워크가 유연한 장면 편집 작업과 다양한 센서 구성을 통해 사실적인 렌더링을 지원한다는 것입니다. 여러 공개 벤치마크에 대한 광범위한 실험을 통해 우리의 방법이 렌더링 품질과 효율성 측면에서 최첨단 방법보다 우수하다는 것을 보여줍니다. 우리 프로젝트 페이지는 https://zju3dv.github.io/lidar-rt에 있습니다."
713,http://arxiv.org/abs/2412.14480 ,GraphEQA: Using 3D Semantic Scene Graphs for Real-time Embodied Question Answering,"Saumya Saxena, Blake Buchanan, Chris Paxton, Peiqi Liu, Bingqing Chen, Narunas Vaskevicius, Luigi Palmieri, Jonathan Francis, Oliver Kroemer","EQA(Embodied Question Answering)에서 에이전트는 상황에 맞는 질문에 자신 있게 대답하기 위해 보이지 않는 환경에 대한 의미론적 이해를 탐색하고 개발해야 합니다. 이 문제는 유용한 의미 표현을 얻고, 이러한 표현을 온라인으로 업데이트하고, 효율적인 계획 및 탐색을 위해 사전 세계 지식을 활용하는 데 어려움이 있기 때문에 로봇공학에서는 여전히 어려운 과제입니다. 이러한 제한 사항을 해결하기 위해 우리는 실시간 3D 메트릭 의미 장면 그래프(3DSG)와 작업 관련 이미지를 비전 언어 모델(VLM) 기반의 다중 모달 메모리로 활용하여 보이지 않는 환경에서 EQA 작업을 수행하는 새로운 접근 방식인 GraphEQA를 제안합니다. 우리는 구조화된 계획 및 의미 기반 탐색을 위해 3DSG의 계층적 특성을 활용하는 계층적 계획 접근 방식을 사용합니다. 우리는 GraphEQA를 HM-EQA 및 OpenEQA라는 두 가지 벤치마크 데이터 세트에 대한 시뮬레이션으로 평가하고, 더 높은 성공률과 더 적은 계획 단계로 EQA 작업을 완료하여 주요 기준을 능가하는 성능을 입증합니다. 우리는 여러 실제 가정 및 사무실 환경에서 GraphEQA를 추가로 시연합니다."
712,http://arxiv.org/abs/2412.13652 ,RelationField: Relate Anything in Radiance Fields,"Sebastian Koch, Johanna Wald, Mirco Colosi, Narunas Vaskevicius, Pedro Hermosilla, Federico Tombari, Timo Ropinski",신경 복사 필드는 새로운 3D 장면 표현이며 최근에는 비전 언어 모델에서 개방형 어휘 기능을 추출하여 장면 이해를 위한 기능을 학습하도록 확장되었습니다. 그러나 현재 방법은 주로 객체 중심 표현에 중점을 두고 객체 분할 또는 감지를 지원하는 반면 객체 간의 의미론적 관계를 이해하는 것은 거의 탐구되지 않은 상태로 남아 있습니다. 이러한 격차를 해결하기 위해 우리는 신경 방사 필드에서 직접 개체 간 관계를 추출하는 첫 번째 방법인 RelationField를 제안합니다. RelationField는 신경 방사 필드 내의 광선 쌍으로 객체 간의 관계를 나타내며 암시적 관계 쿼리를 포함하도록 공식을 효과적으로 확장합니다. RelationField의 복잡하고 개방적인 어휘 관계를 가르치기 위해 관계 지식은 다중 모드 LLM에서 추출됩니다. RelationField를 평가하기 위해 개방형 어휘 3D 장면 그래프 생성 작업과 관계 기반 인스턴스 분할을 해결하여 두 작업 모두에서 최첨단 성능을 달성합니다. https://relationfield.github.io에서 프로젝트 웹사이트를 참조하세요.
711,http://arxiv.org/abs/2412.13646 ,Transmit What You Need: Task-Adaptive Semantic Communications for Visual Information,"Jeonghun Park, Sung Whan Yoon","최근 의미론적 의사소통은 Shannon 이론의 한계를 뛰어넘는 획기적인 개념으로 큰 주목을 받고 있습니다. 특히, 의미론적 통신은 대규모 네트워크 트래픽이 필요한 시각적 작업을 실현하는 데 매우 중요할 것입니다. 컴퓨터 비전 작업에는 매우 독특한 형태의 시각적 의미론이 존재하지만, 어떤 시각적 의미론이 시간에 따라 전송될 수 있는지, 그리고 다른 시각적 의미론이 다양한 시각적 작업을 완료하는 데 필요한지에 대한 철저한 조사는 아직 보고되지 않았습니다. 이를 위해 먼저 제한된 무선 통신 대역폭을 통해 기존 시각적 의미를 전송할 때 달성 가능한 처리량을 면밀히 조사합니다. 또한 각 시각적 의미 체계에 대한 다양한 시각적 작업의 결과 성능을 추가로 보여줍니다. 경험적 테스트를 기반으로 우리는 분류와 같은 간단한 시각적 작업에 대한 기본 의미(예: 주어진 이미지의 개체)와 이미지 재생성과 같은 복잡한 작업에 대한 보다 풍부한 의미(예: 장면 그래프)를 전송하는 시각적 작업에 대한 실시간 의미 통신에 작업 적응형 시각적 의미 체계 선택이 중요하다고 제안합니다. 전송 효율성을 더욱 향상시키기 위해 장면 그래프에 중복된 정보를 삭제하여 주어진 작업을 완료하는 데 필수적인 의미를 보낼 수 있는 장면 그래프 필터링 방법을 제안합니다. 우리는 무선 채널의 광범위한 시뮬레이션을 통해 작업 적응형 의미론적 통신 접근 방식의 효율성을 확인했으며, 이는 원본 데이터의 순진한 전송에 비해 45배 이상 더 큰 처리량을 보여줍니다. 우리의 작업은 다음 소스 코드에서 재현될 수 있습니다: https://github.com/jhpark2024/jhpark.github.io"
710,http://arxiv.org/abs/2412.12788 ,RA-SGG: Retrieval-Augmented Scene Graph Generation Framework via Multi-Prototype Learning,"Kanghoon Yoon, Kibum Kim, Jaehyung Jeon, Yeonjun In, Donghyun Kim, Chanyoung Park","SGG(장면 그래프 생성) 연구는 두 가지 근본적인 문제, 즉 긴 꼬리 술어 분포와 술어 간의 의미적 모호성으로 인해 어려움을 겪었습니다. 이러한 과제는 SGG 모델에서 상위 술어에 대한 편향으로 이어지며, 세밀한 술어를 간과하면서 지배적인 일반 술어를 선호합니다. 본 논문에서는 세밀한 술어의 관련 레이블이 누락된 부분 주석이 있는 다중 레이블 분류 문제로 SGG의 문제를 해결합니다. 새로운 프레임에서 우리는 다중 레이블이 지정될 잠재적 인스턴스를 식별하고 확립된 메모리 뱅크에서 관련 샘플을 검색하여 원래 레이블과 의미상 유사한 다중 레이블로 단일 레이블을 강화하는 RA-SGG(검색 증강 장면 그래프 생성)를 제안합니다. 증강 관계(즉, 발견된 다중 레이블)를 기반으로 다중 프로토타입 학습을 적용하여 SGG 모델을 훈련합니다. 여러 포괄적인 실험을 통해 RA-SGG가 특히 F@K 측면에서 VG에서 최대 3.6%, GQA에서 5.9%까지 최첨단 기준선보다 성능이 우수하다는 사실이 입증되었습니다. 이는 RA-SGG가 술어의 긴 꼬리 분포 및 의미적 모호함으로 인해 발생하는 편향된 예측 문제를 효과적으로 완화한다는 것을 보여줍니다."
709,http://arxiv.org/abs/2412.11396 ,Leveraging Retrieval-Augmented Tags for Large Vision-Language Understanding in Complex Scenes,"Antonio Carlos Rivera, Anthony Moore, Steven Robinson","비전 언어 작업의 객체 인식 추론은 특히 보이지 않는 객체를 처리하고, 환각을 줄이고, 복잡한 시각적 장면에서 세밀한 관계를 캡처하는 데 있어 현재 모델에 심각한 과제를 제기합니다. 이러한 제한 사항을 해결하기 위해 우리는 검색 증강 개체 태그를 프롬프트에 통합하여 LVLM(Large Vision-Language Model)을 향상시키는 생성적 접근 방식인 VRAP(Vision-Aware Retrieval-Augmented Prompting) 프레임워크를 제안합니다. VRAP는 사전 학습된 시각적 인코더와 장면 그래프 파서를 사용하여 개체, 속성 및 관계를 포함한 구조화된 태그를 추출하는 새로운 파이프라인을 도입합니다. 이러한 태그는 외부 지식으로 풍부해지고 LLM의 입력에 통합되어 상세하고 정확한 추론이 가능해집니다. 우리는 VQAv2, GQA, VizWiz 및 COCO를 포함한 여러 비전 언어 벤치마크에서 VRAP를 평가하여 세분화된 추론 및 다중 모드 이해에서 최첨단 성능을 달성합니다. 또한, 우리의 절제 연구는 검색 증강 태그와 대조 학습의 중요성을 강조하는 반면, 인간 평가는 VRAP가 정확하고 상세하며 상황에 맞게 관련 있는 응답을 생성하는 능력을 확인합니다. 특히 VRAP는 런타임 검색을 제거하여 추론 지연 시간을 40% 감소시킵니다. 이러한 결과는 VRAP가 객체 인식 다중 모드 추론을 발전시키기 위한 강력하고 효율적인 프레임워크임을 보여줍니다."
708,http://arxiv.org/abs/2412.11026 ,SceneLLM: Implicit Language Reasoning in LLM for Dynamic Scene Graph Generation,"Hang Zhang, Zhuoling Li, Jun Liu","동적 장면에는 모바일 로봇, UAV 및 자율 주행 시스템이 현명한 결정을 내리는 데 중요한 복잡한 시공간 정보가 포함되어 있습니다. 정확한 장면 그래프 생성(SGG)을 위해 이러한 장면을 의미론적 삼중항 <주체-술어-객체>로 구문 분석하는 것은 변동하는 시공간적 복잡성으로 인해 매우 어렵습니다. LLM(대형 언어 모델)의 추론 기능에서 영감을 받아 LLM을 동적 SGG를 위한 강력한 장면 분석기로 활용하는 새로운 프레임워크인 SceneLLM을 제안합니다. 우리의 프레임워크는 비디오 프레임을 언어 신호(장면 토큰)로 변환하는 V2L(비디오-언어) 매핑 모듈을 도입하여 LLM이 입력을 더 이해하기 쉽게 만듭니다. 공간 정보를 더 잘 인코딩하기 위해 우리는 공간 데이터를 토큰으로 인코딩하는 한자의 구조에서 영감을 받은 공간 정보 집계(SIA) 방식을 고안했습니다. OT(Optimal Transport)를 사용하여 비디오의 시공간 정보를 캡처하는 프레임 수준 토큰 시퀀스에서 암시적 언어 신호를 생성합니다. 이러한 암시적 언어 입력을 처리하는 LLM의 기능을 더욱 향상시키기 위해 LoRA(Low-Rank Adaptation)를 적용하여 모델을 미세 조정합니다. 마지막으로 변환기 기반 SGG 예측기를 사용하여 LLM의 추론을 디코딩하고 의미론적 삼중항을 예측합니다. 우리의 방법은 AG(Action Genome) 벤치마크에서 최첨단 결과를 달성했으며, 광범위한 실험을 통해 정확한 동적 장면 그래프를 이해하고 생성하는 데 있어 SceneLLM의 효율성이 입증되었습니다."
707,http://arxiv.org/abs/2412.10436 ,Benchmarking Federated Learning for Semantic Datasets: Federated Scene Graph Generation,"SeungBum Ha, Taehwan Lee, Jiyoun Lim, Sung Whan Yoon",연합 학습(FL)은 데이터 개인 정보 보호를 유지하면서 분산형 훈련을 가능하게 하지만 기존 FL 벤치마크는 각 샘플에 원-핫 레이블로 주석이 추가되는 상대적으로 간단한 분류 작업을 다룹니다. 그러나 각 샘플이 객체 간의 관계와 같은 다양한 의미 정보를 포함하는 복잡한 의미를 처리하는 FL 벤치마크를 시연하는 데는 거의 관심이 기울이지 않았습니다. 기존 벤치마크는 단일 의미 체계의 좁은 관점에서 데이터를 배포하도록 설계되었기 때문에 FL 벤치마크를 공식화할 때 클라이언트 간의 복잡한 의미 체계 이질성을 관리하는 것은 쉽지 않습니다. 본 논문에서는 클라이언트 간에 제어 가능한 의미 이질성을 갖춘 FL 벤치마크를 설정하기 위한 벤치마크 프로세스를 제안합니다. 두 가지 주요 단계는 (i) 의미론을 사용한 데이터 클러스터링과 (ii) 제어 가능한 의미 이질성을 통해 클라이언트 전체에 데이터를 배포하는 것입니다. 개념 증명으로 연합 PSG 벤치마크를 구성하여 장면 그래프의 의미론적 이질성을 제어할 수 있는 FL 설정에서 기존 PSG 방법의 효율성을 입증합니다. 또한 향상된 성능을 보여주기 위해 데이터 이질성에 강력한 연합 학습 알고리즘을 적용하여 벤치마크의 효율성을 제시합니다. 우리가 아는 바로는 이는 제어된 의미론적 이질성 하에서 다중 의미론적 비전 작업에 대한 연합 학습 및 평가를 가능하게 하는 최초의 벤치마크 프레임워크입니다. 우리 코드는 https://github.com/Seung-B/FL-PSG에서 확인할 수 있습니다.
706,http://arxiv.org/abs/2412.08614 ,Benchmarking Large Vision-Language Models via Directed Scene Graph for Comprehensive Image Captioning,"Fan Lu, Wei Wu, Kecheng Zheng, Shuailei Ma, Biao Gong, Jiawei Liu, Wei Zhai, Yang Cao, Yujun Shen, Zheng-Jun Zha","이미지의 텍스트가 풍부한 시각적 콘텐츠를 포함하는 자세한 캡션을 생성하는 것은 LVLM(Large Vision-Language Model)에 대한 관심이 높아지고 있습니다. 그러나 세부 캡션의 정확성과 포괄성을 측정하기 위해 특별히 맞춤화된 벤치마크를 개발한 연구는 거의 없습니다. 본 논문에서는 방향성 장면 그래프 보기에서 시각적 컨텍스트를 평가하기 위해 CompreCap이라는 상세한 캡션 벤치마크를 소개합니다. 구체적으로, 우리는 먼저 공통 객체 어휘에 따라 이미지를 의미상 의미 있는 영역(즉, 의미론적 분할 마스크)으로 수동으로 분할하는 동시에 모든 해당 영역 내에서 객체의 속성을 구별합니다. 그런 다음 이러한 객체의 방향 관계 레이블에 주석을 달아 이미지의 풍부한 구성 정보를 잘 인코딩할 수 있는 방향성 장면 그래프를 구성합니다. 방향성 장면 그래프를 기반으로 우리는 객체 수준 적용 범위, 속성 설명의 정확성, 주요 관계 점수 등을 포함하여 여러 수준에서 LVLM에서 생성된 세부 캡션을 평가하는 파이프라인을 개발합니다. CompreCap 데이터 세트에 대한 실험 결과는 우리의 평가 방법이 LVLM 전반에 걸쳐 사람의 평가 점수와 밀접하게 일치한다는 것을 확인합니다."
705,http://arxiv.org/abs/2412.08580 ,LAION-SG: An Enhanced Large-Scale Dataset for Training Complex Image-Text Models with Structural Annotations,"Zejian Li, Chenye Meng, Yize Li, Ling Yang, Shengyuan Zhang, Jiarui Ma, Jiayi Li, Guang Yang, Changyuan Yang, Zhiyuan Yang, Jinxiong Chang, Lingyun Sun","최근 T2I(text-to-image) 생성의 발전으로 텍스트에서 고품질 이미지를 생성하는 데 놀라운 성공이 나타났습니다. 그러나 기존 T2I 모델은 여러 객체와 복잡한 관계를 포함하는 구성 이미지 생성에서 성능 저하를 보여줍니다. 우리는 이 문제를 프롬프트만 있는 정확한 객체 간 관계 주석이 부족한 기존 이미지-텍스트 쌍 데이터 세트의 제한 사항으로 인해 발생합니다. 이 문제를 해결하기 위해 우리는 장면 그래프(SG)의 고품질 구조 주석이 포함된 대규모 데이터 세트인 LAION-SG를 구축합니다. 이 데이터 세트는 여러 개체의 속성과 관계를 정확하게 설명하고 복잡한 장면의 의미 구조를 효과적으로 나타냅니다. LAION-SG를 기반으로 구조적 주석 정보를 생성 프로세스에 통합하기 위해 새로운 기반 모델 SDXL-SG를 교육합니다. 광범위한 실험을 통해 LAION-SG에서 훈련된 고급 모델은 기존 데이터 세트의 모델에 비해 복잡한 장면 생성에서 상당한 성능 향상을 자랑하는 것으로 나타났습니다. 또한 구성 이미지 생성에 대한 모델을 평가하는 벤치마크인 CompSG-Bench를 소개하여 이 영역에 대한 새로운 표준을 확립했습니다. 관련 처리 코드, 기초 모델 및 벤치마크 프로토콜이 포함된 주석은 https://github.com/mengcye/LAION-SG에서 공개적으로 제공됩니다."
704,http://arxiv.org/abs/2412.08221 ,Generate Any Scene: Scene Graph Driven Data Synthesis for Visual Generation Training,"Ziqi Gao, Weikai Huang, Jieyu Zhang, Aniruddha Kembhavi, Ranjay Krishna","텍스트-비전 생성의 최근 발전은 시각적 충실도 측면에서 탁월하지만 구성 일반화 및 의미 체계 정렬에 어려움을 겪고 있습니다. 기존 데이터 세트는 시끄럽고 구성이 약하여 복잡한 장면에 대한 모델의 이해가 제한되는 반면 조밀하고 고품질 주석을 위한 확장 가능한 솔루션은 여전히 ​​과제로 남아 있습니다. 가능한 시각적 장면의 조합 배열을 나타내는 장면 그래프를 체계적으로 열거하는 데이터 엔진인 모든 장면 생성을 소개합니다. 모든 장면 생성은 개체, 속성 및 관계의 구조화된 분류에서 다양한 복잡성의 장면 그래프를 동적으로 구성합니다. 샘플링된 장면 그래프가 주어지면 모든 장면 생성은 이를 텍스트-이미지 또는 텍스트-비디오 생성을 위한 캡션으로 변환합니다. 또한 이를 의미 체계 정렬의 자동 평가 및 보상 모델링을 허용하는 일련의 시각적 질문 답변으로 변환합니다. 모든 장면 생성을 사용하여 먼저 모델이 생성된 데이터를 사용하여 성능을 반복적으로 향상시키는 자체 개선 프레임워크를 설계합니다. Stable Diffusion v1.5는 기준선에 비해 평균 4% 개선을 달성하고 CC3M의 미세 조정을 능가합니다. 둘째, 독점 모델의 특정 강점을 오픈 소스 모델로 전달하기 위한 증류 알고리즘도 설계합니다. 800개 미만의 합성 캡션을 사용하여 Stable Diffusion v1.5를 미세 조정하고 구성 및 하드 컨셉 생성에서 TIFA 점수를 10% 높였습니다. 셋째, 저렴한 비용으로 모델 생성을 의미론적 정확성과 일치시키는 보상 모델을 만듭니다. GRPO 알고리즘을 사용하여 SimpleAR-0.5B-SFT를 미세 조정하고 DPG-Bench에서 CLIP 기반 방법을 +5% 능가합니다. 마지막으로, 우리는 이러한 아이디어를 합성 데이터로부터 학습하여 어려운 사례를 식별하도록 모델을 훈련시키는 콘텐츠 조정의 다운스트림 작업에 적용합니다."
703,http://arxiv.org/abs/2412.07160 ,Motion-aware Contrastive Learning for Temporal Panoptic Scene Graph Generation,"Thong Thanh Nguyen, Xiaobao Wu, Yi Bin, Cong-Duy T Nguyen, See-Kiong Ng, Anh Tuan Luu","인공 지능에 시간적 세계에 대한 포괄적인 이해를 제공하기 위해 비디오 및 4D 파노라마 장면 그래프 생성은 시각적 데이터를 노드로 추상화하여 엔터티와 가장자리를 표현하고 시간적 관계를 포착합니다. 기존 방법은 시간 차원(마스크 튜브)에 걸쳐 추적된 엔터티 마스크를 인코딩한 다음 엔터티 관계를 나타내는 모션을 완전히 활용하지 못하는 시간 풀링 작업을 통해 관계를 예측합니다. 이러한 한계를 극복하기 위해 시간적 장면 그래프 생성을 위한 모션 패턴에 초점을 맞춘 대조 표현 학습 프레임워크를 소개합니다. 첫째, 우리의 프레임워크는 모델이 유사한 주제-관계-객체 삼중항의 마스크 튜브에 대한 긴밀한 표현을 학습하도록 권장합니다. 둘째, 우리는 일시적으로 섞인 버전에서 마스크 튜브를 분리하려고 합니다. 또한, 동일한 비디오에 속하지만 서로 다른 삼중항에 속하는 마스크 튜브에 대한 원거리 표현도 학습합니다. 광범위한 실험을 통해 모션 인식 대비 프레임워크가 비디오 및 4D 데이터 세트 모두에서 최첨단 방법을 크게 향상시키는 것으로 나타났습니다."
702,http://arxiv.org/abs/2412.07012 ,ProVision: Programmatically Scaling Vision-centric Instruction Data for Multimodal Language Models,"Jieyu Zhang, Le Xue, Linxin Song, Jun Wang, Weikai Huang, Manli Shu, An Yan, Zixian Ma, Juan Carlos Niebles, Silvio Savarese, Caiming Xiong, Zeyuan Chen, Ranjay Krishna, Ran Xu","다중 모드 애플리케이션이 증가하면서 명령 데이터는 복잡한 이미지 기반 쿼리를 이해할 수 있는 다중 모드 언어 모델을 훈련하는 데 매우 중요해졌습니다. 기존 방식에서는 강력하지만 비용이 많이 드는 LLM(대형 언어 모델) 또는 MLM(다중 모드 언어 모델)을 사용하여 명령 데이터를 생성합니다. 이는 종종 환각, 라이선스 문제가 발생하기 쉽고 생성 프로세스를 확장하고 해석하기 어려운 경우가 많습니다. 이 연구에서는 장면 그래프를 이미지의 상징적 표현으로 사용하고 인간이 작성한 프로그램을 사용하여 비전 중심 명령 데이터를 체계적으로 합성하는 프로그래밍 방식을 제시합니다. 우리의 접근 방식은 데이터 생성 프로세스의 해석 가능성과 제어 가능성을 보장하고 사실적 정확성을 유지하면서 효율적으로 확장합니다. 24개의 단일 이미지, 14개의 다중 이미지 명령 생성기 및 장면 그래프 생성 파이프라인을 구현하여 확장 가능하고 비용 효율적인 시스템인 ProVision을 구축합니다. ProVision은 주어진 이미지에 대해 개체, 속성, 관계, 깊이 등에 관한 다양한 질문-답변 쌍을 생성합니다. Visual Genome 및 DataComp 데이터 세트에 적용하여 천만 개가 넘는 명령 데이터 포인트인 ProVision-10M을 생성하고 MLM의 사전 훈련 및 명령 조정 단계에서 이를 활용합니다. 명령 튜닝 단계에서 채택하면 단일 이미지 명령 데이터는 CVBench의 2D 분할에서 최대 7%, 3D 분할에서 8% 향상되고 QBench2, RealWorldQA 및 MMMU에서 성능이 3% 향상됩니다. 다중 이미지 명령 데이터로 인해 Mantis-Eval이 8% 향상되었습니다. xGen-MM-4B의 사전 훈련 및 미세 조정 단계에 데이터를 통합하면 11개 벤치마크에서 평균 1.6%의 개선이 이루어졌습니다."
701,http://arxiv.org/abs/2412.06322 ,LLaVA-SpaceSGG: Visual Instruct Tuning for Open-vocabulary Scene Graph Generation with Enhanced Spatial Relations,"Mingjie Xu, Mengyang Wu, Yuzhi Zhao, Jason Chun Lok Li, Weifeng Ou","SGG(장면 그래프 생성)는 시각적 장면을 구조화된 그래프 표현으로 변환하여 복잡한 비전 작업에 대한 더 깊은 장면 이해를 제공합니다. 그러나 기존 SGG 모델은 종종 필수적인 공간 관계를 간과하고 개방형 어휘 맥락에서 일반화하는 데 어려움을 겪습니다. 이러한 한계를 해결하기 위해 우리는 향상된 공간 관계 모델링을 갖춘 개방형 어휘 SGG용으로 설계된 다중 모드 대형 언어 모델(MLLM)인 LLaVA-SpaceSGG를 제안합니다. 이를 훈련시키기 위해 SpaceSGG라는 SGG 명령 조정 데이터 세트를 수집합니다. 이 데이터 세트는 공개적으로 사용 가능한 데이터 세트를 결합하고 데이터 구성 파이프라인 내에서 오픈 소스 모델을 사용하여 데이터를 합성하여 구성됩니다. 객체 위치, 객체 관계, 깊이 정보를 결합하여 공간 SGG 설명, 질문 답변, 대화의 세 가지 데이터 형식을 생성합니다. MLLM의 고유 기능을 SGG 작업으로 전환하기 위해 2단계 교육 패러다임을 도입합니다. 실험에 따르면 LLaVA-SpaceSGG는 다른 개방형 어휘 SGG 방법보다 성능이 뛰어나며 기준선에 비해 회상을 8.6%, 평균 회상을 28.4% 향상시키는 것으로 나타났습니다. 우리의 코드베이스, 데이터 세트 및 훈련된 모델은 GitHub의 다음 URL에서 공개적으로 액세스할 수 있습니다: https://github.com/Endlinc/LLaVA-SpaceSGG."
700,http://arxiv.org/abs/2412.05789 ,InfiniteWorld: A Unified Scalable Simulation Framework for General Visual-Language Robot Interaction,"Pengzhen Ren, Min Li, Zhen Luo, Xinshuai Song, Ziwei Chen, Weijia Liufu, Yixuan Yang, Hao Zheng, Rongtao Xu, Zitong Huang, Tongsheng Ding, Luyang Xie, Kaidong Zhang, Changfei Fu, Yang Liu, Liang Lin, Feng Zheng, Xiaodan Liang","구체화된 AI에서 확장 법칙을 실현하는 것이 초점이 되었습니다. 그러나 이전 작업은 통합 인터페이스가 부족한 자산과 모델로 인해 다양한 시뮬레이션 플랫폼에 분산되어 연구의 비효율성을 초래했습니다. 이 문제를 해결하기 위해 우리는 Nvidia Isaac Sim을 기반으로 구축된 일반 비전 언어 로봇 상호 작용을 위한 통합되고 확장 가능한 시뮬레이터인 InfiniteWorld를 소개합니다. InfiniteWorld는 포괄적인 물리 자산 구성 방법 세트와 일반화된 무료 로봇 상호 작용 벤치마크를 포함합니다. 특히, 우리는 먼저 생성 기반 3D 자산 구성, Real2Sim, 자동화된 주석 프레임워크 및 통합 3D 자산 처리의 일련의 개선 사항을 통합하는 구체화된 학습을 위한 통합되고 확장 가능한 시뮬레이션 프레임워크를 구축했습니다. 이 프레임워크는 로봇 상호 작용 및 학습을 위한 통합되고 확장 가능한 플랫폼을 제공합니다. 또한 현실적인 로봇 상호 작용을 시뮬레이션하기 위해 장면 그래프 협업 탐색 및 오픈 월드 소셜 모바일 조작을 포함한 4가지 새로운 일반 벤치마크를 구축했습니다. 전자는 로봇이 환경을 탐색하고 현장 지식을 구축하는 중요한 작업으로 종종 간과되는 반면, 후자는 전자를 기반으로 다양한 수준의 지식 에이전트와 로봇 상호 작용 작업을 시뮬레이션합니다. 환경 이해, 작업 계획 및 실행, 지능형 상호 작용에서 구현된 에이전트의 기능을 보다 포괄적으로 평가할 수 있습니다. 우리는 이 작업이 커뮤니티에 체계적인 자산 인터페이스를 제공하고 고품질 자산 부족의 딜레마를 완화하며 로봇 상호 작용에 대한 보다 포괄적인 평가를 제공할 수 있기를 바랍니다."
699,http://arxiv.org/abs/2412.05722 ,Evaluating Hallucination in Text-to-Image Diffusion Models with Scene-Graph based Question-Answering Agent,"Ziyuan Qin, Dongjie Cheng, Haoyu Wang, Huahui Yi, Yuting Shao, Zhiyuan Fan, Kang Li, Qicheng Lao","현대의 T2I(Text-to-Image) 모델은 합성된 이미지와 텍스트 프롬프트 간의 일관성을 평가하기 위해 질적인 인간 평가에 의존하는 경우가 많습니다. 인간의 평가에는 재현성이 부족하기 때문에 정량적이고 자동적인 평가 도구에 대한 수요가 있습니다. 우리는 효과적인 T2I 평가 지표가 다음을 달성해야 한다고 믿습니다: 생성된 이미지가 텍스트 프롬프트와 일치하지 않는 인스턴스, 즉 T2I 작업에서 '환각 문제'로 정의하는 불일치를 감지합니다. 환각 문제의 유형과 빈도를 기록하여 사용자가 오류의 원인을 이해하는 데 도움을 줍니다. 인간의 기준에 가까운 포괄적이고 직관적인 채점을 제공합니다. 이러한 목적을 달성하기 위해 우리는 추출된 장면 그래프를 사용하여 질문 답변을 수행하는 LLM(대형 언어 모델) 기반 방법을 제안하고 생성된 이미지에 대해 인간이 평가한 점수로 데이터 세트를 생성했습니다. 방법론적 관점에서 우리는 지식이 강화된 질의 응답 작업과 이미지 평가 작업을 결합하여 평가 지표를 보다 쉽게 ​​제어하고 해석할 수 있도록 합니다. 데이터 세트 측면의 기여를 위해 우리는 세 가지 고급 T2I 모델을 사용하여 1,000개의 합성 프롬프트를 기반으로 12,000개의 합성 이미지를 생성했습니다. 그 후, 우리는 모든 합성 이미지와 프롬프트 쌍에 대해 인간 채점을 수행하여 평가 지표로서 우리 방법의 정확성과 효율성을 검증합니다. 생성된 모든 이미지와 사람이 라벨을 붙인 점수는 이 중요한 문제에 대한 지속적인 연구를 촉진하기 위해 향후 공개될 예정입니다. 광범위한 실험을 통해 우리의 방법이 다른 평가 지표보다 인간의 채점 패턴과 더 밀접하게 일치하는 것으로 나타났습니다."
698,http://arxiv.org/abs/2412.05596 ,TB-HSU: Hierarchical 3D Scene Understanding with Contextual Affordances,"Wenting Xu, Viorela Ila, Luping Zhou, Craig T. Jin","기능과 어포던스의 개념은 3D 장면 이해의 중요한 측면이며 작업 지향 목표를 지원합니다. 이 작업에서 우리는 장면의 공간적 구성을 나타내는 3D 계층적 장면 그래프 전반에 걸쳐 기능적 어포던스를 구조화하고 변화시키는 방법을 학습하는 모델을 개발합니다. 다양한 기능적 어포던스는 그래프의 다양한 공간적 맥락과 통합되도록 설계되었습니다. 보다 구체적으로 우리는 장면의 공간적 구성을 포착하는 3D 계층적 장면 그래프(3DHSG)를 구성하는 방법을 학습하는 알고리즘을 개발합니다. 분할된 객체 포인트 클라우드 및 객체 의미 체계 라벨에서 시작하여 방 레이블을 식별하는 최상위 노드, 지역별 어포던스를 사용하여 방 내부의 로컬 공간 영역을 정의하는 하위 노드, 객체 위치 및 객체별 어포던스를 나타내는 손자 노드가 있는 3DHSG를 개발합니다. 이 작업을 지원하기 위해 우리는 지역별 어포던스와 각 개체에 대한 개체별 어포던스를 사용하여 로컬 공간 영역에 대한 실측 데이터를 제공하는 사용자 지정 3DHSG 데이터 세트를 만듭니다. 우리는 3DHSG를 학습하기 위해 변환기 기반 모델을 사용합니다. 우리는 공간 분류를 학습하고 지역별 어포던스를 통해 공간 내 공간 영역을 정의하는 방법을 학습하는 다중 작업 학습 프레임워크를 사용합니다. 우리의 작업은 최첨단 기본 모델의 성능을 향상시키고 변압기 모델을 3D 장면 이해에 적용하고 공간의 공간 구성을 포착하는 3DHSG 생성에 대한 한 가지 접근 방식을 보여줍니다. 코드와 데이터 세트는 공개적으로 사용 가능합니다."
697,http://arxiv.org/abs/2412.02808 ,Temporally Consistent Dynamic Scene Graphs: An End-to-End Approach for Action Tracklet Generation,"Raphael Ruschel, Md Awsafur Rahman, Hardik Prajapati, Suya You, B. S. Manjuanth","비디오 콘텐츠를 이해하는 것은 활동 인식, 자율 시스템, 인간-컴퓨터 상호 작용과 같은 실제 응용 프로그램을 발전시키는 데 매우 중요합니다. 장면 그래프는 개별 프레임의 개체 간 공간 관계를 캡처하는 데 능숙하지만 이러한 표현을 확장하여 비디오 시퀀스 전반에 걸쳐 동적 상호 작용을 캡처하는 것은 여전히 ​​중요한 과제로 남아 있습니다. 이 문제를 해결하기 위해 우리는 시간에 따른 주체-객체 관계를 감지, 추적 및 연결하여 작업 트랙렛, 시간적으로 일관된 엔터티 시퀀스 및 상호 작용을 생성하는 혁신적인 엔드 투 엔드 프레임워크인 일시적으로 일관된 동적 장면 그래프인 TCDSG를 제시합니다. 우리의 접근 방식은 적응형 디코더 쿼리 및 피드백 루프로 강화된 새로운 이분 매칭 메커니즘을 활용하여 확장된 시퀀스에 대한 시간적 일관성과 강력한 추적을 보장합니다. 이 방법은 Action Genome, OpenPVSG 및 MEVA 데이터 세트에서 임시 회상@k를 60% 이상 개선하여 새로운 벤치마크를 확립할 뿐만 아니라 포괄적인 트랙렛 생성을 위한 영구 개체 ID 주석을 사용하여 MEVA의 증강을 개척합니다. 공간적 및 시간적 역학을 완벽하게 통합함으로써 우리의 작업은 다중 프레임 비디오 분석의 새로운 표준을 설정하고 감시, 자율 항법 등의 분야에서 영향력이 큰 응용 프로그램을 위한 새로운 길을 열었습니다."
696,http://arxiv.org/abs/2412.01539 ,"The Bare Necessities: Designing Simple, Effective Open-Vocabulary Scene Graphs","Christina Kassab, Matías Mattamala, Sacha Morin, Martin Büchner, Abhinav Valada, Liam Paull, Maurice Fallon","3D 개방형 어휘 장면 그래프 방법은 구현된 에이전트에 대한 유망한 맵 표현이지만 현재의 많은 접근 방식은 계산 비용이 많이 듭니다. 본 논문에서는 효율성과 성능을 모두 최적화하기 위해 이전 작업에서 확립된 중요한 설계 선택을 재검토합니다. 우리는 일반적인 장면 그래프 프레임워크를 제안하고 이미지 전처리, 특징 융합, 특징 선택에 초점을 맞춘 세 가지 연구를 수행합니다. 우리의 연구 결과에 따르면 일반적으로 사용되는 이미지 전처리 기술은 계산을 3배로 늘리는 동시에 성능 향상을 최소화하는 것으로 나타났습니다(객체 뷰 기준). 또한 다양한 뷰에서 기능 레이블을 평균화하면 성능이 크게 저하된다는 것도 보여줍니다. 불필요한 계산 비용을 추가하지 않고 성능을 향상시키는 대체 기능 선택 전략을 연구합니다. 우리의 연구 결과를 바탕으로 객체별 기능을 갖춘 3D 포인트 클라우드 분할을 위한 계산적으로 균형 잡힌 접근 방식을 소개합니다. 이 접근 방식은 최첨단 분류 정확도를 유지하면서 계산을 3배로 줄입니다."
695,http://arxiv.org/abs/2412.00161 ,STEP: Enhancing Video-LLMs' Compositional Reasoning by Spatio-Temporal Graph-guided Self-Training,"Haiyi Qiu, Minghe Gao, Long Qian, Kaihang Pan, Qifan Yu, Juncheng Li, Wenjie Wang, Siliang Tang, Yueting Zhuang, Tat-Seng Chua","Video-LLM(비디오 대형 언어 모델)은 최근 캡션 작성 및 대략적인 질문 답변과 같은 기본 비디오 이해 작업에서 강력한 성능을 보였지만 개체 관계, 상호 작용 및 이벤트에 대한 다단계 시공간 추론이 필요한 구성 추론에는 어려움을 겪고 있습니다. 이 기능을 향상시키는 데 장애가 되는 장애물로는 광범위한 육체 노동, 기존 데이터의 시공간적 구성 부족, 명시적인 추론 감독 부재 등이 있습니다. 본 논문에서는 Video-LLM이 모든 원본 비디오에서 추론이 풍부한 미세 조정 데이터를 생성하여 자체적으로 개선할 수 있는 새로운 그래프 기반 자가 학습 방법인 STEP을 제안합니다. 구체적으로, 먼저 다양한 비디오의 STSG(Spatio-Temporal Scene Graph) 표현을 유도하여 세밀하고 세분화된 비디오 의미를 캡처합니다. 그런 다음 STSG는 CoT(사고 사슬) 근거를 사용하여 다단계 추론 QA(질문-답변) 데이터 도출을 안내합니다. 답변과 근거는 모두 훈련 목표로 통합되어 명시적 추론 단계에 대한 감독을 통해 모델의 추론 능력을 향상시키는 것을 목표로 합니다. 실험 결과는 다양한 규모의 모델 전반에 걸쳐 STEP의 효율성을 보여 주며, 3개 이상의 추론 단계가 필요한 작업에서 21.3%의 상당한 개선이 이루어졌습니다. 또한, 구성 추론 및 포괄적 이해 벤치마크 모두에서 자체 생성된 이론적 근거가 강화된 최소한의 훈련 샘플로 탁월한 성능을 달성하여 광범위한 적용 가능성과 막대한 잠재력을 강조합니다."
694,http://arxiv.org/abs/2411.19162 ,Lost & Found: Tracking Changes from Egocentric Observations in 3D Dynamic Scene Graphs,"Tjark Behrens, René Zurbrügg, Marc Pollefeys, Zuria Bauer, Hermann Blum","최근 접근 방식은 정적 재구성의 분할에 성공적으로 초점을 맞춰 다운스트림 애플리케이션에 의미론적 3D 이해 기능을 제공했습니다. 그러나 우리가 살고 있는 세상은 환경과 인간 또는 로봇 에이전트 간의 수많은 상호 작용을 특징으로 하는 역동적입니다. 정적 의미 맵은 이 정보를 캡처할 수 없으며 모든 변경 후 환경을 다시 검색하는 순진한 솔루션은 비용이 많이 들고 추적에 비효율적입니다. 서랍 속에 보관되어 있는 물건들. Lost & Found에서는 이러한 제한 사항을 해결하는 접근 방식을 제시합니다. 해당 손 위치 및 카메라 포즈 추정이 포함된 자기중심적 녹음만을 기반으로 감지된 상호 작용 간격 내에서 움직이는 객체의 6DoF 포즈를 추적할 수 있습니다. 이러한 변경 사항은 객체 수준 관계를 캡처하는 변형 가능한 장면 그래프에 온라인으로 적용됩니다. 최첨단 개체 포즈 추적기와 비교할 때 우리의 접근 방식은 까다로운 자기 중심적 관점과 깊이 정보 부족을 처리하는 데 더 안정적입니다. 이는 변환 및 방향 오류에 대해 각각 34% 및 56%만큼 차선책보다 성능이 뛰어나며 눈에 띄게 더 부드러운 6DoF 개체 궤적을 생성합니다. 또한 동적 장면 그래프에서 획득한 상호 작용 정보가 실행 불가능한 로봇 애플리케이션의 맥락에서 어떻게 사용될 수 있는지 설명합니다. 우리의 방법을 통해 어떻게 티치 앤 반복을 통해 모바일 조작기에 명령을 내릴 수 있는지, 이전 상호 작용에 대한 정보를 통해 모바일 조작기가 서랍에 숨겨진 개체를 검색할 수 있는 방법을 보여줍니다. 코드, 비디오 및 해당 데이터는 https://behretj.github.io/LostAndFound에서 액세스할 수 있습니다."
693,http://arxiv.org/abs/2411.18894 ,T2SG: Traffic Topology Scene Graph for Topology Reasoning in Autonomous Driving,"Changsheng Lv, Mengshi Qi, Liang Liu, Huadong Ma","교통 상황을 이해하고 고화질(HD) 지도를 생성하는 것은 자율 주행에 있어 중요한 과제입니다. 본 논문에서는 차선을 명시적으로 모델링하고 다양한 도로 신호(예: 우회전)에 의해 제어 및 안내되는 통합 장면 그래프인 새로운 교통 토폴로지 장면 그래프를 정의했으며, 이전 고화질(HD) 매핑 방법에서는 항상 무시되는 이들 간의 토폴로지 관계를 정의했습니다. T2SG 생성을 위해 우리는 새롭게 설계된 두 개의 레이어를 갖춘 새로운 1단계 토폴로지 장면 그래프 TransFormer인 TopoFormer를 제안합니다. 특히 TopoFormer는 차선 중앙선 사이의 기하학적 거리를 활용하여 글로벌 정보 집계를 안내하는 LAL(차선 집계 계층)을 통합합니다. 또한, 우리는 반사실적 개입이 적용되는 차선 간의 합리적인 도로 구조(예: 교차로, 직선)를 모델링하기 위해 반사실적 개입 계층(CIL)을 제안했습니다. 그러면 생성된 T2SG는 교통 현장의 토폴로지 구조에 대한 보다 정확하고 설명 가능한 설명을 제공할 수 있습니다. 실험 결과에 따르면 TopoFormer는 T2SG 생성 작업에서 기존 방법보다 성능이 뛰어나고 생성된 T2SG는 다운스트림 작업에서 트래픽 토폴로지 추론을 크게 향상시켜 OpenLane-V2 벤치마크에서 46.3 OLS라는 최첨단 성능을 달성했습니다. 소스 코드와 모델을 공개하겠습니다."
692,http://arxiv.org/abs/2411.18666 ,3D Scene Graph Guided Vision-Language Pre-training,"Hao Liu, Yanni Ma, Yan Liu, Haihong Xiao, Ying He","3D 비전 언어(VL) 추론은 3D 물리적 세계와 자연어 설명을 연결할 수 있는 잠재력으로 인해 상당한 주목을 받았습니다. 기존 접근 방식은 일반적으로 작업별로 고도로 전문화된 패러다임을 따릅니다. 따라서 이러한 방법은 제한된 범위의 추론 하위 작업에 중점을 두고 수작업으로 만든 모듈과 보조 손실에 크게 의존합니다. 이는 더 단순하고 통합된 범용 모델의 필요성을 강조합니다. 본 논문에서는 3D 장면 그래프와 자연어 간의 고유한 연결을 활용하여 3D 장면 그래프 기반 VLP(Vision-Language Pre-training) 프레임워크를 제안합니다. 우리의 접근 방식은 양식 인코더, 그래프 컨벌루션 레이어 및 교차 주의 레이어를 활용하여 다양한 3D VL 추론 작업에 적용되는 보편적인 표현을 학습함으로써 작업별 설계가 필요하지 않습니다. 사전 학습 목표는 다음과 같습니다. 1) 3D 장면 그래프와 자연어 간의 강한 상관관계를 활용하여 3D 개체를 다양한 세부 수준의 텍스트 기능과 정렬하는 장면 그래프 기반 대조 학습. 2) 마스킹된 단어와 3D 객체를 재구성하기 위해 교차 양식 정보를 사용하는 마스킹된 양식 학습. 마스크된 객체의 3D 포인트 클라우드를 직접 재구성하는 대신 위치 단서를 사용하여 의미 범주를 예측합니다. 광범위한 실험을 통해 우리의 사전 훈련 모델이 여러 다운스트림 작업에서 미세 조정될 때 3D 시각적 접지, 3D 조밀한 캡션 및 3D 질문 답변과 같은 작업에서 기존 방법과 비슷하거나 더 나은 성능을 달성한다는 것을 보여줍니다."
691,http://arxiv.org/abs/2411.18147 ,Online Knowledge Integration for 3D Semantic Mapping: A Survey,"Felix Igelbrink, Marian Renz, Martin Günther, Piper Powell, Lennart Niecksch, Oscar Lima, Martin Atzmueller, Joachim Hertzberg",의미론적 매핑은 구조화된 환경에서 객체를 작동하고 객체와 상호 작용하는 로봇의 핵심 구성 요소입니다. 전통적으로 의미 지도 내의 기하학적 표현과 지식 표현은 느슨하게 통합되어 있었습니다. 그러나 최근 딥 러닝의 발전으로 지식 그래프나 언어 개념으로 표현되는 사전 지식을 센서 데이터 처리 및 의미 매핑 파이프라인에 완전히 통합할 수 있게 되었습니다. 의미론적 장면 그래프와 언어 모델을 사용하면 그래프 기반 사전 지식을 통합하거나 매핑 프로세스 도중과 이후에 인간 언어의 풍부한 정보를 활용할 수 있는 현대적인 의미론적 매핑 접근 방식이 가능해집니다. 이는 의미 체계 매핑의 상당한 발전을 촉발하여 이전에는 불가능했던 새로운 응용 프로그램을 탄생시켰습니다. 이 설문조사는 의미론적 매핑에 대한 지식의 온라인 통합에 초점을 맞춰 이러한 최근 개발을 포괄적으로 검토합니다. 우리는 특히 암시적 상식 지식과 자연어 개념을 각각 포착하기 위해 상징적 사전 지식과 언어 모델을 통합하기 위해 의미론적 장면 그래프를 사용하는 방법에 중점을 둡니다.
690,http://arxiv.org/abs/2411.18042 ,HyperGLM: HyperGraph for Video Scene Graph Generation and Anticipation,"Trong-Thuan Nguyen, Pha Nguyen, Jackson Cothren, Alper Yilmaz, Khoa Luu","다중 모드 LLM은 고급 비전 언어 작업을 수행하지만 여전히 비디오 장면을 이해하는 데 어려움을 겪습니다. 이러한 격차를 해소하기 위해 비디오 프레임 전반에 걸쳐 다중 객체 관계를 캡처하는 비디오 장면 그래프 생성(VidSGG)이 등장했습니다. 그러나 이전 방법은 쌍별 연결에 의존하므로 복잡한 다중 개체 상호 작용 및 추론을 처리하는 능력이 제한됩니다. 이를 위해 우리는 장면 하이퍼그래프(HyperGLM)에서 다중 모드 LLM을 제안하여 다중 방향 상호 작용 및 고차 관계에 대한 추론을 촉진합니다. 우리의 접근 방식은 개체 간의 공간적 관계를 캡처하는 엔터티 장면 그래프와 개체의 인과적 전환을 모델링하는 절차적 그래프를 고유하게 통합하여 통합된 하이퍼그래프를 형성합니다. 중요한 점은 HyperGLM이 이 통합 HyperGraph를 LLM에 주입하여 추론을 가능하게 한다는 것입니다. 또한 3인칭, 자기중심적, 드론 뷰의 190만 프레임을 특징으로 하는 새로운 VSGR(비디오 장면 그래프 추론) 데이터 세트를 도입하고 장면 그래프 생성, 장면 그래프 예상, 비디오 질문 응답, 비디오 캡션 및 관계 추론의 5가지 작업을 지원합니다. 경험적으로 HyperGLM은 5가지 작업에서 지속적으로 최첨단 방법보다 뛰어난 성능을 발휘하여 다양한 비디오 장면에서 복잡한 관계를 효과적으로 모델링하고 추론합니다."
689,http://arxiv.org/abs/2411.17188 ,Interleaved Scene Graphs for Interleaved Text-and-Image Generation Assessment,"Dongping Chen, Ruoxi Chen, Shu Pu, Zhaoyi Liu, Yanru Wu, Caixi Chen, Benlin Liu, Yue Huang, Yao Wan, Pan Zhou, Ranjay Krishna","많은 실제 사용자 쿼리(예: ""계란 볶음밥을 만드는 방법?"")는 요리책과 유사하게 이미지가 포함된 텍스트 단계로 응답을 생성할 수 있는 시스템의 이점을 누릴 수 있습니다. 인터리브된 텍스트와 이미지를 생성하도록 설계된 모델은 이러한 양식 내에서 일관성을 보장하는 데 어려움을 겪습니다. 이러한 과제를 해결하기 위해 우리는 인터리브된 텍스트 및 이미지 생성을 위한 포괄적인 평가 프레임워크인 ISG를 제시합니다. ISG는 장면 그래프 구조를 활용하여 텍스트와 이미지 블록 간의 관계를 캡처하고 전체적, 구조적, 블록 수준, 이미지별 등 4가지 세부 수준에서 응답을 평가합니다. 이 다단계 평가를 통해 일관성, 일관성 및 정확성에 대한 미묘한 평가가 가능하며 해석 가능한 질문-답변 피드백을 제공합니다. ISG와 함께 우리는 8개 카테고리와 21개 하위 카테고리에 걸쳐 1,150개의 샘플을 포함하는 벤치마크인 ISG-Bench를 도입했습니다. 이 벤치마크 데이터 세트에는 현재 모델의 어려운 영역인 스타일 전송과 같은 비전 중심 작업에서 모델을 효과적으로 평가하기 위한 복잡한 언어 비전 종속성과 황금 답변이 포함되어 있습니다. ISG-Bench를 사용하여 최근 통합 비전 언어 모델이 인터리브된 콘텐츠 생성에 제대로 수행되지 않음을 보여줍니다. 별도의 언어와 이미지 모델을 결합하는 구성적 접근 방식은 전체적인 수준에서 통합 모델에 비해 111% 향상된 성능을 보이지만 블록 및 이미지 수준 모두에서 성능이 여전히 최적이 아닙니다. 향후 작업을 촉진하기 위해 우리는 도구를 호출하기 위해 ""계획-실행-개선"" 파이프라인을 사용하는 기본 에이전트인 ISG-Agent를 개발하여 122% 성능 향상을 달성했습니다."
688,http://arxiv.org/abs/2412.00067 ,Targeted Therapy in Data Removal: Object Unlearning Based on Scene Graphs,"Chenhan Zhang, Benjamin Zi Hao Zhao, Hassan Asghar, Dali Kaafar","사용자는 실수로 MLaaS(Machine Learning as a Service) 제공업체에 개인 식별 정보(PII)를 업로드할 수 있습니다. 사용자가 더 이상 이러한 서비스에서 자신의 PII를 원하지 않는 경우 GDPR 및 COPPA와 같은 규정은 해당 사용자에게 잊어버릴 권리를 요구합니다. 따라서 이러한 서비스는 특정 데이터 포인트의 영향을 제거하는 효율적인 방법을 모색합니다. 따라서 머신 언러닝이 도입되었습니다. 전통적으로 언러닝은 전체 데이터 샘플(샘플 언러닝) 또는 데이터세트 전체의 전체 기능(특성 언러닝)을 제거하여 수행됩니다. 그러나 이러한 접근 방식은 샘플 내의 특정 개체를 학습 해제하는 보다 세부적이고 어려운 작업을 처리할 수 없습니다. 이러한 격차를 해소하기 위해 우리는 장면 그래프 기반 객체 언러닝 프레임워크를 제안합니다. 이 프레임워크는 의미론적 표현이 풍부한 장면 그래프를 활용하고 학습 취소 요청을 실행 가능한 단계로 투명하게 변환합니다. 그 결과, 학습되지 않은 객체를 제외하고 생성된 이미지의 전반적인 의미 무결성이 보존됩니다. 또한, 우리는 학습 취소 프로세스를 근사화하기 위해 영향 함수를 사용하여 높은 계산 오버헤드를 관리합니다. 검증을 위해 이미지 재구성 및 이미지 합성 작업에 따라 출력에서 ​​학습되지 않은 개체의 충실도를 평가합니다. 제안된 프레임워크는 샘플 및 기능 학습 방법과 달리 요청되지 않은 샘플을 보존하여 향상된 객체 학습 해제 결과를 보여줍니다. 이 작업은 전체 데이터 샘플 또는 데이터 세트 기능의 유용성을 희생하지 않고 특정 개체 수준 세부 정보를 잊어버려 대상 기계 학습 해제의 세분성을 높여 중요한 개인 정보 보호 문제를 해결합니다."
687,http://arxiv.org/abs/2411.15714 ,ROOT: VLM based System for Indoor Scene Understanding and Beyond,"Yonghui Wang, Shi-Yong Chen, Zhenxing Zhou, Siyi Li, Haoran Li, Wengang Zhou, Houqiang Li","최근 VLM(Vision Language Model)이 상당한 발전을 이루었지만 이러한 모델은 여전히 ​​실내 장면 내 공간 계층적 추론에 어려움을 겪고 있습니다. 본 연구에서는 실내 장면 분석을 향상시키기 위해 설계된 VLM 기반 시스템인 ROOT를 소개합니다. 구체적으로, 우리는 먼저 실내 장면 내에서 객체 개체를 감지하기 위해 GPT-4V를 사용하는 반복적인 객체 인식 알고리즘을 개발합니다. 그런 다음 경계 상자와 같은 장면에 대한 추가 메타 정보를 얻기 위해 비전 기반 모델을 사용합니다. 이러한 기초 데이터를 바탕으로 공간적 계층적 장면 그래프를 생성하고 실내 환경 내 객체에 대한 거리 정보를 제공할 수 있는 특화된 VLM인 SceneVLM을 제안한다. 이 정보는 실내 장면의 공간 배치에 대한 이해를 향상시킵니다. SceneVLM을 교육하기 위해 다양한 공개 실내 데이터 세트에서 610,000개 이상의 이미지를 수집하고 반자동 기술로 장면 데이터 생성 파이프라인을 구현하여 관계를 설정하고 실내 객체 간의 거리를 추정합니다. 이 풍부한 데이터를 활용하여 다양한 트레이닝 레시피를 실시하고 SceneVLM을 완성합니다. 우리의 실험에서는 \rootname이 실내 장면 이해를 촉진하고 3D 장면 생성 및 구현된 AI와 같은 다양한 다운스트림 애플리케이션에 효과적이라는 것이 입증되었습니다. 코드는 \url{https://github.com/harrytea/ROOT}에서 공개됩니다."
686,http://arxiv.org/abs/2411.17735 ,3D-Mem: 3D Scene Memory for Embodied Exploration and Reasoning,"Yuncong Yang, Han Yang, Jiachen Zhou, Peihao Chen, Hongxin Zhang, Yilun Du, Chuang Gan",특히 장기간에 걸쳐 복잡한 환경에서 효과적으로 구현된 탐색 및 추론을 위해서는 간결하고 유익한 3D 장면 표현을 구성하는 것이 필수적입니다. 객체 중심 3D 장면 그래프와 같은 기존 표현은 제한된 텍스트 관계를 사용하여 장면을 고립된 객체로 모델링하여 공간 관계를 지나치게 단순화하므로 미묘한 공간 이해가 필요한 쿼리를 처리하기가 어렵습니다. 더욱이 이러한 표현에는 적극적인 탐색과 기억 관리를 위한 자연스러운 메커니즘이 부족하여 평생 자율성에 적용하는 데 방해가 됩니다. 본 연구에서는 구현된 에이전트를 위한 새로운 3D 장면 메모리 프레임워크인 3D-Mem을 제안합니다. 3D-Mem은 메모리 스냅샷이라는 유익한 다중 뷰 이미지를 사용하여 장면을 표현하고 탐색된 영역의 풍부한 시각적 정보를 캡처합니다. 프론티어 스냅샷(Frontier Snapshots)을 도입하여 미개척 영역을 간략히 살펴보고 에이전트가 알려진 정보와 잠재적인 새로운 정보를 모두 고려하여 정보에 입각한 결정을 내릴 수 있도록 함으로써 프론티어 기반 탐색을 더욱 통합합니다. 활성 탐색 설정에서 평생 메모리를 지원하기 위해 3D-Mem을 위한 증분 구성 파이프라인과 메모리 관리를 위한 메모리 검색 기술을 제시합니다. 세 가지 벤치마크에 대한 실험 결과는 3D-Mem이 3D 환경에서 에이전트의 탐색 및 추론 능력을 크게 향상시켜 구현된 AI에서 응용 프로그램을 발전시킬 수 있는 잠재력을 강조한다는 것을 보여줍니다.
685,http://arxiv.org/abs/2411.15435 ,What Makes a Scene ? Scene Graph-based Evaluation and Feedback for Controllable Generation,"Zuyao Chen, Jinlin Wu, Zhen Lei, Chang Wen Chen","텍스트-이미지 생성이 광범위하게 연구된 반면, 장면 그래프에서 이미지를 생성하는 것은 주로 공간 관계 및 개체 상호 작용을 정확하게 모델링하는 데 어려움이 있기 때문에 상대적으로 덜 탐구되어 있습니다. 이러한 격차를 메우기 위해 자연스러운 장면 생성 시 사실적 일관성을 평가하고 향상하도록 설계된 포괄적인 벤치마크인 Scene-Bench를 소개합니다. Scene-Bench는 장면 그래프로 주석이 달린 100만 개의 이미지로 구성된 대규모 데이터 세트인 MegaSG로 구성되어 다양하고 복잡한 장면에서 모델의 훈련과 공정한 비교를 용이하게 합니다. 또한 우리는 다중 모달 대형 언어 모델(LLM)의 사고 연쇄 추론 기능을 활용하여 객체 존재와 관계 정확성을 모두 평가하는 새로운 평가 지표인 SGScore를 제안하며, FID 및 CLIPScore와 같은 기존 지표보다 사실적 일관성에 대한 더 효과적인 척도를 제공합니다. 이 평가 프레임워크를 기반으로 장면 그래프와 이미지 간의 불일치를 식별하고 수정하여 생성된 이미지를 반복적으로 개선하는 장면 그래프 피드백 파이프라인을 개발합니다. 광범위한 실험을 통해 Scene-Bench가 특히 복잡한 장면 생성에 대해 기존 벤치마크에 비해 더 포괄적이고 효과적인 평가 프레임워크를 제공한다는 사실이 입증되었습니다. 또한, 우리의 피드백 전략은 이미지 생성 모델의 사실적 일관성을 크게 향상시켜 제어 가능한 이미지 생성 분야를 발전시킵니다."
684,http://arxiv.org/abs/2411.15027 ,Time is on my sight: scene graph filtering for dynamic environment perception in an LLM-driven robot,"Simone Colombani, Luca Brini, Dimitri Ognibene, Giuseppe Boccignone","직장, 병원, 가정 등 역동적인 환경에서 로봇이 점점 더 많이 사용되고 있습니다. 결과적으로 로봇과의 상호 작용은 간단하고 직관적이어야 하며, 로봇 인식은 인간이 유발한 변화에 효율적으로 적응해야 합니다. 본 논문에서는 로봇 상태 표현의 동적 생성과 지속적인 업데이트에 특히 중점을 두고 인간-로봇 상호 작용의 주요 과제를 해결하는 로봇 제어 아키텍처를 제시합니다. 이 아키텍처는 대규모 언어 모델을 사용하여 자연어 명령, 로봇 기술 표현, 인지된 장면의 실시간 동적 의미 매핑을 포함한 다양한 정보 소스를 통합합니다. 이를 통해 복잡하고 역동적인 환경에서 유연하고 적응적인 로봇 동작이 가능해집니다. 기존 로봇 시스템은 사전 프로그래밍된 정적 지침 및 설정에 의존하는 경우가 많아 동적 환경 및 실시간 협업에 대한 적응성이 제한됩니다. 이와 대조적으로 이 아키텍처는 LLM을 사용하여 복잡하고 높은 수준의 지침을 해석하고 인간-로봇 협업을 향상시키는 실행 가능한 계획을 생성합니다. 핵심적으로 시스템 인식 모듈은 RGB-D 센서 데이터를 사용하여 의미론적 장면 그래프를 생성하고 지속적으로 업데이트하여 환경에 대한 상세하고 구조화된 표현을 제공합니다. 동적 실제 환경에서 정확한 개체 위치 파악을 보장하기 위해 입자 필터가 사용됩니다. Planner 모듈은 이 최신 의미 체계 맵을 활용하여 상위 수준 작업을 하위 작업으로 나누고 이를 탐색, 개체 조작(예: PICK 및 PLACE) 및 이동(예: GOTO)과 같은 로봇 기술에 연결합니다. 실시간 인식, 상태 추적, LLM 기반 통신 및 작업 계획을 결합하여 아키텍처는 동적 환경에서 적응성, 작업 효율성 및 인간-로봇 협업을 향상시킵니다."
683,http://arxiv.org/abs/2411.13620 ,Robust SG-NeRF: Robust Scene Graph Aided Neural Surface Reconstruction,"Yi Gu, Dongjun Ye, Zhaorui Wang, Jiaxu Wang, Jiahang Cao, Renjing Xu",신경 표면 재구성은 입력으로 정확한 카메라 포즈에 크게 의존합니다. COLMAP 또는 ARKit과 같은 고급 포즈 추정기를 활용하더라도 카메라 포즈에는 여전히 노이즈가 있을 수 있습니다. 기존 포즈-NeRF 관절 최적화 방법은 작은 노이즈(인라이어)가 있는 포즈를 효과적으로 처리하지만 미러링된 포즈와 같은 큰 노이즈(아웃라이어)로 인해 어려움을 겪습니다. 이 작업에서는 이상치 포즈의 영향을 완화하는 데 중점을 둡니다. 우리의 방법은 데이터 준비 단계에서 수집된 장면 그래프 정보를 활용하여 inlier-outlier 신뢰도 추정 체계를 통합합니다. 렌더링 메트릭을 참조로 직접 사용하는 이전 작업과 달리 모양-광도 모호함으로 인한 영향을 최소화하기 위해 보기 방향을 입력으로 생략하는 분리된 색상 네트워크를 사용합니다. 이 향상된 신뢰도 업데이트 전략은 내부 포즈와 이상치 포즈를 효과적으로 구분하여 내부 포즈에서 더 많은 광선을 샘플링하여 보다 안정적인 복사 필드를 구성할 수 있게 해줍니다. 또한 현재 SDF(Signed Distance Function) 및 포즈 추정을 기반으로 하는 재투영 손실을 도입하여 일치하는 이미지 쌍 간의 제약을 강화합니다. 이상치 포즈의 경우 더 나은 솔루션을 찾기 위해 Monte Carlo 재위치화 방법을 채택합니다. 또한 훈련 과정 전반에 걸쳐 보다 정확한 정보를 제공하기 위해 장면 그래프 업데이트 전략을 고안합니다. 우리는 SG-NeRF 및 DTU 데이터 세트에 대한 접근 방식을 검증합니다. 다양한 데이터 세트에 대한 실험 결과는 우리의 방법이 재구성 품질과 포즈 정확도를 지속적으로 향상시킬 수 있음을 보여줍니다.
682,http://arxiv.org/abs/2411.13287 ,Unbiased Scene Graph Generation by Type-Aware Message Passing on Heterogeneous and Dual Graphs,"Guanglu Sun, Jin Qiu, Lili Liang",편향되지 않은 장면 그래프 생성에 대한 연구는 큰 진전을 이루었지만 여전히 헤드 클래스와 테일 클래스의 예측 성능을 향상시키는 데에는 문제가 있습니다. 이러한 문제를 해결하기 위해 편견 없는 장면 그래프 생성(TA-HDG)이 제안되었습니다. Interactive Graph Construction은 Interactive Graph Construction을 통해 여러 개체 간의 관계를 모델링할 때 이종 그래프와 이중 그래프를 결합하여 개체에 대한 관계의 의존성을 모델링하는 방법을 제안합니다. 또한 무의미한 가장자리를 줄이기 위해 주체-객체 쌍 선택 전략을 구현합니다. 또한 Type-Aware Message Passing은 Intra-Type 및 Inter-Type 단계에서 유형 내 및 유형 간 컨텍스트를 캡처하여 복잡한 상호 작용에 대한 이해를 향상시킵니다. Intra-Type 단계는 상호 관계 및 객체 간 의미적 맥락을 포착합니다. 이를 바탕으로 Inter-Type 단계에서는 각각 상호작용적 관계와 비상호작용적 관계에 대한 객체와 관계 사이의 맥락을 포착합니다. 두 데이터세트에 대한 실험에서는 TA-HDG가 R@K 및 mR@K의 측정항목을 개선한 것으로 나타났습니다. 이는 TA-HDG가 헤드 클래스의 경쟁력 있는 성능을 유지하면서 꼬리 클래스를 정확하게 예측할 수 있음을 입증합니다.
681,http://arxiv.org/abs/2411.13059 ,Towards Unbiased and Robust Spatio-Temporal Scene Graph Generation and Anticipation,"Rohith Peddi,  Saurabh, Ayush Abhay Shrivastava, Parag Singla, Vibhav Gogate","STSG(시공간 장면 그래프)는 객체와 시간이 지남에 따라 진화하는 관계를 모델링하여 동적 장면을 간결하고 표현력 있게 표현합니다. 그러나 실제 시각적 관계는 종종 긴 꼬리 분포를 나타내므로 VidSGG(비디오 장면 그래프 생성) 및 SGA(장면 그래프 예측)와 같은 작업에 대한 기존 방법으로 인해 편향된 장면 그래프가 생성됩니다. 이를 위해 우리는 손실 마스킹과 커리큘럼 학습을 활용하여 시공간 장면 그래프 생성 및 예측의 편견을 완화하는 새로운 교육 프레임워크인 ImparTail을 제안합니다. 편견 없는 추정기를 학습하기 위해 추가적인 아키텍처 구성요소를 추가하는 이전 방법과 달리, 우리는 학습 중에 헤드 클래스의 지배력을 줄이고 과소대표된 꼬리 관계에 초점을 맞추는 공정한 학습 목표를 제안합니다. 우리의 커리큘럼 기반 마스크 생성 전략은 모델이 시간이 지남에 따라 편향 완화 전략을 적응적으로 조정할 수 있도록 더욱 강화하여 보다 균형 있고 강력한 추정을 가능하게 합니다. 다양한 분포 변화에 따른 성능을 철저하게 평가하기 위해 STSG 모델의 복원력을 평가하기 위한 도전적인 벤치마크를 제공하는 두 가지 새로운 작업인 강력한 시공간 장면 그래프 생성과 강력한 장면 그래프 예측도 도입합니다. Action Genome 데이터 세트에 대한 광범위한 실험은 기존 기준에 비해 우리 방법의 탁월한 편견 없는 성능과 견고성을 보여줍니다."
680,http://arxiv.org/abs/2411.12837 ,Anticipatory Planning for Performant Long-Lived Robot in Large-Scale Home-Like Environments,"Md Ridwan Hossain Talukder, Raihan Islam Arnob, Gregory J. Stein","우리는 로봇이 한 번에 하나씩 주어지는 지속적인 대규모 환경에서 일련의 작업을 완료해야 하는 설정을 고려합니다. 기존 작업 계획자는 현재 작업이 미래 작업에 미치는 영향을 고려하지 않고 즉각적인 목표에만 초점을 맞춰 근시안적으로 운영하는 경우가 많습니다. 현재 작업의 즉각적인 계획 비용과 향후 후속 작업과 관련된 예상 비용의 공동 목표를 줄이는 예측 계획은 장기 작업 계획을 개선하기 위한 접근 방식을 제공합니다. 그러나 대규모 환경에서 예측 계획을 적용하는 것은 관련된 자산의 양이 너무 많아 학습 및 계획의 확장성에 부담을 주기 때문에 상당한 어려움을 안겨줍니다. 본 연구에서는 대규모의 현실적인 환경에 맞게 확장할 수 있도록 설계된 모델 기반 예측 작업 계획 프레임워크를 소개합니다. 우리 프레임워크는 특히 3D 장면 그래프에서 영감을 받은 표현을 통해 GNN을 사용하여 주의 예상 비용을 추정하는 데 중요한 환경의 필수 속성과 실용적인 대규모 예측 계획을 위한 샘플링 기반 절차를 학습합니다. 실험 결과에 따르면 플래너는 작업 순서 비용을 집에서는 5.38%, 레스토랑에서는 31.5% 줄였습니다. 우리 모델을 사용하여 미리 준비할 시간이 주어지면 작업 순서 비용이 각각 40.6%와 42.5% 감소합니다."
679,http://arxiv.org/abs/2411.11714 ,Semantic-Geometric-Physical-Driven Robot Manipulation Skill Transfer via Skill Library and Tactile Representation,"Mingchao Qi, Yuanjin Li, Xing Liu, Zhengxiong Liu, Panfeng Huang","구조화되지 않은 환경에서 조작할 수 있는 일반 로봇 시스템을 개발하는 것은 중요한 과제입니다. 특히 관련된 작업은 일반적으로 장거리 및 풍부한 접촉이기 때문에 다양한 작업 시나리오에 걸쳐 효율적인 기술 이전이 필요합니다. 이러한 과제를 해결하기 위해 우리는 지식 그래프 기반 스킬 라이브러리 구축 방법을 제안한다. 이 방법은 각각 작업별 정보와 장면별 정보를 나타내기 위해 ""작업 그래프""와 ""장면 그래프""를 사용하여 조작 지식을 계층적으로 구성합니다. 또한 높은 수준의 작업 계획과 낮은 수준의 장면 정보 간의 상호 작용을 용이하게 하기 위해 ""상태 그래프""를 도입합니다. 이러한 기반을 바탕으로 우리는 기술 이전을 위한 높은 수준의 추론과 실행을 위한 낮은 수준의 정밀도를 통합하는 기술 라이브러리 및 촉각 표현을 기반으로 하는 새로운 계층적 기술 이전 프레임워크를 제안합니다. 작업 수준에서 우리는 LLM(대형 언어 모델)을 활용하고 상황별 학습과 4단계 사고 사슬 프롬프트 패러다임을 결합하여 하위 작업 순서 이전을 달성합니다. 모션 수준에서는 스킬 라이브러리와 경험적 경로 계획 알고리즘을 기반으로 적응형 궤적 전송 방법을 개발합니다. 물리적 수준에서는 촉각 표현을 기반으로 한 적응형 윤곽 추출 및 자세 인식 방법을 제안합니다. 이 방법은 시각적 촉각 이미지로부터 고정밀 윤곽 및 자세 정보를 동적으로 획득하고 접촉 위치 및 자세와 같은 매개변수를 조정하여 새로운 환경에서 기술이 전달되는 효과를 보장합니다. 실험은 다양한 작업 시나리오에 걸쳐 제안된 방법의 기술 이전 및 적응성 기능을 보여줍니다. 프로젝트 웹사이트: https://github.com/MingchaoQi/skill_transfer"
678,http://arxiv.org/abs/2411.10509 ,TESGNN: Temporal Equivariant Scene Graph Neural Networks for Efficient and Robust Multi-View 3D Scene Understanding,"Quang P. M. Pham, Khoi T. N. Nguyen, Lan C. Ngo, Truong Do, Dezhen Song, Truong-Son Hy","장면 그래프는 관계 정보를 간결하고 명시적으로 표현하므로 다양한 장면 이해 작업에 매우 효과적인 것으로 입증되었습니다. 그러나 현재 방법은 3D 포인트 클라우드에서 장면 그래프를 생성할 때 대칭을 유지하는 것의 중요성을 간과하는 경우가 많습니다. 이로 인해 특히 노이즈가 있는 다중 뷰 데이터를 처리할 때 정확성과 견고성이 저하될 수 있습니다. 또한, 이전 접근 방식의 주요 제한 사항은 장면에서 동적으로 진화하는 엔터티 간의 시간 종속 관계를 캡처하는 시간적 모델링이 부족하다는 것입니다. 이러한 문제를 해결하기 위해 우리는 두 가지 주요 구성 요소로 구성된 TESGNN(Temporal Equivariant Scene Graph Neural Network)을 제안합니다. (1) 3D 포인트 클라우드에서 정보를 추출하여 중요한 대칭 속성을 유지하면서 장면 그래프를 생성하는 ESGNN(Equivariant Scene Graph Neural Network), (2) 근사 그래프 일치 알고리즘을 사용하여 여러 시간 시퀀스에 걸쳐 ESGNN에 의해 ​​생성된 장면 그래프를 통합된 전역 표현으로 융합하는 Temporal Graph Matching Network. 우리의 결합된 아키텍처 TESGNN은 장면 그래프 생성의 기존 방법에 비해 효과적인 것으로 나타났으며 더 높은 정확도와 더 빠른 훈련 수렴을 달성했습니다. 또한 대칭 보존 속성을 활용하면 기존 접근 방식에 비해 더 안정적이고 정확한 전역 장면 표현이 생성된다는 것을 보여줍니다. 마지막으로, 계산적으로 효율적이고 기존 프레임워크를 사용하여 쉽게 구현할 수 있으므로 로봇 공학 및 컴퓨터 비전의 실시간 애플리케이션에 매우 적합합니다. 이 접근 방식은 복잡한 다중 뷰 장면 이해 문제에 대한 보다 강력하고 확장 가능한 솔루션을 위한 길을 열어줍니다. 우리의 소스 코드는 https://github.com/HySonLab/TESGraph에서 공개적으로 제공됩니다."
677,http://arxiv.org/abs/2411.10446 ,VeriGraph: Scene Graphs for Execution Verifiable Robot Planning,"Daniel Ekpo, Mara Levy, Saksham Suri, Chuong Huynh, Abhinav Shrivastava","VLM(비전 언어 모델)의 최근 발전은 로봇 작업 계획에 대한 잠재력을 제공하지만 VLM이 잘못된 동작 시퀀스를 생성하는 경향으로 인해 과제가 남아 있습니다. 이러한 한계를 해결하기 위해 우리는 행동 타당성을 검증하면서 로봇 계획을 위해 VLM을 통합하는 새로운 프레임워크인 VeriGraph를 제안합니다. VeriGraph는 장면 그래프를 중간 표현으로 사용하여 주요 개체와 공간 관계를 캡처하여 계획 검증 및 개선을 개선합니다. 시스템은 입력 이미지에서 장면 그래프를 생성하고 이를 사용하여 LLM 기반 작업 플래너에서 생성된 작업 순서를 반복적으로 확인하고 수정하여 제약 조건이 준수되고 작업이 실행 가능하도록 보장합니다. 우리의 접근 방식은 다양한 조작 시나리오에서 작업 완료율을 크게 향상시켜 언어 기반 작업의 경우 기본 방법보다 58%, 이미지 기반 작업의 경우 30% 성능을 향상시킵니다."
676,http://arxiv.org/abs/2411.06048 ,An Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models,"Fatemeh Shiri, Xiao-Yu Guo, Mona Golestan Far, Xin Yu, Gholamreza Haffari, Yuan-Fang Li","LMM(Large Multimodal Model)은 다양한 비전 및 언어 작업에서 강력한 성능을 달성했습니다. 그러나 공간 추론 능력은 충분히 조사되지 않았습니다. 본 논문에서는 LMM의 공간 이해 및 추론 능력을 종합적으로 연구하기 위해 새로운 VQA 데이터 세트인 Spatial-MM을 구축합니다. 객체 관계 및 다중 홉 추론에 대한 우리의 분석은 몇 가지 중요한 결과를 보여줍니다. 첫째, 경계 상자와 장면 그래프는 합성 그래프라도 LMM의 공간 추론을 크게 향상시킬 수 있습니다. 둘째, LMM은 이미지에 대한 카메라 관점보다 인간 관점에서 제기된 질문에 더 많은 어려움을 겪습니다. 셋째, CoT(사고 사슬) 프롬프트는 공간 관계와 관련된 복잡한 다중 홉 질문에 대한 모델 성능을 향상시키지 않습니다. % 더욱이, 공간 추론 단계는 MLLM 전체의 비공간 추론 단계보다 훨씬 덜 정확합니다. 마지막으로 GQA 공간에 대한 섭동 분석을 통해 LMM이 복잡한 공간 추론보다 기본 개체 감지에 훨씬 더 강력하다는 것을 알 수 있습니다. 우리는 벤치마크 데이터 세트와 심층 분석이 LMM 공간 추론에 대한 추가 연구를 촉발할 수 있다고 믿습니다. Spatial-MM 벤치마크는 https://github.com/FatemehShiri/Spatial-MM에서 확인할 수 있습니다."
675,http://arxiv.org/abs/2411.03540 ,VLA-3D: A Dataset for 3D Semantic Scene Understanding and Navigation,"Haochen Zhang, Nader Zantout, Pujith Kachana, Zongyuan Wu, Ji Zhang, Wenshan Wang","최근 LLM(Large Language Model), VLM(Vision-Language Model) 및 기타 일반 기반 모델이 등장하면서 자연어만 입력으로 주어진 다양한 환경에서 작동할 수 있는 다중 모드, 다중 작업 구현 에이전트의 잠재력이 커지고 있습니다. 이러한 응용 분야 중 하나는 자연어 지침을 사용하는 실내 탐색입니다. 그러나 최근의 진전에도 불구하고 이 문제는 특히 세분화된 클래스에 속하는 많은 객체를 포함할 수 있는 임의의 장면에서 필요한 공간적 추론과 의미론적 이해로 인해 여전히 어려운 과제로 남아 있습니다. 이 문제를 해결하기 위해 우리는 기존 데이터세트에서 스캔한 11.5K개 이상의 3D 실내 공간, 객체 간 경험적으로 생성된 의미 관계 23.5M개, 합성적으로 생성된 참조문 970만개로 구성된 3D 장면의 비전 및 언어 기반 동작(VLA-3D)에 대한 최대 규모의 실제 데이터세트를 선별합니다. 우리의 데이터 세트는 처리된 3D 포인트 클라우드, 의미 개체 및 공간 주석, 장면 그래프, 탐색 가능한 여유 공간 주석, 개체 명확화를 위한 뷰 독립적 공간 관계에 특별히 초점을 맞춘 참조 언어 설명으로 구성됩니다. 이러한 기능의 목표는 탐색의 다운스트림 작업을 지원하는 것입니다. 특히 변화하는 장면과 불완전한 언어가 있는 열린 세상에서 일정 수준의 견고성이 보장되어야 하는 실제 시스템에서 더욱 그렇습니다. 우리는 성능 기준을 얻기 위해 현재 최첨단 모델로 데이터 세트를 벤치마킹합니다. 데이터세트를 생성하고 시각화하는 모든 코드는 공개적으로 공개되었습니다. https://github.com/HaochenZ11/VLA-3D를 참조하세요. 이 데이터 세트의 출시를 통해 우리는 변화에 강하고 대화형 실내 내비게이션 시스템 개발에 도움이 되는 의미론적 3D 장면 이해의 발전을 위한 리소스를 제공할 수 있기를 바랍니다."
674,http://arxiv.org/abs/2411.02938 ,Multi-Modal 3D Scene Graph Updater for Shared and Dynamic Environments,"Emilio Olivastri, Jonathan Francis, Alberto Pretto, Niko Sünderhauf, Krishan Rana","일반화된 LLM(대형 언어 모델) 및 VLM(대형 비전 모델)의 출현으로 로봇이 높은 수준의 추론과 계획을 표현에 기초할 수 있도록 하는 의미가 풍부한 맵의 구축이 간소화되었습니다. 가장 널리 사용되는 의미 지도 형식 중 하나는 메트릭(낮은 수준) 정보와 의미(높은 수준) 정보를 모두 캡처하는 3D 장면 그래프입니다. 그러나 이러한 지도는 종종 정적인 세계를 가정하는 반면, 집이나 사무실과 같은 실제 환경은 동적이라고 가정합니다. 이러한 공간의 작은 변화라도 작업 성능에 큰 영향을 미칠 수 있습니다. 로봇을 동적 환경에 통합하려면 실시간으로 변화를 감지하고 장면 그래프를 업데이트해야 합니다. 이 업데이트 프로세스는 본질적으로 다중 모드이므로 인간 에이전트, 로봇 자체 인식 시스템, 시간 및 동작과 같은 다양한 소스의 입력이 필요합니다. 이 작업은 이러한 다중 모드 입력을 활용하여 실시간 작업 중에 장면 그래프의 일관성을 유지하고 유망한 초기 결과를 제시하고 향후 연구를 위한 로드맵의 개요를 설명하는 프레임워크를 제안합니다."
673,http://arxiv.org/abs/2411.02452 ,Goal-Oriented Semantic Communication for Wireless Visual Question Answering,"Sige Liu, Nan Li, Yansha Deng, Tony Q. S. Quek","인공 지능(AI)과 컴퓨터 비전(CV)의 급속한 발전으로 인해 시각적 인식과 자연어 처리를 통합하여 답변을 생성하는 시각적 질문 응답(VQA)과 같은 계산 집약적인 애플리케이션의 개발이 촉진되었습니다. 로컬 계산 리소스로 인해 제한된 기존 VQA의 한계를 극복하기 위해 엣지 컴퓨팅이 통합되어 엣지 측에서 추가 계산 기능을 제공합니다. 한편, 이로 인해 특히 대용량 고해상도 이미지를 전송하는 동안 VQA 성능과 사용자 QoE(사용자 경험 품질)를 저하시키는 제한된 대역폭, 채널 노이즈, 다중 경로 효과 등 로컬과 에지 간의 새로운 통신 문제가 발생합니다. 이러한 병목 현상을 극복하기 위해 우리는 VQA 목표와 가장 관련성이 높은 의미 정보를 효과적으로 추출 및 전송하는 데 중점을 두고 응답 정확도를 높이고 효과성과 효율성을 높이는 목표 지향 의미 통신(GSC) 프레임워크를 제안합니다. 응답 정확도를 극대화하는 것이 목표이며, 질문의 목표에 따라 의미 정보의 우선순위를 정하기 위한 BBox(Bounding Box) 기반 이미지 의미 추출 및 순위 지정 접근 방식을 제안합니다. 그런 다음 복잡한 관계가 있는 질문을 처리하기 위해 장면 그래프(SG) 기반 접근 방식을 통합하여 이를 확장합니다. 실험 결과에 따르면 GSC 프레임워크는 AWGN 채널에서 응답 정확도를 최대 49%, Rayleigh 채널에서 59%까지 향상시키면서 기존 비트 지향 전송에 비해 총 대기 시간을 최대 65%까지 줄이는 것으로 나타났습니다."
672,http://arxiv.org/abs/2411.00578 ,Federated Voxel Scene Graph for Intracranial Hemorrhage,"Antoine P. Sanner, Jonathan Stieber, Nils F. Grauhan, Suam Kim, Marc A. Brockmann, Ahmed E. Othman, Anirban Mukhopadhyay",두개내 출혈은 증상이 매우 다양하고 전세계 임상 센터에 걸쳐 이동하는 잠재적으로 치명적인 상태입니다. 딥러닝 기반 솔루션은 뇌 구조 간의 복잡한 관계를 모델링하기 시작했지만 여전히 일반화하는 데 어려움을 겪고 있습니다. 보다 다양한 데이터를 수집하는 것이 가장 자연스러운 접근 방식이지만 개인 정보 보호 규정으로 인해 의료 데이터 공유가 제한되는 경우가 많습니다. 우리는 연합 장면 그래프 생성(Federated Scene Graph Generation)의 첫 번째 적용을 제안합니다. 우리는 모델이 증가된 교육 데이터 다양성을 활용할 수 있음을 보여줍니다. 장면 그래프 생성의 경우 단일 중앙 집중식 데이터 세트에 대해 훈련된 모델에 비해 데이터 세트 전체에서 임상적으로 관련성이 최대 20% 더 많이 기억될 수 있습니다. 연합 환경에서 구조화된 데이터 표현을 학습하면 이 미세한 정보를 활용하여 클라이언트 전체에서 보다 효과적으로 정규화할 수 있는 새로운 방법을 개발할 수 있는 길을 열 수 있습니다.
671,http://arxiv.org/abs/2410.23968 ,EmbodiedRAG: Dynamic 3D Scene Graph Retrieval for Efficient and Scalable Robot Task Planning,"Meghan Booker, Grayson Byrd, Bethany Kemp, Aurora Schmidt, Corban Rivera","LLM(대형 언어 모델)의 최근 발전은 실제 개방형 환경에서 로봇 계획을 위한 흥미로운 발전을 촉진하는 데 도움이 되었습니다. 3D 장면 그래프(3DSG)는 컴팩트하고 의미가 풍부하기 때문에 LLM 기반 플래너를 기반으로 하는 유망한 환경 표현을 제공합니다. 그러나 로봇의 환경이 확장되고(예: 추적되는 엔터티 수) 장면 그래프 정보의 복잡성이 증가함에 따라(예: 더 많은 속성 유지) 3DSG를 LLM 기반 플래너에 있는 그대로 제공하는 것은 입력 토큰 수 제한과 LLM에 존재하는 주의 편향으로 인해 빠르게 불가능해집니다. LLM 질문 및 답변을 위한 쿼리 관련 문서 청크를 검색하는 RAG(Retrieval-Augmented Generation) 방법의 성공에 영감을 받아 구현된 도메인에 대한 패러다임을 적용합니다. 구체적으로 우리는 자연어 로봇 작업을 실행하기 위해 LLM 기반 플래너를 강화하는 EmbodiedRAG라는 3D 장면 하위 그래프 검색 프레임워크를 제안합니다. 특히, 검색된 하위 그래프는 로봇이 계획을 실행할 때 환경 변화는 물론 작업 관련성 변화에도 적응합니다. 우리는 입력 토큰 수(크기순)와 계획 시간(계획 단계당 평균 시간 최대 70% 감소)을 크게 줄이는 동시에 단일 팔, 모바일 조작기를 사용하여 AI2Thor 시뮬레이션 가사 작업의 성공률을 높이는 EmbodiedRAG의 능력을 보여줍니다. 또한 실제 환경의 가장자리에 로봇을 배포할 때의 성능 이점을 강조하기 위해 조작기가 있는 4족 동물에 EmbodiedRAG를 구현합니다."
670,http://arxiv.org/abs/2410.23963 ,Exploiting Information Theory for Intuitive Robot Programming of Manual Activities,"Elena Merlo, Marta Lagomarsino, Edoardo Lamon, Arash Ajoudani","관찰 학습은 인간이 다른 사람을 관찰하여 새로운 행동을 학습하는 방식을 반영하기 때문에 프로그래밍에 대한 전문 지식이 없는 사람들이 사용자 친화적인 방식으로 로봇에 기술을 전달할 수 있도록 하는 유망한 접근 방식입니다. 기존의 많은 방법은 로봇이 인간의 궤적을 모방하도록 지시하는 데 중점을 두고 있지만 모션 수준 전략은 다양한 환경에서 기술을 일반화하는 데 종종 어려움을 겪습니다. 본 논문에서는 로봇이 RGB 비디오로 기록된 인간이 시연하는 수동 작업을 더 높은 수준으로 이해할 수 있도록 하는 새로운 프레임워크를 제안합니다. 로봇은 작업 구조와 목표를 인식함으로써 관찰된 내용을 보이지 않는 시나리오로 일반화합니다. 우리는 수동 작업에 처음으로 적용되는 Shannon의 정보 이론(IT)에서 작업 표현을 찾았습니다. IT는 활성 장면 요소를 추출하고 손과 물체 간에 공유되는 정보를 정량화하는 데 도움을 줍니다. 우리는 장면 그래프 속성을 활용하여 추출된 상호 작용 기능을 컴팩트한 구조로 인코딩하고 데모를 블록으로 분할하여 로봇 복제본에 대한 비헤이비어 트리 생성을 간소화합니다. 실험을 통해 인간의 한 번의 시연을 통해 로봇 실행 계획을 자동으로 생성하는 IT의 효율성이 검증되었습니다. 또한, 우리는 이 분야에 대한 추가 연구와 평가를 촉진하기 위해 다양한 주제로 시연되는 손 기술의 오픈 소스 데이터세트인 HANDSOME을 제공합니다."
669,http://arxiv.org/abs/2410.22829 ,Situational Scene Graph for Structured Human-centric Situation Understanding,"Chinthani Sugandhika, Chen Li, Deepu Rajan, Basura Fernando","그래프 기반 표현은 비디오 이해에서 시공간 관계를 모델링하는 데 널리 사용되었습니다. 효과적이긴 하지만 기존 그래프 기반 접근 방식은 동작 구성 요소의 세분화된 의미론적 속성을 무시하면서 인간-객체 관계를 캡처하는 데 중점을 둡니다. 이러한 의미적 속성은 작업이 어디서 발생하는지, 어떤 도구가 사용되는지, 개체의 기능적 속성과 같은 현재 상황을 이해하는 데 중요합니다. 본 연구에서는 인간-객체 관계와 해당 의미론적 속성을 모두 인코딩하기 위해 SSG(상황 장면 그래프)라는 그래프 기반 표현을 제안합니다. 의미론적 세부 사항은 원래 단일 동작을 나타내도록 설계된 상황 프레임에서 영감을 받아 미리 정의된 역할과 값으로 표현됩니다. 제안된 표현을 기반으로 상황별 장면 그래프 생성 작업을 소개하고 작업을 해결하기 위한 다단계 파이프라인인 InComNet(Interactive and Complementary Network)을 제안합니다. 기존 데이터 세트가 작업에 적용 가능하지 않다는 점을 고려하여 주석이 인간, 객체 및 인간-객체 관계의 동사 술어에 대한 의미론적 역할-가치 프레임으로 구성된 SSG 데이터 세트를 추가로 소개합니다. 마지막으로 다양한 다운스트림 작업을 테스트하여 제안된 SSG 표현의 효율성을 입증합니다. 실험 결과는 통합 표현이 술어 분류 및 의미론적 역할-가치 분류에 도움이 될 뿐만 아니라 인간 중심 상황 이해에 대한 추론 작업에도 도움이 될 수 있음을 보여줍니다. 곧 코드와 데이터 세트를 공개할 예정입니다."
668,http://arxiv.org/abs/2410.17751 ,VISAGE: Video Synthesis using Action Graphs for Surgery,"Yousef Yeganeh, Rachmadio Lazuardi, Amir Shamseddin, Emine Dari, Yash Thirani, Nassir Navab, Azade Farshad","수술 데이터 과학(SDS)은 수술 전, 수술 중, 수술 후 환자 데이터를 분석하여 수술 결과와 기술을 향상시키는 분야입니다. 그러나 수술 데이터는 부족하고 이질적이며 복잡하여 기존 기계 학습 방법의 적용 가능성이 제한됩니다. 본 연구에서는 복강경 수술에서 미래 비디오 세대의 새로운 과제를 소개합니다. 이 작업을 통해 기존 수술 데이터를 보강하고 강화할 수 있으며 시뮬레이션, 분석, 로봇 보조 수술 등 다양한 응용이 가능해집니다. 궁극적으로 이는 수술의 현재 상태를 이해하는 것뿐만 아니라 수술 절차의 역동적이고 종종 예측할 수 없는 특성을 정확하게 예측하는 것도 포함합니다. 우리가 제안한 방법인 VISAGE(수술용 동작 그래프를 사용한 비디오 합성)는 동작 장면 그래프의 기능을 활용하여 복강경 수술의 순차적 특성을 포착하고 확산 모델을 활용하여 시간적으로 일관된 비디오 시퀀스를 합성합니다. VISAGE는 단일 초기 프레임과 동작 그래프 세 개가 주어지면 미래 프레임을 예측합니다. VISAGE는 액션 그래프를 통해 도메인별 지식을 통합함으로써 생성된 비디오가 실제 복강경 수술에서 관찰된 예상 시각 및 동작 패턴을 준수하도록 보장합니다. 우리의 실험 결과는 SDS에서 다양한 응용을 가능하게 하는 복강경 수술을 위한 고화질 비디오 생성을 보여줍니다."
667,http://arxiv.org/abs/2410.16770 ,"The Scene Language: Representing Scenes with Programs, Words, and Embeddings","Yunzhi Zhang, Zizhang Li, Matt Zhou, Shangzhe Wu, Jiajun Wu","시각적 장면의 구조와 의미, 정체성을 간결하고 정확하게 기술하는 시각적 장면 표현인 Scene Language를 소개합니다. 장면에 있는 엔터티의 계층적 및 관계형 구조를 지정하는 프로그램, 각 엔터티의 의미 클래스를 요약하는 자연어로 된 단어, 각 엔터티의 시각적 정체성을 캡처하는 임베딩이라는 세 가지 주요 구성 요소로 장면을 나타냅니다. 이 표현은 텍스트 또는 이미지 입력이 주어지면 훈련이 필요 없는 추론 기술을 통해 사전 훈련된 언어 모델에서 추론할 수 있습니다. 결과 장면은 기존, 신경 또는 하이브리드 그래픽 렌더러를 사용하여 이미지로 렌더링될 수 있습니다. 이는 고품질 3D 및 4D 장면 생성을 위한 강력하고 자동화된 시스템을 형성합니다. 장면 그래프와 같은 기존 표현과 비교하여 제안된 Scene Language는 정확도가 높은 복잡한 장면을 생성하는 동시에 장면 구조를 명시적으로 모델링하여 정밀한 제어 및 편집이 가능합니다."
666,http://arxiv.org/abs/2410.15517 ,SceneGraMMi: Scene Graph-boosted Hybrid-fusion for Multi-Modal Misinformation Veracity Prediction,"Swarang Joshi, Siddharth Mavani, Joel Alex, Arnav Negi, Rahul Mishra, Ponnurangam Kumaraguru","잘못된 정보는 개인의 지식을 약화시키고 더 넓은 사회적 서술에 영향을 미칩니다. 다중 모드 잘못된 정보 탐지에 대한 연구 커뮤니티의 관심이 증가하고 있음에도 불구하고 기존 방법은 다중 모드 데이터 세트 내에서 의미론적 단서, 핵심 영역 및 교차 모드 유사성을 캡처하는 데 한계가 있습니다. 우리는 감지 성능을 향상시키기 위해 다양한 양식에 걸쳐 장면 그래프를 통합하는 다중 모드 잘못된 정보 진실성 예측을 위한 장면 그래프 강화 하이브리드 융합 접근 방식인 SceneGraMMi를 제안합니다. 4가지 벤치마크 데이터세트에 대한 실험 결과에 따르면 SceneGraMMi는 지속적으로 최첨단 방법보다 뛰어난 성능을 발휘하는 것으로 나타났습니다. 포괄적인 절제 연구에서 우리는 각 구성 요소의 기여도를 강조하고 Shapley 값을 사용하여 모델 의사 결정 과정의 설명 가능성을 조사합니다."
665,http://arxiv.org/abs/2410.15364 ,Scene Graph Generation with Role-Playing Large Language Models,"Guikun Chen, Jin Li, Wenguan Wang","개방형 어휘 장면 그래프 생성(OVSGG)을 위한 현재 접근 방식은 CLIP과 같은 비전 언어 모델을 사용하고 표준 제로샷 파이프라인을 따릅니다. 즉, 쿼리 이미지와 각 범주(예: 텍스트 분류자)에 대한 텍스트 임베딩 간의 유사성을 계산합니다. 이 작업에서 우리는 기존 OVSGG 방법에서 채택한 텍스트 분류자, 즉 카테고리/부분 수준 프롬프트가 컨텍스트 전반에 걸쳐 변경되지 않기 때문에 장면에 구애받지 않는다고 주장합니다. 이러한 고정 텍스트 분류자를 사용하면 변동성이 큰 시각적 관계를 모델링하는 데 어려움을 겪을 뿐만 아니라 고유한 컨텍스트에 적응하는 데도 부족합니다. 이러한 본질적인 단점을 보완하기 위해 텍스트 분류기의 가중치가 시각적 콘텐츠에 따라 적응적으로 조정되는 장면별 설명 기반 OVSGG 프레임워크인 SDSGG를 고안했습니다. 특히 장면에 초점을 맞춘 포괄적이고 다양한 설명을 생성하기 위해 LLM은 다양한 역할(예: 생물학자 및 엔지니어)을 수행하여 주어진 장면의 설명적 특징을 다양한 관점에서 분석하고 논의하도록 요청받습니다. 생성된 설명을 단순히 상호 동등한 텍스트 분류자로 처리하는 이전 노력과 달리 SDSGG는 제시된 장면과의 관련성을 기반으로 각 텍스트 분류자의 영향력을 조정하는 고급 재정규화 메커니즘을 갖추고 있습니다(이것이 ""특정""이라는 용어의 의미입니다). 또한 주체와 객체 간의 복잡한 상호작용을 포착하기 위해 상호 시각적 어댑터라는 새로운 경량 모듈을 제안합니다. 이는 상호작용 인식 의미 공간을 학습하여 관계를 인식하는 CLIP의 능력을 개선합니다. 널리 사용되는 벤치마크에 대한 광범위한 실험을 통해 SDSGG가 확실한 차이로 최고의 방법을 능가하는 것으로 나타났습니다."
664,http://arxiv.org/abs/2410.15312 ,Synergistic Dual Spatial-aware Generation of Image-to-Text and Text-to-Image,"Yu Zhao, Hao Fei, Xiangtai Li, Libo Qin, Jiayi Ji, Hongyuan Zhu, Meishan Zhang, Min Zhang, Jianguo Wei",VSU(시각적 공간 이해) 영역에서는 SI2T(공간적 이미지-텍스트)와 ST2I(공간적 텍스트-이미지)가 이중 형태로 나타나는 두 가지 기본 작업입니다. 독립형 SI2T 또는 ST2I에 대한 기존 방법은 3D 공간 특징 모델링의 어려움으로 인해 공간 이해가 불완전하게 수행됩니다. 이 작업에서는 이중 학습 프레임워크에서 SI2T와 ST2I를 함께 모델링하는 것을 고려합니다. 듀얼 프레임워크 동안 우리는 공유할 수 있고 두 작업 모두에 도움이 될 수 있는 새로운 3D 장면 그래프(3DSG) 표현을 사용하여 3D 공간 장면 특징을 표현할 것을 제안합니다. 또한 더 쉬운 3D$\to$image 및 3D$\to$text 프로세스가 각각 ST2I 및 SI2T에 대칭적으로 존재한다는 직관에 영감을 받아 3D$\to$X 프로세스의 중간 기능을 활용하여 하드 X$\to$3D 프로세스를 안내하여 전체 ST2I와 SI2T가 서로 이익을 얻을 수 있는 SD$^3$(Spatial Dual Discrete Diffusion) 프레임워크를 제안합니다. 시각적 공간 이해 데이터세트 VSD에서 우리 시스템은 주류 T2I 및 I2T 방법보다 훨씬 뛰어난 성능을 발휘합니다. 추가 심층 분석을 통해 우리의 이중 학습 전략이 어떻게 발전하는지 확인할 수 있습니다.
663,http://arxiv.org/abs/2410.13514 ,GraphSCENE: On-Demand Critical Scenario Generation for Autonomous Vehicles in Simulation,"Efimia Panagiotaki, Georgi Pramatarov, Lars Kunze, Daniele De Martini","실제 배포에 앞서 안전이 중요하고 다양한 시나리오에서 자율주행차(AV) 성능을 테스트하고 검증하는 것이 중요합니다. 그러나 시뮬레이션에서 이러한 시나리오를 수동으로 생성하는 것은 여전히 ​​중요하고 시간이 많이 걸리는 과제입니다. 이 작업은 AV 동작, 동적 에이전트 세트 및 중요도 수준과 같은 사용자 정의 기본 설정에 맞춰 주문형으로 다양한 트래픽 시나리오에 해당하는 동적 시간 장면 그래프를 생성하는 새로운 방법을 소개합니다. GNN(시간 그래프 신경망) 모델은 실제 시공간 상호 작용 패턴에 따라 안내되고 예측을 의미상 유효한 링크로 제한하는 온톨로지에 의해 제한되는 자가 차량, 에이전트 및 정적 구조 간의 관계를 예측하는 방법을 학습합니다. 우리 모델은 요청된 시나리오에 해당하는 링크를 정확하게 생성하는 데 있어서 지속적으로 기준선보다 성능이 뛰어납니다. AV 에이전트를 위한 테스트 환경으로서의 효율성을 추가로 입증하기 위해 시뮬레이션에서 예측된 시나리오를 렌더링합니다."
662,http://arxiv.org/abs/2410.13121 ,Trust but Verify: Programmatic VLM Evaluation in the Wild,"Viraj Prabhu, Senthil Purushwalkam, An Yan, Caiming Xiong, Ran Xu","VLM(Vision-Language Model)은 시각적 쿼리에 대해 그럴듯하지만 잘못된 응답을 생성하는 경우가 많습니다. 그러나 개방형 쿼리에 대한 자유 형식 응답에서 이러한 환각의 효과를 안정적으로 정량화하는 것은 응답 내의 각 주장을 시각적으로 확인해야 하기 때문에 어렵습니다. 우리는 개방형 쿼리에 대한 VLM 응답을 평가하기 위한 새로운 벤치마킹 패러다임인 PROVE(Programmatic VLM Evaluation)를 제안합니다. PROVE를 구축하기 위해 우리는 초상세 이미지 캡션으로 구성된 고품질 장면 그래프 표현을 갖춘 대형 언어 모델(LLM)을 제공하고, 다양한 질문-답변(QA) 쌍과 각 QA 쌍을 검증하기 위해 장면 그래프 개체에 대해 실행될 수 있는 프로그램을 생성하도록 유도합니다. 따라서 우리는 까다롭지만 시각적으로 기반이 되는 QA 쌍의 벤치마크를 구성합니다. 다음으로, PROVE의 쿼리에 대한 자유 형식 모델 응답을 평가하기 위해 통합 장면 그래프 기반 프레임워크 내에서 응답의 유용성과 진실성을 모두 측정하는 프로그래밍 방식 평가 전략을 제안합니다. 우리는 PROVE에서 다양한 VLM의 유용성-진실성 절충점을 벤치마킹한 결과 실제로 둘 사이에서 좋은 균형을 이룰 수 있는 VLM이 거의 없다는 사실을 발견했습니다. 프로젝트 페이지: \url{https://prove-explorer.netlify.app/}."
661,http://arxiv.org/abs/2410.11989 ,Dynamic Open-Vocabulary 3D Scene Graphs for Long-term Language-Guided Mobile Manipulation,"Zhijie Yan, Shufei Li, Zuoxu Wang, Lixiu Wu, Han Wang, Jun Zhu, Lijiang Chen, Jihong Liu",모바일 로봇이 역동적인 실제 환경에서 장기적인 작업을 수행할 수 있도록 하는 것은 매우 어려운 일입니다. 특히 인간과 로봇의 상호 작용이나 로봇 자체의 행동으로 인해 환경이 자주 변하는 경우에는 더욱 그렇습니다. 전통적인 방법은 일반적으로 정적 장면을 가정하므로 지속적으로 변화하는 현실 세계에서의 적용 가능성이 제한됩니다. 이러한 한계를 극복하기 위해 우리는 동적 개방형 어휘 3D 장면 그래프와 장기 작업 실행을 위한 언어 기반 작업 계획 모듈을 활용하는 새로운 모바일 조작 프레임워크인 DovSG를 제시합니다. DovSG는 RGB-D 시퀀스를 입력으로 사용하고 객체 감지를 위해 VLM(비전 언어 모델)을 활용하여 높은 수준의 객체 의미론적 특징을 얻습니다. 분할된 객체를 기반으로 하위 수준 공간 관계에 대해 구조화된 3D 장면 그래프가 생성됩니다. 또한 장면 그래프를 로컬로 업데이트하는 효율적인 메커니즘을 통해 로봇은 전체 장면을 재구성할 필요 없이 상호 작용 중에 그래프의 일부를 동적으로 조정할 수 있습니다. 이 메커니즘은 로봇이 장면 변화에 지속적으로 적응하고 장기적인 작업 실행을 효과적으로 지원할 수 있으므로 동적 환경에서 특히 유용합니다. 우리는 다양한 수준의 수동 수정을 통해 실제 환경에서 시스템을 검증하여 장기 작업에서 효율성과 탁월한 성능을 입증했습니다. 우리 프로젝트 페이지는 https://bjhyzj.github.io/dovsg-web에서 확인할 수 있습니다.
660,http://arxiv.org/abs/2410.11815 ,SGEdit: Bridging LLM with Text2Image Generative Model for Scene Graph-based Image Editing,"Zhiyuan Zhang, DongDong Chen, Jing Liao","장면 그래프는 개체와 개체 간의 관계를 상징하는 노드와 가장자리를 사용하여 구조화된 계층적 이미지 표현을 제공합니다. 이는 이미지 편집을 위한 자연스러운 인터페이스 역할을 하여 정확성과 유연성을 획기적으로 향상시킬 수 있습니다. 이러한 이점을 활용하여 장면 그래프 기반 이미지 편집을 위해 LLM(대형 언어 모델)을 Text2Image 생성 모델과 통합하는 새로운 프레임워크를 도입합니다. 이러한 통합을 통해 전반적인 이미지 무결성을 손상시키지 않으면서 개체 수준에서 정확한 수정과 장면의 창의적인 재구성이 가능해졌습니다. 우리의 접근 방식에는 두 가지 기본 단계가 포함됩니다. 1) LLM 기반 장면 파서를 활용하여 이미지의 장면 그래프를 구성하고, 주요 개체와 그 상호 관계를 캡처하고, 개체 마스크 및 설명과 같은 세부적인 속성을 구문 분석합니다. 이러한 주석은 미세 조정된 확산 모델을 통해 개념 학습을 촉진하고 최적화된 토큰과 자세한 설명 프롬프트로 각 개체를 나타냅니다. 2) 이미지 편집 단계에서 LLM 편집 컨트롤러는 특정 영역에 대한 편집을 안내합니다. 그런 다음 이러한 편집은 주의 조절 확산 편집기에 의해 구현되며, 미세 조정된 모델을 활용하여 개체 추가, 삭제, 교체 및 조정을 수행합니다. 광범위한 실험을 통해 우리는 프레임워크가 편집 정밀도와 장면 미학 측면에서 기존 이미지 편집 방법보다 훨씬 뛰어남을 보여줍니다."
659,http://arxiv.org/abs/2410.11187 ,Multiview Scene Graph,"Juexiao Zhang, Gao Zhu, Sihang Li, Xinhao Liu, Haorui Song, Xinran Tang, Chen Feng","적절한 장면 표현은 에이전트가 3D 장면을 강력하게 재구성하고 효율적으로 이해할 수 있는 공간 지능 추구의 핵심입니다. 장면 표현은 3D 재구성의 랜드마크 맵, 객체 감지의 3D 경계 상자 또는 점유 예측의 복셀 그리드와 같은 메트릭이거나 SLAM의 루프 폐쇄가 있는 포즈 그래프 또는 SfM의 가시성 그래프와 같은 토폴로지입니다. 이 작업에서 우리는 포즈가 없는 이미지로부터 MSG(Multiview Scene Graph)를 구축하여 상호 연결된 장소 및 개체 노드를 사용하여 장면을 위상적으로 표현하는 것을 제안합니다. MSG를 구축하는 작업은 제한된 시야와 잠재적으로 큰 관점 변경이 있는 이미지의 시각적 장소 인식, 객체 감지 및 객체 연관을 공동으로 해결해야 하기 때문에 기존 표현 학습 방법으로는 어렵습니다. 이 작업을 처리하는 방법을 평가하기 위해 우리는 공개 3D 데이터세트를 기반으로 MSG 데이터세트와 주석을 개발했습니다. 또한 MSG 에지의 Intersection-over-Union 점수를 기반으로 한 평가 지표를 제안합니다. 또한 우리는 시각적 장소 인식과 객체 연관을 하나의 Transformer 디코더 아키텍처로 결합하여 주류 사전 훈련된 비전 모델을 기반으로 구축된 새로운 기준 방법을 개발합니다. 실험은 우리의 방법이 기존 관련 기준에 비해 우수한 성능을 가지고 있음을 보여줍니다."
658,http://arxiv.org/abs/2410.08189 ,SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation,"Hang Yin, Xiuwei Xu, Zhenyu Wu, Jie Zhou, Jiwen Lu","본 논문에서는 제로샷 객체 탐색을 위한 새로운 프레임워크를 제안합니다. 기존의 제로샷 객체 탐색 방법은 공간적으로 닫힌 객체의 텍스트로 LLM을 유도하는데, 이는 심층 추론을 위한 충분한 장면 컨텍스트가 부족합니다. 환경 정보를 더 잘 보존하고 LLM의 추론 능력을 최대한 활용하기 위해 관찰된 장면을 3D 장면 그래프로 표현할 것을 제안합니다. 장면 그래프는 LLM 친화적인 구조로 개체, 그룹 및 방 간의 관계를 인코딩합니다. 이를 위해 LLM이 노드와 가장자리를 통과하여 장면 컨텍스트에 따라 목표 위치를 추론하는 데 도움이 되는 계층적 사고 체인 프롬프트를 설계합니다. 또한 장면 그래프 표현의 이점을 활용하여 객체 탐색 프레임워크에 인식 오류를 수정할 수 있는 기능을 부여하는 재인식 메커니즘을 추가로 설계합니다. 우리는 MP3D, HM3D 및 RoboTHOR 환경에 대한 광범위한 실험을 수행합니다. 여기서 SG-Nav는 모든 벤치마크에서 10% 이상의 SR로 이전 최첨단 제로샷 방법을 능가하며 결정 프로세스는 설명 가능합니다. 우리가 아는 한, SG-Nav는 까다로운 MP3D 벤치마크에서 지도 객체 탐색 방법보다 훨씬 더 높은 성능을 달성하는 최초의 제로샷 방법입니다."
657,http://arxiv.org/abs/2410.06239 ,OrionNav: Online Planning for Robot Autonomy with Context-Aware LLM and Open-Vocabulary Semantic Scene Graphs,"Venkata Naren Devarakonda, Raktim Gautam Goswami, Ali Umut Kaypak, Naman Patel, Rooholla Khorrambakht, Prashanth Krishnamurthy, Farshad Khorrami","로봇이 알려지지 않은 복잡하고 역동적인 환경을 자율적으로 탐색하고 다양한 작업을 수행할 수 있도록 하는 것은 강력한 자율 물리 에이전트를 개발하는 데 있어 근본적인 과제로 남아 있습니다. 이러한 에이전트는 의사 결정을 위해 세계 지식을 활용하면서 주변 환경을 효과적으로 인식해야 합니다. 최근 접근 방식은 장면 이해 및 계획을 위해 비전 언어 및 대규모 언어 모델을 활용하지만 종종 오프라인 처리, 오프보드 컴퓨팅에 의존하고 환경 및 인식에 대한 가정을 단순화하여 실제 적용 가능성을 제한합니다. 우리는 인식 및 계획 파이프라인 모두에 다단계 추상화를 통합하여 시간이 지남에 따라 변화하는 알 수 없는 환경에서 실시간 온보드 자율 탐색을 위한 새로운 프레임워크를 제시합니다. 우리 시스템은 현지화 및 매핑을 위해 여러 온보드 센서의 데이터를 융합하고 이를 개방형 어휘 의미론과 통합하여 지속적으로 업데이트되는 의미 객체 맵에서 계층적 장면 그래프를 생성합니다. LLM 기반 플래너는 이러한 그래프를 사용하여 자연어로 지정된 탐색 작업을 실행할 때 하위 수준 컨트롤러를 안내하는 다단계 계획을 만듭니다. 시스템의 실시간 작동을 통해 LLM은 장면 그래프 및 작업 실행 상태 업데이트를 기반으로 계획을 조정할 수 있으므로 새로운 상황에 대한 지속적인 적응을 보장하거나 현재 계획으로 작업을 수행할 수 없는 경우 이는 정적 또는 규칙 기반 시스템에 비해 주요 이점입니다. 우리는 네 발 달린 동물 탐색 동적 환경에서 시스템의 효율성을 입증하고 다양한 시나리오에서의 적응성과 견고성을 보여줍니다."
656,http://arxiv.org/abs/2410.00447 ,Scene Graph Disentanglement and Composition for Generalizable Complex Image Generation,"Yunnan Wang, Ziqiang Li, Zequn Zhang, Wenyao Zhang, Baao Xie, Xihui Liu, Wenjun Zeng, Xin Jin","자연어 또는 레이아웃 조건에서 이미지를 생성하는 데 흥미로운 진전이 있었습니다. 그러나 이러한 방법은 여러 객체와 그 관계에 대한 모델링이 부족하여 복잡한 장면을 충실하게 재현하는 데 어려움을 겪습니다. 이 문제를 해결하기 위해 우리는 복잡한 이미지 생성을 위한 강력한 구조적 표현인 장면 그래프를 활용합니다. 생성을 위해 장면 그래프를 직접 사용했던 이전 작업과 달리, 우리는 변형 오토인코더와 확산 모델의 생성 기능을 일반화 가능한 방식으로 사용하여 장면 그래프에서 얽혀 있지 않은 다양한 시각적 단서를 합성합니다. 구체적으로, 우리는 먼저 입력 장면 그래프에서 (레이아웃, 의미론)을 공동으로 도출하는 SL-VAE(Semantics-Layout Variational AutoEncoder)를 제안합니다. 이를 통해 일대다 매핑에서 보다 다양하고 합리적인 생성이 가능합니다. 그런 다음 확산 모델과 통합된 CMA(Compositional Masked Attention)를 개발하고 생성 지침으로 세분화된 속성(레이아웃, 의미론)을 통합합니다. 시각적 콘텐츠의 일관성을 유지하면서 그래프 조작을 더욱 달성하기 위해 ""격리된"" 이미지 편집 효과를 위한 MLS(Multi-Layered Sampler)를 도입했습니다. 광범위한 실험을 통해 우리의 방법이 생성 합리성과 제어 가능성 측면에서 텍스트, 레이아웃 또는 장면 그래프를 기반으로 하는 최근 경쟁사보다 우수한 것으로 나타났습니다."
655,http://arxiv.org/abs/2410.00253 ,MM-Conv: A Multi-modal Conversational Dataset for Virtual Humans,"Anna Deichler, Jim O'Regan, Jonas Beskow","본 논문에서는 물리 시뮬레이터(AI2-THOR) 내 참가자 간의 대화를 기록하기 위해 VR 헤드셋을 사용하여 캡처한 새로운 데이터 세트를 제시합니다. 우리의 주요 목표는 참조 설정 내에 풍부한 상황 정보를 통합하여 공동 음성 제스처 생성 분야를 확장하는 것입니다. 참가자들은 모두 참조 의사소통 작업을 기반으로 다양한 대화 시나리오에 참여했습니다. 데이터 세트는 모션 캡처, 음성, 시선 및 장면 그래프와 같은 다양한 다중 모드 기록 세트를 제공합니다. 이 포괄적인 데이터 세트는 다양하고 상황에 맞게 풍부한 데이터를 제공하여 3D 장면에서 제스처 생성 모델의 이해와 개발을 향상시키는 것을 목표로 합니다."
654,http://arxiv.org/abs/2409.18743 ,OpenObject-NAV: Open-Vocabulary Object-Oriented Navigation Based on Dynamic Carrier-Relationship Scene Graph,"Yujie Tang, Meiling Wang, Yinan Deng, Zibo Zheng, Jiagui Zhong, Yufeng Yue","일상생활에서 컵과 같이 자주 사용되는 물건은 위치가 고정되지 않고 동일한 카테고리 내에서 여러 인스턴스가 있는 경우가 많으며, 담는 사람도 자주 변경됩니다. 결과적으로 로봇이 특정 인스턴스를 효율적으로 탐색하는 것이 어려워집니다. 이 과제를 해결하려면 로봇은 장면 변화와 계획을 지속적으로 캡처하고 업데이트해야 합니다. 그러나 현재의 객체 탐색 접근 방식은 주로 의미 수준에 초점을 맞추고 있으며 장면 표현을 동적으로 업데이트하는 기능이 부족합니다. 이 백서는 자주 사용되는 객체와 정적 캐리어 간의 관계를 포착합니다. 개방형 어휘 CRSG(Carrier-Relationship Scene Graph)를 구성하고 로봇 탐색 중 운반 상태를 업데이트하여 장면의 동적 변화를 반영합니다. CRSG를 기반으로 탐색 프로세스를 Markov 결정 프로세스로 모델링하는 인스턴스 탐색 전략을 추가로 제안합니다. 각 단계에서 결정은 대형 언어 모델의 상식적 지식과 시각적 언어 기능 유사성을 바탕으로 이루어집니다. 우리는 Habitat 시뮬레이터에서 자주 사용되는 일상 항목에 대한 일련의 긴 순서 탐색 작업을 설계했습니다. 결과는 CRSG를 업데이트함으로써 로봇이 이동된 목표를 효율적으로 탐색할 수 있음을 보여줍니다. 또한 실제 로봇에 알고리즘을 배포하고 실제 효율성을 검증했습니다."
653,http://arxiv.org/abs/2409.15684 ,SYNERGAI: Perception Alignment for Human-Robot Collaboration,"Yixin Chen, Guoxi Zhang, Yaowei Zhang, Hongming Xu, Peiyuan Zhi, Qing Li, Siyuan Huang",최근에는 LLM(대형 언어 모델)이 인간-로봇 상호 작용 및 협업을 촉진하는 데 강력한 잠재력을 보여주었습니다. 그러나 기존 LLM 기반 시스템은 인간과 로봇 인식 간의 불일치를 간과하는 경우가 많아 효과적인 의사소통과 실제 로봇 배포를 방해합니다. 이 문제를 해결하기 위해 우리는 지각 정렬과 인간-로봇 협업을 모두 달성하도록 설계된 통합 시스템인 SYNERGAI를 소개합니다. SYNERGAI는 핵심적으로 3D 장면 그래프(3DSG)를 명시적이고 자연스러운 표현으로 사용합니다. 이를 통해 시스템은 LLM을 활용하여 복잡한 작업을 세분화하고 중간 단계에서 적절한 도구를 할당하여 3DSG에서 관련 정보를 추출하거나 구조를 수정하거나 응답을 생성할 수 있습니다. 중요한 점은 SYNERGAI가 온라인 상호 작용으로 3DSG를 업데이트하여 사용자의 지각 불일치 교정을 가능하게 하는 자동 메커니즘을 통합했다는 것입니다. SYNERGAI는 제로샷 방식으로 ScanQA의 데이터 기반 모델과 비슷한 성능을 달성합니다. 10개의 실제 장면에 대한 포괄적인 실험을 통해 SYNERGAI는 인간과 공통 기반을 구축하는 데 있어 효율성을 입증하고 정렬 작업에서 61.9%의 성공률을 실현했습니다. 또한 정렬 중에 얻은 지식을 전달하여 새로운 작업의 성공률을 3.7%에서 45.68%로 크게 향상시킵니다.
652,http://arxiv.org/abs/2409.14908 ,KARMA: Augmenting Embodied AI Agents with Long-and-short Term Memory Systems,"Zixuan Wang, Bo Yu, Junzhe Zhao, Wenhao Sun, Sai Hou, Shuai Liang, Xing Hu, Yinhe Han, Yiming Gan","상호 연결된 긴 순서의 가사 작업을 담당하는 내장된 AI 에이전트는 상황 내 메모리 문제에 직면하여 작업 실행 시 비효율성과 오류가 발생하는 경우가 많습니다. 이 문제를 해결하기 위해 우리는 장기 및 단기 메모리 모듈을 통합하고 메모리 증강 프롬프트를 통해 구체화된 에이전트의 계획을 위한 LLM(대형 언어 모델)을 향상시키는 혁신적인 메모리 시스템인 KARMA를 소개합니다. KARMA는 장기 기억과 단기 기억을 구별합니다. 장기 기억은 환경의 표현으로 포괄적인 3D 장면 그래프를 캡처하는 반면, 단기 기억은 물체의 위치와 상태의 변화를 동적으로 기록합니다. 이 듀얼 메모리 구조를 통해 에이전트는 관련 과거 장면 경험을 검색할 수 있으므로 작업 계획의 정확성과 효율성이 향상됩니다. 단기 기억은 효과적이고 적응력이 뛰어난 메모리 교체 전략을 사용하여 관련성이 덜한 데이터를 삭제하면서 중요한 정보를 보존합니다. 메모리로 강화된 최첨단 구현 에이전트와 비교하여 메모리 증강 구현 AI 에이전트는 AI2-THOR 시뮬레이터 내 복합 작업 및 복합 작업에서 성공률을 각각 1.3배 및 2.3배 향상시키고 작업 실행 효율성을 3.4배 및 62.7배 향상시킵니다. 또한 KARMA의 플러그 앤 플레이 기능을 통해 모바일 조작 플랫폼과 같은 실제 로봇 시스템에 원활하게 배포할 수 있음을 보여줍니다. 이 플러그 앤 플레이 메모리 시스템을 통해 KARMA는 일관되고 상황에 맞게 적절한 계획을 생성하는 내장된 에이전트의 능력을 크게 향상시켜 복잡한 가사 작업을 보다 효율적으로 실행할 수 있습니다. 해당 작품의 실험 영상은 https://youtu.be/4BT7fnw9ehs에서 확인하실 수 있습니다. 우리 코드는 https://github.com/WZX0Swarm0Robotics/KARMA/tree/master에서 확인할 수 있습니다."
651,http://arxiv.org/abs/2409.13612 ,FIHA: Autonomous Hallucination Evaluation in Vision-Language Models with Davidson Scene Graphs,"Bowen Yan, Zhengsong Zhang, Liqiang Jing, Eftekhar Hossain, Xinya Du","LVLM(Large Vision-Language Model)의 급속한 개발로 인해 광범위한 환각 문제가 발생하는 경우가 많아 비용 효율적이고 포괄적인 평가가 점점 더 중요해지고 있습니다. 현재 접근 방식은 주로 비용이 많이 드는 주석에 의존하며 관계, 속성 및 측면 간의 종속성과 같은 모든 측면을 평가한다는 측면에서 포괄적이지 않습니다. 따라서 LLM 및 주석 없는 방식으로 환각 LVLM에 액세스하고 다양한 유형의 환각 간의 종속성을 모델링할 수 있는 FIHA(LVLM의 자율적 세분화된 환각 평가 평가)를 소개합니다. FIHA는 최소한의 비용으로 모든 이미지 데이터 세트에 대해 Q&A 쌍을 생성하여 이미지와 캡션 모두에서 환각 평가를 가능하게 합니다. 이러한 접근 방식을 바탕으로 MSCOCO와 Foggy의 다양한 이미지에 대한 다양한 질문으로 구성된 FIHA-v1이라는 벤치마크를 소개합니다. 또한 DSG(Davidson Scene Graph)를 사용하여 Q&A 쌍 간의 구조를 구성하여 평가의 신뢰성을 높일 수 있습니다. FIHA-v1을 사용하여 대표 모델을 평가하여 한계와 과제를 강조합니다. 우리는 코드와 데이터를 공개했습니다."
650,http://arxiv.org/abs/2409.11972 ,Generation of Uncertainty-Aware High-Level Spatial Concepts in Factorized 3D Scene Graphs via Graph Neural Networks,"Jose Andres Millan-Romera, Muhammad Shaheer, Miguel Fernandez-Cortizas, Martin R. Oswald, Holger Voos, Jose Luis Sanchez-Lopez","로봇이 3D 장면 그래프 내의 기본 기하학적 관찰(예: 평면 표면)에서 높은 수준의 공간 개념(예: 방 및 벽)을 자율적으로 발견할 수 있도록 하는 것은 강력한 실내 탐색 및 매핑에 필수적입니다. 이러한 그래프는 그러한 개념이 구성되는 계층적 메트릭-의미론적 표현을 제공합니다. 그래프-SLAM 성능을 더욱 향상시키기 위해 Factorized 3D Scene Graphs는 이러한 개념을 상대 기하학을 제한하고 전역 일관성을 강화하는 최적화 요소로 통합합니다. 그러나 이 프로세스의 두 단계 모두 대부분 수동으로 유지됩니다. 개념은 일반적으로 손으로 만든 개념별 경험적 방법을 사용하여 파생되는 반면 요인과 공분산도 마찬가지로 수동으로 설계됩니다. 수동 사양에 대한 이러한 의존은 다양한 환경에서의 일반화와 새로운 개념 클래스에 대한 확장성을 제한합니다. 이 논문에서는 관찰된 수직 평면에서 온라인으로 공간 개념을 추론하고 이를 SLAM 백엔드 내에서 최적화 가능한 요소로 도입하여 개념 생성, 요소 설계 및 공분산 사양을 직접 작성할 필요가 없는 새로운 학습 기반 방법을 제시합니다. 우리는 복잡한 레이아웃이 있는 시뮬레이션 환경에서 우리의 접근 방식을 평가하여 공간 감지를 20.7%, 궤적 추정을 19.2% 향상시켰으며, 실제 건설 현장에서 이를 검증하여 공간 감지가 5.3% 향상되고 지도 일치 정확도가 3.8% 향상되었습니다. 결과는 학습된 요소가 SLAM 시스템에서 수작업으로 만들어진 요소를 개선할 수 있고 이 접근 방식을 새로운 공간 개념으로 확장하기 위한 기반이 될 수 있음을 확인합니다."
649,http://arxiv.org/abs/2409.11870 ,SpotLight: Robotic Scene Understanding through Interaction and Affordance Detection,"Tim Engelbracht, René Zurbrügg, Marc Pollefeys, Hermann Blum, Zuria Bauer","가정용 로봇 공학에 대한 연구 노력이 증가하고 있음에도 불구하고 가정용 환경에 배치할 로봇은 주로 제한된 작업별 이해 및 상호 작용 기능으로 인해 서랍이나 전등 스위치와 같은 기능 요소와 상호 작용하는 등 더 복잡한 작업에 여전히 어려움을 겪고 있습니다. 이러한 작업에는 감지 및 포즈 추정뿐만 아니라 이러한 요소가 제공하는 어포던스에 대한 이해도 필요합니다. 이러한 과제를 해결하고 로봇 장면에 대한 이해를 높이기 위해 SpotLight를 소개합니다. 기능 요소, 특히 조명 스위치와의 로봇 상호 작용을 위한 포괄적인 프레임워크입니다. 또한 이 프레임워크를 통해 로봇은 상호작용을 통해 환경에 대한 이해를 향상시킬 수 있습니다. VLM 기반 어포던스 예측을 활용하여 전등 스위치 상호 작용에 대한 모션 프리미티브를 추정하여 실제 실험에서 최대 84%의 작동 성공률을 달성했습니다. 또한 715개의 이미지가 포함된 특수 데이터 세트와 전등 스위치 감지를 위한 사용자 정의 감지 모델을 소개합니다. 로봇이 환경을 탐색하고 장면 그래프 표현에서 이전에 알려지지 않은 관계를 발견하도록 함으로써 프레임워크가 물리적 상호 작용을 통해 로봇 학습을 어떻게 촉진할 수 있는지 보여줍니다. 마지막으로 우리는 스윙 도어와 같은 다른 기능적 상호 작용을 수용하여 유연성을 보여주는 프레임워크 확장을 제안합니다. 비디오 및 코드: timengelbracht.github.io/SpotLight/"
648,http://arxiv.org/abs/2409.10350 ,Point2Graph: An End-to-end Point Cloud-based 3D Open-Vocabulary Scene Graph for Robot Navigation,"Yifan Xu, Ziming Luo, Qianwei Wang, Vineet Kamat, Carol Menassa","현재 개방형 어휘 장면 그래프 생성 알고리즘은 3D 장면 포인트 클라우드 데이터와 포즈를 취한 RGB-D 이미지에 크게 의존하므로 RGB-D 이미지 또는 카메라 포즈를 쉽게 사용할 수 없는 시나리오에서는 적용이 제한됩니다. 이 문제를 해결하기 위해 우리는 포즈를 취한 RGB-D 이미지 시리즈의 요구 사항을 제거하는 새로운 엔드 투 엔드 포인트 클라우드 기반 3D 개방형 어휘 장면 그래프 생성 프레임워크인 Point2Graph를 제안합니다. 이 계층적 프레임워크에는 공간 및 객체 감지/분할 및 개방형 어휘 분류가 포함되어 있습니다. 룸 레이어의 경우 학습 기반 영역 감지와 기하학 기반 경계 감지 알고리즘을 병합하여 룸을 분할하고 개방형 어휘 룸 분류를 위한 ""스냅 조회"" 프레임워크를 생성하는 이점을 활용합니다. 또한 3D 포인트 클라우드 데이터만을 기반으로 3D 객체를 감지하고 분류하기 위해 객체 레이어에 대한 엔드투엔드 파이프라인을 생성합니다. 우리의 평가 결과는 우리의 프레임워크가 널리 사용되는 실제 장면 데이터세트에서 현재의 최첨단(SOTA) 개방형 어휘 개체 및 공간 분할 및 분류 알고리즘보다 성능이 뛰어날 수 있음을 보여줍니다."
647,http://arxiv.org/abs/2409.10262 ,Hydra-SGG: Hybrid Relation Assignment for One-stage Scene Graph Generation,"Minghan Chen, Guikun Chen, Wenguan Wang, Yi Yang","DETR은 장면 그래프 생성(SGG)을 위한 단순화된 1단계 프레임워크를 도입하지만 희소 감독 및 위음성 샘플 문제에 직면해 있습니다. 전자는 일반적으로 각 이미지에 10개 미만의 관계 주석이 포함되어 있는 반면 DETR 기반 SGG 모델은 100개가 넘는 관계 쿼리를 사용하기 때문에 발생합니다. 각 정답 관계는 훈련 중에 하나의 쿼리에만 할당됩니다. 후자는 하나의 실측 관계에 유사한 일치 점수를 가진 여러 쿼리가 있을 수 있어 최적이 아닌 일치 쿼리가 음수 샘플로 처리되는 경우에 발생합니다. 이를 해결하기 위해 우리는 Hybrid Relation Assignment를 특징으로 하는 1단계 SGG 방법인 Hydra-SGG를 제안합니다. 이 접근 방식은 일대일 관계 할당과 IoU 기반 일대다 관계 할당을 결합하여 긍정적인 훈련 샘플을 늘리고 희소 감독을 완화합니다. 또한 관계 쿼리 간의 self-attention을 제거하면 중복 예측이 발생하고 이는 실제로 제안된 일대다 관계 할당에 이점이 있음을 경험적으로 보여줍니다. 이러한 통찰력을 바탕으로 Self-Attention 레이어가 없는 보조 디코더인 Hydra Branch를 도입하여 동일한 관계 예측을 위해 다양한 쿼리를 승격시켜 일대다 관계 할당을 더욱 향상시킵니다. Hydra-SGG는 VG150(16.0 mR@50), Open Images V6(50.1 가중치 점수) 및 GQA(12.7 mR@50)를 포함한 여러 데이터 세트에서 최첨단 성능을 달성합니다."
646,http://arxiv.org/abs/2409.06625 ,Towards Localizing Structural Elements: Merging Geometrical Detection with Semantic Verification in RGB-D Data,"Ali Tourani, Saad Ejaz, Hriday Bavle, Jose Luis Sanchez-Lopez, Holger Voos","RGB-D 카메라는 장면 이해, 지도 재구성, 위치 파악 등 다양한 로봇 공학 작업을 위해 풍부하고 조밀한 시각 및 공간 정보를 제공합니다. 깊이와 시각적 정보를 통합하면 위치 파악 및 요소 매핑, 3D 장면 그래프 생성 및 VSLAM(Visual Simultaneous Localization and Mapping)과 같은 응용 프로그램을 발전시키는 로봇에 도움이 될 수 있습니다. 이러한 정보를 포함하는 포인트 클라우드 데이터는 주로 장면 이해를 향상시키는 데 사용되지만, 풍부한 의미 정보를 캡처하고 표현하는 잠재력을 활용하는 것은 아직 적절하게 목표화되지 않았습니다. 본 논문에서는 순수 3D 평면 감지를 위한 기하학적 계산을 통합한 다음 RGB-D 카메라의 포인트 클라우드 데이터를 사용하여 의미 범주를 검증함으로써 벽 및 지표면을 포함한 건물 구성 요소의 위치를 ​​파악하기 위한 실시간 파이프라인을 제시합니다. 여기에는 환경에서 감지된 모든 평면의 자세와 방정식을 정확하게 추정하고, 범광학 분할 검증을 사용하여 지도 구조를 형성하는 평면을 필터링하고, 검증된 건물 구성요소만 유지하는 병렬 다중 스레드 아키텍처가 있습니다. 제안된 방법을 VSLAM 프레임워크에 통합하면 감지된 환경 기반 의미 요소로 지도를 제한하면 장면 이해와 지도 재구성 정확도가 향상될 수 있음이 확인되었습니다. 또한 이러한 감지된 구성요소를 통합 3D 장면 그래프로 (재)연결하여 기하학적 정확성과 의미론적 이해 간의 격차를 해소할 수 있습니다. 또한 파이프라인을 사용하면 레이아웃을 기반으로 건물 구성 요소 간의 관계를 식별하여 방과 같은 잠재적인 상위 구조 개체를 감지할 수 있습니다."
645,http://arxiv.org/abs/2409.05392 ,Leveraging Computation of Expectation Models for Commonsense Affordance Estimation on 3D Scene Graphs,"Mario A. V. Saucedo, Nikolaos Stathoulopoulos, Akash Patel, Christoforos Kanellakis, George Nikolakopoulos","이 기사에서는 도시 환경에서 구현된 로봇 에이전트의 인간에 가까운 작업 계획 및 작업 최적화를 가능하게 하기 위한 상식적인 개체 어포던스 개념을 연구합니다. 객체 어포던스의 초점은 작업 실행 중에 객체에 내재된 유용성을 효과적으로 식별하는 방법을 추론하는 데 있으며, 이는 3차원 장면 그래프의 희소 정보의 맥락적 관계 분석을 통해 가능합니다. 제안된 프레임워크는 Graph Convolutional Network를 사용하여 확률 분포를 학습하는 상관 정보(CECI) 모델을 개발하여 의미 클래스의 개별 구성원에 대한 상식적 어포던스를 추출할 수 있습니다. 전체 프레임워크는 실제 실내 환경에서 실험적으로 검증되었으며, 인간의 상식에 맞는 방법의 능력을 보여주었습니다. 실험적 시연을 보여주는 기사의 비디오를 보려면 다음 링크를 참조하십시오: https://youtu.be/BDMVx2GiQE"
644,http://arxiv.org/abs/2409.02389 ,Multi-modal Situated Reasoning in 3D Scenes,"Xiongkun Linghu, Jiangyong Huang, Xuesong Niu, Xiaojian Ma, Baoxiong Jia, Siyuan Huang","구현된 AI 에이전트의 3D 장면을 이해하고 추론하려면 상황 인식이 필수적입니다. 그러나 상황 이해를 위한 기존 데이터 세트와 벤치마크는 데이터 양식, 다양성, 규모 및 작업 범위가 제한되어 있습니다. 이러한 제한 사항을 해결하기 위해 우리는 다양한 범위의 실제 3D 장면에서 3D 장면 그래프와 VLM(비전 언어 모델)을 활용하여 확장 가능하게 수집된 대규모 다중 모드 상황 추론 데이터 세트인 다중 모드 상황별 질문 응답(MSQA)을 제안합니다. MSQA에는 9개의 서로 다른 질문 범주에 걸쳐 251K 위치의 질문 답변 쌍이 포함되어 있으며 3D 장면 내의 복잡한 시나리오를 다루고 있습니다. 상황 및 질문 설명을 위한 텍스트, 이미지 및 포인트 클라우드를 제공하기 위해 벤치마크에 새로운 인터리브 다중 모드 입력 설정을 도입하여 이전 단일 모드 규칙(예: 텍스트)의 모호성을 해결했습니다. 또한 우리는 탐색에 대한 모델의 상황 추론을 평가하기 위해 MSNN(Multi-modal Should Next-step Navigation) 벤치마크를 고안했습니다. MSQA 및 MSNN에 대한 종합적인 평가는 기존 비전 언어 모델의 한계를 강조하고 다중 모드 인터리브 입력 및 상황 모델링 처리의 중요성을 강조합니다. 데이터 스케일링 및 도메인 간 전송에 대한 실험은 보다 강력한 상황 추론 모델을 개발하기 위한 사전 훈련 데이터 세트로 MSQA를 활용하는 것의 효율성을 추가로 보여줍니다."
643,http://arxiv.org/abs/2408.16760 ,OmniRe: Omni Urban Scene Reconstruction,"Ziyu Chen, Jiawei Yang, Jiahui Huang, Riccardo de Lutio, Janick Martinez Esturo, Boris Ivanovic, Or Litany, Zan Gojcic, Sanja Fidler, Marco Pavone, Li Song, Yue Wang","온디바이스 로그에서 역동적인 실제 장면의 충실도 높은 디지털 트윈을 효율적으로 생성하기 위한 포괄적인 시스템인 OmniRe를 소개합니다. 신경장 또는 Gaussian Splatting을 사용하는 최근 방법은 주로 차량에 중점을 두므로 인간 행동 시뮬레이션과 같은 다운스트림 애플리케이션에서 요구되는 모든 동적 전경에 대한 전체적인 프레임워크를 방해합니다. OmniRe는 차량 모델링을 넘어 도시 장면의 다양한 동적 개체를 전체 길이로 정확하게 재구성할 수 있도록 확장합니다. 우리의 접근 방식은 3DGS에 장면 그래프를 구축하고 차량, 보행자, 자전거 타는 사람 등 다양한 동적 행위자를 모델링하는 표준 공간에 여러 가우스 표현을 구성합니다. OmniRe를 사용하면 장면의 모든 동적 개체를 전체적으로 재구성하여 보행자 행동 시뮬레이션 및 인간-차량 상호 작용과 같은 인간 참여 시나리오를 포함하는 고급 시뮬레이션(~60Hz)을 가능하게 합니다. 이 포괄적인 시뮬레이션 기능은 기존 방법과 비교할 수 없습니다. Waymo 데이터세트에 대한 광범위한 평가는 우리의 접근 방식이 이전의 최첨단 방법보다 양적, 질적으로 큰 차이로 뛰어남을 보여줍니다. 우리는 일반적인 도시 현장에서의 일반화 가능성을 입증하기 위해 결과를 5개의 추가 인기 운전 데이터 세트로 확장합니다."
642,http://arxiv.org/abs/2408.16621 ,Towards Infusing Auxiliary Knowledge for Distracted Driver Detection,"Ishwar B Balappanawar, Ashmit Chamoli, Ruwan Wickramarachchi, Aditya Mishra, Ponnurangam Kumaraguru, Amit P. Sheth","부주의한 운전은 전 세계적으로 교통사고의 주요 원인입니다. 주의 산만 운전을 식별하려면 차량 내 카메라 피드에서 다양한 형태의 운전자 산만(예: 문자 메시지, 식사 또는 차량 내 장치 사용)을 안정적으로 감지하고 분류하여 도로 안전을 강화해야 합니다. 이 작업은 광범위한 주석이 달린 데이터 세트가 필요하지 않고 다양한 운전자 행동 세트를 일반화할 수 있는 강력한 모델이 필요하기 때문에 어렵습니다. 본 논문에서는 장면 내 엔터티 간의 의미 관계와 운전자 자세의 구조적 구성에 대한 보조 지식을 주입하여 산만한 운전자 감지(DDD)를 위한 새로운 방법인 KiD3를 제안합니다. 구체적으로, 우리는 장면 그래프와 운전자 자세 정보를 비디오 프레임의 시각적 단서와 통합하여 운전자의 행동을 전체적으로 표현하는 통합 프레임워크를 구축합니다. 우리의 결과는 KiD3가 이러한 보조 지식을 시각적 정보와 통합함으로써 비전 전용 기준에 비해 13.64%의 정확도 향상을 달성한 것으로 나타났습니다."
641,http://arxiv.org/abs/2408.16224 ,LLaVA-SG: Leveraging Scene Graphs as Visual Semantic Expression in Vision-Language Models,"Jingyi Wang, Jianzhong Ju, Jian Luan, Zhidong Deng",대규모 VLM(비전 언어 모델)의 최근 발전은 일반적으로 ViT(Vision Transformer) 아키텍처를 기반으로 하는 비전 인코더를 사용합니다. ViT에 의해 이미지를 패치로 분할하면 인식이 단편화되어 VLM의 시각적 이해 기능이 저하됩니다. 본 논문에서는 VLM에 SGE(Scene Graph Expression) 모듈을 도입하여 이러한 제한을 해결하는 혁신적인 개선 사항을 제안합니다. 이 모듈은 이미지 내의 복잡한 의미 정보를 추출하고 구조적으로 표현함으로써 VLM의 기본 인식 및 이해 능력을 향상시킵니다. 광범위한 실험을 통해 SGE 모듈을 통합하면 비전 언어 작업에서 VLM의 성능이 크게 향상되어 복잡한 의미 세부 사항을 보존하고 더 나은 시각적 이해를 촉진하는 효과가 있음이 입증되었습니다.
640,http://arxiv.org/abs/2408.14941 ,BOX3D: Lightweight Camera-LiDAR Fusion for 3D Object Detection and Localization,"Mario A. V. Saucedo, Nikolaos Stathoulopoulos, Vidya Sumathy, Christoforos Kanellakis, George Nikolakopoulos",객체 감지 및 전역 위치 파악은 자율주행차부터 의미론적 장면 이해를 위한 다층 3D 장면 그래프에 이르기까지 다양한 응용 분야에 걸쳐 로봇 공학에서 중요한 역할을 합니다. 이 기사에서는 RGB 카메라와 3D LiDAR의 정보를 융합하여 관심 객체의 위치를 ​​파악하기 위한 새로운 다중 모드 및 경량 방식인 BOX3D를 제안합니다. BOX3D는 3개 계층 아키텍처를 중심으로 구성되어 수신되는 순차 센서 데이터에 대한 로컬 인식부터 이상값 및 각 개체 관찰의 일반적인 일관성을 포괄하는 전역 인식 개선까지 구축됩니다. 보다 구체적으로 첫 번째 레이어는 초기 3D 경계 상자 추출을 위해 카메라와 LiDAR 데이터의 낮은 수준 융합을 처리합니다. 두 번째 레이어는 각 LiDAR의 스캔 3D 경계 상자를 세계 좌표계로 변환하고 공간 페어링 및 병합 메커니즘을 적용하여 다양한 관점에서 관찰된 개체의 고유성을 유지합니다. 마지막으로 BOX3D는 객체에 속하는 전역 지도의 모든 지점을 식별하기 위해 점 대 복셀 비교를 사용하여 전역 지도에서 결과의 일관성을 반복적으로 감독하는 세 번째 레이어를 통합합니다. 제안된 새로운 아키텍처의 벤치마킹 결과는 도시 환경에 대한 공공 최첨단 대규모 데이터 세트에 대한 여러 실험 시험에서 전시됩니다.
639,http://arxiv.org/abs/2408.14187 ,Ensemble Predicate Decoding for Unbiased Scene Graph Generation,"Jiasong Feng, Lichun Wang, Hongbo Xu, Kai Xu, Baocai Yin","SGG(장면 그래프 생성)는 주어진 시나리오의 의미 정보를 정확하게 캡처하는 포괄적인 그래픽 표현을 생성하는 것을 목표로 합니다. 그러나 더 세분화된 술어를 예측하는 SGG 모델의 성능은 상당한 술어 편향으로 인해 방해를 받습니다. 기존 연구에 따르면 훈련 데이터에서 술어의 롱테일 분포로 인해 편향된 장면 그래프가 생성됩니다. 그러나 술어 카테고리 간의 의미적 중복은 술어 예측을 어렵게 만들고, 의미상 유사한 술어의 표본 크기에 상당한 차이가 있어 술어 예측을 더욱 어렵게 만듭니다. 따라서 모델의 식별 능력에 대한 더 높은 요구 사항이 적용됩니다. 이러한 문제를 해결하기 위해 본 논문에서는 다중 디코더를 사용하여 편향되지 않은 장면 그래프 생성을 달성하는 EPD(Ensemble Predicate Decoding)를 제안합니다. 모델의 판별 능력을 향상시키기 위해 저주파 조건자에 대해 훈련된 두 개의 보조 디코더가 사용됩니다. VG에 대해 광범위한 실험이 수행되었으며, 실험 결과는 EPD가 모델의 술어 표현 능력을 향상시키는 것으로 나타났습니다. 또한, 우리는 우리의 접근 방식이 이전의 편견 없는 SGG 방법에 비해 더 빈번한 조건자에 대해 상대적으로 우수한 예측 기능을 보장한다는 것을 발견했습니다."
638,http://arxiv.org/abs/2408.13499 ,R2G: Reasoning to Ground in 3D Scenes,"Yixuan Li, Zan Wang, Wei Liang","우리는 추론 방식으로 3D 장면 내 대상 개체를 접지하는 신경 기호 모델인 R2G(Reasoning to Ground)를 제안합니다. 이전 작업과 달리 R2G는 의미론적 개념 기반 장면 그래프를 사용하여 3D 장면을 명시적으로 모델링합니다. 객체 개체 간에 전달되는 주의를 반복적으로 시뮬레이션합니다. 따라서 가장 높은 확률로 대상 물체를 접지하는 과정을 해석할 수 있습니다. 구체적으로, 우리는 사전 정의된 의미 어휘를 활용하여 그래프 노드 내에 여러 개체 속성을 포함하고 가장자리 내 엔터티 간의 공간 관계를 각각 포함합니다. 주의 전달을 안내하기 위해 우리는 학습 또는 프롬프트 기반 방법을 사용하여 참조 발언을 분석하고 이를 동일한 의미 공간 내에서 추론 지침으로 변환합니다. 각 추론 라운드에서 R2G는 (1) 명령어와 내장된 엔터티 속성 간의 유사성과 현재 주의 분포를 병합하거나 (2) 명령어와 내장된 공간 관계 간의 유사성을 기반으로 장면 그래프 전체에서 주의를 이동합니다. Sr3D/Nr3D 벤치마크에 대한 실험에서는 R2G가 향상된 해석성을 유지하면서 이전 작업과 비슷한 결과를 달성하여 3D 언어 기반을 위한 새로운 길을 개척했음을 보여줍니다."
637,http://arxiv.org/abs/2408.12093 ,LLM-enhanced Scene Graph Learning for Household Rearrangement,"Wenhao Li, Zhiyuan Yu, Qijin She, Zhinan Yu, Yuqing Lan, Chenyang Zhu, Ruizhen Hu, Kai Xu","집안 정리 작업에는 장면에서 잘못 배치된 물건을 찾아내고 적절한 위치에 배치하는 작업이 포함됩니다. 객관적인 측면에서는 상식적인 지식, 주관적인 측면에서는 인간 사용자 선호도에 따라 달라집니다. 이러한 작업을 달성하기 위해 우리는 사람의 개입에 의존하지 않고 장면 자체에서 직접 사용자 선호도 정렬을 통해 객체 기능을 마이닝할 것을 제안합니다. 이를 위해 우리는 장면 그래프 표현 작업을 수행하고 입력 장면 그래프를 정보가 강화된 노드와 새로 발견된 에지(관계)가 있는 AEG(어포던스 강화 그래프)로 변환하는 LLM 강화 장면 그래프 학습을 제안합니다. AEG에서는 콘센트 개체에 해당하는 노드가 어떤 종류의 운반 가능한 개체를 배치할 수 있는지 인코딩하는 컨텍스트 유도 어포던스로 강화됩니다. 새로 발견된 비국소 관계를 통해 새로운 가장자리가 발견됩니다. AEG를 사용하면 잘못 배치된 캐리어를 감지하고 각각의 적절한 배치를 결정하여 장면 재배치를 위한 작업 계획을 수행합니다. 우리는 시뮬레이터에서 조수 로봇을 구현하여 방법을 테스트하고 우리가 구축한 새로운 벤치마크에 대한 평가를 수행합니다. 광범위한 평가는 우리의 방법이 잘못된 배치 감지 및 다음 재배열 계획에 대한 최첨단 성능을 달성한다는 것을 보여줍니다."
636,http://arxiv.org/abs/2408.09429 ,"Reefknot: A Comprehensive Benchmark for Relation Hallucination Evaluation, Analysis and Mitigation in Multimodal Large Language Models","Kening Zheng, Junkai Chen, Yibo Yan, Xin Zou, Xuming Hu","환각 문제는 다중 모드 대형 언어 모델(MLLM)에 계속 영향을 미치고 있으며, 기존 연구는 주로 객체 수준 또는 속성 수준 환각을 다루고 고급 추론이 필요한 더 복잡한 관계 환각을 무시합니다. 관계 환각에 대한 현재 벤치마크에는 상세한 평가와 효과적인 완화가 부족하며, 해당 데이터 세트는 체계적인 주석 프로세스로 인해 편향으로 인해 어려움을 겪는 경우가 많습니다. 이러한 문제를 해결하기 위해 우리는 20,000개가 넘는 실제 샘플로 구성된 관계 환각을 대상으로 하는 포괄적인 벤치마크인 Reefknot을 소개합니다. 우리는 지각적 관점과 인지적 관점을 통합하여 관계 환각에 대한 체계적인 정의를 제공하고 Visual Genome 장면 그래프 데이터 세트를 사용하여 관계 기반 코퍼스를 구성합니다. 우리의 비교 평가는 관계 환각을 처리하는 현재 MLLM의 능력에 상당한 한계가 있음을 보여줍니다. 또한 Reefknot을 포함한 세 가지 데이터 세트에서 환각 비율을 평균 9.75% 줄이는 새로운 신뢰 기반 완화 전략을 제안합니다. 우리의 작업은 신뢰할 수 있는 다중 모드 인텔리전스를 달성하기 위한 귀중한 통찰력을 제공합니다."
635,http://arxiv.org/abs/2408.08305 ,Towards Flexible Visual Relationship Segmentation,"Fangrui Zhu, Jianwei Yang, Huaizu Jiang","시각적 관계 이해는 인간-객체 상호작용(HOI) 탐지, 장면 그래프 생성(SGG), 참조 관계(RR) 작업에서 별도로 연구되었습니다. 이러한 작업의 복잡성과 상호 연결성을 고려할 때 이러한 작업을 응집력 있는 방식으로 효과적으로 처리할 수 있는 유연한 프레임워크를 갖는 것이 중요합니다. 본 연구에서는 위의 세 가지 측면을 표준적이고 신속한 시각적 관계 분할에 완벽하게 통합하고 더 나아가 새로운 시나리오에 적응할 수 있는 개방형 어휘 분할 기능을 갖춘 단일 모델인 FleVRS를 제안합니다. FleVRS는 텍스트와 이미지 양식 간의 시너지 효과를 활용하여 이미지의 다양한 유형의 관계를 기반으로 하고 비전 언어 모델의 텍스트 기능을 사용하여 시각적 개념 이해에 이릅니다. 다양한 데이터 세트에 대한 경험적 검증을 통해 우리 프레임워크는 표준, 프롬프트 및 개방형 어휘 작업에서 기존 모델보다 성능이 우수하다는 것을 보여줍니다(예: HICO-DET에서 +1.9 $mAP$, VRD에서 +11.4 $Acc$, 보이지 않는 HICO-DET에서 +4.7 $mAP$). FleVRS는 시각적 관계에 대한 보다 직관적이고 포괄적이며 확장 가능한 이해를 향한 중요한 단계를 나타냅니다."
634,http://arxiv.org/abs/2408.06926 ,SceneGPT: A Language Model for 3D Scene Understanding,Shivam Chandhok,"3D 감독 훈련 및 대규모 훈련 체제를 위한 데이터 소스가 부족하기 때문에 3D 장면을 이해하고 추론할 수 있는 모델을 구축하는 것은 어렵습니다. 이 작업에서 우리는 3D 사전 훈련 없이 사전 훈련된 언어 모델의 지식을 어떻게 3D 장면 이해에 활용할 수 있는지 묻습니다. 이 작업의 목적은 사전 훈련된 LLM이 3D 공간 추론에 필요한 사전 지식/지식을 보유하고 있는지 확인하고, 3D 공간 추론 및 객체 이해에 사용될 수 있도록 어떻게 유도할 수 있는지 확인하는 것입니다. 이를 위해 훈련이나 명시적인 3D 감독 없이 3D 공간 추론을 수행할 수 있는 LLM 기반 장면 이해 시스템인 SceneGPT를 제시합니다. 우리 프레임워크의 주요 구성 요소는 다음과 같습니다. 1) 장면 표현 역할을 하고 장면의 객체와 공간 관계를 인코딩하는 3D 장면 그래프 2) 3D 공간 추론을 위해 상황 학습에 적응할 수 있는 사전 훈련된 LLM입니다. 우리는 객체 의미론, 물리적 속성 및 어포던스(객체 수준) 및 공간 이해(장면 수준)를 포함한 객체 및 장면 이해 작업에 대한 프레임워크를 질적으로 평가합니다."
633,http://arxiv.org/abs/2408.04979 ,Mesh-based Object Tracking for Dynamic Semantic 3D Scene Graphs via Ray Tracing,"Lennart Niecksch, Alexander Mock, Felix Igelbrink, Thomas Wiemann, Joachim Hertzberg","본 논문에서는 거리 센서와 RGB 카메라를 이용한 3차원 기하학적 장면 그래프 생성을 위한 새로운 방법을 제시합니다. 우리는 먼저 YOLOv8s 모델을 사용하여 인스턴스별 키포인트를 감지하고 PnP를 해결하여 알려진 객체의 6D 포즈 추정을 계산합니다. 우리는 객체 인스턴스의 메쉬 모델로 구성된 기하학적 장면 그래프를 추적하기 위해 광선 추적 접근 방식을 사용합니다. 기존의 지점 간 일치와 달리 이는 특히 개체 인스턴스 간의 폐색에서 더욱 강력한 결과를 가져옵니다. 우리는 이 하이브리드 전략을 사용하면 강력한 자체 위치 파악, 범위 센서 데이터의 사전 분할 및 동일한 환경 표현을 사용하여 물체의 정확한 자세 추적이 가능하다는 것을 보여줍니다. 감지된 모든 객체는 의미론적 장면 그래프에 통합됩니다. 이 장면 그래프는 의미론적 매핑 프레임워크의 프런트 엔드 역할을 하여 공간적 추론을 가능하게 합니다."
632,http://arxiv.org/abs/2408.04852 ,MSG-Chart: Multimodal Scene Graph for ChartQA,"Yue Dai, Soyeon Caren Han, Wei Liu","자동 차트 질문 응답(ChartQA)은 차트에 명시적으로 표시되지 않는 기본 데이터 패턴이 있는 차트 요소의 복잡한 분포로 인해 어렵습니다. 이 문제를 해결하기 위해 우리는 차트 요소와 해당 패턴 간의 관계를 명시적으로 나타내기 위해 차트용 공동 다중 모드 장면 그래프를 설계했습니다. 우리가 제안하는 다중 모드 장면 그래프에는 차트의 구조적, 의미적 지식을 공동으로 포착하기 위한 시각적 그래프와 텍스트 그래프가 포함됩니다. 이 그래프 모듈은 유도 바이어스로서 다양한 비전 변환기와 쉽게 통합될 수 있습니다. 우리의 실험은 제안된 그래프 모듈을 통합하면 차트 요소의 구조와 의미에 대한 이해가 향상되어 공개적으로 사용 가능한 벤치마크인 ChartQA 및 OpenCQA의 성능이 향상된다는 것을 보여줍니다."
631,http://arxiv.org/abs/2407.21580 ,Voxel Scene Graph for Intracranial Hemorrhage,"Antoine P. Sanner, Nils F. Grauhan, Marc A. Brockmann, Ahmed E. Othman, Anirban Mukhopadhyay","두개내 출혈(ICH) 환자는 잠재적으로 생명을 위협하는 상태에 직면해 있으며, 가능한 임상 합병증으로 인해 환자 중심의 개별화된 치료는 여전히 어려운 상황입니다. 딥러닝 기반 방법은 일상적으로 획득한 두부 CT를 효율적으로 분석하여 임상 의사 결정을 지원할 수 있습니다. 초기 연구의 대부분은 무형문화유산의 탐지와 세분화에 초점을 맞추고 있지만 무형문화유산과 인접한 뇌 구조 사이의 복잡한 관계를 모델링하지는 않습니다. 이 작업에서 우리는 임상 대뇌 장면의 전체적인 표현을 학습하기 위해 분할 기반 장면 그래프 생성(SGG) 방법과 결합한 ICH용 맞춤형 개체 감지 방법을 설계합니다. 우리가 아는 한, 이는 3D 복셀 이미지에 SGG를 적용한 최초의 사례입니다. 우리는 두 개의 머리 CT 데이터 세트에 대한 방법을 평가하고 모델이 임상적으로 관련된 관계의 최대 74%를 회상할 수 있음을 보여줍니다. 이 작업은 3D 복셀 데이터를 위한 SGG의 기반을 마련합니다. 생성된 장면 그래프는 이미 임상의에게 통찰력을 제공할 수 있을 뿐만 아니라 간결하고 해석 가능한 표현으로서 모든 다운스트림 작업에 유용합니다."
630,http://arxiv.org/abs/2407.20214 ,SANGRIA: Surgical Video Scene Graph Optimization for Surgical Workflow Prediction,"Çağhan Köksal, Ghazal Ghazaei, Felix Holm, Azade Farshad, Nassir Navab",그래프 기반의 전체적인 장면 표현은 수술 흐름 이해를 촉진하며 최근 상당한 성공을 거두었습니다. 그러나 이 작업은 조밀하게 주석이 달린 수술 장면 데이터의 제한된 가용성으로 인해 종종 방해를 받습니다. 이 작업에서는 다운스트림 작업에서 수술 장면 그래프를 생성하고 최적화하기 위한 엔드투엔드 프레임워크를 소개합니다. 우리의 접근 방식은 그래프 기반 스펙트럼 클러스터링의 유연성과 기초 모델의 일반화 기능을 활용하여 학습 가능한 속성을 갖춘 비지도 장면 그래프를 생성합니다. 우리는 시간적 이웃에 걸쳐 시간적으로 일관된 클러스터를 예측하기 위해 연속 프레임 간의 로컬 일치를 사용하여 희박한 시간적 연결로 초기 공간 그래프를 강화합니다. 위상 분할의 다운스트림 작업과 함께 동적 장면 그래프의 시공간 관계 및 노드 기능을 공동으로 최적화함으로써 약한 수술 위상 레이블만 사용하여 수술 비디오에서 의미론적 장면 이해 및 장면 그래프 생성이라는 비용이 많이 들고 주석 부담이 큰 작업을 해결합니다. 또한 파이프라인 내에 효과적인 중간 장면 표현 분리 단계를 통합함으로써 당사의 솔루션은 CATARACTS 데이터세트의 SOTA보다 수술 워크플로우 인식에서 8% 정확도와 10% F1 점수를 능가합니다.
629,http://arxiv.org/abs/2407.19259 ,Fine-Grained Scene Graph Generation via Sample-Level Bias Prediction,"Yansheng Li, Tingzhu Wang, Kang Wu, Linlin Wang, Xin Guo, Wenbin Wang","SGG(장면 그래프 생성)는 이미지의 개체 간 관계를 탐색하고 장면 요약 그래프를 얻어 다운스트림 작업을 더 효과적으로 처리하는 것을 목표로 합니다. 그러나 롱테일 문제는 장면 그래프의 품질에 부정적인 영향을 미쳤습니다. 예측은 대략적인 관계에 의해 지배되며 보다 유익한 세부적인 관계가 부족합니다. 하나의 개체 쌍(즉, 하나의 샘플)의 결합 영역에는 풍부하고 전용된 상황별 정보가 포함되어 있어 원래 관계 예측을 개선하기 위한 샘플별 편향을 예측할 수 있습니다. 따라서 우리는 세분화된 SGG(SBG)에 대한 새로운 샘플 수준 바이어스 예측(SBP) 방법을 제안합니다. 먼저, 우리는 고전적인 SGG 모델을 훈련시키고, 하나의 고전적인 SGG 모델을 사용하여 정답 레이블과 예측 레이블 사이의 마진을 계산하여 수정 편향 세트를 구성합니다. 그런 다음 구성된 수정 편향을 예측하는 방법을 학습하는 편향 지향 생성적 적대 신경망(BGAN)을 고안합니다. 이는 거친 관계에서 세분화된 관계로 원래 예측을 수정하는 데 사용할 수 있습니다. VG, GQA 및 VG-1800 데이터 세트에 대한 광범위한 실험 결과는 SBG가 Motif, VCtree 및 Transformer의 세 가지 주류 SGG 모델에서 Average@K 측면에서 최첨단 방법보다 성능이 우수하다는 것을 보여줍니다. VG의 데이터 세트 수준 수정 방법과 비교하여 SBG는 PredCls, SGCls 및 SGDet 작업에 대해 Average@K에서 각각 5.6%, 3.9% 및 3.2%의 상당한 평균 개선을 보여줍니다. 코드는 https://github.com/Zhuzi24/SBG에서 확인할 수 있습니다."
628,http://arxiv.org/abs/2407.18715 ,BCTR: Bidirectional Conditioning Transformer for Scene Graph Generation,"Peng Hao, Weilong Wang, Xiaobing Wang, Yingying Jiang, Hanchao Jia, Shaowei Cui, Junhang Wei, Xiaoshuai Hao","장면 그래프 생성(SGG)은 구성 특성으로 인해 여전히 어려운 작업입니다. 이전 접근 방식은 엔드투엔드 학습을 통해 예측 효율성을 향상시킵니다. 그러나 이러한 방법은 엔터티와 조건자 간의 단방향 조건을 가정하여 효과적인 정보 상호 작용을 제한하므로 성능이 제한됩니다. 이러한 한계를 해결하기 위해 우리는 SGG의 의미 정렬 공간에서 새로운 양방향 조건화 인수분해를 제안하여 엔터티와 조건자 간의 효율적이고 일반화 가능한 상호 작용을 가능하게 합니다. 특히 이 인수분해를 구현하기 위해 BCTR(양방향 조건 변환기)이라는 엔드투엔드 장면 그래프 생성 모델을 소개합니다. BCTR은 두 가지 핵심 모듈로 구성됩니다. 첫째, BCG(양방향 조건 생성기)는 엔터티와 조건자 간의 다단계 대화형 기능 보강을 수행하여 이러한 예측 간의 상호 향상을 가능하게 합니다. 둘째, 사전 훈련된 모델에서 다중 모드 지식을 추출하여 기능 공간을 정규화하기 위해 RFA(Random Feature Alignment)가 제공됩니다. 이 정규화된 특징 공간 내에서 BCG는 훈련 중에 다양한 관계에 걸쳐 상호 작용 패턴을 캡처하는 것이 가능하며, 학습된 상호 작용 패턴은 추론 중에 보이지 않지만 의미적으로 관련된 관계로 일반화될 수 있습니다. Visual Genome 및 Open Image V6에 대한 광범위한 실험에서는 BCTR이 두 벤치마크 모두에서 최고 수준의 성능을 달성한 것으로 나타났습니다."
627,http://arxiv.org/abs/2407.17398 ,3D Question Answering for City Scene Understanding,"Penglei Sun, Yaoxian Song, Xiang Liu, Xiaofei Yang, Qiang Wang, Tiefeng Li, Yang Yang, Xiaowen Chu","3D 다중 모드 질문 응답(MQA)은 지능형 에이전트가 3D 환경에서 주변 환경을 이해할 수 있도록 함으로써 장면 이해에 중요한 역할을 합니다. 기존 연구는 주로 실내 가사 작업과 실외 길가 자율주행 작업에 중점을 두었지만, 도시 수준의 장면 이해 작업에 대한 탐색은 제한적이었습니다. 또한 기존 연구는 도시 수준의 공간 의미 정보와 인간-환경 상호 작용 정보가 부족하여 도시 장면을 이해하는 데 어려움을 겪고 있습니다. 이러한 문제를 해결하기 위해 데이터 세트 및 방법 관점에서 3D MQA를 조사합니다. 데이터 세트 관점에서 도시 내 장면 의미론 및 인간-환경 상호 작용 작업을 통합한 최초의 데이터 세트인 도시 수준 장면 이해를 위한 City-3DQA라는 새로운 3D MQA 데이터 세트를 소개합니다. 방법론적으로 장면 그래프를 활용하여 공간적 의미를 도입하는 장면 그래프 강화된 도시 수준 이해 방법(Sg-CityU)을 제안합니다. 새로운 벤치마크가 보고되었으며 제안된 Sg-CityU는 City-3DQA의 다양한 설정에서 63.94% 및 63.76%의 정확도를 달성했습니다. 실내 3D MQA 방법 및 고급 LLM(대형 언어 모델)을 사용한 제로샷과 비교하여 Sg-CityU는 견고성과 일반화 측면에서 최첨단(SOTA) 성능을 보여줍니다."
626,http://arxiv.org/abs/2407.15396 ,Semantic Diversity-aware Prototype-based Learning for Unbiased Scene Graph Generation,"Jaehyeong Jeon, Kibum Kim, Kanghoon Yoon, Chanyoung Park","SGG(장면 그래프 생성) 작업에는 이미지 내의 개체를 감지하고 개체 간의 관계를 나타내는 조건자를 예측하는 작업이 포함됩니다. 그러나 SGG 벤치마크 데이터세트에서는 단일 조건자가 다양한 의미(즉, 의미적 다양성)를 나타낼 수 있더라도 각 주체-객체 쌍에 단일 조건자로 주석이 달려 있으며, 기존 SGG 모델은 각 쌍에 대한 유일한 조건자를 예측하도록 훈련되었습니다. 이는 결과적으로 SGG 모델이 술어에 존재할 수 있는 의미론적 다양성을 간과하게 하여 편향된 예측으로 이어집니다. 본 논문에서는 술어의 의미적 다양성에 대한 이해를 바탕으로 편견 없는 예측을 가능하게 하는 새로운 모델 독립적인 의미적 다양성 인식 프로토타입 기반 학습(DPL) 프레임워크를 제안합니다. 특히 DPL은 단일 조건자가 나타낼 수 있는 다양한 의미 체계를 구별하기 위해 각 조건자가 다루는 의미 공간의 영역을 학습합니다. 광범위한 실험을 통해 우리가 제안한 모델 독립적 DPL 프레임워크가 기존 SGG 모델에 상당한 성능 향상을 가져오고 술어의 의미론적 다양성을 효과적으로 이해한다는 사실이 입증되었습니다."
625,http://arxiv.org/abs/2407.12667 ,SG-NeRF: Neural Surface Reconstruction with Scene Graph Optimization,"Yiyang Chen, Siyan Dong, Xulong Wang, Lulu Cai, Youyi Zheng, Yanchao Yang","이미지로부터 3D 표면을 재구성하는 것은 다양한 응용 분야에 필수적입니다. 최근에는 NeRF(Neural Radiance Fields)가 3D 모델링을 위한 유망한 프레임워크로 등장했습니다. 그러나 NeRF는 정확한 카메라 포즈를 입력으로 요구하며 기존 방법은 실제 시나리오에서 흔히 발생하는 상당히 시끄러운 포즈 추정(예: 이상치)을 처리하는 데 어려움을 겪습니다. 이 과제를 해결하기 위해 우리는 장면 그래프로 발광 필드를 최적화하여 이상치 포즈의 영향을 완화하는 새로운 접근 방식을 제시합니다. 우리의 방법은 장면 그래프를 기반으로 하는 적응형 inlier-outlier 신뢰도 추정 방식을 통합하여 이웃과의 높은 호환성과 렌더링 품질의 일관성을 강조하는 이미지를 강조합니다. 또한 훈련을 용이하게 하기 위한 대략적인 전략과 함께 카메라 자세와 표면 형상을 최적화하기 위한 효과적인 IoU(Intersection-over-Union) 손실을 도입합니다. 또한, 자세한 평가를 위해 일반적인 이상값 포즈를 포함하는 새로운 데이터 세트를 제안합니다. 다양한 데이터 세트에 대한 실험 결과는 기존 접근 방식에 비해 우리 방법의 효율성과 우수성을 일관되게 입증하여 이상값을 처리하고 고품질 3D 재구성을 생성하는 데 있어 견고성을 보여줍니다. 우리의 코드와 데이터는 \url{https://github.com/Iris-cyy/SG-NeRF}에서 확인할 수 있습니다."
624,http://arxiv.org/abs/2407.11213 ,OpenPSG: Open-set Panoptic Scene Graph Generation via Large Multimodal Models,"Zijian Zhou, Zheng Zhu, Holger Caesar, Miaojing Shi","PSG(Panoptic Scene Graph Generation)는 객체를 분할하고 관계를 인식하여 이미지를 구조적으로 이해하는 것을 목표로 합니다. 이전 방법은 미리 정의된 객체 및 관계 범주를 예측하는 데 중점을 두었기 때문에 오픈 월드 시나리오에서의 적용이 제한되었습니다. LMM(Large Multimodal Model)의 급속한 발전으로 개방형 객체 감지 및 분할에서 상당한 진전이 이루어졌지만 PSG의 개방형 관계 예측은 아직 탐구되지 않았습니다. 본 논문에서는 진정한 오픈 세트 팬옵틱 장면 그래프 생성(OpenPSG)을 달성하기 위해 사전 훈련된 오픈 세트 팬옵틱 분할 모델과 통합된 오픈 세트 관계 예측 작업에 중점을 둡니다. OpenPSG는 LMM을 활용하여 자동 회귀 방식으로 개방형 관계 예측을 달성합니다. 객체 쌍의 시각적 특징을 효율적으로 추출하고 이들 사이의 관계 존재를 추정하기 위해 관계 쿼리 변환기를 소개합니다. 후자는 관련 없는 쌍을 필터링하여 예측 효율성을 향상시킬 수 있습니다. 마지막으로 PSG에서 자동회귀적으로 개방형 관계 예측을 수행하기 위한 생성 및 판단 명령을 설계합니다. 우리가 아는 한, 우리는 공개 세트 PSG 작업을 제안한 최초의 사람입니다. 광범위한 실험을 통해 우리의 방법이 개방형 관계 예측 및 파노라마 장면 그래프 생성에서 최첨단 성능을 달성한다는 것을 보여줍니다. 코드는 \url{https://github.com/franciszzj/OpenPSG}에서 확인할 수 있습니다."
623,http://arxiv.org/abs/2407.09216 ,A Fair Ranking and New Model for Panoptic Scene Graph Generation,"Julian Lorenz, Alexander Pest, Daniel Kienzle, Katja Ludwig, Rainer Lienhart","PSGG(팬옵틱 장면 그래프 생성)에서 모델은 팬옵틱 분할 마스크에 의해 기반이 되는 이미지 내 개체 간의 상호 작용을 검색합니다. Panoptic 장면 그래프에 대한 이전 평가는 동일한 객체에 대한 여러 마스크가 마스크-마스크 쌍당 여러 관계 분포로 이어질 수 있는 잘못된 평가 프로토콜의 적용을 받았습니다. 이를 활용하여 최종 점수를 높일 수 있습니다. 우리는 이 결함을 수정하고 다양한 기존 PSGG 모델에 대해 공정한 순위를 제공합니다. 기존 방법에서 관찰된 점수는 모든 2단계 방법에서 최대 7.4mR@50까지 증가하는 반면, 모든 1단계 방법에서는 최대 19.3mR@50까지 떨어지므로 올바른 평가의 중요성이 강조됩니다. 최근 출판물과 달리 기존의 2단계 방법이 1단계 방법과 경쟁적임을 보여줍니다. 이를 바탕으로 수정된 평가에서 +11 mR@50 및 +10 mNgR@50의 큰 차이로 기존의 모든 장면 그래프 모델보다 성능이 뛰어난 새로운 2단계 모델인 De커플링 SceneFormer(DSFormer)를 도입하여 새로운 SOTA를 설정합니다. 핵심 디자인 원리로서 DSFormer는 주제 및 객체 마스크를 특징 공간으로 직접 인코딩합니다."
622,http://arxiv.org/abs/2407.09191 ,From Easy to Hard: Learning Curricular Shape-aware Features for Robust Panoptic Scene Graph Generation,"Hanrong Shi, Lin Li, Jun Xiao, Yueting Zhuang, Long Chen","PSG(Panoptic Scene Graph Generation)는 Panoptic 분할 마스크를 기반으로 포괄적인 그래프 구조 표현을 생성하는 것을 목표로 합니다. PSG의 놀라운 발전에도 불구하고 거의 모든 기존 방법은 본질적으로 객체의 윤곽과 경계에 초점을 맞추는 형상 인식 기능의 중요성을 무시합니다. 이러한 격차를 해소하기 위해 우리는 PSG를 위한 모델에 구애받지 않는 CAFE(Currical shApe-aware FEature) 학습 전략을 제안합니다. 특히, 우리는 모양 인식 기능(예: 마스크 기능 및 경계 기능)을 PSG에 통합하여 bbox 기능에만 의존하는 것을 넘어섰습니다. 또한 인간의 인지에서 영감을 얻어 형태 인식 기능을 쉽고 어려운 방식으로 통합할 것을 제안합니다. 이를 달성하기 위해 우리는 인지 학습 난이도에 따라 술어를 세 그룹으로 분류하고 이에 따라 훈련 과정을 세 단계로 나눕니다. 각 단계에서는 특수한 관계 분류기를 활용하여 특정 술어 그룹을 구별합니다. 술어의 학습 난이도가 높아질수록 이러한 분류기는 복잡성이 높아지는 특징을 갖습니다. 우리는 또한 이전 단계에서 획득한 지식을 유지하기 위해 지식 증류를 통합합니다. 모델에 구애받지 않는 특성으로 인해 CAFE는 모든 PSG 모델에 원활하게 통합될 수 있습니다. 견고한 PSG와 제로샷 PSG에서 두 가지 PSG 작업에 대한 광범위한 실험과 절제를 통해 제안된 CAFE의 우수성과 견고성이 입증되었으며, 이는 기존 최첨단 방법보다 훨씬 뛰어난 성능을 발휘합니다."
621,http://arxiv.org/abs/2407.05910 ,Enhancing Vision-Language Models with Scene Graphs for Traffic Accident Understanding,"Aaron Lohner, Francesco Compagno, Jonathan Francis, Alessandro Oltramari","교통사고를 인지하는 것은 자율주행이나 도로 모니터링 시스템의 필수적인 부분입니다. 사고는 다양한 형태로 나타날 수 있으며, 어떤 사고가 발생하고 있는지 파악하는 것이 재발 방지에 도움이 될 수 있습니다. 이 작업은 교통 상황을 특정 사고 유형으로 분류하는 데 중점을 둡니다. 우리는 교통 상황을 그래프로 표현하고, 자동차와 같은 객체를 노드로 표현하고, 객체 사이의 상대적인 거리와 방향을 엣지로 표현함으로써 문제에 접근합니다. 이러한 교통 현장 표현을 장면 그래프라고 하며 사고 분류기의 입력으로 사용할 수 있습니다. 장면 그래프 입력을 시각적 및 텍스트 표현과 융합하는 분류기를 사용하면 더 나은 결과를 얻을 수 있습니다. 이 작업에서는 교통 사고 영상을 전처리하고 이를 장면 그래프로 인코딩하며 분류 작업을 실행하기 전에 이 표현을 시각 및 언어 양식에 맞춰 정렬하는 다단계 다중 모드 파이프라인을 소개합니다. 4개 클래스에 대해 교육을 했을 때 우리의 방법은 인기 있는 DoTA(Detection of Traffic Anomaly) 벤치마크의 (불균형) 하위 집합에서 57.77%의 균형 잡힌 정확도 점수를 달성했습니다. 이는 장면 그래프 정보를 고려하지 않은 경우보다 5% 가까이 증가한 수치입니다."
620,http://arxiv.org/abs/2407.15852 ,BSH for Collision Detection in Point Cloud models,"Mauro Figueiredo, João Pereira, João Oliveira, Bruno Araujo","포인트 클라우드 모델은 여러 가지 이유로 일반적인 모양 표현입니다. 3차원 스캐닝 장치는 오늘날 널리 사용되고 있으며 점은 복잡한 형상을 렌더링하는 데 매력적인 기본 요소입니다. 그럼에도 불구하고 포인트 클라우드 모델의 충돌 감지에 관한 문헌은 많지 않습니다. 본 논문에서는 복셀, 옥트리 및 경계 구 계층 구조(BSH)를 사용하여 대규모 포인트 클라우드 모델에 대한 새로운 충돌 감지 알고리즘을 제시합니다. 장면 그래프는 복셀 단위로 나뉩니다. 각 복셀의 객체는 옥트리로 구성됩니다. 장면의 포인트 수가 많기 때문에 옥트리의 비어 있지 않은 각 셀은 R-트리 계층 구조와 같은 경계 영역 계층 구조로 구성됩니다. BSH 계층은 인접한 지점을 그룹화하고 다른 모델과 상호 작용하지 않는 객체의 일부를 매우 빠르게 필터링하는 데 사용됩니다. 레이저 스캔 데이터에서 파생된 점은 일반적으로 분할되지 않으며 임의의 공간 해상도를 가질 수 있으므로 계산 및 모델링 문제가 발생할 수 있습니다. 우리는 이러한 문제를 해결했으며 결과는 제안된 충돌 감지 알고리즘이 경계 볼륨 확인 및 업데이트 횟수를 줄일 수 있기 때문에 포인트 클라우드 모델 간의 교차점을 효과적으로 찾는다는 것을 보여줍니다."
619,http://arxiv.org/abs/2407.02473 ,Open Scene Graphs for Open World Object-Goal Navigation,"Joel Loo, Zhanxin Wu, David Hsu","새로운 장면에서 대상 물체를 검색하는 것과 같은 개방형 의미론적 탐색 작업을 위한 로봇을 어떻게 구축할 수 있습니까? 기초 모델은 이러한 작업에 필요한 풍부한 지식과 일반화를 갖추고 있지만 이를 완전한 로봇 시스템에 연결하려면 적절한 장면 표현이 필요합니다. 우리는 이러한 모델에 대한 공개 세트 장면 정보를 유지 및 구성하고 다양한 환경 유형에 맞게 구성할 수 있는 구조를 갖는 위상 의미론적 표현인 OSG(Open Scene Graph)를 통해 이 문제를 해결합니다. 우리는 기본 모델과 OSG를 Open World Object-Goal Navigation용 OpenSearch 시스템에 통합합니다. 이는 자연어로 지정된 개방형 개체를 검색하는 동시에 다양한 환경과 실시예에 걸쳐 제로 샷을 일반화할 수 있습니다. 우리의 OSG는 LLM(대형 언어 모델)을 통해 추론을 강화하여 기존 LLM 접근 방식을 능가하는 강력한 객체-목표 탐색을 가능하게 합니다. 시뮬레이션과 실제 실험을 통해 다양한 환경, 로봇 및 새로운 지침에 대한 OpenSearch의 일반화를 검증합니다."
618,http://arxiv.org/abs/2407.00609 ,ESGNN: Towards Equivariant Scene Graph Neural Network for 3D Scene Understanding,"Quang P. M. Pham, Khoi T. N. Nguyen, Lan C. Ngo, Truong Do, Truong Son Hy","장면 그래프는 간결하고 명시적인 특성으로 인해 다양한 장면 이해 작업에 유용한 것으로 입증되었습니다. 그러나 기존 접근 방식은 3D 포인트 클라우드에서 장면 그래프를 생성할 때 대칭 보존 속성을 유지하는 것의 중요성을 간과하는 경우가 많습니다. 이러한 감독은 특히 노이즈가 있는 다중 뷰 3D 데이터를 처리할 때 결과 장면 그래프의 정확성과 견고성을 감소시킬 수 있습니다. 우리가 아는 한, 이 작업은 장면 이해를 위해 3D 포인트 클라우드에서 의미론적 장면 그래프 생성에 등변 그래프 신경망을 구현한 최초의 작업입니다. 우리가 제안한 방법인 ESGNN은 기존의 최첨단 접근 방식보다 성능이 뛰어나며 더 빠른 수렴으로 장면 추정이 크게 향상되었음을 보여줍니다. ESGNN은 낮은 계산 리소스를 요구하며 사용 가능한 프레임워크에서 쉽게 구현하여 로봇 공학 및 컴퓨터 비전과 같은 실시간 애플리케이션을 위한 길을 열어줍니다."
617,http://arxiv.org/abs/2406.19316 ,Enhanced Data Transfer Cooperating with Artificial Triplets for Scene Graph Generation,"KuanChao Chu, Satoshi Yamazaki, Hideki Nakayama",이 작업은 장면 그래프 생성(SGG)을 위한 정보 관계형 삼중항의 교육 데이터 세트 향상에 중점을 둡니다. 효과적인 감독이 부족하기 때문에 현재 SGG 모델 예측은 훈련 샘플이 부적절한 유익한 관계형 삼중항에 대해 제대로 수행되지 않습니다. 따라서 우리는 FSTA(Feature Space Triplet Augmentation)와 Soft Transfer라는 두 가지 새로운 훈련 데이터 세트 향상 모듈을 제안합니다. FSTA는 관계형 삼중항으로 개체 표현을 생성하도록 훈련된 특징 생성기를 활용합니다. FSTA의 편향된 예측 기반 샘플링은 까다로운 항목에 초점을 맞춘 인공 삼중항을 효율적으로 강화합니다. 또한 유익한 술어 클래스에 대해 더 많은 감독을 효과적으로 수행하기 위해 소프트 술어 레이블을 일반 관계형 삼중항에 할당하는 소프트 전송을 도입합니다. 실험 결과에 따르면 FSTA와 Soft Transfer를 통합하면 Visual Genome 데이터 세트에서 높은 수준의 Recall과 평균 Recall이 달성되는 것으로 나타났습니다. Recall과 Mean Recall의 평균은 기존의 모든 모델 불가지론적 방법 중에서 가장 높습니다.
616,http://arxiv.org/abs/2406.19255 ,Enhancing Video-Language Representations with Structural Spatio-Temporal Alignment,"Hao Fei, Shengqiong Wu, Meishan Zhang, Min Zhang, Tat-Seng Chua, Shuicheng Yan","사전 교육된 대규모 비디오 언어 모델(VLM)은 다양한 다운스트림 비디오 언어 작업에 대한 놀라운 잠재력을 보여 주었지만 기존 VLM은 여전히 ​​일반적으로 볼 수 있는 특정 제한(예: 대략적인 교차 모달 정렬, 시간 역학의 과소 모델링, 분리된 비디오 언어 보기)으로 인해 어려움을 겪을 수 있습니다. 이 연구에서는 세분화된 구조적 시공간 정렬 학습 방법(즉, Finsta)을 사용하여 VLM을 향상시키는 것을 목표로 합니다. 우선, 우리는 세밀한 장면 그래프(SG) 구조로 입력 텍스트와 비디오를 표현하며, 두 가지 모두 두 가지 양식을 연결하기 위해 전체적인 SG(HSG)로 통합됩니다. 그런 다음 텍스트 SG(TSG)가 그래프 변환기로 인코딩되는 반면, 비디오 동적 SG(DSG) 및 HSG는 공간 및 시간 특성 전파를 위한 새로운 순환 그래프 변환기로 모델링되는 SG 기반 프레임워크가 구축됩니다. 공간-시간적 가우스 미분 그래프 변환기는 공간적 및 시간적 차원에 걸쳐 객체의 변화에 ​​대한 감각을 강화하기 위해 더욱 고안되었습니다. 다음으로 TSG와 DSG의 세밀한 구조적 특징을 바탕으로 객체 중심 공간 정렬과 술어 중심 시간 정렬을 각각 수행하여 공간성과 시간성 모두에서 비디오 언어 접지를 향상시킵니다. 우리는 처음부터 훈련하거나 다운스트림 애플리케이션에서 SG 주석에 의존하지 않고 추가 표현 확대를 위해 잘 훈련된 기존 VLM에 통합할 수 있는 플러그 앤 플레이 시스템으로 방법을 설계합니다. 표준 및 장편 비디오 시나리오 모두에서 12개 데이터 세트에 대한 6개의 대표적인 VL 모델링 작업에서 Finsta는 기존의 강력한 성능을 발휘하는 13개의 VLM을 지속적으로 개선하고 미세 조정 및 제로샷 설정 모두에서 현재 최첨단 최종 작업 성능을 크게 새로 고칩니다."
615,http://arxiv.org/abs/2406.13302 ,"SituationalLLM: Proactive language models with scene awareness for dynamic, contextual task guidance","Muhammad Saif Ullah Khan, Muhammad Zeshan Afzal, Didier Stricker","LLM(대형 언어 모델)은 텍스트 기반 작업에서 놀라운 성공을 거두었지만 실제 물리적 환경에서 실행 가능한 지침을 제공하는 데 어려움을 겪는 경우가 많습니다. 이는 사용자의 물리적 상황에 대한 제한된 이해를 인식할 수 없기 때문입니다. 구조화된 장면 정보를 LLM에 통합하여 사전에 상황을 인식하는 지원을 제공하는 새로운 접근 방식인 SituationalLLM을 소개합니다. SituationalLLM은 사용자 정의 장면 그래프 언어로 개체, 속성 및 관계를 인코딩함으로써 환경적 맥락의 격차를 적극적으로 식별하고 사용자 상호 작용 중에 설명을 찾습니다. 이러한 동작은 다양한 시나리오별 장면 그래프와 반복적인 대화 기반 개선을 결합한 SAD-Instruct(Situational Awareness Database for Instruct-Tuning)에 대한 교육에서 나타납니다. 실험 결과에 따르면 SituationalLLM은 작업 특이성, 신뢰성 및 적응성 측면에서 일반 LLM 기준을 능가하여 실제 제약 조건 하에서 강력하고 사용자 중심적인 지침을 제공할 수 있는 환경 인식 AI 도우미를 위한 길을 닦았습니다."
614,http://arxiv.org/abs/2406.12736 ,Beyond Visual Appearances: Privacy-sensitive Objects Identification via Hybrid Graph Reasoning,"Zhuohang Jiang, Bingkui Tong, Xia Du, Ahmed Alhammadi, Jizhe Zhou","POI(개인 정보 보호 개체 식별) 작업은 장면의 개인 정보 보호 개체에 대한 경계 상자를 할당합니다. POI의 핵심은 객체의 개인 정보 보호 등급(개인 정보 보호에 민감하거나 민감하지 않음)을 설정하는 것입니다. 객체의 시각적 외양에 따라 결정되는 기존 객체 클래스와 달리, 한 객체의 프라이버시 클래스는 장면 컨텍스트에서 파생되며 시각적 외양을 넘어 다양한 암시적 요인의 영향을 받습니다. 즉, 시각적으로 유사한 객체는 프라이버시 등급이 완전히 반대일 수 있습니다. 장면 컨텍스트에서 객체의 프라이버시 클래스를 명시적으로 도출하기 위해 본 논문에서는 POI 작업을 장면에 있는 각 객체의 프라이버시를 목표로 하는 시각적 추론 작업으로 해석합니다. 이러한 해석에 따라 우리는 POI를 위한 PrivacyGuard 프레임워크를 제안합니다. PrivacyGuard에는 세 가지 단계가 있습니다. i) 구조화: 구조화되지 않은 이미지는 먼저 풍부한 장면 컨텍스트를 포함하는 구조화된 이종 장면 그래프로 변환됩니다. ii) 데이터 확대: 장면 그래프에서 약간 교란된 개인 정보 보호 개체를 생성하여 개인 정보 클래스의 편향된 분포의 균형을 맞추는 상황별 섭동 오버샘플링 전략이 제안됩니다. iii) 하이브리드 그래프 생성 및 추론: 균형 잡힌 이종 장면 그래프는 추가 ""노드-노드"" 및 ""에지-에지"" 동종 경로를 부여하여 하이브리드 그래프로 변환됩니다. 이러한 동종 경로를 사용하면 노드나 에지 간에 직접 메시지를 전달할 수 있으므로 추론이 가속화되고 미묘한 컨텍스트 변경을 쉽게 포착할 수 있습니다. 이 하이브리드 그래프를 기반으로... **전체 초록을 보려면 원본 논문을 참조하세요.**"
613,http://arxiv.org/abs/2406.11820 ,Composing Object Relations and Attributes for Image-Text Matching,"Khoi Pham, Chuong Huynh, Ser-Nam Lim, Abhinav Shrivastava","우리는 이미지-텍스트 매칭을 위한 시각적 의미 임베딩 문제를 연구합니다. 대부분의 기존 작업은 맞춤형 교차 주의 메커니즘을 활용하여 두 이미지와 텍스트 양식에 걸쳐 로컬 정렬을 수행합니다. 이는 유니모달 듀얼 인코더 접근 방식보다 더 강력하지만 계산 비용이 많이 듭니다. 이 작업에서는 장면 그래프를 활용하여 관계형 가장자리로 상호 연결된 개체 및 속성에 대한 노드가 있는 캡션을 나타내는 이중 인코더 이미지-텍스트 일치 모델을 소개합니다. 그래프 어텐션 네트워크를 활용하여 우리 모델은 객체-속성 및 객체-객체 의미론적 관계를 효율적으로 인코딩하여 강력하고 빠른 성능의 시스템을 만듭니다. 캡션을 장면 그래프로 표현하면 그래프 신경망의 강력한 관계형 귀납적 편향을 활용하여 객체-속성 및 객체-객체 관계를 효과적으로 학습할 수 있는 기능을 제공합니다. 모델을 훈련하기 위해 우리는 전체적인 수준(이미지-캡션)과 로컬 수준(이미지-객체 엔터티) 모두에서 이미지와 캡션을 정렬하는 손실을 제안하며, 이것이 모델 성공의 핵심임을 보여줍니다. 우리 모델은 CORA인 객체 관계 및 속성을 위한 구성 모델이라고 합니다. 두 가지 주요 이미지-텍스트 검색 벤치마크인 Flickr30K와 MSCOCO에 대한 실험 결과는 CORA가 이중 인코더의 빠른 계산 속도를 달성하면서 재현 점수와 관련하여 계산적으로 비용이 많이 드는 기존 최첨단 교차 주의 방법보다 성능이 우수하다는 것을 보여줍니다."
612,http://arxiv.org/abs/2406.10100 ,SkySenseGPT: A Fine-Grained Instruction Tuning Dataset and Model for Remote Sensing Vision-Language Understanding,"Junwei Luo, Zhen Pang, Yongjun Zhang, Tingzhu Wang, Linlin Wang, Bo Dang, Jiangwei Lao, Jian Wang, Jingdong Chen, Yihua Tan, Yansheng Li","RSLMM(Remote Sensing Large Multi-Modal Model)은 빠르게 발전하고 있으며 RSI(원격 감지 이미지) 이해에 있어 중요한 기능을 선보이고 있습니다. 그러나 기존 데이터 세트의 한계로 인해 RSLMM은 복잡한 원격 감지 장면에서 객체 간의 풍부한 의미 관계를 이해하는 데 단점이 있습니다. RSLMM의 복잡한 이해 능력을 잠금 해제하기 위해 우리는 1,800,851개의 명령어 샘플을 포함하는 대규모 명령어 튜닝 데이터 세트 FIT-RS를 제안합니다. FIT-RS는 일반적인 해석 작업을 다루며 관계 추론부터 이미지 수준 장면 그래프 생성에 이르기까지 난이도가 높아지는 여러 가지 복잡한 이해 작업을 혁신적으로 도입합니다. FIT-RS를 기반으로 FIT-RSFG 벤치마크를 구축합니다. 또한 FIT-RSRC라는 LMM의 세분화된 관계 이해 기능을 평가하기 위한 새로운 벤치마크를 설정합니다. 결합된 명령 데이터를 기반으로 공개 데이터 세트와 FIT-RSFG 모두에서 기존 RSLMM을 능가하는 뛰어난 성능을 달성하는 SkySenseGPT를 제안합니다. FIT-RS 데이터 세트가 RSLMM의 관계 이해 기능을 향상시키고 원격 감지 커뮤니티에 대규모의 세분화된 데이터 소스를 제공할 수 있기를 바랍니다. 데이터 세트는 https://github.com/Luo-Z13/SkySenseGPT에서 사용할 수 있습니다."
611,http://arxiv.org/abs/2406.09410 ,STAR: A First-Ever Dataset and A Large-Scale Benchmark for Scene Graph Generation in Large-Size Satellite Imagery,"Yansheng Li, Linlin Wang, Tingzhu Wang, Xue Yang, Junwei Luo, Qi Wang, Youming Deng, Wenbin Wang, Xian Sun, Haifeng Li, Bo Dang, Yongjun Zhang, Yi Yu, Junchi Yan","위성 이미지(SAI)의 장면 그래프 생성(SGG)은 인식에서 인지까지 지리공간 시나리오에 대한 이해를 높이는 데 도움이 됩니다. SAI에서 객체는 크기와 종횡비에서 큰 변화를 나타내며 객체 간에(공간적으로 분리된 객체 간에도) 풍부한 관계가 존재하므로 대형 VHR(초고해상도) SAI에서 SGG를 전체적으로 수행하는 것이 매력적입니다. 그러나 그러한 SGG 데이터 세트가 부족합니다. 대규모 SAI의 복잡성으로 인해 마이닝 트리플렛 <주체, 관계, 객체>는 장거리 상황 추론에 크게 의존합니다. 결과적으로 작은 크기의 자연 이미지를 위해 설계된 SGG 모델은 큰 크기의 SAI에 직접 적용할 수 없습니다. 이 논문은 512 x 768에서 27,860 x 31,096 픽셀 범위의 이미지 크기를 가진 대형 VHR SAI에서 SGG를 위한 대규모 데이터 세트를 구성하며, STAR(Scene graph generaTion in lArge-size Satellite imageRy)라고 하며 210K 이상의 객체와 400K 이상의 세 쌍을 포함합니다. 대규모 SAI에서 SGG를 구현하기 위해 SGG에 대한 객체 감지(OBD), 쌍 가지치기 및 관계 예측과 관련된 SAI를 이해하기 위한 상황 인식 계단식 인지(CAC) 프레임워크를 제안합니다. 우리는 또한 도전적인 STAR 데이터 세트에 대해 우리가 고안한 모듈을 통해 추가 적응이 필요한 약 30개의 OBD 및 10개의 SGG 방법을 포함하는 SAI 지향 SGG 툴킷을 출시합니다. 데이터 세트와 툴킷은 https://linlin-dev.github.io/project/STAR에서 사용할 수 있습니다."
610,http://arxiv.org/abs/2406.07113 ,Beyond Bare Queries: Open-Vocabulary Object Grounding with 3D Scene Graph,"Sergey Linok, Tatiana Zemskova, Svetlana Ladanova, Roman Titkov, Dmitry Yudin, Maxim Monastyrny, Aleksei Valenkov","자연어로 설명된 객체를 찾는 것은 자율 에이전트에게 중요한 과제입니다. 기존 CLIP 기반 개방형 어휘 방법은 간단한(베어) 쿼리로 3D 객체 접지를 성공적으로 수행하지만 객체 관계에 대한 이해를 요구하는 모호한 설명에는 대처할 수 없습니다. 이 문제를 해결하기 위해 우리는 BBQ(Beyond Bare Queries)라는 모듈식 접근 방식을 제안합니다. 이는 미터법 및 의미 공간 가장자리로 3D 장면 그래프 표현을 구성하고 연역적 장면 추론 알고리즘을 통해 대규모 언어 모델을 인간-에이전트 인터페이스로 활용합니다. BBQ는 강력한 DINO 기반 연결을 사용하여 3D 객체 중심 맵을 구성하고 2D 비전 언어 모델을 갖춘 고급 레이캐스팅 알고리즘을 사용하여 이를 그래프 노드로 설명합니다. Replica 및 ScanNet 데이터세트에서 우리는 BBQ가 다른 제로샷 방법에 비해 개방형 어휘 3D 의미론적 분할에서 선두 자리를 차지하고 있음을 입증했습니다. 또한 공간 관계를 활용하는 것이 동일한 의미 클래스의 여러 엔터티를 포함하는 장면에 특히 효과적이라는 것을 보여줍니다. 까다로운 Sr3D+, Nr3D 및 ScanRefer 벤치마크에서 우리의 연역적 접근 방식은 다른 최첨단 방법에 비해 복잡한 쿼리로 객체 기반을 기반으로 하는 상당한 개선을 보여줍니다. 우리의 설계 선택과 소프트웨어 구현의 결합으로 로봇 온보드 컴퓨터에 대한 실험에서 상당한 데이터 처리 속도가 나타났습니다. 이러한 유망한 성능을 통해 지능형 로봇 프로젝트에 우리의 접근 방식을 적용할 수 있습니다. 우리는 코드를 https://linukc.github.io/BeyondBareQueries/에서 공개적으로 사용할 수 있도록 만들었습니다."
609,http://arxiv.org/abs/2406.06438 ,Multimodal Contextualized Semantic Parsing from Speech,"Jordan Voas, Raymond Mooney, David Harwath",다중 모드 입력을 이전 컨텍스트와 통합하여 인공 에이전트의 컨텍스트 인식을 향상시키도록 설계된 작업인 SPICE(Semantic Parsing in Contextual Environments)를 소개합니다. SPICE는 에이전트의 지식을 새로운 정보로 동적으로 업데이트하고 인간 의사소통의 복잡성을 반영하는 구조화되고 해석 가능한 프레임워크를 제공함으로써 전통적인 의미 분석을 뛰어넘습니다. 우리는 음성 대화 교환을 통해 시각적 장면 그래프를 구성하여 음성 및 시각적 데이터 통합을 강조하는 에이전트에 도전하도록 제작된 VG-SPICE 데이터 세트를 개발합니다. 또한 VG-SPICE용으로 개발된 AViD-SP(Audio-Vision Dialogue Scene Parser)도 소개합니다. 이러한 혁신은 다중 모드 정보 처리 및 통합을 개선하는 것을 목표로 합니다. VG-SPICE 데이터 세트와 AViD-SP 모델은 모두 공개적으로 제공됩니다.
608,http://arxiv.org/abs/2406.06028 ,ReCon1M:A Large-scale Benchmark Dataset for Relation Comprehension in Remote Sensing Imagery,"Xian Sun, Qiwei Yan, Chubo Deng, Chenglong Liu, Yi Jiang, Zhongyan Hou, Wanxuan Lu, Fanglong Yao, Xiaoyu Liu, Lingxiang Hao, Hongfeng Yu","SGG(장면 그래프 생성)는 이미지에서 개체(예: 개체)와 개체의 상호 관계를 추출하는 것을 목표로 하는 높은 수준의 시각적 이해 및 추론 작업입니다. 최근 몇 년 동안 자연 이미지에 대한 SGG 연구에서 상당한 진전이 있었지만 원격 감지 이미지 영역에서의 탐색은 여전히 ​​매우 제한적입니다. 원격 감지 이미지의 복잡한 특성으로 인해 자연 이미지에 비해 주석을 추가하는 데 더 많은 시간과 수동 해석 비용이 필요합니다. 대규모 공공 SGG 벤치마크의 부족은 항공 이미지 분야의 SGG 관련 연구 발전에 주요 장애물입니다. 본 논문에서는 ReCon1M이라는 원격탐사 이미지 분야 최초의 공개적으로 이용 가능한 대규모, 백만 수준의 관계 데이터세트를 소개합니다. 특히, 우리의 데이터 세트는 Fair1M을 기반으로 구축되었으며 21,392개의 이미지로 구성됩니다. 여기에는 60개의 서로 다른 카테고리에 걸쳐 859,751개의 객체 경계 상자에 대한 주석과 이러한 경계 상자를 기반으로 하는 64개 카테고리에 걸쳐 1,149,342개의 관계 삼중항이 포함되어 있습니다. 데이터세트의 특성과 통계정보에 대한 자세한 설명을 제공합니다. 우리는 이 데이터 세트에 대해 SGG 내에서 두 가지 객체 감지 작업과 세 가지 하위 작업을 수행하여 이러한 작업에 대한 주류 방법의 성능을 평가했습니다."
607,http://arxiv.org/abs/2406.03865 ,Semantic Similarity Score for Measuring Visual Similarity at Semantic Level,"Senran Fan, Zhicheng Bao, Chen Dong, Haotai Liang, Xiaodong Xu, Ping Zhang","의미론적 커뮤니케이션은 혁신적인 커뮤니케이션 아키텍처로서 유망한 새로운 커뮤니케이션 패러다임으로 간주됩니다. 기존의 기호 기반 오류 없는 통신 시스템과 달리 의미 기반 시각적 통신 시스템은 의미 수준에서 이미지를 추출, 압축, 전송 및 재구성합니다. 그러나 픽셀 기반 MSE, PSNR 또는 구조 기반 MS-SSIM 등 널리 사용되는 이미지 유사성 평가 메트릭은 시스템 전송 중 소스의 의미 수준 정보 손실을 정확하게 측정하는 데 어려움을 겪습니다. 이는 특히 기존 통신 시스템과 비교할 때 시각적 의미 통신 시스템의 성능을 평가하는 데 어려움을 겪습니다. 이를 해결하기 위해 우리는 장면 그래프 생성 및 그래프 매칭을 기반으로 이미지 간 유사성 점수를 의미 수준의 그래프 매칭 점수로 전환하는 의미론적 평가 지표인 SeSS(Semantic 유사성 점수)를 제안합니다. 한편, 수만 개의 이미지 쌍에 대한 의미론적 유사성 점수는 그래프 일치 알고리즘의 하이퍼파라미터를 미세 조정하기 위해 수동으로 주석을 달고 측정항목을 인간의 의미론적 인식에 더 가깝게 정렬합니다. SeSS의 성능은 (1) 다양한 압축률로 기존 및 의미론적 통신 시스템에 의해 전송된 이미지, (2) 다양한 신호 대 잡음비로 기존 및 의미론적 통신 시스템에 의해 전송된 이미지, (3) 다양한 노이즈 수준이 도입된 대규모 모델에서 생성된 이미지, (4) 특정 특수 변환이 적용된 이미지의 경우를 포함한 다양한 데이터 세트에서 테스트되었습니다. 실험은 메트릭이 이미지의 의미 수준 정보의 의미 수준 차이를 측정할 수 있고 시각적 의미 통신 시스템의 평가에 사용될 수 있음을 나타내는 SeSS의 효율성을 보여줍니다."
606,http://arxiv.org/abs/2406.18579 ,Hire: Hybrid-modal Interaction with Multiple Relational Enhancements for Image-Text Matching,"Xuri Ge, Fuhai Chen, Songpei Xu, Fuxiang Tao, Jie Wang, Joemon M. Jose","ITM(이미지-텍스트 일치)은 컴퓨터 비전의 근본적인 문제입니다. 핵심 문제는 시각적 표현과 텍스트 표현을 공동으로 학습하여 유사성을 정확하게 추정하는 데 있습니다. 대부분의 기존 방법은 양식 내의 특징 강화 또는 양식 간 특징 상호 작용에 중점을 두지만, 이는 풍부한 문맥 의미론을 사용하여 해당 문장을 일치시키는 개체 간 관계를 기반으로 개체 표현의 문맥 정보를 무시합니다. 본 논문에서는 이미지-텍스트 매칭을 위한 다중 관계 향상(\textit{Hire}라고 함)을 사용한 하이브리드 모달 상호 작용을 제안합니다. 이는 암시적 및 명시적 관계 모델링을 통해 객체와 단어 사이의 모달 내 및 모달 간 의미를 연관시킵니다. 특히, 명시적 모달 내 공간 의미 그래프 기반 추론 네트워크는 객체의 공간 위치와 장면 그래프의 명시적 관계에 따라 눈에 띄는 공간 및 의미 관계 연결성을 통해 시각적 객체의 상황별 표현을 개선하도록 설계되었습니다. 명시적 관계 검색의 내결함성을 향상시키기 위해 명시적 모델링 전에 잠재적인 관계 상호 작용에 대한 암시적 관계 모델링을 사용합니다. 그런 다음 시각적 및 텍스트 의미론적 표현은 모달 간 대화형 주의 및 모달 간 정렬을 통해 공동으로 개선됩니다. 개체의 컨텍스트를 텍스트 컨텍스트와 연관시키기 위해 교차 수준 개체 문장 및 단어 이미지 기반 대화형 주의를 통해 시각적 의미 표현을 더욱 구체화합니다. 광범위한 실험을 통해 제안된 암시적 및 명시적 모델링과의 하이브리드-모달 상호 작용이 이미지-텍스트 일치에 더 유리하다는 것을 검증했습니다. 그리고 제안된 \textit{Hire}는 MS-COCO 및 Flickr30K 벤치마크에서 새로운 최첨단 결과를 얻습니다."
605,http://arxiv.org/abs/2406.03175 ,Dynamic 3D Gaussian Fields for Urban Areas,"Tobias Fischer, Jonas Kulhanek, Samuel Rota Bulò, Lorenzo Porzi, Marc Pollefeys, Peter Kontschieder","우리는 대규모의 역동적인 도시 지역에서 NVS(novel-view 종합)을 위한 효율적인 신경 3D 장면 표현을 제시합니다. 기존 작업은 제한된 시각적 품질과 비대화형 렌더링 속도로 인해 혼합 현실이나 폐쇄 루프 시뮬레이션과 같은 응용 프로그램에 적합하지 않습니다. 최근에는 래스터화 기반 접근 방식을 통해 인상적인 속도로 고품질 NVS를 달성했습니다. 그러나 이러한 방법은 소규모의 동질적인 데이터로 제한됩니다. 즉, 날씨, 계절, 조명으로 인한 심각한 외관 및 기하학적 변화를 처리할 수 없으며 수천 개의 이미지가 있는 더 크고 동적 영역으로 확장할 수 없습니다. 우리는 대규모의 역동적인 도시 지역으로 확장되고, 이질적인 입력 데이터를 처리하며, 렌더링 속도를 크게 향상시키는 신경 장면 표현인 4DGF를 제안합니다. 우리는 3D 가우시안을 효율적인 기하학 비계로 사용하는 동시에 신경장을 작고 유연한 외관 모델로 사용합니다. 우리는 변형을 통해 로컬 수준에서 관절 동작을 모델링하는 동시에 전역 규모의 장면 그래프를 통해 장면 역학을 통합합니다. 이러한 분해 방식을 사용하면 실제 애플리케이션에 적합한 유연한 장면 구성이 가능합니다. 실험에서는 PSNR에서 3dB 이상, 렌더링 속도에서 200배 이상 최첨단 기술을 능가했습니다."
604,http://arxiv.org/abs/2406.02038 ,Leveraging Predicate and Triplet Learning for Scene Graph Generation,"Jiankai Li, Yunhong Wang, Xiefan Guo, Ruijie Yang, Weixin Li","장면 그래프 생성(SGG)은 개체를 식별하고 시각적 장면에서 세 쌍의 \textit{\textless subject, predicate, object\textgreater } 관계를 예측하는 것을 목표로 합니다. 동일한 술어에서도 주체-객체 쌍의 대규모 시각적 변형이 널리 퍼져 있다는 점을 감안할 때 이러한 쌍에 걸쳐 술어 표현을 직접 모델링하고 개선하는 것은 매우 어려울 수 있지만 이는 대부분의 기존 SGG 방법에서 채택하는 일반적인 전략입니다. 우리는 동일한 삼중항 내의 시각적 변화가 상대적으로 작고 특정 관계 단서가 동일한 유형의 삼중항에서 공유되어 잠재적으로 SGG에서 관계 학습을 용이하게 할 수 있음을 관찰합니다. 또한 SGG 작업에서 널리 연구된 롱테일 문제의 경우 꼬리 술어에서 제한된 유형과 수량의 삼중항을 처리하는 것도 중요합니다. 따라서 본 논문에서는 거친 술어 큐 외에 세밀한 삼중항 큐를 활용하는 DRM(Dual-granularity Relation Modeling) 네트워크를 제안합니다. DRM은 이중 세분성 제약 조건을 통해 술어 및 삼중항의 컨텍스트 및 의미를 활용하여 관계 인식을 용이하게 하기 위해 두 가지 관점에서 간결하고 균형 잡힌 표현을 생성합니다. 또한, 긴 꼬리 문제를 완화하기 위해 꼬리 클래스의 패턴 다양성을 풍부하게 하는 것을 목표로 머리 술어/삼중항에서 꼬리 술어로 변형을 전송하기 위해 DKT(이중 세분성 지식 전달) 전략이 도입되었습니다. 광범위한 실험을 통해 Visual Genome, Open Image 및 GQA 데이터 세트에 대한 새로운 최첨단 성능을 확립하는 방법의 효율성이 입증되었습니다. 우리 코드는 \url{https://github.com/jkli1998/DRM}에서 확인할 수 있습니다."
603,http://arxiv.org/abs/2406.01584 ,SpatialRGPT: Grounded Spatial Reasoning in Vision Language Models,"An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, Sifei Liu","VLM(Vision Language Model)은 2D 비전 및 언어 작업에서 놀라운 성능을 보여주었습니다. 그러나 공간 배치에 대해 추론하는 능력은 여전히 ​​제한적입니다. 이 작업에서는 VLM의 공간 인식 및 추론 기능을 향상시키기 위해 Spatial Region GPT(SpatialRGPT)를 도입합니다. SpatialRGPT는 (1) 3D 장면 그래프에서 지역 표현을 효과적으로 학습할 수 있는 데이터 큐레이션 파이프라인과 (2) 깊이 정보를 기존 VLM의 시각적 인코더에 통합하기 위한 유연한 플러그인 모듈이라는 두 가지 주요 혁신을 통해 VLM의 공간 이해를 향상시킵니다. 추론 중에 사용자가 지정한 영역 제안이 제공되면 SpatialRGPT는 상대적인 방향과 거리를 정확하게 인식할 수 있습니다. 또한 VLM의 3D 공간 인식을 평가하기 위해 실내, 실외 및 시뮬레이션 환경을 포괄하는 실제 3D 주석을 갖춘 벤치마크인 SpatialRGBT-Bench를 제안합니다. 우리의 결과는 SpatialRGPT가 로컬 영역 프롬프트 유무에 관계없이 공간 추론 작업의 성능을 크게 향상시키는 것을 보여줍니다. 이 모델은 또한 강력한 일반화 기능을 보여 복잡한 공간 관계를 효과적으로 추론하고 로봇 작업에 대한 지역 인식 조밀한 보상 주석 역할을 합니다. 코드, 데이터 세트 및 벤치마크는 https://www.anjiecheng.me/SpatialRGPT에서 공개됩니다."
602,http://arxiv.org/abs/2406.01029 ,CYCLO: Cyclic Graph Transformer Approach to Multi-Object Relationship Modeling in Aerial Videos,"Trong-Thuan Nguyen, Pha Nguyen, Xin Li, Jackson Cothren, Alper Yilmaz, Khoa Luu","비디오 장면 그래프 생성(VidSGG)은 비디오 시퀀스에서 객체 간의 복잡한 관계와 객체의 시간적 역학을 캡처하고 해석하는 혁신적인 접근 방식으로 등장했습니다. 본 논문에서는 항공 영상의 다중 객체 관계 모델링에 초점을 맞춘 새로운 AeroEye 데이터 세트를 소개합니다. AeroEye 데이터 세트는 다양한 드론 장면을 특징으로 하며 객체 간의 복잡한 관계와 공간 배열을 포착하는 시각적으로 포괄적이고 정확한 조건자 컬렉션을 포함합니다. 이를 위해 우리는 순환 방식으로 상호 작용 기록을 지속적으로 업데이트하여 모델이 직접 및 장거리 시간 종속성을 모두 캡처할 수 있는 새로운 CYCLO(Cyclic Graph Transformer) 접근 방식을 제안합니다. 제안된 접근 방식을 사용하면 고유한 순환 패턴을 가진 시퀀스를 처리하고 올바른 순차적 순서로 개체 관계를 처리할 수 있습니다. 따라서 정보 손실을 최소화하면서 주기적이고 중복되는 관계를 효과적으로 포착할 수 있습니다. AeroEye 데이터세트에 대한 광범위한 실험은 제안된 CYCLO 모델의 효율성을 입증하고, 드론 비디오에서 장면 이해를 수행할 수 있는 잠재력을 보여줍니다. 마지막으로 CYCLO 방법은 PVSG와 ASPIRe라는 두 가지 현장 그래프 생성 벤치마크에서 지속적으로 SOTA(State-of-the-Art) 결과를 달성합니다."
601,http://arxiv.org/abs/2405.19442 ,Large-scale DSM registration via motion averaging,"Ningli Xu, Rongjun Qin","광역 디지털 표면 모델(DSM)을 생성하려면 다수의 개별 DSM과 부분적으로 중첩된 DSM을 등록해야 합니다. 이는 일반적인 등록 알고리즘에 어려운 문제를 제시합니다. 왜냐하면 이러한 여러 DSM에서 많은 수의 관찰을 고려할 때 메모리 오버플로가 쉽게 발생할 수 있기 때문입니다. 순차 등록 알고리즘은 계산을 크게 줄일 수 있지만 작은 중첩 쌍에 특히 취약하여 큰 오류가 누적됩니다. 이 연구에서는 모션 평균화 문제로 DSM 등록 작업을 구축하는 새로운 솔루션을 제안합니다. 쌍별 DSM은 DSM 간의 상대 포즈를 나타내는 가장자리를 사용하여 장면 그래프를 구축하기 위해 등록됩니다. 구체적으로, 대형 DSM의 그리드 구조를 기반으로 새로운 최근접 이웃 검색 방법을 사용하여 쌍별 등록을 수행합니다. 우리는 장면 그래프가 O(N) 복잡도(N은 이미지 수를 나타냄)의 매우 빠른 모션 평균 알고리즘을 통해 최적화될 수 있음을 보여줍니다. 고해상도 위성 기반 DSM의 평가는 계산 및 정확도가 크게 향상되었음을 보여줍니다."
600,http://arxiv.org/abs/2405.18948 ,Learning to Recover from Plan Execution Errors during Robot Manipulation: A Neuro-symbolic Approach,"Namasivayam Kalithasan, Arnav Tuli, Vishal Bindal, Himanshu Gaurav Singh, Parag Singla, Rohan Paul",오류를 자동으로 감지하고 복구하는 것은 자율 로봇에게 중요하지만 어려운 문제입니다. 데모를 통한 계획 학습에 대한 최근 작업의 대부분은 명시적인 상태 표현 및/또는 (하위) 목표 확인 기능이 없는 경우 오류를 감지하고 복구하는 기능이 부족합니다. 우리는 주석이 달린 실패 데이터 없이 자동화된 오류 발견 및 복구를 위한 접근 방식(기호 검색과 학습 혼합)을 제안합니다. 우리 접근 방식의 핵심은 환경 내에 존재하는 객체를 기반으로 구조화된 조밀한 장면 그래프 형태의 신경 기호 상태 표현입니다. 이를 통해 오류를 식별할 뿐만 아니라 오류를 국지화하는 판별기 및 전이 함수를 효율적으로 학습할 수 있어 경험적 거리 함수 계산을 통해 빠른 재계획이 가능해집니다. 우리는 또한 마지막 올바른 상태로 복구하는 대신 재계획 예산을 고려하여 목표까지의 총 거리를 최소화하면서 원래 계획에서 하위 목표를 검색하는 언제든지 버전의 알고리즘을 제시합니다. 다양한 시뮬레이션 오류가 포함된 물리 시뮬레이터에 대한 실험은 복구 메커니즘의 효율성과 정확성 측면에서 기존 기준과 비교하여 우리 접근 방식의 효율성을 보여줍니다.
599,http://arxiv.org/abs/2405.16925 ,OED: Towards One-stage End-to-End Dynamic Scene Graph Generation,"Guan Wang, Zhimin Li, Qingchao Chen, Yang Liu","DSGG(동적 장면 그래프 생성)는 비디오의 시공간 영역 내에서 시각적 관계를 식별하는 데 중점을 둡니다. 기존 접근 방식에서는 일반적으로 객체 감지, 시간 연관 및 다중 관계 분류로 구성된 다단계 파이프라인을 사용하는 경우가 많습니다. 그러나 이러한 방법은 여러 단계의 분리로 인해 본질적인 한계를 나타내며 이러한 하위 문제를 독립적으로 최적화하면 차선책이 나올 수 있습니다. 이러한 제한 사항을 해결하기 위해 우리는 DSGG 파이프라인을 간소화하는 OED라는 1단계 엔드투엔드 프레임워크를 제안합니다. 이 프레임워크는 작업을 설정된 예측 문제로 재구성하고 쌍별 기능을 활용하여 장면 그래프 내의 각 주체-객체 쌍을 나타냅니다. 또한 DSGG의 또 다른 과제는 시간적 종속성을 캡처하는 것입니다. 추가 추적기나 손으로 만든 궤적의 제약 없이 시간적 컨텍스트를 집계하기 위한 PRM(Progressively Refined Module)을 도입하여 네트워크의 엔드투엔드 최적화를 가능하게 합니다. Action Genome 벤치마크에서 수행된 광범위한 실험은 우리 설계의 효율성을 입증합니다. 코드와 모델은 \url{https://github.com/guanw-pku/OED}에서 확인할 수 있습니다."
598,http://arxiv.org/abs/2405.16401 ,Understanding the Effect of using Semantically Meaningful Tokens for Visual Representation Learning,"Neha Kalibhat, Priyatham Kattakinda, Sumit Nawathe, Arman Zarei, Nikita Seleznev, Samuel Sharpe, Senthil Kumar, Soheil Feizi",비전 변환기는 처리하기 전에 이미지를 균일한 크기의 청크로 패치하는 선례를 확립했습니다. 우리는 이러한 디자인 선택이 시각적 데이터로부터 포괄적이고 구성적인 표현을 학습하는 데 있어 모델을 제한할 수 있다고 가정합니다. 이 문서에서는 비전 언어 사전 훈련 프레임워크 내에서 변환기 인코더에 의미상 의미 있는 시각적 토큰을 제공하는 개념을 탐구합니다. 기성 분할 및 장면 그래프 모델을 활용하여 인스턴스 분할 마스크(유형 토큰이라고 함)와 관계 및 작업(무형 토큰이라고 함)의 표현을 추출합니다. 그런 다음 새로 추출된 토큰을 통합하고 결과 임베딩을 텍스트 측 인코더의 캡션 임베딩과 정렬하여 비전 측 변환기를 사전 훈련합니다. 시각적 토큰 간의 구조적 및 의미적 관계를 포착하기 위해 self-attention 점수를 계산하는 데 사용되는 추가 주의 가중치를 도입합니다. COCO에 대한 우리의 실험은 텍스트-이미지(+47%) 및 이미지-텍스트 검색(+44%) 작업 전반에 걸쳐 학습된 표현 품질에서 ViT에 비해 눈에 띄게 개선되었음을 보여줍니다. 또한 ARO(+18%) 및 Winoground(+10%)와 같은 구성성 벤치마크의 장점을 보여줍니다.
597,http://arxiv.org/abs/2405.16116 ,REACT: Real-time Efficiency and Accuracy Compromise for Tradeoffs in Scene Graph Generation,"Maëlic Neau, Paulo E. Santos, Anne-Gwenn Bosser, Cédric Buche, Akihiro Sugimoto","SGG(장면 그래프 생성)는 이미지 내 객체 간의 시각적 관계를 그래프 구조로 인코딩하는 작업입니다. SGG는 구체화된 에이전트에 대한 추론과 같은 다운스트림 작업을 위한 기본 구성 요소로서 상당한 가능성을 보여줍니다. 실시간 애플리케이션을 활성화하려면 SGG는 성능과 추론 속도 간의 균형을 해결해야 합니다. 그러나 현재 방법은 세 가지 목표의 균형을 동시에 맞추는 것을 목표로 하지 않고 (1) 관계 예측 정확도 향상, (2) 객체 감지 정확도 향상, (3) 지연 시간 감소 중 하나에 초점을 맞추는 경향이 있습니다. 이러한 한계를 해결하기 위해 우리는 REACT(Real-time Efficiency and Accuracy Compromise for Tradeoffs in Scene Graph Generation) 아키텍처를 제안합니다. 이 아키텍처는 기존 SGG 모델 중에서 가장 높은 추론 속도를 달성하고 관계 예측 성능을 저하시키지 않으면서 객체 감지 정확도를 향상시킵니다. REACT는 최첨단 접근 방식에 비해 속도가 2.7배 빠르고 객체 감지 정확도가 58% 향상됩니다. 또한 우리의 제안은 평균 5.5배 더 적은 매개변수로 모델 크기를 크게 줄입니다. 코드는 https://github.com/Maelic/SGG-Benchmark에서 확인할 수 있습니다."
596,http://arxiv.org/abs/2405.15321 ,SG-Adapter: Enhancing Text-to-Image Generation with Scene Graph Guidance,"Guibao Shen, Luozhou Wang, Jiantao Lin, Wenhang Ge, Chaozhe Zhang, Xin Tao, Yuan Zhang, Pengfei Wan, Zhongyuan Wang, Guangyong Chen, Yijun Li, Ying-Cong Chen","텍스트-이미지 생성의 최근 발전은 확산 모델과 다중 양식 학습의 개발로 인해 촉진되었습니다. 그러나 텍스트는 일반적으로 이러한 모델에서 순차적으로 표현되므로 정확한 상황화 및 구조적 제어를 제공하는 데 종종 부족합니다. 따라서 생성된 이미지는 특히 여러 개체와 관계가 포함된 복잡한 시나리오에서 인간의 기대와 일관되게 일치하지 않습니다. 본 논문에서는 장면 그래프의 구조화된 표현을 활용하여 원본 텍스트 임베딩의 부정확성을 수정하는 장면 그래프 어댑터(SG-Adapter)를 소개합니다. SG-Adapter의 명시적 및 불완전 연결 그래프 표현은 완전히 연결된 변환기 기반 텍스트 표현을 크게 향상시킵니다. 이러한 향상된 기능은 여러 관계가 포함된 시나리오에서 정확한 대응을 유지하는 데 특히 주목할 만합니다. Visual Genome과 같이 주석이 달린 저품질 데이터 세트로 인한 문제를 해결하기 위해 우리는 매우 깨끗한 다중 관계 장면 그래프-이미지 쌍 데이터 세트 MultiRels를 수동으로 선별했습니다. 또한, 이미지와 장면 그래프 간의 일치성을 효과적이고 철저하게 측정하기 위해 GPT-4V에서 파생된 세 가지 메트릭을 설계합니다. 정성적 및 정량적 결과 모두 다중 관계의 서신을 제어하는 ​​데 있어 우리 접근 방식의 효율성을 검증합니다."
595,http://arxiv.org/abs/2405.12648 ,Scene Graph Generation Strategy with Co-occurrence Knowledge and Learnable Term Frequency,"Hyeongjin Kim, Sangwon Kim, Dasom Ahn, Jong Taek Lee, Byoung Chul Ko",장면 그래프 생성(SGG)은 이미지 속 개체 간의 관계를 그래프 구조로 표현하여 개체 간의 의미 관계를 직관적으로 이해할 수 있도록 하기 때문에 이미지 이해에 있어 중요한 작업입니다. 이전 SGG 연구에서는 MPNN(메시지 전달 신경망)을 사용하여 주변 객체에 대한 정보를 효과적으로 반영할 수 있는 기능을 업데이트했습니다. 그러나 이러한 연구에서는 SGG 생성 중 객체의 동시 발생을 반영하지 못했습니다. 또한 샘플링 및 학습 방법의 관점에서 훈련 데이터 세트의 롱테일 문제만 해결했습니다. 이 두 가지 문제를 해결하기 위해 우리는 객체 간 Co-occurrence Knowledge를 반영한 ​​CooK와 롱테일 문제를 해결하기 위한 학습 가능한 용어 주파수-역 문서 빈도(TF-l-IDF)를 제안합니다. 제안된 모델을 SGG 벤치마크 데이터셋에 적용한 결과 SGGen 하위 작업에서 기존 최첨단 모델 대비 최대 3.8%의 성능 향상을 보였다. 제안된 방법은 얻은 결과로부터 일반화 능력을 보여 모든 MPNN 모델에 대해 균일한 성능 향상을 보여줍니다.
594,http://arxiv.org/abs/2405.11483 ,MICap: A Unified Model for Identity-aware Movie Descriptions,"Haran Raajesh, Naveen Reddy Desanur, Zeeshan Khan, Makarand Tapaswi","캐릭터는 스토리라인의 중요한 측면이며 스토리를 이해하려면 캐릭터를 식별하고 설명에 포함하는 것이 필요합니다. 이전 작업에서는 신원을 대부분 무시하고 누군가(익명화된 이름)와 함께 캡션을 생성했지만 최근 작업에서는 ID 인식 캡션을 공백 채우기(FITB) 작업으로 공식화합니다. 여기서 공백이 있는 캡션이 주어지면 사람 ID 레이블을 예측하는 것이 목표입니다. 그러나 ID를 사용하여 캡션을 예측하려면 먼저 누군가와 캡션을 예측한 다음 ID를 입력하는 2단계 접근 방식이 필요합니다. 이 작업에서는 공백이 있는 캡션이 제공될 때 ID 인식 캡션 생성 또는 FITB 간에 원활하게 전환할 수 있는 새로운 단일 단계 접근 방식을 제시합니다. 우리 모델인 MICap(Movie-Identity Captioner)은 FITB 및 전체 캡션 생성 목표를 통한 교육의 이점을 누리는 공유 자동 회귀 디코더를 사용하는 반면, 인코더는 입력으로 공백이 있는 캡션의 이점을 얻거나 무시할 수 있습니다. ID 인식 캡션의 또 다른 과제는 개인 ID 간의 미묘한 차이를 포착할 수 있는 측정 기준이 부족하다는 것입니다. 이를 위해 중간 장면 그래프를 통해 생성된 ID 튜플에 초점을 맞춘 캡션 평가 메트릭인 iSPICE를 소개합니다. LSMDC(Large-Scale Movie Description Challenge)에서 MICap을 평가했는데, 여기서 FITB 정확도는 4.2% 향상되었고 클래식 캡션 측정 항목은 1~2% 향상되었습니다."
593,http://arxiv.org/abs/2405.10305 ,4D Panoptic Scene Graph Generation,"Jingkang Yang, Jun Cen, Wenxuan Peng, Shuai Liu, Fangzhou Hong, Xiangtai Li, Kaiyang Zhou, Qifeng Chen, Ziwei Liu","우리는 4차원인 시간을 통해 앞으로 나아가면서 3차원 공간에 살고 있습니다. 인공지능이 이러한 4D 환경에 대한 포괄적인 이해를 개발할 수 있도록 역동적인 4D 세계에서 인식된 원시 시각적 데이터와 높은 수준의 시각적 이해를 연결하는 새로운 표현인 4D Panoptic Scene Graph(PSG-4D)를 소개합니다. 특히 PSG-4D는 풍부한 4D 감각 데이터를 정확한 위치 및 상태 정보가 있는 개체를 나타내는 노드와 시간적 관계를 캡처하는 가장자리로 추상화합니다. 이 새로운 영역에 대한 연구를 촉진하기 위해 우리는 총 1M 프레임의 3K RGB-D 비디오로 구성된 풍부한 주석이 달린 PSG-4D 데이터세트를 구축했습니다. 각 프레임에는 4D 팬옵틱 분할 마스크와 세분화된 동적 장면 그래프가 표시되어 있습니다. PSG-4D를 해결하기 위해 우리는 Panoptic 분할 마스크를 예측하고 시간 축을 따라 마스크를 추적하며 관계 구성 요소를 통해 해당 장면 그래프를 생성할 수 있는 Transformer 기반 모델인 PSG4DFormer를 제안합니다. 새로운 데이터 세트에 대한 광범위한 실험은 우리의 방법이 PSG-4D에 대한 향후 연구를 위한 강력한 기준이 될 수 있음을 보여줍니다. 마지막으로, 대규모 언어 모델을 PSG-4D 시스템에 통합하여 동적 장면 이해를 달성할 수 있는 방법을 보여주기 위해 실제 적용 사례를 제공합니다."
592,http://arxiv.org/abs/2405.09822 ,SEEK: Semantic Reasoning for Object Goal Navigation in Real World Inspection Tasks,"Muhammad Fadhil Ginting, Sung-Kyun Kim, David D. Fan, Matteo Palieri, Mykel J. Kochenderfer, Ali-akbar Agha-Mohammadi","이 논문은 실제 환경의 자율 검사에서 객체-목표 탐색 문제를 다룹니다. 객체-목표 탐색은 다양한 설정에서 효과적인 검사를 가능하게 하는 데 중요하며, 종종 로봇이 넓은 검색 공간 내에서 대상 객체를 식별해야 합니다. 현재의 물체 검사 방법은 일반적으로 인간처럼 사전 지식과 상식 지식을 부트스트랩할 수 없기 때문에 인간 효율성이 부족합니다. 본 논문에서는 로봇이 환경의 사전 공간 구성에서 얻은 의미 지식과 의미 상식 지식을 사용할 수 있도록 하는 프레임워크를 소개합니다. 우리는 의미론적 사전 지식과 로봇의 관찰을 결합하여 대상 객체를 보다 효율적으로 검색하고 탐색하는 SEEK(Semantic Reasoning for Object Inspection Tasks)를 제안합니다. SEEK는 DSG(Dynamic Scene Graph)와 RSN(Relational Semantic Network)이라는 두 가지 표현을 유지합니다. RSN은 DSG의 공간 요소 전반에 걸쳐 대상 객체를 찾을 확률을 추정하는 간결하고 실용적인 모델입니다. 우리는 관계형 의미 지식을 사용하여 객체를 검색하기 위한 새로운 확률론적 계획 프레임워크를 제안합니다. 우리의 시뮬레이션 분석은 SEEK가 개체 목표 검사 작업의 효율성 측면에서 본 연구에서 검토된 고전적 계획 및 LLM(대형 언어 모델) 기반 방법보다 성능이 우수하다는 것을 보여줍니다. 우리는 도시 환경에서 실제 다리가 있는 로봇에 대한 접근 방식을 검증하여 실제 검사 시나리오에서 실용성과 효율성을 보여주었습니다."
591,http://arxiv.org/abs/2405.05792 ,RoboHop: Segment-based Topological Map Representation for Open-World Visual Navigation,"Sourav Garg, Krishan Rana, Mehdi Hosseinzadeh, Lachlan Mares, Niko Sünderhauf, Feras Dayoub, Ian Reid","매핑은 공간 추론, 계획 및 로봇 탐색에 매우 중요합니다. 기존 접근 방식은 정확한 형상 기반 최적화가 필요한 미터법부터 노드 기반 이미지 기반 그래프에 명시적인 개체 수준 추론 및 상호 연결성이 부족한 순전히 토폴로지에 이르기까지 다양합니다. 본 논문에서는 의미상 의미가 있고 개방형 어휘 쿼리가 가능한 ""이미지 세그먼트""를 기반으로 하는 환경의 새로운 토폴로지 표현을 제안하여 픽셀 수준 기능을 기반으로 한 이전 작업에 비해 몇 가지 장점을 제공합니다. 3D 장면 그래프와 달리 우리는 세그먼트를 노드로 사용하여 a) 연속 이미지 쌍 사이에 세그먼트 수준 설명자를 연결하고 b) 픽셀 중심을 사용하여 이미지 내 인접 세그먼트를 연결하여 가장자리를 형성하는 순수 토폴로지 그래프를 만듭니다. 이는 이미지 내 이웃과 함께 세그먼트의 이미지 간 지속성에 의해 정의되는 ""장소의 연속적인 감각""을 드러냅니다. 또한 그래프 컨볼루션 레이어를 사용한 인접 집계를 통해 세그먼트 수준 설명자를 표현하고 업데이트할 수 있어 세그먼트 수준 검색을 기반으로 로봇 위치 파악이 향상됩니다. 실제 데이터를 사용하여 우리는 제안된 지도 표현을 사용하여 i) ""세그먼트를 통한 홉"" 형태로 탐색 계획을 생성하고 ii) 객체의 공간 관계를 설명하는 자연어 쿼리를 사용하여 대상 객체를 검색하는 방법을 보여줍니다. 또한, 동일한 장소를 다시 방문할 때 매핑 및 세그먼트 수준 위치 파악 중 이미지 간 연결을 뒷받침하는 세그먼트 수준에서 데이터 연관성을 정량적으로 분석합니다. 마지막으로, 세그먼트 수준의 '호핑' 기반 제로 샷 실제 탐색에 대한 예비 시험을 보여줍니다. 추가 세부 정보가 포함된 프로젝트 페이지: oravus.github.io/RoboHop/"
590,http://arxiv.org/abs/2405.04732 ,Is the House Ready For Sleeptime? Generating and Evaluating Situational Queries for Embodied Question Answering,"Vishnu Sashank Dorbala, Prasoon Goyal, Robinson Piramuthu, Michael Johnston, Reza Ghanadhan, Dinesh Manocha","우리는 가정 환경에서 S-EQA(Situational Queries)를 사용한 EQA(Embodied Question Answering) 문제를 제시하고 해결합니다. 대상 개체 및 속성을 직접 참조하는 간단한 쿼리(""자동차의 색상은 무엇입니까?"")를 다루는 이전 EQA 작업과 달리 상황별 쿼리(예: ""집은 잠잘 준비가 되었나요?"")는 에이전트가 여러 개체 상태(문: 닫힘, 조명: 꺼짐 등)를 올바르게 식별하고 답변을 위해 해당 상태에 대한 합의에 도달해야 하기 때문에 어렵습니다. 이 목표를 위해 먼저 LLM의 출력을 둘러싸 고유한 상황 쿼리와 해당 합의 객체 정보를 생성하는 새로운 PGE(Prompt-Generate-Evaluate) 체계를 소개합니다. PGE는 VirtualHome 시뮬레이터에서 2K 데이터 포인트를 생성하는 데 사용되며, M-Turk에서 수행된 대규모 사용자 연구를 통해 실제 답변에 대한 주석이 추가됩니다. 이 연구에서 높은 응답률(97.26%)을 통해 LLM이 상황 데이터 생성에 능숙하다는 사실을 입증했습니다. 그러나 LLM을 사용하여 데이터를 평가할 때 실제 인간 주석과 46.2%의 낮은 상관 관계가 관찰되었습니다. LLM은 상황별 데이터를 생성하는 데 능숙하지만 합의에 따라 답변하는 데 어려움을 겪고 있음을 나타냅니다. 추론을 요청하면 LLM이 답변을 정당화하는 데 있어 상식에 어긋나는 경우가 종종 있습니다. 마지막으로 PGE를 활용하여 실제 환경에서 상황별 데이터를 생성하고 구조화된 장면 그래프를 사용할 수 없을 때 신뢰할 수 있는 객체 상태를 생성하는 LLM 환각을 노출합니다. 우리가 아는 한, 이는 상황 쿼리의 맥락에서 EQA를 소개한 최초의 작업이자 쿼리 생성을 위한 생성적 접근 방식을 제시한 최초의 작업입니다. 우리는 이 작업을 통해 구체화된 에이전트의 실제 사용성을 향상시키는 연구를 육성하는 것을 목표로 합니다."
589,http://arxiv.org/abs/2501.03230 ,Video-of-Thought: Step-by-Step Video Reasoning from Perception to Cognition,"Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Meishan Zhang, Mong-Li Lee, Wynne Hsu","비디오 이해에 대한 기존 연구는 주로 두 가지 주요 병목 현상, 즉 세밀한 시공간 지각 이해와 인지 수준의 비디오 장면 이해에 대한 탐구가 부족하기 때문에 복잡한 비디오에 대한 심층적인 이해와 추론을 달성하는 데 여전히 어려움을 겪고 있습니다. 본 논문은 새로운 솔루션을 제시하여 격차를 해소합니다. 먼저 비디오 공간-시간 장면 그래프(STSG) 표현을 통합하여 세밀한 픽셀 수준의 공간-시간 비디오 접지를 달성하는 새로운 비디오 MLLM(Multimodal Large Language Model), MotionEpic을 소개합니다. 그런 다음 MotionEpic을 기반으로 VoT(생각의 비디오) 추론 프레임워크를 개발합니다. VoT는 CoT(Chain-of-Thought) 코어를 상속하여 복잡한 작업을 더 간단하고 관리 가능한 하위 문제로 나누고 낮은 수준의 픽셀 인식에서 높은 수준의 인지 해석까지 단계별로 해결합니다. 다양하고 복잡한 비디오 QA 벤치마크에 대한 광범위한 실험을 통해 우리의 전반적인 프레임워크가 기존의 최첨단 기술을 눈에 띄게 향상시키는 것으로 나타났습니다. 우리가 아는 한, 이는 인간 수준의 비디오 추론을 달성하기 위한 CoT 기술을 성공적으로 구현하려는 첫 번째 시도이며, 이를 더 광범위한 비디오 이해 시나리오로 확장할 수 있는 큰 잠재력을 보여줍니다. 프로젝트는 https://haofei.vip/VoT에서 공개됩니다."
588,http://arxiv.org/abs/2405.03650 ,Generated Contents Enrichment,"Mahdi Naseri, Jiayan Qiu, Zhou Wang","본 논문에서는 생성된 콘텐츠 강화(GCE)라는 새로운 인공지능 생성 작업을 조사합니다. 기존 AI 콘텐츠 생성은 제한된 의미론적 설명을 기반으로 주어진 텍스트 설명을 암시적으로 풍부하게 하여 시각적으로 사실적인 콘텐츠를 생성합니다. 이러한 전통적인 작업과 달리 우리가 제안한 GCE는 시각적 및 텍스트 영역 모두에서 명시적으로 콘텐츠 강화를 수행하려고 노력합니다. 목표는 시각적으로 사실적이고, 구조적으로 일관되고, 의미적으로 풍부한 콘텐츠를 생성하는 것입니다. GCE를 해결하기 위해 우리는 강화 프로세스 동안 의미 체계와 의미 간 관계를 명시적으로 탐색하는 심층적인 엔드투엔드 적대적 방법을 제안합니다. 우리의 접근 방식은 먼저 입력 설명을 장면 그래프로 모델링합니다. 여기서 노드는 객체를 나타내고 가장자리는 객체 간 관계를 캡처합니다. 그런 다음 입력 장면 설명 위에 그래프 컨볼루셔널 네트워크를 채택하여 추가적인 강화 객체와 기존 객체와의 관계를 예측합니다. 마지막으로, 보강된 설명은 이미지 합성 모델에 전달되어 해당 시각적 콘텐츠를 생성합니다. Visual Genome 데이터 세트에 대해 수행된 실험은 우리 방법의 효율성을 입증하여 유망하고 시각적으로 그럴듯한 결과를 생성합니다."
587,http://arxiv.org/abs/2405.02826 ,Nip in the Bud: Forecasting and Interpreting Post-exploitation Attacks in Real-time through Cyber Threat Intelligence Reports,"Tiantian Zhu, Jie Ying, Tieming Chen, Chunlin Xiong, Wenrui Cheng, Qixuan Yuan, Aohan Zheng, Mingqi Lv, Yan Chen","지능형 지속 위협(APT) 공격은 전 세계적으로 심각한 피해를 입혔습니다. 기업에서는 잠재적인 위협에 맞서 싸우기 위해 다양한 EDR(엔드포인트 탐지 및 대응) 시스템을 배포합니다. 그러나 EDR은 높은 오탐률로 인해 어려움을 겪고 있습니다. 정상적인 운영에 영향을 미치지 않기 위해 분석가는 대응 조치를 취하기 전에 탐지 결과를 조사하고 필터링해야 하며, 과도한 수작업과 알람 피로로 인해 분석가가 최적의 응답 시간을 놓치게 되어 정보 유출 및 파괴로 이어집니다. 따라서 우리는 공격 후 공격 시 다음 움직임을 자동으로 예측하고 이를 기술 수준에서 설명하고, 사전 강화를 위해 EDR에 전략을 파견할 수 있는 실시간 공격 예측 및 해석 시스템인 EFI(Endpoint Forecasting and Interpreting)를 제안한다. 먼저 CTI(사이버 위협 인텔리전스) 보고서를 사용하여 하위 수준 시스템 로그에 매핑할 수 있는 공격 장면 그래프(ASG)를 추출하여 공격 샘플을 강화합니다. 둘째, EDR에서 제공하는 공격 프로버넌스 그래프(APG)와 결합하여 공격 예측 그래프(AFG)를 생성하여 다음 움직임을 예측하는 직렬형 그래프 예측 모델을 구축합니다. 마지막으로 공격 템플릿 그래프(ATG)와 그래프 정렬 및 기술 수준 해석 알고리즘을 활용하여 EDR에 대한 전략을 자동으로 파견하여 시스템을 사전에 강화합니다. EFI는 기존 EDR 오탐지의 영향을 방지하고 정상적인 작동에 영향을 주지 않고 시스템의 공격 표면을 줄일 수 있습니다. 우리는 총 3,484개의 CTI 보고서를 수집하고, 1,429개의 ASG를 생성하고, 8,000개의 문장에 레이블을 지정하고, 10,451개의 엔터티에 태그를 지정하고, 256개의 ATG를 구성합니다. DARPA Engagement와 대규모 CTI 데이터 세트에 대한 실험 결과는 EFI에서 예측한 AFG와 실제 공격 그래프 간의 정렬 점수가 0.8을 초과할 수 있고 EFI의 예측 및 해석 정확도가 91.8%에 도달할 수 있음을 보여줍니다."
586,http://arxiv.org/abs/2405.00915 ,EchoScene: Indoor Scene Generation via Information Echo over Scene Graph Diffusion,"Guangyao Zhai, Evin Pınar Örnek, Dave Zhenyu Chen, Ruotong Liao, Yan Di, Nassir Navab, Federico Tombari, Benjamin Busam","장면 그래프에서 3D 실내 장면을 생성하는 대화형 및 제어 가능한 생성 모델인 EchoScene을 소개합니다. EchoScene은 장면 그래프에 동적으로 적응하는 이중 분기 확산 모델을 활용합니다. 기존 방법은 다양한 노드 수, 다중 에지 조합 및 조작자로 인한 노드-에지 작업으로 인해 장면 그래프를 처리하는 데 어려움을 겪습니다. EchoScene은 각 노드를 잡음 제거 프로세스와 연결하여 이를 극복하고 공동 정보 교환을 가능하게 하여 글로벌 제약 조건에 대한 제어 가능하고 일관된 생성 인식을 향상시킵니다. 이는 모양 및 레이아웃 분기 모두에서 정보 반향 체계를 통해 달성됩니다. 모든 잡음 제거 단계에서 모든 프로세스는 그래프 컨볼루션을 사용하여 이러한 업데이트를 결합하는 정보 교환 장치와 잡음 제거 데이터를 공유합니다. 이 체계는 장면 그래프에 대한 전체적인 이해에 의해 노이즈 제거 프로세스가 영향을 받도록 보장하여 전 세계적으로 일관된 장면의 생성을 촉진합니다. 입력 장면 그래프를 편집하고 확산 모델에서 노이즈를 샘플링하여 추론 중에 결과 장면을 조작할 수 있습니다. 광범위한 실험을 통해 장면 제어성을 유지하고 생성 충실도에서 이전 방법을 능가하는 우리의 접근 방식을 검증했습니다. 또한 생성된 장면은 고품질이므로 기성 텍스처 생성과 직접 호환됩니다. 코드와 훈련된 모델은 오픈 소스입니다."
585,http://arxiv.org/abs/2405.00552 ,Long-Term Human Trajectory Prediction using 3D Dynamic Scene Graphs,"Nicolas Gorlo, Lukas Schmid, Luca Carlone","우리는 실내 인간 중심 환경에서 장기 인간 궤적 예측을 위한 새로운 접근 방식을 제시합니다. 이는 이러한 환경에서 장거리 로봇 계획에 필수적입니다. 최첨단 인간 궤적 예측 방법은 충돌 회피 및 단기 계획에 초점을 맞추고 인간과 환경의 복잡한 상호 작용을 모델링할 수 없다는 점으로 인해 제한됩니다. 대조적으로, 우리의 접근 방식은 인간과 환경의 상호 작용 순서를 예측하고 이 정보를 사용하여 최대 60년대의 궤적 예측을 안내함으로써 이러한 한계를 극복합니다. 우리는 LLM(대형 언어 모델)을 활용하여 장면에 대한 풍부한 맥락 정보에 대한 LLM 예측을 조정함으로써 환경과의 상호 작용을 예측합니다. 이 정보는 환경의 기하학, 의미, 횡단 가능성을 계층적 표현으로 인코딩하는 3D 동적 장면 그래프로 제공됩니다. 그런 다음 연속 시간 마르코프 체인을 기반으로 한 확률적 접근 방식을 사용하여 인간 위치에 대한 다중 모드 시공간 분포로 이러한 상호 작용 시퀀스를 기반으로 합니다. 우리의 접근 방식을 평가하기 위해 복잡한 실내 환경에서 인간의 장기 궤적에 대한 새로운 반합성 데이터 세트를 소개합니다. 여기에는 인간과 사물의 상호 작용에 대한 주석도 포함됩니다. 우리는 철저한 실험적 평가를 통해 우리의 접근 방식이 60초의 시간 범위에 대해 최고의 비특권(즉, 데이터세트에서 제로샷 방식으로 평가됨) 기준선에 비해 54% 더 낮은 평균 음의 로그 우도와 26.5% 더 낮은 20개 최고 변위 오류를 달성한다는 것을 보여줍니다."
584,http://arxiv.org/abs/2404.19379 ,SemanticFormer: Holistic and Semantic Traffic Scene Representation for Trajectory Prediction using Knowledge Graphs,"Zhigang Sun, Zixu Wang, Lavdim Halilaj, Juergen Luettin","자율 주행의 궤적 예측은 교통 참가자, 도로 토폴로지, 교통 표지판은 물론 서로의 의미론적 관계를 포함하여 운전 장면의 모든 관련 컨텍스트를 정확하게 표현하는 데 의존합니다. 이 문제에 대한 관심이 높아졌음에도 불구하고 대부분의 궤도 예측 접근 방식은 이러한 모든 요소를 ​​충분히 고려하지 않습니다. 우리는 하이브리드 접근 방식을 사용하여 의미론적 교통 장면 그래프를 추론하여 다중 모드 궤적을 예측하는 접근 방식인 SemanticFormer를 제시합니다. 이는 메타 경로 형태의 높은 수준의 정보를 활용합니다. 즉, 에이전트가 지식 그래프에서 구동할 수 있는 궤적은 다중 주의 메커니즘을 기반으로 하는 새로운 파이프라인에 의해 처리되어 정확한 궤적을 예측합니다. SemanticFormer는 에이전트 전체는 물론 에이전트와 도로 요소 간의 시공간 및 관계 정보를 캡처하기 위한 계층적 이종 그래프 인코더로 구성됩니다. 또한 다양한 인코딩을 융합하고 궤적을 확률로 디코딩하는 예측기가 포함되어 있습니다. 마지막으로, 개선 모듈은 허용된 궤도의 메타 경로와 속도 프로필을 평가하여 최종 예측 궤도를 얻습니다. nuScenes 벤치마크 평가에서는 여러 SOTA 방법에 비해 향상된 성능을 보여줍니다. 또한 우리는 지식 그래프를 두 가지 그래프 기반 기존 SOTA 방법인 VectorNet 및 Laformer에 쉽게 추가하여 원래의 동종 그래프를 대체할 수 있음을 보여줍니다. 평가 결과는 지식 그래프를 추가함으로써 원래 방법의 성능이 각각 5%와 4% 향상되었음을 시사합니다."
583,http://arxiv.org/abs/2404.17179 ,Meta-Objects: Interactive and Multisensory Virtual Objects Learned from the Real World for Use in Augmented Reality,"Dooyoung Kim, Taewook Ha, Jinseok Hong, Seonji Kim, Selin Choi, Heejeong Ko, Woontack Woo","실제 세계의 형태, 속성, 기능을 상속받아 물리적 세계와 가상 세계 간의 원활한 동기화, 상호 작용 및 공유를 가능하게 하는 차세대 가상 개체인 메타 개체의 개념을 소개합니다. 오늘날의 많은 가상 개체는 일부 감각 피드백과 동적 동작을 제공하지만 메타 개체는 구조화된 데이터 프레임워크 내에서 대화형 및 다감각 기능을 완전히 통합하여 포스트 메타버스 지능형 시뮬레이션 플랫폼에서 실시간 몰입형 경험을 가능하게 합니다. 포스트 메타버스에서 메타 객체의 활용을 뒷받침하는 세 가지 주요 구성 요소는 물리적 및 동작 현실성을 위한 속성 내장 모델링, 사용자 상호 작용에 맞춘 적응형 다감각 피드백, 확장 가능하고 효율적인 생태계 통합을 위한 장면 그래프 기반 인텔리전스 시뮬레이션 플랫폼입니다. 웨어러블 AR/VR 장치를 통해 메타 객체를 활용함으로써 포스트 메타버스는 공간적, 시간적 장벽을 초월하는 원활한 상호 작용을 촉진하여 혁신적인 현실-가상 융합을 위한 길을 열어줍니다."
582,http://arxiv.org/abs/2404.14565 ,"""Where am I?"" Scene Retrieval with Language","Jiaqi Chen, Daniel Barath, Iro Armeni, Marc Pollefeys, Hermann Blum","구현된 AI에 대한 자연어 인터페이스는 우리 일상 생활에서 더욱 보편화되고 있습니다. 이는 사용자가 에이전트에게 특정 위치에서 일부 작업을 실행하도록 구두로 지시하는 것과 같이 구현된 에이전트와 언어 기반 상호 작용을 위한 추가 기회를 열어줍니다. 예를 들어, ""그릇을 냉장고 옆 찬장에 다시 넣으세요."" 또는 ""빨간색 표지판 아래 교차로에서 만나요."" 따라서 자연어와 환경의 지도 표현을 연결하는 방법이 필요합니다. 이를 위해 우리는 3D 장면 그래프로 표현되는 장면을 식별하기 위해 개방형 자연어 쿼리를 사용할 수 있는지에 대한 질문을 탐색합니다. 우리는 이 작업을 ""언어 기반 장면 검색""으로 정의하고 ""대략적인 위치 파악""과 밀접하게 관련되어 있지만 대신 대규모 연속 맵이 아닌 분리된 장면 모음에서 일치하는 항목을 검색합니다. 텍스트 설명과 장면 그래프 간의 결합 임베딩을 학습하여 일치하는지 확인하는 ""장면 검색"" 파이프라인인 Text2SceneGraphMatcher를 소개합니다. 코드, 훈련된 모델 및 데이터세트는 공개됩니다."
581,http://arxiv.org/abs/2404.14285 ,LLM-Personalize: Aligning LLM Planners with Human Preferences via Reinforced Self-Training for Housekeeping Robots,"Dongge Han, Trevor McInroe, Adam Jelley, Stefano V. Albrecht, Peter Bell, Amos Storkey","LLM(대형 언어 모델)은 언어 이해 및 텍스트 생성 기능을 활용하여 로봇 공학 애플리케이션, 특히 작업 계획에 대한 상당한 잠재력을 보여주었습니다. 그러나 가정용 로봇 공학과 같은 응용 분야에서는 이러한 모델을 개별 사용자 선호도에 맞게 개인화하는 데 심각한 격차가 남아 있습니다. 가정용 로봇공학을 위한 LLM 플래너를 개인화하도록 설계된 최적화 파이프라인을 갖춘 새로운 프레임워크인 LLM-Personalize를 소개합니다. 당사의 LLM-개인화 프레임워크에는 로컬 관찰로 구성된 장면 그래프를 활용하여 다중 방, 부분적으로 관찰 가능한 가정 시나리오에서 반복 계획을 수행하는 LLM 플래너가 포함되어 있습니다. 생성된 계획은 이후에 컨트롤러에 의해 실행되는 일련의 상위 수준 작업으로 구성됩니다. 우리 접근 방식의 핵심은 모방 학습과 반복적인 자체 훈련을 결합하여 LLM 플래너를 개인화하는 최적화 파이프라인입니다. 특히, 모방 학습 단계에서는 시연을 통해 초기 LLM 정렬을 수행하고 모델을 부트스트랩하여 효과적인 반복 자체 훈련을 촉진합니다. 이를 통해 모델을 사용자 선호도에 따라 추가로 탐색하고 정렬합니다. 우리는 가정 재배치를 위한 도전적인 시뮬레이션 실제 3D 벤치마크인 Housekeep에서 LLM-Personalize를 평가하고 LLM-Personalize가 기존 LLM 플래너에 비해 성공률이 30% 이상 증가하여 인간 선호도에 대한 정렬이 크게 개선되었음을 보여줍니다. 프로젝트 페이지: https://gdg94.github.io/projectllmpersonalize/."
580,http://arxiv.org/abs/2404.13696 ,Clio: Real-time Task-Driven Open-Set 3D Scene Graphs,"Dominic Maggio, Yun Chang, Nathan Hughes, Matthew Trang, Dan Griffith, Carlyn Dougherty, Eric Cristofalo, Lukas Schmid, Luca Carlone",클래스에 구애받지 않는 이미지 분할(예: SegmentAnything) 및 개방형 의미 이해(예: CLIP)를 위한 최신 도구는 로봇 인식 및 매핑을 위한 전례 없는 기회를 제공합니다. 기존의 폐쇄 집합 메트릭 의미 맵은 수십 또는 수백 개의 의미 클래스로 제한되었지만 이제는 과다한 객체와 셀 수 없이 많은 의미 변형이 포함된 맵을 구축할 수 있습니다. 이는 우리에게 근본적인 질문을 남깁니다. 로봇이 지도 표현에 포함해야 하는 객체(그리고 보다 일반적으로 의미론적 개념)에 대한 올바른 세분성은 무엇입니까? 관련 작업에서는 개체 감지에 대한 임계값을 조정하여 세분성 수준을 암묵적으로 선택하지만 이러한 선택은 본질적으로 작업에 따라 다르다고 주장합니다. 이 논문의 첫 번째 기여는 작업 중심의 3D 장면 이해 문제를 제안하는 것입니다. 여기서 로봇은 자연어로 작업 목록을 제공받고 작업을 완료하는 데 충분한 맵에 유지하기 위해 개체 및 장면 구조의 세분성과 하위 집합을 선택해야 합니다. 우리는 이 문제가 확립된 정보 이론 프레임워크인 정보 병목 현상(IB)을 사용하여 자연스럽게 공식화될 수 있음을 보여줍니다. 두 번째 기여는 환경의 3D 프리미티브를 작업 관련 개체 및 영역으로 클러스터링하고 점진적으로 실행할 수 있는 Agglomerative IB 접근 방식을 기반으로 하는 작업 중심 3D 장면 이해를 위한 알고리즘입니다. 세 번째 기여는 작업 중심 클러스터링 알고리즘을 Clio라는 실시간 파이프라인에 통합하는 것입니다. Clio는 로봇이 탐색할 때 온보드 컴퓨팅만 사용하여 온라인 환경의 계층적 3D 장면 그래프를 구성합니다. 우리의 마지막 기여는 Clio가 컴팩트한 오픈 세트 3D 장면 그래프의 실시간 구성을 허용할 뿐만 아니라 지도를 관련 의미론적 개념으로 제한하여 작업 실행의 정확성을 향상시키는 것을 보여주는 광범위한 실험 캠페인입니다.
579,http://arxiv.org/abs/2404.11267 ,Towards Human Awareness in Robot Task Planning with Large Language Models,"Yuchen Liu, Luigi Palmieri, Sebastian Koch, Ilche Georgievski, Marco Aiello","최근 LLM(대형 언어 모델) 연구의 획기적인 발전으로 인해 여러 연구 영역에 걸쳐 변화가 촉발되었습니다. 특히, LLM의 통합으로 로봇 작업 및 동작 계획(TAMP)의 성능이 크게 향상되었습니다. 그러나 이전 접근 방식은 동적 환경, 즉 인간과 같은 동적 개체의 존재에 대한 고려를 종종 무시합니다. 본 논문에서는 인간의 인식을 LLM 기반 로봇 작업 계획에 통합하여 이러한 격차를 해소하기 위한 새로운 접근 방식을 제안합니다. 동적 환경을 효과적으로 표현하기 위해 우리의 접근 방식은 인간의 정보를 계층적 장면 그래프에 통합합니다. 계획의 실행 가능성을 보장하기 위해 우리는 LLM을 활용하여 환경 토폴로지와 실행 가능한 지식을 공식 계획 언어로 기반화합니다. 가장 중요한 것은 LLM을 사용하여 미래의 인간 활동을 예측하고 예측을 고려하여 로봇의 작업을 계획한다는 것입니다. 우리의 기여는 인간 인식을 LLM 기반 로봇 작업 계획에 통합하는 개발을 촉진하고 동적 환경에서 사전 로봇 의사 결정을 위한 길을 열어줍니다."
578,http://arxiv.org/abs/2404.09616 ,A Review and Efficient Implementation of Scene Graph Generation Metrics,"Julian Lorenz, Robin Schön, Katja Ludwig, Rainer Lienhart",장면 그래프 생성은 컴퓨터 비전의 주요 연구 분야로 떠오르며 최근 몇 년간 상당한 발전을 이루었습니다. 그러나 이러한 발전에도 불구하고 장면 그래프 생성 모델을 평가하는 데 사용되는 측정 항목에 대한 정확하고 철저한 정의가 부족합니다. 본 논문에서는 장면 그래프 생성에서 일반적으로 사용되는 측정항목에 대한 검토 및 정확한 정의를 제공하여 문헌의 이러한 격차를 해결합니다. 우리의 포괄적인 조사는 이러한 측정항목의 기본 원칙을 명확히 하며 장면 그래프 측정항목에 대한 참조 또는 소개 역할을 할 수 있습니다.   또한 이러한 측정항목의 사용을 촉진하기 위해 정의된 모든 측정항목을 효율적으로 구현하여 연구 커뮤니티에 대한 접근성을 보장하는 SGBench라는 독립형 Python 패키지를 도입합니다. 또한 연구자들이 장면 그래프 생성 방식을 한 곳에서 비교하고 새로운 방식에 대한 가시성을 높일 수 있는 장면 그래프 벤치마킹 웹 서비스를 제시합니다.   모든 코드는 https://lorjul.github.io/sgbench/에서 찾을 수 있습니다.
577,http://arxiv.org/abs/2404.09231 ,Tri-modal Confluence with Temporal Dynamics for Scene Graph Generation in Operating Rooms,"Diandian Guo, Manxi Lin, Jialun Pei, He Tang, Yueming Jin, Pheng-Ann Heng","수술 장면에 대한 포괄적인 이해를 통해 수술 과정을 모니터링하고 사고 발생을 줄이고 의료 전문가의 효율성을 높일 수 있습니다. SGG(장면 그래프 생성) 작업인 수술실 내 의미론적 모델링은 장기간에 걸쳐 미묘한 수술 동작을 연속적으로 인식해야 하기 때문에 어렵습니다. 이 문제를 해결하기 위해 우리는 TriTemp-OR이라는 시간 역학 프레임워크와의 삼중 모드(예: 이미지, 포인트 클라우드 및 언어) 합류를 제안합니다. 메모리 그래프를 통해 시간 정보를 통합하는 이전 접근 방식과 달리, 우리의 방법은 두 가지 장점을 수용합니다. 1) 계층적 기능 상호 작용을 위해 비디오 스트리밍에서 바이모달 시간 정보를 직접 활용하고, 2) 수술실의 클래스 불균형 문제를 완화하기 위해 LLM(Large Language Model)의 사전 지식이 내장되어 있습니다. 특히, 우리 모델은 축척 적응형 다중 뷰 시간적 상호 작용(ViewTemp) 및 기하학적-시간적 점 집계(PointTemp)를 포함하여 2D 프레임과 3D 포인트 클라우드 전반에 걸쳐 시간적 상호 작용을 수행합니다. 또한, 우리는 수술 중 관계에 대한 이해를 심화하기 위해 생의학 LLM인 LLaVA-Med의 지식을 이전합니다. 제안된 TriTemp-OR은 관계 인식 통합을 통해 삼중 모드 특징의 집합을 통해 관계를 예측하여 장면 그래프를 생성할 수 있습니다. 4D-OR 벤치마크의 실험 결과는 장기 OR 스트리밍에 대한 우리 모델의 탁월한 성능을 보여줍니다."
576,http://arxiv.org/abs/2404.08827 ,"""Don't forget to put the milk back!"" Dataset for Enabling Embodied Agents to Detect Anomalous Situations","James F. Mullen, Prasoon Goyal, Robinson Piramuthu, Michael Johnston, Dinesh Manocha, Reza Ghanadan","홈 로봇은 사용자의 삶을 더욱 편리하게 만들려고 합니다. 우리의 작업은 로봇이 사용자에게 집에서 위험하거나 비위생적인 이상 현상을 알릴 수 있도록 함으로써 이 목표를 지원합니다. 이러한 이상 현상의 예로는 사용자가 우유를 빼내거나, 스토브를 끄는 것을 잊어버리거나, 어린이가 독약에 접근할 수 있도록 남겨두는 등이 있습니다. 이러한 기능을 갖춘 가정용 로봇을 활성화하기 위해 우리는 SafetyDetect라고 하는 새로운 데이터 세트를 만들었습니다. SafetyDetect 데이터 세트는 1000개의 변칙적인 가정 장면으로 구성되며 각 장면에는 에이전트가 감지할 수 있는 안전하지 않거나 비위생적인 상황이 포함되어 있습니다. 우리의 접근 방식은 장면의 그래프 표현과 장면 내 개체 간의 관계와 함께 LLM(대형 언어 모델)을 활용합니다. 우리의 핵심 통찰력은 연결된 장면 그래프와 인코딩된 개체 관계를 통해 LLM이 장면에 대해 더 나은 추론을 할 수 있다는 것입니다. 특히 위험하거나 비위생적인 상황을 감지하는 것과 관련이 있을 때 더욱 그렇습니다. 우리의 가장 유망한 접근 방식은 GPT-4를 활용하고 장면 그래프의 객체 관계를 정상, 위험, 비위생적, 어린이에게 위험한 것으로 분류하는 분류 기술을 추구합니다. 이 방법은 SafetyDetect 데이터 세트에서 비정상적인 시나리오의 90% 이상을 정확하게 식별할 수 있습니다. 또한 우리는 실제 장면의 시각적 자료로부터 장면 그래프를 생성하고 수정 없이 접근 방식을 실행하는 ClearPath TurtleBot에서 실제 실험을 수행합니다. 이 설정으로 인해 성능 손실이 거의 발생하지 않았습니다. SafetyDetect 데이터세트와 코드는 이 논문이 출판되면 대중에게 공개될 것입니다."
575,http://arxiv.org/abs/2404.07788 ,AUG: A New Dataset and An Efficient Model for Aerial Image Urban Scene Graph Generation,"Yansheng Li, Kun Li, Yongjun Zhang, Linlin Wang, Dingwen Zhang","SGG(장면 그래프 생성)는 주어진 이미지로부터 시각적 개체와 의미 관계를 이해하는 것을 목표로 합니다. 지금까지 눈높이 뷰를 포함하는 SGG 데이터세트는 많이 출시되었지만 오버헤드 뷰를 포함하는 SGG 데이터세트는 거의 연구되지 않았습니다. SGG를 방해하는 눈높이 뷰의 객체 폐색 문제와 달리 오버헤드 뷰는 지상 장면에서 객체의 공간적 관계에 대한 명확한 인식을 제공하여 SGG를 촉진하는 데 도움이 되는 새로운 관점을 제공합니다. 본 논문에서는 조감도 데이터셋의 공백을 메우기 위해 항공영상 도시 장면 그래프 생성(AUG) 데이터셋을 구축하여 공개한다. AUG 데이터 세트의 이미지는 낮은 자세의 오버헤드 뷰로 캡처됩니다. AUG 데이터세트에는 25,594개의 객체, 16,970개의 관계, 27,175개의 속성이 수동으로 주석 처리되어 있습니다. 복잡한 공중 도시 현장에서 지역적 맥락이 압도되는 것을 피하기 위해 본 논문에서는 새로운 지역성 보존 그래프 컨벌루션 네트워크(LPG)를 제안합니다. SGG의 전역 컨텍스트를 캡처하는 자연스러운 이점을 갖는 기존 그래프 컨볼루션 네트워크와 달리 LPG의 컨볼루션 계층은 객체의 비파괴 초기 특징을 동적으로 업데이트되는 이웃 정보와 통합하여 전역 컨텍스트 마이닝을 전제로 로컬 컨텍스트를 보존합니다. 엄청나게 많은 수의 잠재적 객체 관계 쌍이 존재하지만 그 중 일부만이 AUG에서 의미가 있는 문제를 해결하기 위해 우리는 무의미한 관계 쌍을 지능적으로 정리하기 위한 잠재적 관계 탐지를 위한 적응형 경계 상자 배율 인수(ABS-PRD)를 제안합니다. AUG 데이터 세트에 대한 광범위한 실험을 통해 우리의 LPG가 최첨단 방법과 제안된 지역성 보존 전략의 효율성을 크게 능가할 수 있음을 보여줍니다."
574,http://arxiv.org/abs/2404.07031 ,ORacle: Large Vision-Language Models for Knowledge-Guided Holistic OR Domain Modeling,"Ege Özsoy, Chantal Pellegrini, Matthias Keicher, Nassir Navab","매일 전 세계적으로 수많은 수술이 수행되고 있으며 각 수술실(OR)은 설정뿐만 아니라 사용되는 인력, 도구 및 장비도 다양합니다. 이러한 고유한 다양성은 모델이 초기 훈련 데이터 세트 이상으로 일반화해야 하므로 OR에 대한 전체적인 이해를 달성하는 데 상당한 어려움을 야기합니다. 이러한 격차를 줄이기 위해 우리는 다중 뷰 및 시간 기능을 통합하고 추론 중에 외부 지식을 활용하여 이전에 볼 수 없었던 수술 시나리오에 적응할 수 있는 전체적 OR 도메인 모델링을 위해 설계된 고급 비전 언어 모델인 ORacle을 소개합니다. 이 기능은 교육 데이터 세트를 크게 다양화하여 제공된 지식을 효과적으로 적용하는 Oracle의 숙련도를 보장하는 새로운 데이터 확대 프레임워크를 통해 더욱 향상됩니다. 엄격한 테스트, 장면 그래프 생성 및 4D-OR 데이터 세트의 다운스트림 작업에서 ORacle은 최첨단 성능을 보여줄 뿐만 아니라 기존 모델보다 적은 데이터를 필요로 합니다. 또한, 보이지 않는 풍경과 행동, 도구와 장비의 모습을 해석하는 능력을 통해 적응력을 발휘합니다. 이는 OR 도메인 모델링의 확장성과 경제성을 크게 향상시키고 수술 데이터 과학의 미래 발전을 위한 길을 열어주는 Oracle의 잠재력을 보여줍니다. 승인되면 코드와 데이터를 공개합니다."
573,http://arxiv.org/abs/2404.04565 ,SportsHHI: A Dataset for Human-Human Interaction Detection in Sports Videos,"Tao Wu, Runyu He, Gangshan Wu, Limin Wang","비디오 장면 그래프 생성과 같은 비디오 기반 시각적 관계 감지 작업은 세밀한 비디오 이해에 중요한 역할을 합니다. 그러나 현재의 비디오 시각적 관계 탐지 데이터 세트에는 이 분야의 연구 진행을 방해하는 두 가지 주요 제한 사항이 있습니다. 첫째, 다중 사용자 시나리오에서 복잡한 인간 간 상호 작용을 탐구하지 않습니다. 둘째, 기존 데이터 세트의 관계 유형은 상대적으로 낮은 수준의 의미를 가지며, 자세한 시공간적 맥락 추론 없이도 외관이나 단순한 사전 정보로 인식할 수 있는 경우가 많습니다. 그럼에도 불구하고 인간 사이의 높은 수준의 상호 작용을 이해하는 것은 스포츠 및 감시 영상과 같은 복잡한 다중 사용자 영상을 이해하는 데 중요합니다. 이 문제를 해결하기 위해 우리는 새로운 비디오 시각적 관계 탐지 작업인 비디오 인간-인간 상호 작용 탐지를 제안하고 이를 위해 SportsHHI라는 데이터 세트를 구축합니다. SportsHHI에는 농구, 배구 스포츠의 34개 고급 상호작용 수업이 포함되어 있습니다. 118,075개의 인간 경계 상자와 50,649개의 상호 작용 인스턴스가 11,398개의 키프레임에 주석으로 추가되었습니다. 이를 벤치마킹하기 위해 우리는 2단계 기준 방법을 제안하고 성공적인 인간-인간 상호 작용 탐지기의 핵심 요소를 밝히기 위해 광범위한 실험을 수행합니다. SportsHHI가 영상 속 인간 상호작용 이해에 대한 연구를 활성화하고, 영상 시각적 관계 탐지에서 시공간적 맥락 모델링 기술 개발을 촉진할 수 있기를 바랍니다."
572,http://arxiv.org/abs/2404.03275 ,DELTA: Decomposed Efficient Long-Term Robot Task Planning using Large Language Models,"Yuchen Liu, Luigi Palmieri, Sebastian Koch, Ilche Georgievski, Marco Aiello",최근 LLM(대형 언어 모델)의 발전으로 인해 여러 연구 분야에 혁명이 일어났습니다. 로봇공학에서는 LLM의 상식적 지식을 작업 및 동작 계획에 통합함으로써 전례 없는 수준의 상황 인식을 실현함으로써 해당 분야를 획기적으로 발전시켰습니다. 방대한 지식 수집에도 불구하고 대규모 언어 모델은 환각이나 도메인 정보 누락으로 인해 실행 불가능한 계획을 생성할 수 있습니다. 이러한 과제를 해결하고 계획 타당성 및 계산 효율성을 향상시키기 위해 새로운 LLM 기반 작업 계획 접근 방식인 DELTA를 소개합니다. LLM 내의 환경 표현으로 장면 그래프를 사용함으로써 DELTA는 정확한 계획 문제 설명을 신속하게 생성합니다. 계획 성과를 향상시키기 위해 DELTA는 LLM을 사용하여 장기 작업 목표를 하위 목표의 자동 회귀 시퀀스로 분해하여 자동화된 작업 계획자가 복잡한 문제를 효율적으로 해결할 수 있도록 합니다. 광범위한 평가를 통해 우리는 DELTA가 효율적이고 완전 자동화된 작업 계획 파이프라인을 지원하여 최신 기술에 비해 더 높은 계획 성공률과 훨씬 더 짧은 계획 시간을 달성한다는 것을 보여줍니다. 프로젝트 웹페이지: https://delta-llm.github.io/
571,http://arxiv.org/abs/2404.02838 ,I-Design: Personalized LLM Interior Designer,"Ata Çelen, Guo Han, Konrad Schindler, Luc Van Gool, Iro Armeni, Anton Obukhov, Xi Wang","인테리어 디자인을 통해 우리는 우리 자신이 되고 원하는 방식으로 생활할 수 있습니다. 각 디자인은 우리의 독특한 개성만큼 독특합니다. 그러나 이를 표현하고 구체화하려면 기능적, 시각적 기대치를 물리적 공간의 제약과 일치시켜야 하기 때문에 비전문가가 이를 표현하고 구체화하는 것은 결코 쉬운 일이 아닙니다. 이것은 인테리어 디자인을 럭셔리하게 만듭니다. 보다 쉽게 ​​접근할 수 있도록 자연어 소통을 통해 사용자가 디자인 목표를 생성하고 시각화할 수 있는 맞춤형 인테리어 디자이너 아이디자인(I-Design)을 선보입니다. I-Design은 서로 대화와 논리적 추론에 참여하는 대규모 언어 모델 에이전트 팀으로 시작하여 텍스트 사용자 입력을 상대적 개체 관계가 있는 실행 가능한 장면 그래프 디자인으로 변환합니다. 그 후, 효과적인 배치 알고리즘이 장면 내 각 객체의 최적 위치를 결정합니다. 그런 다음 기존 개체 데이터베이스에서 자산을 검색하고 통합하여 최종 디자인을 3D로 구성합니다. 또한 비전 언어 모델을 활용하고 설계 파이프라인을 보완하는 새로운 평가 프로토콜을 제안합니다. 광범위한 정량적 및 정성적 실험을 통해 I-Design은 고품질 3D 디자인 솔루션을 제공하고 사용자 입력과 일치하는 추상 개념을 정렬하는 데 있어 기존 방법보다 뛰어난 성능을 보여 상세한 3D 배열 및 개념적 충실도 전반에 걸쳐 I-Design의 장점을 보여줍니다."
570,http://arxiv.org/abs/2404.02527 ,Weakly-Supervised 3D Scene Graph Generation via Visual-Linguistic Assisted Pseudo-labeling,"Xu Wang, Yifan Li, Qiudan Zhang, Wenhui Wu, Mark Junjie Li, Jianmin Jinag","3D 장면 그래프를 구축하는 방법을 배우는 것은 체계적이고 풍부한 방식으로 실제 세계를 인식하는 데 필수적입니다. 그러나 기존의 3D 장면 그래프 생성 방법은 완전 지도 학습 방식을 활용하고 개체 및 관계에 대한 대량의 개체 수준 주석 데이터가 필요하므로 이를 얻는 데 리소스가 많이 소모되고 지루합니다. 이 문제를 해결하기 위해 우리는 시각적 언어 보조 의사 라벨링을 통한 약한 지도 3D 장면 그래프 생성 방법인 3D-VLAP를 제안합니다. 특히, 3D-VLAP은 현재 대규모 시각 언어 모델의 우수한 기능을 활용하여 텍스트와 2D 이미지 간의 의미 체계는 물론 2D 이미지와 3D 포인트 클라우드 간의 자연적으로 존재하는 대응성을 정렬함으로써 암묵적으로 텍스트와 3D ​​포인트 클라우드 간의 대응성을 구성합니다. 먼저, 카메라 고유 및 외부 매개변수를 통해 3D 포인트 클라우드에서 2D 이미지로의 위치 대응을 설정하여 3D 포인트 클라우드와 2D 이미지의 정렬을 달성합니다. 그 후, 대규모 교차 모달 시각 언어 모델을 사용하여 2D 이미지를 개체 범주 레이블과 일치시켜 3D 인스턴스를 개체의 텍스트 범주 레이블과 간접적으로 정렬합니다. 그런 다음 시각적 언어 모델에 의해 인코딩된 개체 및 관계의 시각적 임베딩과 텍스트 범주 임베딩 간의 유사성을 계산하여 3D-VLAP 모델 교육을 위해 개체 및 관계에 대한 의사 레이블이 생성됩니다. 궁극적으로 우리는 3D 포인트 클라우드 장면의 장면 그래프를 생성하기 위해 Edge Self-Attention 기반 그래프 신경망을 설계합니다. 광범위한 실험을 통해 우리의 3D-VLAP이 현재의 고급 완전 감독 방법과 비교 가능한 결과를 달성하는 동시에 데이터 주석의 부담을 크게 완화한다는 것을 보여줍니다."
569,http://arxiv.org/abs/2404.02072 ,EGTR: Extracting Graph from Transformer for Scene Graph Generation,"Jinbae Im, JeongYeon Nam, Nokyung Park, Hyungmin Lee, Seunghyun Park","SGG(장면 그래프 생성)는 객체를 감지하고 객체 간의 관계를 예측하는 어려운 작업입니다. DETR이 개발된 이후 1단계 물체 탐지기를 기반으로 한 1단계 SGG 모델이 활발히 연구되었습니다. 그러나 객체 간의 관계를 예측하기 위해 복잡한 모델링이 사용되었으며 객체 탐지기의 Multi-Head Self-Attention에서 학습된 객체 쿼리 간의 고유한 관계는 무시되었습니다. 우리는 DETR 디코더의 다중 헤드 self-attention 레이어에서 학습된 다양한 관계로부터 관계 그래프를 추출하는 경량 1단계 SGG 모델을 제안합니다. Self-Attention 부산물을 충분히 활용함으로써 얕은 관계 추출 헤드를 사용하여 관계 그래프를 효과적으로 추출할 수 있습니다. 객체 탐지 ​​작업에 대한 관계 추출 작업의 종속성을 고려하여, 우리는 탐지된 객체의 품질에 따라 관계 레이블을 적응적으로 조정하는 새로운 관계 평활화 기술을 제안합니다. 관계 평활화를 통해 학습 초기에는 객체 검출 작업에 초점을 맞춘 지속적인 커리큘럼에 따라 모델을 학습하고 객체 검출 성능이 점차 향상되면서 다중 작업 학습을 수행합니다. 또한, 관계 추출의 보조 작업으로 개체 쌍 사이에 관계가 존재하는지 예측하는 연결성 예측 작업을 제안합니다. 우리는 Visual Genome 및 Open Image V6 데이터 세트에 대한 방법의 효율성과 효율성을 입증합니다. 우리의 코드는 https://github.com/naver-ai/egtr에서 공개적으로 제공됩니다."
568,http://arxiv.org/abs/2404.01887 ,3D scene generation from scene graphs and self-attention,"Pietro Bonazzi, Mengqi Wang, Diego Martin Arroyo, Fabian Manhardt, Nico Messikomer, Federico Tombari, Davide Scaramuzza","현실적이고 다양한 실내 3D 장면 레이아웃을 제어 가능한 방식으로 합성하면 시뮬레이션된 내비게이션 및 가상 현실 애플리케이션이 열립니다. 장면을 간결하고 강력하게 표현하는 장면 그래프는 생성된 레이아웃의 의미론적 제어에 매우 적합한 것으로 입증되었습니다. 우리는 장면 그래프와 평면도에서 3D 장면을 합성하기 위해 조건부 변형 자동 인코더(cVAE) 모델의 변형을 제시합니다. 우리는 self-attention 레이어의 속성을 활용하여 장면에 있는 개체 간의 높은 수준의 관계를 캡처하고 이를 모델의 구성 요소로 사용합니다. 우리 모델은 그래프 변환기를 활용하여 주어진 장면 그래프의 관계를 만족시키면서 방에 있는 객체의 크기, 치수 및 방향을 추정합니다. 우리의 실험에서는 self-attention 레이어가 더 희박하고(Graphto3D에 비해 7.9배) 장면이 더 다양하다는 것을 보여줍니다(16%)."
567,http://arxiv.org/abs/2404.00906 ,From Pixels to Graphs: Open-Vocabulary Scene Graph Generation with Vision-Language Models,"Rongjie Li, Songyang Zhang, Dahua Lin, Kai Chen, Xuming He",SGG(장면 그래프 생성)는 시각적 장면을 다운스트림 추론 작업을 위한 중간 그래프 표현으로 구문 분석하는 것을 목표로 합니다. 최근의 발전에도 불구하고 기존 방법은 새로운 시각적 관계 개념을 사용하여 장면 그래프를 생성하는 데 어려움을 겪고 있습니다. 이 문제를 해결하기 위해 시퀀스 생성을 기반으로 하는 새로운 개방형 어휘 SGG 프레임워크를 도입합니다. 우리 프레임워크는 이미지-그래프 생성 패러다임을 통합하여 비전 언어 사전 훈련 모델(VLM)을 활용합니다. 특히 VLM을 사용하여 이미지-텍스트 생성을 통해 장면 그래프 시퀀스를 생성한 다음 이러한 시퀀스에서 장면 그래프를 구성합니다. 이를 통해 우리는 개방형 어휘 SGG를 위한 VLM의 강력한 기능을 활용하고 VL 작업을 향상시키기 위해 명시적 관계형 모델링을 원활하게 통합합니다. 실험 결과는 우리의 디자인이 개방형 어휘로 우수한 성능을 달성할 뿐만 아니라 명시적인 관계 모델링 지식을 통해 다운스트림 비전 언어 작업 성능을 향상한다는 것을 보여줍니다.
566,http://arxiv.org/abs/2404.00469 ,SceneGraphLoc: Cross-Modal Coarse Visual Localization on 3D Scene Graphs,"Yang Miao, Francis Engelmann, Olga Vysotska, Federico Tombari, Marc Pollefeys, Dániel Béla Baráth","우리는 새로운 문제, 즉 3D 장면 그래프 데이터베이스로 표현되는 다중 모드 참조 맵 내에서 입력 이미지의 위치 파악을 소개합니다. 이러한 그래프는 객체 수준 포인트 클라우드, 이미지, 속성 및 객체 간 관계를 포함한 여러 양식으로 구성되어 광범위한 이미지 데이터베이스에 의존하는 기존 방법에 대한 가볍고 효율적인 대안을 제공합니다. 사용 가능한 양식이 주어지면 제안된 방법인 SceneGraphLoc은 장면 그래프의 각 노드(객체 인스턴스를 나타냄)에 대해 고정된 크기의 임베딩을 학습하여 입력 쿼리 이미지에 표시되는 객체와 효과적인 매칭을 가능하게 합니다. 이 전략은 이미지를 지도 임베딩에 통합하지 않고도 다른 교차 모드 방법보다 훨씬 뛰어납니다. 이미지를 활용하면 SceneGraphLoc은 대규모 이미지 데이터베이스에 따라 최첨단 기술에 가까운 성능을 달성하는 동시에 3배 더 적은 스토리지를 필요로 하고 10배 더 빠르게 작동합니다. 코드는 공개될 예정입니다."
565,http://arxiv.org/abs/2404.00343 ,Commonsense Scene Graph-based Target Localization for Object Search,"Wenqi Ge, Chao Tang, Hong Zhang",물체 검색은 가정용 로봇의 기본 기술이지만 핵심 문제는 대상 물체를 정확하게 찾는 로봇의 능력에 있습니다. 사용자가 일상적인 물건을 임의로 배치하는 것을 특징으로 하는 가정 환경의 역동적인 특성으로 인해 대상 위치 파악이 어렵습니다. 대상 물체를 효율적으로 찾으려면 로봇이 물체 수준과 방 수준 모두에 대한 지식을 갖추고 있어야 합니다. 그러나 기존 접근 방식은 한 가지 유형의 지식에만 의존하므로 객체 위치 파악 성능이 만족스럽지 못하고 결과적으로 객체 검색 프로세스가 비효율적입니다. 이 문제를 해결하기 위해 우리는 가정 환경에서 대상 개체 검색을 향상시키기 위해 상식적인 장면 그래프 기반 대상 위치 파악 CSG-TL을 제안합니다. 고정된 항목이 포함된 사전 구축된 맵이 주어지면 로봇은 LLM(대형 언어 모델)에서 생성된 객체 수준 상식 지식을 사용하여 상식 장면 그래프(CSG)로 모델링하여 CSG-TL에 대한 두 가지 유형의 지식을 모두 지원합니다. 대상 위치 파악에 대한 CSG-TL의 우수성을 입증하기 위해 실제 ScanNet 데이터 세트와 AI2THOR 시뮬레이터에서 광범위한 실험이 수행됩니다. 또한 시뮬레이션 환경과 실제 환경 모두에서 검증된 객체 검색 프레임워크인 CSG-OS로 CSG-TL을 확장했습니다. 코드와 동영상은 https://sites.google.com/view/csg-os에서 확인할 수 있습니다.
564,http://arxiv.org/abs/2404.00168 ,Multi-Level Neural Scene Graphs for Dynamic Urban Environments,"Tobias Fischer, Lorenzo Porzi, Samuel Rota Bulò, Marc Pollefeys, Peter Kontschieder","우리는 다양한 환경 조건에서 여러 차량 캡처를 통해 대규모 동적 영역의 복사장을 추정합니다. 이 영역의 이전 작품은 정적 환경으로 제한되거나, 하나의 짧은 비디오 이상으로 확장되지 않거나, 동적 개체 인스턴스를 별도로 표현하는 데 어려움을 겪습니다. 이를 위해 우리는 역동적인 도시 환경을 위한 새롭고 분해 가능한 복사장 접근 방식을 제시합니다. 우리는 빠르게 움직이는 수백 개의 객체가 포함된 수십 개의 시퀀스에서 수천 개의 이미지로 확장되는 다단계 신경 장면 그래프 표현을 제안합니다. 표현을 효율적으로 훈련하고 렌더링할 수 있도록 빠른 합성 광선 샘플링 및 렌더링 방식을 개발합니다. 도시 주행 시나리오에서 접근 방식을 테스트하기 위해 새롭고 참신한 뷰 합성 벤치마크를 도입했습니다. 우리는 우리의 접근 방식이 확립된 벤치마크와 제안된 벤치마크 모두에서 상당한 차이로 이전 기술을 능가하는 동시에 교육 및 렌더링 속도가 더 빠르다는 것을 보여줍니다."
563,http://arxiv.org/abs/2403.19474 ,SG-PGM: Partial Graph Matching Network with Semantic Geometric Fusion for 3D Scene Graph Alignment and Its Downstream Tasks,"Yaxu Xie, Alain Pagani, Didier Stricker","장면 그래프는 최근 장면에 대한 포괄적인 표현으로 3D 공간 이해에 도입되었습니다. 3D 장면 그래프 간의 정렬은 장면 그래프 지원 포인트 클라우드 등록, 모자이크, 중복 검사 및 로봇 탐색과 같은 많은 다운스트림 작업의 첫 번째 단계입니다. 본 연구에서는 3D 장면 그래프 정렬을 부분 그래프 매칭 문제로 취급하고 이를 그래프 신경망으로 해결하는 것을 제안합니다. 우리는 포인트 클라우드 등록 방법으로 학습된 기하학적 특징을 재사용하고 설계된 특징 융합 모듈을 통해 클러스터링된 포인트 수준 기하학적 특징을 노드 수준 의미론적 특징과 연결합니다. 부분 매칭은 학습 가능한 방법을 사용하여 상위 k 유사 노드 쌍을 선택함으로써 활성화됩니다. 포인트 클라우드 등록과 같은 후속 다운스트림 작업은 일치하는 지역 내에서 사전 훈련된 등록 네트워크를 실행하여 달성됩니다. 우리는 또한 3D 장면 그래프의 노드별 정렬을 사용하여 사전 학습된 포인트 클라우드 등록 방법에서 매칭 후보에 다시 가중치를 부여하는 포인트 매칭 재점수 방법을 제안합니다. 특히 낮은 중첩 사례에서 추정된 잘못된 점 대응을 줄입니다. 실험에 따르면 우리의 방법은 낮은 중첩 및 무작위 변환 시나리오에서 정렬 정확도를 10~20% 향상시키고 여러 다운스트림 작업에서 기존 작업보다 성능이 뛰어난 것으로 나타났습니다."
562,http://arxiv.org/abs/2403.19098 ,GraphAD: Interaction Scene Graph for End-to-end Autonomous Driving,"Yunpeng Zhang, Deheng Qian, Ding Li, Yifeng Pan, Yong Chen, Zhenbao Liang, Zhiyao Zhang, Shurui Zhang, Hongxu Li, Maolei Fu, Yun Ye, Zhujin Liang, Yi Shan, Dalong Du","자가 차량, 도로 에이전트, 지도 요소 간의 복잡한 상호 작용을 모델링하는 것은 안전이 중요한 자율 주행에 중요한 부분이었습니다. 엔드 투 엔드 자율 주행에 대한 이전 연구는 이종 상호 작용을 처리하기 위한 주의 메커니즘에 의존하는데, 이는 기하학적 사전을 포착하지 못하고 계산 집약적이기도 합니다. 본 논문에서는 자율주행차, 도로 에이전트, 지도 요소 간의 상호작용을 모델링하기 위한 통합 방법으로 ISG(Interaction Scene Graph)를 제안합니다. ISG의 표현을 통해 운전 에이전트는 충돌 가능성이 있는 도로 에이전트와 따라야 할 지도 요소를 포함하여 가장 영향력 있는 요소로부터 필수 정보를 집계합니다. 불필요한 상호작용이 대량으로 생략되므로 보다 효율적인 장면 그래프 기반 프레임워크는 필수적인 연결에 집중할 수 있어 더 나은 성능을 얻을 수 있습니다. 우리는 nuScenes 데이터세트에서 엔드투엔드 자율주행을 위해 제안된 방법을 평가합니다. 강력한 기준과 비교했을 때 우리의 방법은 인식, 예측 및 계획을 포함한 전체 스택 운전 작업에서 훨씬 뛰어난 성능을 발휘합니다. 코드는 https://github.com/zhangyp15/GraphAD에서 공개됩니다."
561,http://arxiv.org/abs/2403.17995 ,Semi-Supervised Image Captioning Considering Wasserstein Graph Matching,Yang Yang,"이미지 캡션은 주어진 이미지에 대한 캡션을 자동으로 생성할 수 있으며, 핵심 과제는 시각적 특징에서 자연어 특징으로의 매핑 기능을 배우는 것입니다. 기존 접근 방식은 대부분 감독 방식입니다. 즉, 각 이미지에는 훈련 세트에 해당 문장이 있습니다. 그러나 이미지를 기술하는 데는 항상 막대한 인력이 필요하다는 점을 고려하면 일반적으로 설명되는 이미지(즉, 이미지-텍스트 쌍)의 양은 제한되어 있고 실제 응용 프로그램에서는 설명되지 않은 이미지의 수가 많습니다. 따라서 딜레마는 ""반 감독 이미지 캡션""입니다. 이 문제를 해결하기 위해 우리는 생성된 문장을 감독하기 위해 원본 이미지 입력을 채택하는 Wasserstein Graph Matching(SSIC-WGM)을 고려한 새로운 Semi-Supervised Image Captioning 방법을 제안합니다. 전통적인 단일 모달 반지도 방법과 달리 반지도 교차 모달 학습의 어려움은 이종 양식 간에 중간 수준으로 비교할 수 있는 정보를 구성하는 데 있습니다. 본 논문에서 SSIC-WGM은 성공적인 장면 그래프를 중간 정보로 채택하고 생성된 문장을 다음 두 가지 측면에서 제한합니다. 1) 모드 간 일관성. SSIC-WGM은 원시 이미지와 생성된 문장의 장면 그래프를 각각 구성한 다음 Wasserstein 거리를 사용하여 다양한 그래프의 영역 임베딩 간의 유사성을 더 잘 측정합니다. 2) 모달 내 일관성. SSIC-WGM은 원시 이미지에 대한 데이터 증대 기술을 채택한 다음 증강된 이미지와 생성된 문장 간의 일관성을 제한합니다. 결과적으로 SSIC-WGM은 설명되지 않은 이미지를 효율적으로 사용하기 위해 교차 모달 의사 감독과 구조 불변 측정을 결합하고 보다 합리적인 매핑 기능을 학습합니다."
560,http://arxiv.org/abs/2403.17846 ,Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation,"Abdelrhman Werby, Chenguang Huang, Martin Büchner, Abhinav Valada, Wolfram Burgard","최근의 개방형 어휘 로봇 매핑 방법은 사전 훈련된 시각적 언어 기능을 통해 조밀한 기하학적 지도를 풍부하게 합니다. 이러한 맵을 사용하면 특정 언어 개념에 대해 쿼리할 때 점별 돌출 맵을 예측할 수 있지만, 개체 수준을 넘어서는 대규모 환경과 추상 쿼리는 여전히 상당한 장애물을 제기하여 궁극적으로 언어 기반 로봇 탐색을 제한합니다. 이 연구에서는 언어 기반 로봇 탐색을 위한 계층적 개방형 어휘 3D 장면 그래프 매핑 접근 방식인 HOV-SG를 제시합니다. 개방형 어휘 비전 기반 모델을 활용하여 먼저 3D로 최첨단 개방형 어휘 세그먼트 수준 맵을 얻은 다음 각각 개방형 어휘 기능이 풍부한 바닥, 공간 및 개체 개념으로 구성된 3D 장면 그래프 계층 구조를 구성합니다. 우리의 접근 방식은 다층 건물을 나타낼 수 있으며 층간 보로노이 그래프를 사용하여 건물을 로봇으로 탐색할 수 있습니다. HOV-SG는 세 가지 별개의 데이터 세트로 평가되었으며 객체, 방 및 바닥 수준의 개방형 어휘 의미 정확도에서 이전 기준을 능가하는 동시에 조밀한 개방형 어휘 맵에 비해 표현 크기를 75% 줄였습니다. HOV-SG의 효율성과 일반화 기능을 입증하기 위해 실제 다중 저장 환경에서 성공적인 장거리 언어 조절 로봇 탐색을 선보입니다. 우리는 http://hovsg.github.io/에서 코드 및 평가판 비디오 데이터를 제공합니다."
559,http://arxiv.org/abs/2403.16184 ,Predicate Debiasing in Vision-Language Models Integration for Scene Graph Generation Enhancement,"Yuxuan Wang, Xiaoyuan Liu",SGG(장면 그래프 생성)는 시각적 장면의 기본 언어 표현을 제공하므로 모델이 개체 간의 복잡하고 다양한 의미를 파악해야 합니다. SGG의 이러한 복잡성과 다양성으로 인해 삼중 라벨의 일부가 드물거나 훈련 중에 표시되지 않아 부정확한 예측이 발생하는 과소표현이 발생합니다. 이 문제를 해결하기 위해 사전 학습된 Vision 언어 모델을 통합하여 표현을 향상할 것을 제안합니다. 그러나 사전 훈련과 SGG 사이의 차이로 인해 SGG에서 사전 훈련된 VLM을 직접 추론하면 사전 훈련 언어 집합의 불균형한 술어 분포로 인해 심각한 편향이 발생합니다. 편향을 완화하기 위해 달성할 수 없는 술어 분포를 근사화하는 새로운 LM 추정을 도입합니다. 마지막으로 편향이 제거된 VLM을 SGG 모델과 앙상블하여 표현을 강화합니다. 여기서 각 샘플에 점수를 매기고 앙상블 가중치를 동적으로 조정하는 확실성 인식 지표를 설계합니다. 우리의 훈련 없는 방법은 사전 훈련된 VLM의 술어 편향을 효과적으로 해결하고 SGG의 표현을 향상시키며 성능을 크게 향상시킵니다.
558,http://arxiv.org/abs/2403.14886 ,DSGG: Dense Relation Transformer for an End-to-end Scene Graph Generation,"Zeeshan Hayder, Xuming He","장면 그래프 생성은 불완전한 레이블 지정, 긴 꼬리 관계 범주 및 관계형 의미 중첩으로 인해 어려운 이미지 내 개체 간의 자세한 공간적 및 의미적 관계를 캡처하는 것을 목표로 합니다. 기존 Transformer 기반 방법은 객체와 술어에 대해 고유한 쿼리를 사용하거나 관계 삼중항에 대한 전체적 쿼리를 활용하므로 저주파 관계를 학습하는 데 용량이 제한되는 경우가 많습니다. 본 논문에서는 장면 그래프 감지를 고유한 그래프 인식 쿼리 세트를 기반으로 한 직접 그래프 예측 문제로 보는 DSGG라는 새로운 Transformer 기반 방법을 제시합니다. 특히, 각 그래프 인식 쿼리는 훈련 과정에서 완화된 하위 그래프 매칭을 활용하여 얻은 노드와 그래프의 모든 관계를 간략하게 표현합니다. 또한 관계형 의미 중첩 문제를 해결하기 위해 의미 관계의 여러 인스턴스를 효율적으로 학습하는 것을 목표로 하는 관계 증류 전략을 활용합니다. VG 및 PSG 데이터세트에 대한 광범위한 실험에서는 우리 모델이 최첨단 결과를 달성하여 장면 그래프 생성 작업에 대해 mR@50 및 mR@100에서 3.5% 및 6.7%의 상당한 개선을 보여주고, 팬옵틱 장면 그래프 생성 작업에 대해 mR@50 및 mR@100에서 8.5% 및 10.3%의 훨씬 더 실질적인 개선을 달성하는 것으로 나타났습니다. 코드는 \url{https://github.com/zeeshanhayder/DSGG}에서 확인할 수 있습니다."
557,http://arxiv.org/abs/2403.14270 ,Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship Detection,"Tim Salzmann, Markus Ryll, Alex Bewley, Matthias Minderer","시각적 관계 감지는 이미지에서 객체와 객체의 관계를 식별하는 것을 목표로 합니다. 이전 방법에서는 기존 개체 감지 아키텍처에 별도의 관계 모듈 또는 디코더를 추가하여 이 작업에 접근했습니다. 이러한 분리는 복잡성을 증가시키고 엔드 투 엔드 교육을 방해하여 성능을 제한합니다. 우리는 개방형 어휘 시각적 관계 감지를 위한 간단하고 매우 효율적인 디코더 없는 아키텍처를 제안합니다. 우리 모델은 객체를 토큰으로 표현하고 객체의 관계를 암시적으로 모델링하는 Transformer 기반 이미지 인코더로 구성됩니다. 관계 정보를 추출하기 위해 관계를 형성할 가능성이 있는 개체 쌍을 선택하는 Attention 메커니즘을 도입합니다. 우리는 객체와 관계 감지 데이터의 혼합을 통해 이 모델을 훈련하는 단일 단계 레시피를 제공합니다. 우리의 접근 방식은 Visual Genome과 대규모 어휘 GQA 벤치마크에서 실시간 추론 속도로 최첨단 관계 탐지 성능을 달성합니다. 우리는 절제, 실제 정성적 사례, 제로샷 성능 분석을 제공합니다."
556,http://arxiv.org/abs/2403.12848 ,Planner3D: LLM-enhanced graph prior meets 3D indoor scene explicit regularization,"Yao Wei, Martin Renqiang Min, George Vosselman, Li Erran Li, Michael Ying Yang","구성적 3D 장면 합성은 실제 다중 객체 환경의 복잡성을 밀접하게 반영하므로 로봇 공학, 영화, 비디오 게임 등 다양한 산업 분야에 걸쳐 다양하게 응용됩니다. 기존 작업은 일반적으로 제한된 모양 다양성으로 인해 어려움을 겪는 모양 검색 기반 프레임워크를 사용합니다. 형태 충실도를 높이는 확산 모델과 같은 생성 모델을 사용하여 객체 형태 생성에 최근 진전이 이루어졌습니다. 그러나 이러한 접근 방식은 3D 모양 생성과 레이아웃 생성을 별도로 처리합니다. 합성된 장면은 일반적으로 레이아웃 충돌로 인해 방해를 받습니다. 이는 장면 수준 충실도가 아직 충분히 탐구되지 않았음을 의미합니다. 본 논문에서는 장면 그래프를 통해 현실적이고 합리적인 3차원 실내 장면을 생성하는 것을 목표로 한다. 주어진 장면 그래프 입력의 사전을 강화하기 위해 대규모 언어 모델을 활용하여 전역적 기능을 로컬 노드별 및 에지별 기능과 집계합니다. 통합 그래프 인코더를 사용하면 그래프 특징을 추출하여 조인트 레이아웃 모양 생성을 안내합니다. 생성된 3D 레이아웃을 명시적으로 제한하기 위해 추가 정규화가 도입되었습니다. SG-FRONT 데이터 세트를 벤치마킹한 우리의 방법은 특히 장면 수준 충실도 측면에서 더 나은 3D 장면 합성을 달성합니다. 소스코드는 공개 후 공개될 예정입니다."
555,http://arxiv.org/abs/2403.12488 ,DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM,"Yixuan Wu, Yizhou Wang, Shixiang Tang, Wenhao Wu, Tong He, Wanli Ouyang, Philip Torr, Jian Wu","우리는 GPT-4V 및 Gemini와 같은 다중 모드 대형 언어 모델(MLLM)의 제로샷 객체 감지 기능을 활용하기 위해 새로운 프롬프트 패러다임인 DetToolChain을 제시합니다. 우리의 접근 방식은 고정밀 탐지 사전에 영감을 받은 탐지 프롬프트 툴킷과 이러한 프롬프트를 구현하기 위한 새로운 사고 사슬로 구성됩니다. 특히, 툴킷의 프롬프트는 MLLM이 지역 정보(예: 확대)에 초점을 맞추고 측정 표준에 따라 좌표를 읽고(예: 눈금자 및 나침반 오버레이) 상황 정보에서 추론(예: 장면 그래프 오버레이)하도록 안내하도록 설계되었습니다. 이러한 도구를 기반으로 구축된 새로운 감지 체인은 자동으로 작업을 간단한 하위 작업으로 분해하고, 예측을 진단하고, 점진적인 상자 개선을 계획할 수 있습니다. 우리 프레임워크의 효율성은 다양한 탐지 작업, 특히 어려운 사례에서 입증됩니다. 기존의 최첨단 방법과 비교하여 DetToolChain이 포함된 GPT-4V는 개방형 어휘 감지를 위한 MS COCO Novel 클래스 세트에서 +21.5% AP50, 제로샷 참조 표현 이해를 위한 RefCOCO 값 세트에서 +24.23% Acc, D-큐브 설명 객체 감지 FULL 설정에서 +14.5% AP로 최첨단 객체 감지기를 향상시킵니다."
554,http://arxiv.org/abs/2403.12033 ,HiKER-SGG: Hierarchical Knowledge Enhanced Robust Scene Graph Generation,"Ce Zhang, Simon Stepputtis, Joseph Campbell, Katia Sycara, Yaqi Xie","시각적 장면을 이해하는 능력은 자율 주행, 로봇 공학 및 기타 비전 기반 접근 방식을 포함한 많은 다운스트림 작업의 전조입니다. 시각적 데이터를 추론하는 기능을 가능하게 하는 일반적인 접근 방식은 SGG(장면 그래프 생성)입니다. 그러나 기존의 많은 접근 방식은 시야가 방해받지 않는다고 가정합니다. 즉, 안개, 눈, 연기와 같은 실제 손상은 물론 햇빛이나 물방울과 같은 불균일한 섭동이 없다고 가정합니다. 이 작업에서 우리는 절차적으로 생성된 날씨 손상 및 Visual Genome 데이터 세트에 대한 기타 변환을 포함하는 새로운 SGG 벤치마크를 제안합니다. 또한 HiKER-SGG(Hierarchical Knowledge Enhanced Robust Scene Graph Generation)라는 해당 접근 방식을 도입하여 이러한 까다로운 설정에서 장면 그래프 생성을 위한 강력한 기준을 제공합니다. HiKER-SGG는 기본적으로 계층적 지식 그래프를 활용하여 대략적인 초기 추정에서 상세한 예측으로 예측을 개선합니다. 광범위한 실험에서 우리는 HiKER-SGG가 제로샷 방식으로 손상된 이미지에 대해 우수한 성능을 보여줄 뿐만 아니라 손상되지 않은 SGG 작업에 대한 현재의 최첨단 방법보다 성능이 우수하다는 것을 보여줍니다. 코드는 https://github.com/zhangce01/HiKER-SGG에서 확인할 수 있습니다."
553,http://arxiv.org/abs/2403.10119 ,URS-NeRF: Unordered Rolling Shutter Bundle Adjustment for Neural Radiance Fields,"Bo Xu, Ziao Liu, Mengqi Guo, Jiancheng Li, Gim Hee Lee","우리는 정렬되지 않은 RS(롤링 셔터) 이미지를 활용하여 암시적 3D 표현을 얻는 NeRF(신경 복사 필드)에 대한 새로운 롤링 셔터 번들 조정 방법을 제안합니다. 기존 NeRF 방법은 이미지의 RS 효과로 인해 이미지 품질이 낮고 초기 카메라 자세가 정확하지 않은 반면, RS를 NeRF에 통합하는 이전 방법은 엄격한 순차적 데이터 입력이 필요하므로 광범위한 적용 가능성이 제한됩니다. 지속적으로 우리의 방법은 카메라 자세와 속도를 추정하여 RS 이미지의 물리적 형성을 복구함으로써 순차 데이터에 대한 입력 제약을 제거합니다. 또한 장면 그래프의 쌍별 프레임에 대한 RS 에피폴라 제약 조건을 사용하여 로컬 최소값에 해당하는 카메라 포즈를 감지하는 대략적에서 정밀한 훈련 전략을 채택합니다. 이상치로 검출된 포즈는 이웃 포즈와의 보간법을 통해 보정됩니다. 실험 결과는 최첨단 작업에 대한 우리 방법의 효율성을 검증하고 3D 표현의 재구성이 비디오 시퀀스 입력 요구 사항에 의해 제한되지 않음을 보여줍니다."
552,http://arxiv.org/abs/2403.08605 ,Language-Grounded Dynamic Scene Graphs for Interactive Object Search with Mobile Manipulation,"Daniel Honerkamp, Martin Büchner, Fabien Despinoy, Tim Welschehold, Abhinav Valada","모바일 조작 로봇의 기능을 최대한 활용하려면 탐색되지 않은 대규모 환경에서 장거리 작업을 자율적으로 실행할 수 있어야 합니다. LLM(대형 언어 모델)은 임의 작업에 대한 새로운 추론 기술을 보여주었지만 기존 작업은 주로 탐색된 환경에 중점을 두고 있으며 일반적으로 탐색 또는 조작 작업에 독립적으로 중점을 둡니다. 이 작업에서 우리는 개방형 어휘 장면 그래프에서 파생된 구조화된 표현 내에서 언어 모델을 기반으로 하고 환경을 탐색함에 따라 동적으로 업데이트되는 새로운 접근 방식인 MoMa-LLM을 제안합니다. 우리는 이러한 표현을 객체 중심 행동 공간과 긴밀하게 인터리브합니다. 객체 감지를 고려하면 결과적인 접근 방식은 제로샷, 개방형 어휘이며 다양한 모바일 조작 및 가정용 로봇 작업으로 쉽게 확장 가능합니다. 우리는 대규모의 현실적인 실내 환경에서 새로운 의미론적 대화형 검색 작업에서 MoMa-LLM의 효율성을 입증합니다. 시뮬레이션과 실제 세계의 광범위한 실험에서 기존 기준선 및 최첨단 접근 방식에 비해 검색 효율성이 크게 향상되었으며 보다 추상적인 작업에 대한 적용 가능성도 나타났습니다. 우리는 http://moma-llm.cs.uni-freiburg.de에서 코드를 공개적으로 제공합니다."
551,http://arxiv.org/abs/2403.08094 ,Task and Motion Planning in Hierarchical 3D Scene Graphs,"Aaron Ray, Christopher Bradley, Luca Carlone, Nicholas Roy","3D 장면 그래프 구축에 대한 최근 작업을 통해 모바일 로봇은 세계에 대한 대규모 미터법-의미론적 계층적 표현을 구축할 수 있게 되었습니다. 이러한 세부 모델에는 계획에 유용한 정보가 포함되어 있지만 실행 가능한 계획을 효율적으로 계산할 수 있는 3D 장면 그래프에서 계획 영역을 파생하는 방법은 아직 해결되지 않은 질문입니다. 본 연구에서는 계층적 3D 장면 그래프를 사용하여 대규모 환경에서 작업 및 동작 계획 문제를 정의하고 해결하기 위한 새로운 접근 방식을 제시합니다. 우리는 큰 장면으로 계획을 확장할 수 있는 희소 문제 인스턴스를 구축하는 방법을 설명하고, 장면 그래프의 관련 없는 요소에 대한 계산을 최소화하는 계획 시간 동안 해당 도메인에 객체를 점진적으로 추가하는 기술을 제안합니다. 우리는 KITTI 데이터 세트로 구성된 그래프를 포함하여 인식을 바탕으로 구축된 두 개의 실제 장면 그래프에서 접근 방식을 평가합니다. 또한 우리는 실제 로봇 모바일 매니퓰레이터에서 표현을 구축하고 계획을 세우고 해당 계획을 실행하는 등 현실 세계에서의 접근 방식을 보여줍니다. 추가 동영상은 \url{https://youtu.be/v8fkwLjBn58}에서 확인하실 수 있습니다."
550,http://arxiv.org/abs/2403.07508 ,MoAI: Mixture of All Intelligence for Large Language and Vision Models,"Byung-Kwan Lee, Beomchan Park, Chae Won Kim, Yong Man Ro","대형 언어 모델(LLM) 및 명령어 튜닝의 증가로 인해 명령어 튜닝 대형 언어 및 비전 모델(LLVM)의 현재 추세가 나타났습니다. 이러한 추세에는 특정 목표에 맞는 수많은 명령 튜닝 데이터 세트를 꼼꼼하게 관리하거나 LLVM을 확대하여 방대한 양의 VL(비전 언어) 데이터를 관리하는 것이 포함됩니다. 그러나 현재 LLVM은 분할, 감지, 장면 그래프 생성(SGG) 및 광학 문자 인식(OCR)과 같은 시각적 인식 작업에서 특수 컴퓨터 비전(CV) 모델에서 사용할 수 있는 상세하고 포괄적인 실제 장면 이해를 무시했습니다. 대신 기존 LLVM은 LLM 백본의 대용량 및 긴급 기능에 주로 의존합니다. 따라서 우리는 외부 분할, 감지, SGG 및 OCR 모델의 출력에서 ​​얻은 보조 시각적 정보를 활용하는 새로운 LLVM인 MoAI(Mixture of All Intelligence)를 제시합니다. MoAI는 새로 도입된 두 가지 모듈인 MoAI-Compressor와 MoAI-Mixer를 통해 작동합니다. MoAI-Compressor는 외부 CV 모델의 출력을 언어화한 후 이를 정렬하고 압축하여 VL 작업에 관련 보조 시각 정보를 효율적으로 사용합니다. MoAI-Mixer는 Mixture of Experts 개념을 활용하여 (1) 시각적 기능, (2) 외부 CV 모델의 보조 기능, (3) 언어 기능의 세 가지 유형의 지능을 혼합합니다. 이러한 통합을 통해 MoAI는 수많은 제로샷 VL 작업, 특히 모델 크기를 확대하거나 추가 시각적 명령 조정 데이터 세트를 큐레이팅하지 않고도 객체 존재, 위치, 관계 및 OCR과 같은 실제 장면 이해와 관련된 작업에서 오픈 소스 및 폐쇄 소스 LLVM보다 훨씬 뛰어난 성능을 발휘합니다."
549,http://arxiv.org/abs/2403.07076 ,Mapping High-level Semantic Regions in Indoor Environments without Object Recognition,"Roberto Bigazzi, Lorenzo Baraldi, Shreyas Kousik, Rita Cucchiara, Marco Pavone",로봇이 인간 환경에서 효율적이고 설명 가능한 방식으로 작동하려면 주변 환경에 대한 의미론적 이해가 필요합니다. 문헌에서는 객체 라벨링 및 철저한 장면 그래프 생성에 중점을 두었습니다. 큰 의미 영역을 순수하게 식별하고 매핑하는 작업에 더 적은 노력이 집중되었습니다. 본 연구는 실내 환경에서 구체화된 탐색을 통해 의미 영역 매핑을 위한 방법을 제안하여 에이전트 지식에 대한 높은 수준의 표현을 생성합니다. 지역 식별을 활성화하기 위해 이 방법은 비전-언어 모델을 사용하여 매핑을 위한 장면 정보를 제공합니다. 제안된 방법은 자기 중심적인 장면 이해를 전역 프레임에 투영함으로써 각 위치에서 가능한 지역 레이블에 대한 분포로 의미 지도를 생성합니다. 이 매핑 절차는 훈련된 탐색 정책과 결합되어 자율 지도 생성을 가능하게 합니다. 제안된 방법은 사실적 시뮬레이터에서의 실험에서 객체 기반 시스템과 사전 훈련된 장면 분류기를 포함한 다양한 기준선보다 훨씬 뛰어난 성능을 보였습니다.
548,http://arxiv.org/abs/2403.06514 ,Structure Your Data: Towards Semantic Graph Counterfactuals,"Angeliki Dimitriou, Maria Lymperaiou, Giorgos Filandrianos, Konstantinos Thomas, Giorgos Stamou","개념에 기반한 반사실적 설명(CE)은 어떤 상위 수준의 의미론적 특징이 특정 모델 예측에 기여했는지 이해하기 위해 대체 시나리오를 고려하는 설명입니다. 본 연구에서는 보다 설명적이고 정확하며 인간에 맞춰진 설명을 달성하기 위해 입력 데이터와 함께 제공되는 의미 그래프를 기반으로 CE를 제안합니다. 최첨단(SoTA) 개념적 시도를 기반으로 모델에 구애받지 않는 편집 기반 접근 방식을 채택하고 효율적인 GED(Graph Edit Distance) 계산을 위해 GNN을 활용하는 방법을 소개합니다. 시각적 영역에 초점을 맞춰 이미지를 장면 그래프로 표현하고 GNN 임베딩을 얻어 CE 계산 프로세스의 필수 부분인 모든 입력 쌍에 대한 NP-하드 그래프 유사성 문제를 해결하지 않습니다. 우리는 의미론적 주석의 난이도와 가용성이 다양한 벤치마크 및 실제 데이터세트에 우리 방법을 적용합니다. 다양한 분류기에 대한 테스트를 통해 CE가 화이트 박스와 블랙 박스는 물론 개념적 및 픽셀 수준 접근 방식을 포함한 의미론을 기반으로 하는 이전 SoTA 설명 모델보다 성능이 뛰어난 것으로 나타났습니다. 이들의 우월성은 인간 피험자에 의해 검증된 바와 같이 양적 및 질적으로 입증되었으며, 복잡한 관계가 있는 경우 의미론적 가장자리를 활용하는 것의 중요성을 강조합니다. 모델에 구애받지 않는 그래프 기반 접근 방식은 널리 적용 가능하고 쉽게 확장 가능하여 다양한 상황에서 실행 가능한 설명을 생성합니다."
547,http://arxiv.org/abs/2403.05687 ,Scene Graph Aided Radiology Report Generation,"Jun Wang, Lixing Zhu, Abhir Bhalerao, Yulan He",방사선학 보고서 생성(RRG) 방법은 임상적으로 정확한 보고서를 생성하기에 충분한 의학 지식이 부족한 경우가 많습니다. 장면 그래프에는 이미지의 개체를 설명하는 풍부한 정보가 포함되어 있습니다. 우리는 현재 RRG 문헌에서 수행되지 않은 장면 그래프를 통해 RRG에 대한 의학 지식을 풍부하게 탐구합니다. 이를 위해 우리는 영역 수준의 시각적 특징을 생성하고 해부학적 속성을 예측하며 자동으로 생성된 장면 그래프를 활용하여 엔드투엔드 방식으로 의학 지식 증류를 달성하는 프레임워크인 SGRRG(Scene Graph Aided RRG) 네트워크를 제안합니다. SGRRG는 장면 그래프 변환을 담당하는 전용 장면 그래프 인코더와 패치 수준 및 지역 수준의 시각 정보를 모두 활용하는 장면 그래프 지원 디코더로 구성됩니다. 장면 그래프 정보를 더 잘 분류할 수 있도록 세분화된 문장 수준 주의 방법이 설계되었습니다. 광범위한 실험을 통해 SGRRG는 보고서 생성에서 이전의 최첨단 방법보다 성능이 뛰어나고 비정상적인 발견을 더 잘 포착할 수 있음이 입증되었습니다.
546,http://arxiv.org/abs/2403.04899 ,Towards Scene Graph Anticipation,"Rohith Peddi, Saksham Singh,  Saurabh, Parag Singla, Vibhav Gogate",시공간 장면 그래프는 장면을 개별 개체와 쌍별 시간 관계로 분해하여 비디오의 상호 작용을 나타냅니다. 개체 간의 세밀한 쌍별 관계를 장기적으로 예측하는 것은 어려운 문제입니다. 이를 위해 SGA(Scene Graph Anticipation) 작업을 소개합니다. 우리는 최신 장면 그래프 생성 방법을 기준으로 적용하여 객체 간의 미래 쌍별 관계를 예측하고 새로운 접근 방식인 SceneSayer를 제안합니다. SceneSayer에서는 객체 중심 관계 표현을 활용하여 관찰된 비디오 프레임을 추론하고 객체 간 관계의 진화를 모델링합니다. 우리는 연속적인 시간 관점을 취하고 각각 NeuralODE 및 NeuralSDE의 개념을 사용하여 객체 상호 작용 진화의 잠재 역학을 모델링합니다. 우리는 각각 상미분 방정식과 확률적 미분 방정식을 풀어 미래 관계의 표현을 추론합니다. Action Genome 데이터 세트에 대한 광범위한 실험을 통해 제안된 방법의 효율성이 검증되었습니다.
545,http://arxiv.org/abs/2403.01248 ,SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code,"Ziniu Hu, Ahmet Iscen, Aashi Jain, Thomas Kipf, Yisong Yue, David A. Ross, Cordelia Schmid, Alireza Fathi","이 문서에서는 텍스트 설명을 최대 100개의 3D 자산으로 복잡한 장면을 렌더링하는 Blender 실행 가능 Python 스크립트로 변환하는 LLM(대형 언어 모델) 에이전트인 SceneCraft를 소개합니다. 이 과정에는 복잡한 공간 계획과 배치가 필요합니다. 우리는 고급 추상화, 전략 계획 및 도서관 학습을 결합하여 이러한 과제를 해결합니다. SceneCraft는 먼저 장면 그래프를 청사진으로 모델링하여 장면 내 자산 간의 공간 관계를 자세히 설명합니다. 그런 다음 SceneCraft는 이 그래프를 기반으로 Python 스크립트를 작성하여 관계를 자산 레이아웃에 대한 수치 제약 조건으로 변환합니다. 다음으로 SceneCraft는 GPT-V와 같은 비전 언어 기반 모델의 지각적 강점을 활용하여 렌더링된 이미지를 분석하고 장면을 반복적으로 개선합니다. 이 프로세스 외에도 SceneCraft는 공통 스크립트 기능을 재사용 가능한 라이브러리로 컴파일하여 값비싼 LLM 매개변수 조정 없이 지속적인 자체 개선을 촉진하는 라이브러리 학습 메커니즘을 갖추고 있습니다. 우리의 평가는 SceneCraft가 제약 조건 준수와 호의적인 인간 평가를 통해 알 수 있듯이 복잡한 장면을 렌더링하는 데 있어 기존 LLM 기반 에이전트를 능가한다는 것을 보여줍니다. 또한 Sintel 영화의 상세한 3D 장면을 재구성하고 생성된 장면을 중간 제어 신호로 사용하여 비디오 생성 모델을 안내함으로써 SceneCraft의 광범위한 응용 가능성을 보여줍니다."
544,http://arxiv.org/abs/2402.17213 ,VCD: A Dataset for Visual Commonsense Discovery in Images,"Xiangqing Shen, Fanfan Wang, Siwei Wu, Rui Xia","시각적 상식은 시각적 세계를 이해하고 추론하는 데 중요한 역할을 합니다. ConceptNet과 같은 상식적 지식 기반은 일반적인 사실의 구조화된 컬렉션을 제공하지만 시각적 기반 표현이 부족합니다. Visual Genome과 같은 장면 그래프 데이터 세트는 객체 수준 설명이 풍부하지만 주로 직접적으로 관찰 가능한 정보에 초점을 맞추고 상식 지식의 체계적인 분류가 부족합니다. 우리는 이러한 격차를 해소하는 100,000개 이상의 이미지와 1,400만 개 이상의 개체-상식 쌍을 포함하는 대규모 데이터 세트인 VCD(Visual Commonsense Dataset)를 제시합니다. VCD는 시각적 상식에 대한 새로운 3단계 분류법을 도입하여 속성, 동작 및 공간 측면에서 보이는(직접 관찰 가능) 상식과 보이지 않는(추론 가능한) 상식을 모두 통합합니다. 각 상식은 헤드 엔터티가 이미지의 객체 경계 상자에 기반을 두는 트리플로 표현되므로 장면 종속적이고 객체별 시각적 상식 표현이 가능합니다. VCD의 유용성을 입증하기 위해 우리는 비전 언어 모델과 명령어 조정을 결합하여 이미지에서 다양한 시각적 상식을 발견하는 생성 모델인 VCM을 개발합니다. 광범위한 평가를 통해 VCD의 높은 품질과 시각적으로 기초한 상식적 이해 및 추론을 향상시키는 리소스로서의 가치가 모두 입증되었습니다. 우리의 데이터세트와 코드는 https://github.com/NUSTM/VCD에 공개될 예정입니다."
543,http://arxiv.org/abs/2403.10534 ,VISREAS: Complex Visual Reasoning with Unanswerable Questions,"Syeda Nahida Akter, Sangwu Lee, Yingshan Chang, Yonatan Bisk, Eric Nyberg","사용자가 불완전한 지침을 제공할 수 있는 실제 애플리케이션에서는 답변하기 전에 질문의 유효성을 확인하는 것이 중요합니다. 이 시나리오에서 이상적인 모델은 가능한 최상의 답변을 생성하기보다는 쿼리의 불일치를 해결하고 이를 사용자에게 전달해야 합니다. 이 요구 사항을 해결하기 위해 우리는 객체, 속성 및 관계 간의 공통점과 차이점을 탐색하고 교란하여 공식화된 응답 가능 및 응답 불가능 시각적 쿼리로 구성된 새로운 구성적 시각적 질문 응답 데이터 세트인 VISREAS를 소개합니다. VISREAS에는 Visual Genome 장면 그래프를 사용하여 자동으로 생성된 의미론적으로 다양한 207M 쿼리가 포함되어 있습니다. 답변하기 전에 이미지에 대한 질문 답변 가능성을 검증하는 이 작업의 고유한 기능과 최첨단 모델의 열악한 성능은 답변을 생성하기 위해 외부 모듈 없이 의사 코드를 생성하고 실행하여 추론하는 새로운 모듈 기준선인 LOGIC2VISION의 설계에 영감을 주었습니다. LOGIC2VISION은 VISREAS의 생성 모델보다 성능이 뛰어나며(LLaVA-1.5에 비해 +4.82%, InstructBLIP에 비해 +12.23%) 분류 모델에 비해 성능이 크게 향상되었습니다."
542,http://arxiv.org/abs/2402.15487 ,RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for Robotic Manipulation,"Hanxiao Jiang, Binghao Huang, Ruihai Wu, Zhuoran Li, Shubham Garg, Hooshang Nayyeri, Shenlong Wang, Yunzhu Li","로봇이 자동으로 환경을 탐색하고 기본 환경의 구조를 포착하는 동작 조건 장면 그래프(ACSG)를 생성하는 대화형 장면 탐색의 새로운 작업을 소개합니다. ACSG는 장면의 하위 수준 정보(기하학 및 의미 체계)와 상위 수준 정보(다른 엔터티 간의 동작 조건 관계)를 모두 설명합니다. 이를 위해 LMM(Large Multimodal Model)과 명시적 메모리 설계를 통합하여 시스템 성능을 향상시키는 RoboEXP(로보틱 탐색) 시스템을 제시합니다. 로봇은 물체를 탐색할 대상과 방법을 추론하고 상호작용 과정을 통해 새로운 정보를 축적하며 점진적으로 ACSG를 구축합니다. 구성된 ACSG를 활용하여 강체, 연결 개체, 중첩 개체 및 변형 가능한 개체와 관련된 광범위한 실제 조작 작업을 촉진하는 RoboEXP 시스템의 효과와 효율성을 설명합니다."
541,http://arxiv.org/abs/2402.14461 ,S^2Former-OR: Single-Stage Bi-Modal Transformer for Scene Graph Generation in OR,"Jialun Pei, Diandian Guo, Jingyang Zhang, Manxi Lin, Yueming Jin, Pheng-Ann Heng","수술 절차의 장면 그래프 생성(SGG)은 수술실(OR)에서 전체적인 인지 지능을 향상시키는 데 매우 중요합니다. 그러나 이전 작업에서는 생성된 의미론적 장면 그래프가 포즈 추정 및 객체 감지를 포함한 중간 프로세스에 의존하는 다단계 학습에 주로 의존했습니다. 이 파이프라인은 다중 모달 표현 학습의 유연성을 잠재적으로 손상시켜 결과적으로 전반적인 효율성을 제한할 수 있습니다. 본 연구에서는 엔드투엔드 방식으로 SGG용 멀티 뷰 2D 장면과 3D 포인트 클라우드를 보완적으로 활용하는 것을 목표로 하는 S^2Former-OR이라고 하는 OR의 SGG용 새로운 단일 스테이지 바이모달 변환기 프레임워크를 소개합니다. 구체적으로, 우리 모델은 다중 뷰 시각적 정보 상호 작용을 장려하기 위해 View-Sync Transfusion 체계를 수용합니다. 동시에 Geometry-Visual Cohesion 작업은 시너지 효과가 있는 2D 의미론적 기능을 3D 포인트 클라우드 기능에 통합하도록 설계되었습니다. 또한, 증강된 특징을 기반으로 동적 엔터티 쌍 쿼리와 관계형 특성 사전을 포함하는 새로운 관계 감지 변환기 디코더를 제안합니다. 이를 통해 중간 단계 없이 그래프 생성을 위한 엔터티 쌍 관계를 직접 예측할 수 있습니다. 광범위한 실험을 통해 현재 OR-SGG 방법과 비교하여 4D-OR 벤치마크에서 S^2Former-OR의 뛰어난 SGG 성능과 낮은 계산 비용이 검증되었습니다(예: 모델 매개변수의 정밀도가 3% 증가하고 24.2M 감소). 우리는 포괄적인 평가를 위해 더 광범위한 측정 기준을 사용하여 우리의 방법을 일반적인 단일 단계 SGG 방법과 비교했으며 지속적으로 더 나은 성능을 달성했습니다."
540,http://arxiv.org/abs/2402.14123 ,DeiSAM: Segment Anything with Deictic Prompting,"Hikaru Shindo, Manuel Brack, Gopika Sudhakaran, Devendra Singh Dhami, Patrick Schramowski, Kristian Kersting","사전 훈련된 대규모 신경망은 제로샷 이미지 분할을 포함한 다양한 작업에서 강력한 기능을 입증했습니다. 복잡한 장면에서 구체적인 사물을 식별하기 위해 인간은 본능적으로 ""책상 위에 있고 컵 뒤에 있는 물체""와 같이 문맥에 따라 무언가를 언급하는 자연어의 직설적 설명에 의존합니다. 그러나 딥러닝 접근 방식은 복잡한 시나리오에서 추론 능력이 부족하기 때문에 이러한 직시적 표현을 안정적으로 해석할 수 없습니다. 이 문제를 해결하기 위해 우리는 사전 훈련된 대규모 신경망과 미분 가능한 논리 추론기를 결합한 Deictic Prompable Segmentation을 위한 DeiSAM을 제안합니다. 복잡한 텍스트 분할 설명이 주어지면 DeiSAM은 LLM(대형 언어 모델)을 활용하여 1차 논리 규칙을 생성하고 생성된 장면 그래프에 대해 차별화 가능한 순방향 추론을 수행합니다. 이후 DeiSAM은 객체를 논리적으로 추론된 이미지 영역과 일치시켜 객체를 분할합니다. 평가의 일환으로 우리는 쌍을 이루는 시각적 입력과 복잡하고 직설적인 텍스트 프롬프트를 포함하는 DeiVG(Deictic Visual Genome) 데이터세트를 제안합니다. 우리의 경험적 결과는 DeiSAM이 직접적인 프롬프트 가능 세분화를 위한 순수 데이터 기반 기준에 비해 상당한 개선이 이루어졌음을 보여줍니다."
539,http://arxiv.org/abs/2402.12728 ,Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering,"Junnan Dong, Qinggang Zhang, Huachi Zhou, Daochen Zha, Pai Zheng, Xiao Huang","지식 기반 시각적 질문 답변(KVQA)은 지식 그래프(KG)와 같은 외부 지식을 사용하여 시각적 질문에 답하기 위해 광범위하게 연구되었습니다. LLM(대형 언어 모델)을 암시적 지식 소스로 활용하려는 여러 시도가 제안되었지만 LLM이 환각을 일으킬 수 있기 때문에 여전히 어려운 과제입니다. 더욱이 이미지, KG, LLM 등 여러 지식 소스를 복잡한 시나리오에 맞게 쉽게 정렬할 수 없습니다. 이러한 문제를 해결하기 위해 우리는 KVQA(MAIL)용 LLM과의 새로운 양식 인식 통합을 제시합니다. 이미지 이해와 지식 추론 모두를 위해 다중 모드 지식을 신중하게 활용합니다. 구체적으로, (i) 우리는 이미지를 상세한 시각적 특징을 가진 장면 그래프로 조밀하게 구현하기 위해 LLM을 사용한 2단계 프롬프트 전략을 제안합니다. (ii) 언급된 개체를 외부 사실과 연결하여 결합 개념 그래프를 구성합니다. (iii) 맞춤형 의사 샴 그래프 매체 융합은 충분한 다중 모드 융합을 위해 설계되었습니다. 우리는 두 그래프에서 공유된 언급된 엔터티를 매체로 활용하여 긴밀한 모달 간 교환을 연결하는 동시에 매체 내 융합을 제한하여 통찰력 있는 모달 내 학습을 최대한 보존합니다. 두 개의 벤치마크 데이터 세트에 대한 광범위한 실험은 24배 적은 리소스로 MAIL의 우수성을 보여줍니다."
538,http://arxiv.org/abs/2402.12259 ,Open3DSG: Open-Vocabulary 3D Scene Graphs from Point Clouds with Queryable Objects and Open-Set Relationships,"Sebastian Koch, Narunas Vaskevicius, Mirco Colosi, Pedro Hermosilla, Timo Ropinski","3D 장면 그래프 예측을 위한 현재 접근 방식은 레이블이 지정된 데이터 세트를 사용하여 알려진 객체 클래스 및 관계 범주의 고정 세트에 대한 모델을 교육합니다. 우리는 라벨이 붙은 장면 그래프 데이터 없이 열린 세상에서 3D 장면 그래프 예측을 학습하는 대안적인 접근 방식인 Open3DSG를 제시합니다. 우리는 3D 장면 그래프 예측 백본의 기능을 강력한 오픈 월드 2D 비전 언어 기반 모델의 기능 공간과 함께 통합했습니다. 이를 통해 공개 어휘에서 객체 클래스를 쿼리하고 장면 그래프 기능과 쿼리된 객체 클래스를 컨텍스트로 사용하여 기반 LLM에서 객체 간 관계를 예측함으로써 제로샷 방식으로 3D 포인트 클라우드에서 3D 장면 그래프를 예측할 수 있습니다. Open3DSG는 명시적인 개방형 어휘 객체 클래스뿐만 아니라 사전 정의된 라벨 세트에 국한되지 않는 개방형 관계도 예측하는 최초의 3D 포인트 클라우드 방식으로, 예측된 3D 장면 그래프에서 특정 객체 및 관계는 물론 희귀 객체까지 표현이 가능합니다. 우리의 실험에서는 Open3DSG가 임의의 개체 클래스뿐만 아니라 공간적, 지원적, 의미적, 비교 관계를 설명하는 복잡한 개체 간 관계를 예측하는 데 효과적이라는 것을 보여줍니다."
537,http://arxiv.org/abs/2402.07630 ,G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering,"Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V. Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, Bryan Hooi","텍스트 속성이 있는 그래프가 주어지면 사용자는 '그래프와 채팅'할 수 있습니다. 즉, 대화형 인터페이스를 사용하여 그래프에 대해 질문할 수 있습니다. 사용자의 질문에 대한 응답으로 우리의 방법은 텍스트 응답을 제공하고 그래프의 관련 부분을 강조 표시합니다. 기존 작업에서는 LLM(대형 언어 모델)과 GNN(그래프 신경망)을 다양한 방식으로 통합하지만 대부분 기존 그래프 작업(예: 노드, 에지, 그래프 분류)이나 작은 그래프 또는 합성 그래프에 대한 간단한 그래프 쿼리에 응답하는 데 중점을 둡니다. 대조적으로, 우리는 장면 그래프 이해, 상식 추론, 지식 그래프 추론을 포함한 다양한 응용 프로그램에 적용할 수 있는 실제 텍스트 그래프를 대상으로 하는 유연한 질문 답변 프레임워크를 개발합니다. 이 목표를 위해 먼저 다양한 작업에서 수집된 데이터를 사용하여 GraphQA(그래프 질문 응답) 벤치마크를 개발합니다. 그런 다음 소프트 프롬프트를 통해 그래프 이해를 향상시키기 위해 미세 조정할 수 있는 일반 텍스트 그래프에 대한 최초의 RAG(검색 증강 생성) 접근 방식을 도입하는 G-Retriever 방법을 제안합니다. 환각을 방지하고 LLM의 컨텍스트 창 크기를 크게 초과하는 텍스트 그래프를 허용하기 위해 G-Retriever는 이 작업을 Prize-Collecting Steiner Tree 최적화 문제로 공식화하여 그래프에 대해 RAG를 수행합니다. 경험적 평가에 따르면 우리의 방법은 여러 도메인의 텍스트 그래프 작업에 대한 기준보다 성능이 뛰어나고 더 큰 그래프 크기에 맞게 확장되며 환각을 완화합니다.~\footnote{우리의 코드와 데이터 세트는 다음에서 사용할 수 있습니다: \url{https://github.com/XiaoxinHe/G-Retriever}}"
536,http://arxiv.org/abs/2402.07537 ,UAV-assisted Visual SLAM Generating Reconstructed 3D Scene Graphs in GPS-denied Environments,"Ahmed Radwan, Ali Tourani, Hriday Bavle, Holger Voos, Jose Luis Sanchez-Lopez","공중 로봇은 환경과 관련된 로봇의 상황 인식이 기본적으로 요구되는 다양한 응용 분야에서 중요한 역할을 합니다. 이러한 사용 사례 중 하나로서 GPS 거부 환경의 드론에는 자세 추정 및 위치 파악을 수행하는 동안 신뢰할 수 있는 감지 결과를 제공하는 다양한 센서(예: 비전 센서)가 장착되어 있어야 합니다. 본 논문에서는 드론에 장착된 카메라를 이용하여 높은 수준의 표현을 위한 3차원 장면 그래프 생성과 함께 실내 환경 지도를 재구성하는 것을 목표로 한다. 이에, 저자가 제안한 VSLAM(Visual Simultaneous Localization and Mapping) 프레임워크와 적절하게 통합될 수 있도록 컴패니언 컴퓨터와 RGB-D 카메라를 갖춘 항공 로봇을 제작하고 채택했습니다. 지도를 재구성하는 동안 로봇의 상황 인식을 향상시키기 위해 문과 벽을 포함한 다양한 구조 요소에 인쇄된 기준 마커를 표시하고 이들 간의 위상 관계 사전을 시스템에 공급했습니다. VSLAM 시스템은 마커를 감지하고 복도와 방을 포함하여 더 높은 수준의 의미 엔터티가 풍부한 실내 영역의 지도를 재구성합니다. 또 다른 성과는 실내 환경의 향상된 계층적 표현을 포함하는 다층 비전 기반 상황 그래프를 생성하는 것입니다. 이와 관련하여 VSLAM을 사용되는 드론에 통합하는 것이 GPS 거부 환경을 위한 엔드투엔드 로봇 애플리케이션을 제공하는 본 논문의 주요 목표입니다. 시스템의 실용성을 보여주기 위해 구조 레이아웃이 서로 다른 실내 시나리오에서 다양한 실제 조건 실험이 수행되었습니다. 평가에 따르면 제안된 드론 애플리케이션이 적절하게 성능을 발휘할 수 있는 것으로 나타났습니다. 실측 데이터와 그 기준선."
535,http://arxiv.org/abs/2402.03840 ,Belief Scene Graphs: Expanding Partial Scenes with Objects through Computation of Expectation,"Mario A. V. Saucedo, Akash Patel, Akshit Saradagi, Christoforos Kanellakis, George Nikolakopoulos","본 논문에서는 부분 3D 장면 그래프의 유틸리티 기반 확장으로 부분 정보를 사용하여 효율적이고 높은 수준의 작업 계획을 가능하게 하는 새로운 개념의 Belief 장면 그래프를 제안합니다. 우리는 주어진 3D 장면 그래프에 대한 믿음(기대라고도 함) 계산을 위한 그래프 기반 학습 방법을 제안하며, 이는 로봇 임무와 관련된 새 노드(블라인드 노드라고도 함)를 전략적으로 추가하는 데 사용됩니다. 우리는 사용 가능한 훈련 데이터로부터 히스토그램을 학습함으로써 실제 믿음/기대를 합리적으로 근사화하기 위한 상관 정보(CECI) 기반 기대 계산 방법을 제안합니다. 3D 장면 그래프 저장소에서 CECI를 학습하기 위해 새로운 GCN(Graph Convolutional Neural Network) 모델이 개발되었습니다. 새로운 CECI 모델 훈련을 위한 3D 장면 그래프 데이터베이스가 없기 때문에 의미상 주석이 달린 실제 3D 공간을 기반으로 3D 장면 그래프 데이터 세트를 생성하는 새로운 방법론을 제시합니다. 생성된 데이터세트는 제안된 CECI 모델을 훈련하고 제안된 방법의 광범위한 검증을 위해 활용됩니다. 우리는 기대치를 추상적 표현에 통합하기 위한 핵심 구성 요소로 \textit{Belief Scene Graphs}(BSG)라는 새로운 개념을 확립했습니다. 이 새로운 개념은 고전적인 3D 장면 그래프 개념의 진화이며 다양한 로봇 임무의 작업 계획 및 최적화를 위한 높은 수준의 추론을 가능하게 하는 것을 목표로 합니다. 전체 프레임워크의 효율성은 객체 검색 시나리오에서 평가되었으며, 보이지 않는 객체에 대한 인간의 상식을 모방하기 위한 실제 실험에서도 테스트되었습니다. 실험 시연을 보여주는 기사의 비디오를 보려면 다음 링크를 참조하십시오: https://youtu.be/hsGlSCa12iY"
534,http://arxiv.org/abs/2401.14626 ,Towards Lifelong Scene Graph Generation with Knowledge-ware In-context Prompt Learning,"Tao He, Tongtong Wu, Dongyang Zhang, Guiduo Duan, Ke Qin, Yuan-Fang Li","SGG(장면 그래프 생성)는 이미지 내 개체 쌍 간의 시각적 관계를 예측하려고 노력합니다. 널리 사용되는 SGG 방법은 전통적으로 SGG에 대한 일회성 학습 프로세스를 가정합니다. 이러한 기존 패러다임에서는 새로운 관계가 나타날 때마다 이전에 관찰한 모든 샘플에 대해 반복적인 훈련이 필요할 수 있으므로 이전에 획득한 지식을 잊어버릴 위험이 완화됩니다. 이 작업은 이전 관계 예측 모음에 내재된 이러한 함정을 해결하려고 합니다. 사전 훈련된 언어 모델의 상황 내 학습 성과에 동기를 부여받은 우리의 접근 방식은 치명적인 망각에 굴복하지 않고 관계를 예측하고 지속적으로 새로운 지식을 획득할 수 있는 기능을 모델에 부여합니다. 이 목표를 달성하기 위해 우리는 조건자 같은 작업이 스트리밍 방식으로 전개되는 평생 장면 그래프 생성(LSGG)이라는 장면 그래프 생성을 위한 새롭고 실용적인 프레임워크를 소개합니다. 이 프레임워크에서 모델은 제한된 수의 예시를 제외하고 이전에 접한 학습 데이터에 대한 액세스가 없는 현재 작업에 대한 배타적 학습으로 제한되지만, 모델은 지금까지 접한 모든 조건자를 추론하는 작업을 맡습니다. 엄격한 실험은 다양한 측정 항목에 걸쳐 LSGG의 맥락에서 최첨단 SGG 모델에 비해 우리가 제안한 방법의 우월성을 보여줍니다. 게다가 두 가지 주류 벤치마크 데이터 세트인 VG와 Open-Image(v6)에 대한 광범위한 실험은 지속적인 학습 및 기존 설정 측면에서 제안된 모델이 여러 경쟁 SGG 모델보다 우수함을 보여줍니다. 또한 포괄적인 절제 실험은 모델의 각 구성 요소의 효율성을 보여줍니다."
533,http://arxiv.org/abs/2401.14111 ,Image Synthesis with Graph Conditioning: CLIP-Guided Diffusion Models for Scene Graphs,"Rameshwar Mishra, A V Subramanyam",생성 모델의 발전으로 인해 특정 구조 지침을 준수하면서 이미지를 생성하는 데 상당한 관심이 촉발되었습니다. 장면 그래프에서 이미지 생성까지의 작업은 주어진 장면 그래프와 일치하는 이미지를 생성하는 작업 중 하나입니다. 그러나 시각적 장면의 복잡성으로 인해 장면 그래프 내에서 지정된 관계를 기반으로 개체를 정확하게 정렬하는 데 어려움이 있습니다. 기존 방법은 먼저 장면 레이아웃을 예측하고 적대적 훈련을 사용하여 이러한 레이아웃에서 이미지를 생성함으로써 이 작업에 접근합니다. 이 작업에서는 중간 레이아웃을 예측할 필요가 없는 장면 그래프에서 이미지를 생성하는 새로운 접근 방식을 소개합니다. 사전 훈련된 텍스트-이미지 확산 모델과 CLIP 지침을 활용하여 그래프 지식을 이미지로 변환합니다. 이를 위해 먼저 GAN 기반 학습을 사용하여 그래프 특징을 해당 이미지의 CLIP 특징과 정렬하도록 그래프 인코더를 사전 학습합니다. 또한 그래프 기능을 주어진 장면 그래프에 있는 객체 레이블의 CLIP 임베딩과 융합하여 그래프와 일관된 CLIP 안내 조건 신호를 생성합니다. 조건 입력에서 객체 임베딩은 이미지의 대략적인 구조를 제공하고 그래프 기능은 객체 간의 관계를 기반으로 구조적 정렬을 제공합니다. 마지막으로 재구성 및 CLIP 정렬 손실이 있는 그래프 일치 조건화 신호를 사용하여 사전 훈련된 확산 모델을 미세 조정합니다. 정교한 실험에 따르면 우리의 방법은 COCO 항목 및 Visual Genome 데이터 세트의 표준 벤치마크에서 기존 방법보다 성능이 뛰어난 것으로 나타났습니다.
532,http://arxiv.org/abs/2401.12835 ,SGTR+: End-to-end Scene Graph Generation with Transformer,"Rongjie Li, Songyang Zhang, Xuming He","장면 그래프 생성(SGG)은 구성 특성으로 인해 여전히 어려운 시각적 이해 작업으로 남아 있습니다. 대부분의 이전 작업은 상향식, 2단계 또는 포인트 기반, 1단계 접근 방식을 채택했는데, 이는 종종 시간 복잡성이 높거나 최적이 아닌 설계로 인해 어려움을 겪습니다. 본 연구에서는 앞서 언급한 문제를 해결하기 위한 새로운 SGG 방법을 제안하고 작업을 이분 그래프 구성 문제로 공식화합니다. 위의 문제를 해결하기 위해 변환기 기반 엔드투엔드 프레임워크를 생성하여 엔터티 및 엔터티 인식 조건자 제안 세트를 생성하고 방향성 에지를 추론하여 관계 삼중항을 형성합니다. 또한, 엔터티 인식 구조를 기반으로 이분 장면 그래프의 연결성을 추론하는 그래프 조립 모듈을 설계하여 엔드 투 엔드 방식으로 장면 그래프를 생성할 수 있습니다. 이분 그래프 조립 패러다임을 기반으로 우리는 엔터티 인식 모델링의 효율성과 그래프 조립의 최적화 안정성을 해결하기 위한 새로운 기술 설계를 추가로 제안합니다. 향상된 엔터티 인식 설계를 갖춘 우리의 방법은 최적의 성능과 시간 복잡성을 달성합니다. 광범위한 실험 결과에 따르면 우리의 설계는 세 가지 까다로운 벤치마크에서 최첨단 또는 이에 필적하는 성능을 달성하여 기존 접근 방식의 대부분을 능가하고 추론에서 더 높은 효율성을 누릴 수 있음을 보여줍니다. 코드는 https://github.com/Scarecrow0/SGTR에서 확인할 수 있습니다."
531,http://arxiv.org/abs/2401.12479 ,TD^2-Net: Toward Denoising and Debiasing for Dynamic Scene Graph Generation,"Xin Lin, Chong Shi, Yibing Zhan, Zuopeng Yang, Yaqi Wu, Dacheng Tao","동적 장면 그래프 생성(SGG)은 비디오에서 객체를 감지하고 해당 객체의 쌍 관계를 결정하는 데 중점을 둡니다. 기존 동적 SGG 방법은 일반적으로 1) 일부 프레임에 가려지거나 흐릿한 개체가 포함될 수 있으므로 상황에 따른 노이즈를 비롯한 여러 문제로 인해 어려움을 겪습니다. 2) 라벨 편향. 주로 소수의 긍정적인 관계 샘플과 다수의 부정적인 관계 샘플 사이의 높은 불균형으로 인해 발생합니다. 또한 관계 분포는 긴 꼬리 패턴을 나타냅니다. 위의 문제를 해결하기 위해 본 논문에서는 동적 SGG에 대한 잡음 제거 및 편향 제거를 목표로 하는 TD$^2$-Net이라는 네트워크를 소개합니다. 구체적으로, 우리는 먼저 강력한 상황 정보로 객체 표현을 향상시키는 잡음 제거 시공간 변환기 모듈을 제안합니다. 이는 각 객체에 대한 관련 이웃을 선택하기 위해 gumbel-softmax 샘플링 전략을 활용하는 미분 가능한 Top-K 객체 선택기를 설계함으로써 달성됩니다. 둘째, 라벨 편향 문제를 완화하기 위해 비대칭 재가중 손실을 도입합니다. 이 손실 함수는 비대칭 초점 요소와 샘플 볼륨을 통합하여 개별 샘플에 할당된 가중치를 조정합니다. 체계적인 실험 결과는 Action Genome 데이터베이스에 대한 기존의 최첨단 접근 방식보다 우리가 제안한 TD$^2$-Net의 우수성을 보여줍니다. 더 자세히 살펴보면, TD$^2$-Net은 조건자 분류에 대한 평균-Recall@10에서 2위 경쟁사보다 12.7\% 더 나은 성능을 보입니다."
530,http://arxiv.org/abs/2401.09786 ,Adaptive Self-training Framework for Fine-grained Scene Graph Generation,"Kibum Kim, Kanghoon Yoon, Yeonjun In, Jinyoung Moon, Donghyun Kim, Chanyoung Park",SGG(장면 그래프 생성) 모델은 긴 꼬리 조건자 분포 및 주석 누락 문제와 같은 벤치마크 데이터 세트와 관련된 고유한 문제로 어려움을 겪었습니다. 본 연구에서는 주석이 없는 삼중항을 활용하여 SGG의 긴 꼬리 문제를 완화하는 것을 목표로 합니다. 이를 위해 SGG 모델이 훈련되는 기반으로 주석이 없는 삼중항에 의사 레이블을 할당하는 SGG용 자체 훈련 프레임워크(ST-SGG)를 소개합니다. 이미지 인식을 위한 자체 학습에서 상당한 진전이 있었지만 SGG 작업을 위한 자체 학습 프레임워크를 설계하는 것은 의미적 모호성 및 조건자 클래스의 긴 꼬리 분포와 같은 고유한 특성으로 인해 더욱 어렵습니다. 따라서 우리는 기존 SGG 모델에 적용할 수 있는 모델 독립적 프레임워크인 CATM(Class-Specific Adaptive Thresholding with Momentum)이라는 SGG를 위한 새로운 의사 라벨링 기술을 제안합니다. 또한 제안된 자체 학습 프레임워크를 MPNN(메시지 전달 신경망) 기반 SGG 모델에 적용할 때 유용한 그래프 구조 학습기(GSL)를 고안했습니다. 우리의 광범위한 실험은 특히 세분화된 조건자 클래스의 성능을 향상시키는 데 있어 다양한 SGG 모델에서 ST-SGG의 효율성을 검증합니다.
529,http://arxiv.org/abs/2401.09340 ,SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding,"Baoxiong Jia, Yixin Chen, Huangyue Yu, Yan Wang, Xuesong Niu, Tengyu Liu, Qing Li, Siyuan Huang","언어를 3D 물리적 환경에 맞추는 데 초점을 맞춘 3D 비전 언어 접지는 구체화된 에이전트 개발의 초석입니다. 2D 영역의 최근 발전과 비교하여 3D 장면의 기반 언어는 몇 가지 중요한 과제에 직면해 있습니다. (i) 다양한 객체 구성, 풍부한 속성 및 복잡한 관계로 인한 3D 장면의 고유한 복잡성; (ii) 기초 학습을 지원하기 위한 쌍을 이루는 3D 비전-언어 데이터의 부족; (iii) 기반 3D 데이터에서 지식을 추출하기 위한 통합 학습 프레임워크가 부족합니다. 본 연구에서 우리는 실내 환경에서 3D 비전 언어 학습을 체계적으로 확대하는 잠재력을 조사함으로써 3D 비전 언어의 세 가지 주요 과제를 해결하는 것을 목표로 합니다. 우리는 약 68K 3D 실내 장면을 포함하고 인간 주석과 확장 가능한 장면 그래프 기반 생성 접근 방식에서 파생된 250만 개의 비전 언어 쌍으로 구성된 최초의 백만 규모 3D 비전 언어 데이터 세트인 SceneVerse를 소개합니다. 우리는 이러한 스케일링이 3D 비전 언어 학습을 위한 통합 사전 훈련 프레임워크인 GPS(Grounded Pre-training for Scenes)를 허용한다는 것을 보여줍니다. 광범위한 실험을 통해 기존의 모든 3D 시각적 접지 벤치마크에서 최첨단 성능을 달성하여 GPS의 효율성을 보여줍니다. SceneVerse 및 GPS의 엄청난 잠재력은 까다로운 3D 비전 언어 작업에서 제로샷 전송 실험을 통해 공개됩니다. 프로젝트 웹사이트: https://scene-verse.github.io."
528,http://arxiv.org/abs/2401.01130 ,Joint Generative Modeling of Grounded Scene Graphs and Images via Diffusion Models,"Bicheng Xu, Qi Yan, Renjie Liao, Lele Wang, Leonid Sigal","우리는 고차원, 다중 모드 구조 데이터와 관련된 어려운 작업인 이미지 생성이라는 공동 기반 장면 그래프를 위한 프레임워크를 소개합니다. 이 복잡한 결합 분포를 효과적으로 모델링하기 위해 우리는 인수분해 접근 방식을 채택했습니다. 먼저 접지 장면 그래프를 생성한 다음 생성된 접지 장면 그래프를 조건으로 이미지를 생성합니다. 조건부 이미지 생성은 문헌에서 널리 연구되었지만 우리의 주요 초점은 이미지 생성 프로세스에 대한 효율적이고 해석 가능한 제어를 제공하는 노이즈로부터 접지된 장면 그래프를 생성하는 것입니다. 이 작업에는 연속 속성(예: 객체 경계 상자)과 이산 속성(예: 객체 및 관계 범주)을 포함하는 노드(객체)와 가장자리(객체 간 관계) 모두에 대한 이질적인 속성이 있는 그럴듯한 기반 장면 그래프를 생성해야 합니다. 이러한 문제를 해결하기 위해 우리는 이종 노드와 에지 속성을 공동으로 모델링하는 새로운 확산 모델인 DiffuseSG를 소개합니다. 범주형 데이터를 효과적으로 처리하기 위해 다양한 인코딩 전략을 탐색합니다. 그래프 변환기를 노이즈 제거기로 활용하는 DiffuseSG는 구조화된 출력을 생성하기 위해 이산화하기 전에 연속 공간에서 접지된 장면 그래프 표현을 점진적으로 개선합니다. 또한 경험적 성능을 향상시키기 위해 IoU 기반 정규화 용어를 도입합니다. 우리 모델은 VG 및 COCO-Stuff 데이터 세트의 기본 장면 그래프 생성에서 기존 방법보다 성능이 뛰어나며 작업의 복잡성을 보다 정확하게 포착하는 표준 및 새로 도입된 측정 항목 모두에서 탁월합니다. 또한 우리는 두 가지 중요한 다운스트림 작업에서 DiffuseSG의 광범위한 적용 가능성을 보여줍니다. 1) 다양한 접지 장면 그래프 완성 작업에서 우수한 결과 달성, 2) DiffuseSG에서 생성된 추가 훈련 샘플을 활용하여 접지 장면 그래프 감지 모델 향상."
527,http://arxiv.org/abs/2312.17425 ,ALF: Adaptive Label Finetuning for Scene Graph Generation,"Qishen Chen, Jianzhi Liu, Xinyu Lyu, Lianli Gao, Heng Tao Shen, Jingkuan Song","SGG(장면 그래프 생성)는 주어진 이미지에서 피사체와 객체 간의 관계를 예측하려고 노력합니다. 그럼에도 불구하고 관계의 롱테일 분포는 종종 거친 레이블에 대한 편향된 예측으로 이어져 SGG에 상당한 장애물을 제시합니다. 이 문제를 해결하기 위해 연구자들은 편파적이지 않은 SGG에 중점을 두고 전체 데이터 세트에서 대략적인 조건부를 세분화된 조건부로 전송하는 데이터 전송 방법을 도입합니다. 그러나 이러한 방법은 두 가지 주요 문제에 직면합니다. 1) 주체-객체 쌍에 의해 부과된 고유한 컨텍스트 제약을 간과하여 잘못된 관계 전송을 초래합니다. 2) 데이터 전송 후 추가적인 재훈련 과정이 필요하며, 이로 인해 상당한 전산 비용이 발생합니다. 이러한 한계를 극복하기 위해 우리는 ALF(Adaptive Label Finetuning)라고 불리는 최초의 플러그 앤 플레이 1단계 데이터 전송 파이프라인을 SGG에 도입했습니다. 이 파이프라인은 추가 재교육 세션의 필요성을 없애고 동시에 다양한 SGG 벤치마크 접근 방식에서 모델의 관계 인식 기능을 크게 향상시킵니다. 특히 ALF는 적응형 레이블 구성(ALC)과 적응형 반복 학습(AIL)의 두 가지 구성 요소로 구성됩니다. ALC는 관계 공간 내에 Predicate-Context Constraints를 부과함으로써 제한 기반 판단 기술을 활용하는 모델의 예측 로짓을 참조하여 후보 관계의 순위를 다시 매기고 선택하여 강력한 관계 전송을 달성합니다. ALC에서 전송된 레이블을 감독하는 AIL은 자동 회귀 방식으로 SGG 모델을 반복적으로 미세 조정하여 재교육 프로세스에서 발생하는 상당한 계산 비용을 완화합니다. 광범위한 실험을 통해 ALF는 일반적인 SGG 방법인 Motif에 비해 mR@100이 16% 향상되었으며 최첨단 방법인 IETrans에 비해 계산 비용이 6%만 증가한 것으로 나타났습니다."
526,http://arxiv.org/abs/2312.13219 ,Interactive Visual Task Learning for Robots,"Weiwei Gu, Anant Sah, Nakul Gopalan","우리는 인간 사용자와의 현장 언어 상호 작용을 통해 로봇이 새로운 시각적 개념과 작업을 학습할 수 있는 프레임워크를 제시합니다. 이전 접근 방식에서는 사전 훈련된 대규모 시각적 모델을 사용하여 새로운 개체를 제로샷으로 추론하거나 개념 계층 구조에 속성 및 표현과 함께 새로운 개념을 추가했습니다. 우리는 새로운 개념을 배우고 보이지 않는 로봇 공학 작업을 해결할 수 있도록 함으로써 시각적 개념 계층 학습에 초점을 맞춘 접근 방식을 확장합니다. 시각적 개념 학습자가 로봇 공학 작업을 한 번에 해결할 수 있도록 하기 위해 우리는 두 가지 고유한 기술을 개발했습니다. 첫째, 개념 계층 내에서 상위 노드에 새로운 개념의 정보를 증가시키는 새로운 접근 방식인 Hi-Viscont(HIerarchical VISual CONcept learner for Task)를 제안합니다. 이러한 정보 전파를 통해 지속적인 학습 환경에서 새로운 개념을 가르칠 때 계층 구조의 모든 개념이 업데이트될 수 있습니다. 둘째, 시각적 작업을 언어 주석이 포함된 장면 그래프로 표현하여 현장에서 시연된 작업 제로 샷의 새로운 순열을 만들 수 있습니다. 우리는 두 가지 결과 세트를 제시합니다. 먼저, Hi-Viscont와 시각적 질의응답(VQA) 기준 모델(FALCON)을 세 가지 영역에서 비교합니다. Hi-Viscont는 리프 레벨 개념의 기본 모델과 비교되는 동시에 리프가 아닌 개념에서 평균 9% 이상의 개선을 달성합니다. 모델의 성능을 기본 FALCON 모델과 비교합니다. 우리의 프레임워크는 기본 모델에 비해 성공률 지표가 33% 향상되고 개체 수준 정확도가 19% 향상되었습니다. 이 두 결과를 통해 우리는 로봇의 지속적인 학습 환경에서 작업과 개념을 학습하는 모델의 능력을 보여줍니다."
525,http://arxiv.org/abs/2312.11713 ,Indoor and Outdoor 3D Scene Graph Generation via Language-Enabled Spatial Ontologies,"Jared Strader, Nathan Hughes, William Chen, Alberto Speranzon, Luca Carlone","본 논문에서는 임의의 실내 및 실외 환경에서 3D 장면 그래프를 구축하는 접근 방식을 제안합니다. 그러한 확장은 어려운 일입니다. 실외 환경을 설명하는 개념 계층은 실내보다 더 복잡하며 이러한 계층을 수동으로 정의하는 것은 시간이 많이 걸리고 확장되지 않습니다. 또한 훈련 데이터가 부족하여 실내 환경에서 사용되는 학습 기반 도구를 직접 적용하기가 어렵습니다. 이러한 문제를 해결하기 위해 우리는 두 가지 새로운 확장을 제안합니다. 첫째, 실내 및 실외 로봇 작동과 관련된 개념과 관계를 정의하는 공간 온톨로지를 구축하는 방법을 개발합니다. 특히 우리는 이러한 온톨로지를 구축하기 위해 LLM(Large Language Model)을 사용하므로 필요한 수동 작업의 양을 크게 줄입니다. 둘째, LTN(Logic Tensor Networks)을 사용하여 3D 장면 그래프 구성을 위한 공간 온톨로지를 활용하여 논리적 규칙 또는 공리(예: ""해변에 모래가 포함되어 있음"")를 추가합니다. 이는 훈련 시간에 추가 감독 신호를 제공하여 레이블이 지정된 데이터의 필요성을 줄이고 더 나은 예측을 제공하며 훈련 시간에 보이지 않는 개념 예측도 허용합니다. 우리는 실내, 시골 및 해안 환경을 포함한 다양한 데이터 세트에서 접근 방식을 테스트하고 주석이 거의 없는 데이터를 사용하여 3D 장면 그래프 생성의 품질을 크게 향상시키는 것으로 나타났습니다."
524,http://arxiv.org/abs/2312.10251 ,Advancing Surgical VQA with Scene Graph Knowledge,"Kun Yuan, Manasi Kattel, Joel L. Lavanchy, Nassir Navab, Vinkle Srivastav, Nicolas Padoy","현대의 수술실은 점점 더 복잡해지고 있으며 혁신적인 수술 내 지원 시스템이 필요합니다. 수술 데이터 과학의 초점은 주로 비디오 분석에 맞춰져 있지만, 수술 컴퓨터 비전과 언어 기능의 통합이 필수 사항으로 대두되고 있습니다. 우리의 작업은 장면 그래프 지식을 통해 수술 상황에서 시각적 질문 응답(VQA)을 발전시키는 것을 목표로 하며, 현재 수술 VQA 시스템의 두 가지 주요 과제를 해결합니다. 즉, 수술 VQA 데이터세트에서 질문 조건 편향을 제거하고 수술 VQA 모델 설계에 장면 인식 추론을 통합하는 것입니다. 먼저 공개적으로 사용 가능한 데이터 세트에 대한 분할 및 감지 모델을 사용하여 생성된 수술 장면 그래프 기반 데이터 세트 SSG-QA를 제안합니다. 기구와 해부체의 공간정보와 동작정보를 활용하여 수술 장면 그래프를 구축합니다. 이 그래프는 질문 엔진에 입력되어 다양한 QA 쌍을 생성합니다. SSG-QA 데이터세트는 기존 수술용 VQA 데이터세트에 비해 더 복잡하고, 다양하며, 기하학적으로 기반이 있고, 편파적이지 않은 수술 조치 중심의 데이터세트를 제공합니다. 그런 다음 텍스트와 장면 기능 간의 교차 주의를 사용하여 VQA 모델 설계에 기하학적 장면 지식을 통합하는 경량 장면 내장 상호 작용 모듈(SIM)을 통합한 새로운 외과용 VQA 모델인 SSG-QA-Net을 제안합니다. SSG-QA 데이터 세트에 대한 포괄적인 분석에 따르면 SSG-QA-Net은 다양한 질문 유형과 복잡성에 걸쳐 기존 방법보다 성능이 뛰어납니다. 우리는 현재 수술용 VQA 시스템의 주요 한계가 복잡한 질문에 답할 수 있는 현장 지식이 부족하다는 점을 강조합니다. 우리는 새로운 수술용 VQA 데이터세트와 모델을 제시하고 VQA 모델 디자인에 기하학적 장면 특징을 통합하면 결과가 크게 향상될 수 있음을 보여줍니다. 소스 코드와 데이터 세트는 https://github.com/CAMMA-public/SSG-QA에서 공개적으로 제공됩니다."
523,http://arxiv.org/abs/2312.09076 ,ProSGNeRF: Progressive Dynamic Neural Scene Graph with Frequency Modulated Foundation Model in Urban Scenes,"Tianchen Deng, Yanbo Wang, Yejia Liu, Chenpeng Su, Jingchuan Wang, Danwei Wang, Shao-Yuan Lo, Weidong Chen","암시적 신경 표현은 다양한 장면의 3D 재구성에서 유망한 결과를 보여주었습니다. 그러나 기존 접근 방식은 빠르게 움직이는 물체를 모델링하는 데 어려움을 겪거나 도시 환경에서 대규모 카메라 자아 동작을 처리할 수 없습니다. 이로 인해 대규모 도시 장면의 품질이 낮은 합성 뷰가 생성됩니다. 본 논문에서는 보다 실용적이고 도전적인 대규모 장면과 빠르게 움직이는 차량으로 인해 발생하는 문제를 공동으로 해결하는 것을 목표로 합니다. 이를 위해 우리는 동적 객체와 글로벌 도시 장면의 로컬 장면 표현을 학습하기 위한 진보적인 장면 그래프 네트워크 아키텍처를 제안합니다. 점진적 학습 아키텍처는 시간 창 내의 프레임에 대해 훈련된 새로운 로컬 장면 그래프를 동적으로 할당하고 창 크기를 자동으로 결정하므로 표현을 임의로 큰 장면으로 확장할 수 있습니다. 게다가, 우리의 관찰에 따르면, 동적 객체의 트레이닝 뷰는 빠른 움직임에 따라 상대적으로 희박하여 동적 객체의 재구성 정확도가 크게 저하됩니다. 따라서 우리는 잠재 코드를 인코딩하기 위해 기초 모델 네트워크를 활용합니다. 특히 시각적 기반 모델 DINOv2의 일반화 기능을 활용하여 모양 및 모양 코드를 추출하고 대규모 도시 장면 객체 데이터 세트에서 네트워크를 훈련하여 희소 뷰 동적 입력을 처리하기 위한 사전 모델링 기능을 향상시킵니다. 이와 동시에 물체의 주파수 스펙트럼을 정규화하여 주파수 영역 관점에서 희소 이미지 입력을 모델링하는 문제를 해결하는 주파수 변조 모듈을 소개합니다. 실험 결과는 우리의 방법이 다양한 장면에서 최첨단 뷰 합성 정확도, 객체 조작 및 장면 로밍 능력을 달성한다는 것을 보여줍니다."
522,http://arxiv.org/abs/2312.07621 ,Spatiotemporal Event Graphs for Dynamic Scene Understanding,Salman Khan,"동적 장면 이해는 실제 장면의 비디오에 있는 시각적 정보를 해석하고 이해하는 컴퓨터 시스템의 능력입니다. 본 논문에서는 자율주행 관점의 도로 이벤트 감지부터 복잡한 영상 활동 감지까지 동적 장면 이해를 위한 일련의 프레임워크를 제시하고, 모델의 평생 학습을 위한 지속적인 학습 접근 방식을 제시합니다. 먼저, 자율주행을 위한 ROAD(ROad event Awareness Dataset)를 최초로 소개합니다. 공식적으로 지정된 논리적 요구 사항을 갖춘 데이터 세트가 부족하기 때문에 해당 분야에서 신경 기호 연구를 추진하기 위한 도구로 논리적 제약 조건으로 표현된 요구 사항을 갖춘 자율 주행을 위한 최초의 공개 데이터 세트인 ROAD-R(논리적 요구 사항이 포함된 ROad 이벤트 인식 데이터 세트)도 소개합니다. 다음으로, 두 가지 복잡한 활동 감지 방법을 제안하여 이벤트 감지를 전체적인 장면 이해로 확장합니다. 첫 번째 방법에서는 액션 튜브 감지, 구성 액션 튜브의 유연하고 변형 가능한 형상을 학습하기 위해 설계된 3D 변형 가능한 RoI 풀링 레이어, 모든 부품을 노드로 간주하고 서로 다른 의미론을 기반으로 연결하여 구성된 장면 그래프의 세 가지 주요 구성 요소로 구성된 변형 가능한 시공간 장면 그래프 접근 방식을 제시합니다. 첫 번째 접근 방식에서 발전한 두 번째 접근 방식에서는 로컬(단기) 동적 장면의 그래프 인코딩에 적용되는 주의와 전체 장기간 활동을 모델링하는 시간 그래프를 결합하는 하이브리드 그래프 신경망을 제안합니다. 마지막으로 논문의 마지막 부분은 새로운 연속 준지도 학습(CSSL) 패러다임을 제시하는 것입니다."
521,http://arxiv.org/abs/2312.04314 ,GPT4SGG: Synthesizing Scene Graphs from Holistic and Region-specific Narratives,"Zuyao Chen, Jinlin Wu, Zhen Lei, Zhaoxiang Zhang, Changwen Chen",자연어 캡션을 사용한 SGG(장면 그래프 생성) 모델 훈련은 자연어가 제공하는 풍부하고 비용 효율적인 개방형 일반화 감독 신호로 인해 점점 인기를 얻고 있습니다. 그러나 이러한 구조화되지 않은 캡션 데이터와 처리는 정확하고 포괄적인 장면 그래프를 학습하는 데 심각한 문제를 야기합니다. 과제는 세 가지 측면으로 요약될 수 있습니다. 1) 언어적 표현을 기반으로 하는 전통적인 장면 그래프 파서는 캡션 데이터에서 의미 있는 관계 삼중항을 추출하지 못하는 경우가 많습니다. 2) 구문 분석된 삼중항의 지역화되지 않은 객체를 접지하면 시각적 언어 정렬의 모호성 문제가 발생합니다. 3) 캡션 데이터는 일반적으로 드물고 이미지 콘텐츠의 부분적인 관찰에 대한 편견을 나타냅니다. 이러한 문제를 해결하기 위해 우리는 보다 정확하고 포괄적인 장면 그래프 신호를 얻기 위해 \textit{GPT4SGG}라는 새로운 프레임워크를 사용하여 분할 정복 전략을 제안합니다. 이 프레임워크는 복잡한 장면을 여러 개의 간단한 영역으로 분해하여 지역별 내러티브 세트를 생성합니다. 이러한 지역별 내러티브(부분 관찰)와 이미지에 대한 전체적인 내러티브(전역적 관찰)를 통해 LLM(대형 언어 모델)은 정확하고 포괄적인 장면 그래프를 합성하기 위한 관계 추론을 수행합니다. 실험 결과는 \textit{GPT4SGG}가 이미지 캡션 데이터로 훈련된 SGG 모델의 성능을 크게 향상시키는 것으로 나타났습니다. 여기서 모호성 문제와 롱테일 편향은 보다 정확하고 포괄적인 장면 그래프를 통해 잘 처리되었습니다.
520,http://arxiv.org/abs/2312.03391 ,Action Scene Graphs for Long-Form Understanding of Egocentric Videos,"Ivan Rodin, Antonino Furnari, Kyle Min, Subarna Tripathi, Giovanni Maria Farinella","우리는 자기 중심적 비디오에 대한 장기적인 이해를 위한 새로운 표현인 EASG(자기 중심적 액션 장면 그래프)를 제시합니다. EASG는 상호 작용하는 객체, 객체의 관계, 시간에 따라 동작이 전개되는 방식을 포함하여 카메라 착용자가 수행하는 동작에 대해 시간적으로 진화하는 그래프 기반 설명을 제공함으로써 동사-명사 동작 레이블과 같은 자기 중심적 비디오의 수동으로 주석이 달린 표준 표현을 확장합니다. 새로운 주석 절차를 통해 우리는 오랫동안 자기 중심적인 비디오 이해를 위해 설계된 풍부한 주석 세트를 제공하는 수동으로 레이블이 지정된 Egocentric Action Scene Graph를 추가하여 Ego4D 데이터 세트를 확장합니다. 따라서 우리는 EASG 생성 작업을 정의하고 기본 접근 방식을 제공하여 예비 벤치마크를 설정합니다. 자기중심적 행동 예상과 자기중심적 활동 요약이라는 두 가지 다운스트림 작업에 대한 실험은 긴 형식의 자기중심적 비디오 이해를 위한 EASG의 효과를 강조합니다. 실험과 주석을 복제하기 위한 데이터세트와 코드를 공개할 예정입니다."
519,http://arxiv.org/abs/2312.03050 ,HIG: Hierarchical Interlacement Graph Approach to Scene Graph Generation in Video Understanding,"Trong-Thuan Nguyen, Pha Nguyen, Khoa Luu","시각적 장면 내에서 시각적 상호작용을 이해하는 것은 컴퓨터 비전에서 중요한 과제를 제시합니다. 기존 방법은 단순한 관계 모델을 활용하면서 복잡한 상호 작용에 중점을 둡니다. 그러나 이러한 방법은 영상 속 모습, 상황, 위치, 상호작용, 관계 등의 다양성 때문에 어려움을 겪습니다. 이러한 제한은 피사체의 복잡한 시각적 역동성 내에서 상호 작용을 완전히 이해하는 능력을 방해합니다. 본 논문에서는 인간과 사물 간의 밀집된 상호작용으로부터 장면 그래프 표현을 도출하여 시각적 콘텐츠 내에서의 상호작용 이해를 탐구합니다. 이 목표를 달성하기 위해 우리는 먼저 ASPIRE라는 Appearance-Situation-Position-Interaction-Relation 조건자를 포함하는 새로운 데이터 세트를 제시하고 광범위한 상호 작용으로 표시되는 광범위한 비디오 컬렉션을 제공합니다. 그런 다음 계층 구조 내의 통합 레이어와 그래프를 활용하여 5가지 개별 작업 전반에 걸쳐 장면 변화에 대한 깊은 통찰력을 제공하는 HIG(계층적 인터레이스 그래프)라는 새로운 접근 방식을 제안합니다. 우리의 접근 방식은 다양한 시나리오에서 수행된 광범위한 실험을 통해 다른 방법보다 우수한 성능을 보여줍니다."
518,http://arxiv.org/abs/2312.03032 ,ZeroReg: Zero-Shot Point Cloud Registration with Foundation Models,"Weijie Wang, Wenqi Ren, Guofeng Mei, Bin Ren, Xiaoshui Huang, Fabio Poiesi, Nicu Sebe, Bruno Lepri","최첨단 3D 포인트 클라우드 등록 방법은 훈련을 위해 레이블이 지정된 3D 데이터 세트에 의존하므로 실제 시나리오에서의 실제 적용이 제한되고 종종 보이지 않는 장면에 대한 일반화를 방해합니다. 기초 모델의 제로샷 기능을 활용하면 이러한 과제에 대한 유망한 솔루션을 제공할 수 있습니다. 본 논문에서는 2D 기반 모델을 활용하여 3D 대응을 예측하는 제로샷 등록 접근 방식인 ZeroReg를 소개합니다. 구체적으로 ZeroReg는 기초 모델을 사용하여 다중 시점 이미지에서 객체 위치 파악 및 의미론적 특징 추출을 시작으로 객체 간 매칭 전략을 채택합니다. 객체 일치 단계에서 의미론적 특징은 뷰 전반에 걸쳐 객체 간의 대응 관계를 식별하는 데 도움이 됩니다. 그러나 의미론적 특징에만 의존하면 특히 동일한 카테고리의 여러 인스턴스가 있는 장면에서 모호성이 발생할 수 있습니다. 이를 해결하기 위해 장면 그래프를 구성하여 객체 간의 공간적 관계를 포착하고 그래프에 그래프 매칭 알고리즘을 적용하여 일치하는 객체를 정확하게 식별합니다. 마지막으로 SuperGlue 및 LoFTR과 같은 알고리즘을 사용하여 일치하는 객체 영역 내에서 세밀한 포인트 수준 대응을 계산하면 강력한 포인트 클라우드 등록이 달성됩니다. 3DMatch, 3DLoMatch 및 ScanNet과 같은 벤치마크에 대한 평가는 ZeroReg의 경쟁력 있는 성능을 보여주며, 기초 모델의 의미론적 기능을 통합하여 포인트 클라우드 등록을 발전시킬 수 있는 잠재력을 강조합니다."
517,http://arxiv.org/abs/2312.00093 ,GraphDreamer: Compositional 3D Scene Synthesis from Scene Graphs,"Gege Gao, Weiyang Liu, Anpei Chen, Andreas Geiger, Bernhard Schölkopf",사전 학습된 텍스트-이미지 확산 모델이 점점 더 강력해짐에 따라 텍스트 기반 3D 모델을 최적화하기 위해 이러한 텍스트-이미지 사전 학습 모델에서 지식을 추출하려는 최근 노력이 이루어졌습니다. 기존 방법의 대부분은 일반 텍스트 입력에서 전체적인 3D 모델을 생성합니다. 벡터화된 텍스트 임베딩은 본질적으로 여러 엔터티와 관계가 있는 복잡한 설명을 캡처할 수 없기 때문에 텍스트가 여러 개체가 포함된 복잡한 장면을 설명하는 경우 문제가 될 수 있습니다. 전체 장면의 전체적인 3D 모델링은 텍스트 엔터티와 개념의 정확한 기반을 더욱 방해합니다. 이러한 제한 사항을 해결하기 위해 우리는 장면 그래프에서 구성적인 3D 장면을 생성하는 새로운 프레임워크인 GraphDreamer를 제안합니다. 여기서 개체는 노드로 표시되고 해당 상호 작용은 가장자리로 표시됩니다. 장면 그래프의 노드 및 에지 정보를 활용함으로써 우리의 방법은 미리 훈련된 텍스트-이미지 확산 모델을 더 잘 활용하고 이미지 수준 감독 없이 다양한 객체를 완전히 풀 수 있습니다. 객체별 관계의 모델링을 용이하게 하기 위해 우리는 부호 있는 거리 필드를 표현으로 사용하고 객체의 상호 침투를 피하기 위해 제약 조건을 부과합니다. 수동 장면 그래프 생성을 피하기 위해 ChatGPT가 텍스트 입력을 기반으로 장면 그래프를 생성하도록 텍스트 프롬프트를 디자인합니다. 우리는 얽힌 개체 개체를 사용하여 충실도가 높은 구성 3D 장면을 생성하는 데 있어 GraphDreamer의 효과를 검증하기 위해 정성적 및 정량적 실험을 모두 수행합니다.
516,http://arxiv.org/abs/2311.18553 ,Heterogeneous Graph-based Trajectory Prediction using Local Map Context and Social Interactions,"Daniel Grimm, Maximilian Zipfl, Felix Hertlein, Alexander Naumann, Jürgen Lüttin, Steffen Thoma, Stefan Schmid, Lavdim Halilaj, Achim Rettinger, J. Marius Zöllner","주변 교통 참가자의 미래 궤적을 정확하게 예측하는 것은 교통 주체, 지도 컨텍스트 및 교통 규칙 간의 복잡한 상호 작용으로 인해 자율 주행에서 중요하지만 어려운 문제입니다. 벡터 기반 접근 방식은 최근 궤도 예측 벤치마크에서 최고의 성능을 달성하는 것으로 나타났습니다. 이러한 방법은 교통 에이전트 간의 간단한 상호 작용을 모델링하지만 관계 유형과 도로상의 거리와 같은 속성을 구별하지 않습니다. 또한 중앙선을 나타내는 일련의 벡터로만 차선을 나타내며 차선 구분선 및 기타 도로 요소와 같은 상황 정보를 무시합니다. 우리는 세 가지 중요한 정보 소스를 활용하여 이러한 단점을 해결하는 벡터 기반 궤적 예측을 위한 새로운 접근 방식을 제시합니다. 먼저, 관계의 성격과 중요한 특징을 설명하는 의미론적 장면 그래프를 통해 교통 에이전트 간의 상호 작용을 모델링합니다. 둘째, 에이전트 중심의 이미지 기반 지도 특징을 추출하여 로컬 지도 컨텍스트를 모델링합니다. 마지막으로 허용된 궤적에 대해서만 다중 모드 예측 정책을 시행하기 위한 앵커 경로를 생성합니다. 이러한 세 가지 향상된 기능은 각각 기본 모델 HoliGraph에 비해 장점을 보여줍니다."
515,http://arxiv.org/abs/2312.07740 ,HAtt-Flow: Hierarchical Attention-Flow Mechanism for Group Activity Scene Graph Generation in Videos,"Naga VS Raviteja Chappa, Pha Nguyen, Thi Hoang Ngan Le, Khoa Luu","그룹 활동 장면 그래프(GASG) 생성은 비디오 시퀀스에서 피사체와 개체 간의 관계를 예측하고 설명하는 것을 목표로 하는 컴퓨터 비전의 어려운 작업입니다. 전통적인 비디오 장면 그래프 생성(VidSGG) 방법은 회고적 분석에 중점을 두므로 예측 기능이 제한됩니다. 장면 이해 기능을 강화하기 위해 \textit{Appearance, Interaction, Position, Relationship 및 Situation} 속성과 관련된 미묘한 주석으로 JRDB 데이터 세트를 확장하는 GASG 데이터 세트를 도입했습니다. 이 작업은 또한 GASG 성능을 향상시키기 위해 흐름 네트워크 이론에 기초한 혁신적인 접근 방식인 \textbf{H}계층적 \textbf{Att}ention-\textbf{Flow}(HAtt-Flow) 메커니즘을 소개합니다. Flow-Attention은 흐름 보존 원칙을 통합하여 소스 경쟁을 촉진하고 싱크 할당을 촉진하여 사소한 주의 생성을 효과적으로 방지합니다. 우리가 제안한 접근 방식은 기존의 ""가치""와 ""키""가 각각 소스와 싱크로 변환되어 주의 기반 모델을 위한 새로운 프레임워크를 생성하는 주의 메커니즘에 대한 고유한 관점을 제공합니다. 광범위한 실험을 통해 우리는 Hatt-Flow 모델의 효율성과 제안된 Flow-Attention 메커니즘의 우수성을 입증합니다. 이 작업은 예측 비디오 장면 이해의 상당한 발전을 나타내며 비디오 데이터의 실시간 관계 예측이 필요한 애플리케이션에 귀중한 통찰력과 기술을 제공합니다."
514,http://arxiv.org/abs/2311.17058 ,Panoptic Video Scene Graph Generation,"Jingkang Yang, Wenxuan Peng, Xiangtai Li, Zujin Guo, Liangyu Chen, Bo Li, Zheng Ma, Kaiyang Zhou, Wayne Zhang, Chen Change Loy, Ziwei Liu","포괄적인 실제 시각적 인식 시스템을 구축하기 위해 PVSG(Panoptic Scene Graph Generation)라는 새로운 문제를 제안하고 연구합니다. PVSG는 기존 비디오 장면 그래프 생성(VidSGG) 문제와 관련이 있으며, 이는 비디오의 경계 상자를 기반으로 하는 인간과 객체 간의 시간적 상호 작용에 중점을 둡니다. 그러나 단단하지 않은 물체와 배경을 감지하는 데 있어 경계 상자의 제한으로 인해 VidSGG는 포괄적인 비디오 이해에 중요한 주요 세부 정보를 놓치는 경우가 많습니다. 이와 대조적으로 PVSG에서는 전체적인 장면 이해를 용이하게 하는 보다 정확한 픽셀 수준 분할 마스크를 사용하여 장면 그래프의 노드를 기반으로 해야 합니다. 이 새로운 영역에 대한 연구를 발전시키기 위해 우리는 400개의 비디오(289개의 3인칭 + 111개의 자기중심적 비디오)로 구성된 PVSG 데이터 세트를 제공하며, 이는 파놉틱 분할 마스크와 미세한 시간 장면 그래프로 레이블이 지정된 총 150,000개의 프레임을 포함합니다. 또한 다양한 기본 방법을 제공하고 향후 작업에 유용한 설계 사례를 공유합니다."
513,http://arxiv.org/abs/2311.17076 ,Compositional Chain-of-Thought Prompting for Large Multimodal Models,"Chancharik Mitra, Brandon Huang, Trevor Darrell, Roei Herzig","강력한 시각적 백본과 LLM(대형 언어 모델) 추론의 결합으로 인해 LMM(대형 다중 모달 모델)이 광범위한 VL(비전 및 언어) 작업에 대한 현재 표준이 되었습니다. 그러나 최근 연구에 따르면 가장 발전된 LMM조차도 객체 간의 속성 및 관계와 같은 구성적 시각적 추론 측면을 포착하는 데 여전히 어려움을 겪고 있는 것으로 나타났습니다. 한 가지 해결책은 장면 그래프(SG)를 활용하는 것입니다. 이는 시각적 영역과 텍스트 영역 사이의 가교로 광범위하게 사용되는 객체와 객체의 관계 및 속성을 형식화한 것입니다. 그러나 장면 그래프 데이터에는 장면 그래프 주석이 필요하며 수집 비용이 많이 들고 쉽게 확장할 수 없습니다. 더욱이 SG 데이터를 기반으로 LMM을 미세 조정하면 사전 훈련 목표를 망각하게 될 수 있습니다. 이를 극복하기 위해 생각 연쇄 방법에서 영감을 얻어 LMM에서 구성 지식을 추출하기 위해 SG 표현을 활용하는 새로운 제로샷 생각 연쇄 프롬프트 방법인 CCoT(Compositional Chain-of-Thought)를 제안합니다. 구체적으로, 먼저 LMM을 사용하여 SG를 생성한 다음 프롬프트에서 해당 SG를 사용하여 응답을 생성합니다. 광범위한 실험을 통해 우리는 제안된 CCoT 접근 방식이 여러 비전 및 언어 VL 구성 벤치마크에서 LMM 성능을 향상시킬 뿐만 아니라 미세 조정이나 주석이 달린 실제 SG 없이 일반 다중 모달 벤치마크에서 여러 인기 있는 LMM의 성능도 향상한다는 사실을 발견했습니다. 코드: https://github.com/chancharikmitra/CCoT"
512,http://arxiv.org/abs/2311.16492 ,VLPrompt: Vision-Language Prompting for Panoptic Scene Graph Generation,"Zijian Zhou, Miaojing Shi, Holger Caesar","PSG(Panoptic Scene Graph Generation)는 객체를 분할하고 객체 간의 관계를 예측하는 동시에 포괄적인 이미지 이해를 달성하는 것을 목표로 합니다. 그러나 관계 간의 롱테일 문제로 인해 실제 적용에서는 만족스럽지 못한 결과가 발생합니다. 기존의 방법들은 주로 시각정보에 의존하거나 객체명, 관계명 등 제한된 언어정보를 활용하여 언어정보의 유용성을 간과하였다. LLM(대형 언어 모델)의 최근 발전을 활용하여 특히 희귀 관계에 대한 관계 예측을 지원하기 위해 언어 정보를 사용할 것을 제안합니다. 이를 위해 이미지에서 비전 정보를, LLM에서 언어 정보를 획득하는 VLPrompt(Vision-Language Prompting) 모델을 제안합니다. 그리고 Attention 메커니즘을 기반으로 한 프롬프터 네트워크를 통해 정확한 관계 예측을 달성합니다. 우리의 광범위한 실험에 따르면 VLPrompt는 PSG 데이터 세트에 대한 이전의 최첨단 방법보다 훨씬 뛰어난 성능을 보여 언어 정보를 통합하고 관계의 롱테일 문제를 완화하는 효율성을 입증했습니다. 코드는 \url{https://github.com/franciszzj/TP-SIS}에서 확인할 수 있습니다."
511,http://arxiv.org/abs/2311.16479 ,Mitigating Hallucination in Visual Language Models with Visual Supervision,"Zhiyang Chen, Yousong Zhu, Yufei Zhan, Zhaowen Li, Chaoyang Zhao, Jinqiao Wang, Ming Tang","대형 시각 언어 모델(LVLM)은 환각으로 인해 고통을 많이 받으며 때때로 이미지 내용과 명백히 모순되는 반응을 생성합니다. 핵심 문제는 다중 모드 상황에서 세부적인 내용을 이해하는 능력이 약하다는 데 있는데, 이는 주로 훈련 데이터와 손실 함수의 두 가지 요소에 기인할 수 있습니다. 비전 명령 데이터 세트는 주로 전역 설명에 중점을 두고 있으며 자동 회귀 손실 기능은 이미지 이해보다는 텍스트 모델링을 선호합니다. 이 논문에서는 LVLM의 훈련을 용이하게 하기 위해 더 자세한 비전 주석과 더 식별력 있는 비전 모델을 제공하여 환각을 겪지 않고 더 정확한 반응을 생성할 수 있도록 합니다. 한편으로 우리는 Panoptic 장면 그래프 데이터 세트(PSG)에서 자세한 관계 주석을 사용하여 이미지-텍스트 쌍을 생성합니다. 이러한 대화는 이미지의 세부 사실에 더 많은 주의를 기울이고 모델이 다중 모드 컨텍스트를 기반으로 질문에 답하도록 장려합니다. 반면에 우리는 SAM과 마스크 예측 손실을 보조 감독으로 통합하여 LVLM이 상황 관련 객체를 식별할 수 있는 능력을 갖게 함으로써 보다 정확한 응답을 생성하고 환각을 완화할 수 있습니다. 또한 LVLM의 환각에 대한 심층적인 평가를 제공하기 위해 새로운 벤치마크인 RAH-Bench를 제안합니다. 환상 환각을 잘못된 범주, 속성 또는 관계로 이미지와 모순되는 세 가지 유형으로 나누고 각 유형에 대한 세부 하위 지표로 False Positive Rate를 도입합니다. 이 벤치마크에서 우리의 접근 방식은 원래 LLaVA에 비해 +8.4% 향상된 성능을 보여 주며 다른 모델 전반에 걸쳐 광범위한 성능 향상을 달성합니다."
510,http://arxiv.org/abs/2311.12889 ,Enhancing Scene Graph Generation with Hierarchical Relationships and Commonsense Knowledge,"Bowen Jiang, Zhijun Zhuang, Shreyas S. Shivakumar, Camillo J. Taylor","이 작업은 관계 계층 구조와 상식 지식을 모두 통합하여 장면 그래프 생성에 대한 향상된 접근 방식을 소개합니다. 구체적으로, 우리는 정보가 풍부한 계층 구조를 활용하는 계층 관계 헤드를 제안하는 것으로 시작합니다. 각 슈퍼 카테고리 아래의 세부 관계와 함께 이미지의 객체 쌍 간의 관계 슈퍼 카테고리를 공동으로 예측합니다. 이에 따라 우리는 기초 모델을 활용하여 장면 그래프 예측 시스템의 결과를 비판하고 작은 언어 전용 모델에서도 무의미한 조건자를 제거하는 강력한 상식 검증 파이프라인을 구현합니다. Visual Genome 및 OpenImage V6 데이터 세트에 대한 광범위한 실험은 제안된 모듈이 기존 장면 그래프 생성 알고리즘에 대한 플러그 앤 플레이 향상 기능으로 원활하게 통합될 수 있음을 보여줍니다. 결과는 데이터 세트 주석을 넘어서는 광범위한 합리적인 예측 세트를 통해 상당한 개선을 보여줍니다. 코드는 https://github.com/bowen-upenn/scene_graph_commonsense에서 확인할 수 있습니다."
509,http://arxiv.org/abs/2311.10988 ,Expanding Scene Graph Boundaries: Fully Open-vocabulary Scene Graph Generation via Visual-Concept Alignment and Retention,"Zuyao Chen, Jinlin Wu, Zhen Lei, Zhaoxiang Zhang, Changwen Chen","SGG(장면 그래프 생성)는 많은 컴퓨터 비전 애플리케이션에 중요한 구조화된 표현을 제공합니다. 그러나 기존 SGG 접근 방식은 폐쇄 집합 가정으로 인해 사전 정의된 개체 및 관계 범주만 인식하는 기능이 제한됩니다. 이를 극복하기 위해 SGG 시나리오를 노드와 에지에 따라 Closed-set SGG, Open Vocabulary(객체) 탐지 기반 SGG(OvD-SGG), Open Vocabulary Relation-based SGG(OvR-SGG), Open Vocabulary 탐지 + 관계 기반 SGG(OvD+R-SGG)의 네 가지 설정으로 분류합니다. 객체 중심 개방형 어휘 SGG가 최근 연구되었지만 관계 관련 개방형 어휘 SGG의 더 어려운 문제는 상대적으로 탐구되지 않은 상태로 남아 있습니다. 이러한 격차를 메우기 위해 우리는 전체적인 관점에서 완전 개방형 어휘 SGG를 향한 OvSGTR이라는 통합 프레임워크를 제안합니다. 제안된 프레임워크는 노드와 에지 모두에 대한 시각적 개념 정렬을 학습하여 모델이 보이지 않는 범주를 인식할 수 있도록 하는 엔드 투 엔드 변환기 아키텍처입니다. 관계 관련 개방형 어휘 SGG의 보다 어려운 설정을 위해 제안된 접근 방식은 이미지 캡션 데이터를 활용한 관계 인식 사전 훈련을 통합하고 지식 증류를 통해 시각적 개념 정렬을 유지합니다. Visual Genome 벤치마크에 대한 종합적인 실험 결과는 제안된 프레임워크의 효율성과 우수성을 입증합니다. 우리 코드는 https://github.com/gpt4vision/OvSGTR/에서 확인할 수 있습니다."
508,http://arxiv.org/abs/2311.06746 ,Two Stream Scene Understanding on Graph Embedding,"Wenkai Yang, Wenyuan Sun, Runxaing Huang","이 논문은 컴퓨터 비전에서 장면 이해를 향상시키기 위한 새로운 2-스트림 네트워크 아키텍처를 제시합니다. 이 아키텍처는 그래프 기능 스트림과 이미지 기능 스트림을 활용하여 이미지 분류 및 장면 그래프 생성 작업의 성능을 향상시키기 위해 두 가지 양식의 장점을 병합하는 것을 목표로 합니다. 그래프 특징 스트림 네트워크는 분할 구조, 장면 그래프 생성 및 그래프 표현 모듈로 구성됩니다. 분할 구조는 잔여 네트워크, Vit 또는 Swin Transformer가 될 수 있는 백본을 갖춘 UPSNet 아키텍처를 사용합니다. 장면 그래프 생성 구성 요소는 의미 지도에서 객체 레이블과 이웃 관계를 추출하여 장면 그래프를 만드는 데 중점을 둡니다. GCN(Graph Convolutional Networks), GraphSAGE 및 GAT(Graph Attention Networks)는 노드 기능 및 상호 연결 캡처에 중점을 두고 그래프 표현에 사용됩니다. 반면, 이미지 특징 스트림 네트워크는 Vision Transformer 및 Swin Transformer 모델을 사용하여 이미지 분류에 중점을 둡니다. 두 스트림은 다양한 데이터 융합 방법을 사용하여 융합됩니다. 이 융합은 그래프 기반 기능과 이미지 기반 기능의 상호 보완적인 장점을 활용하도록 설계되었습니다. ADE20K 데이터세트에서 수행된 실험은 제안된 2스트림 네트워크가 기존 방법에 비해 이미지 분류 정확도를 향상시키는 효과를 보여줍니다. 이 연구는 그래프 기반 접근 방식과 이미지 기반 접근 방식을 효과적으로 결합함으로써 컴퓨터 비전 분야, 특히 장면 이해 및 이미지 분류 분야에 상당한 기여를 합니다."
507,http://arxiv.org/abs/2311.04192 ,JaSPICE: Automatic Evaluation Metric Using Predicate-Argument Structures for Image Captioning Models,"Yuiga Wada, Kanta Kaneda, Komei Sugiura","이미지 캡션 연구는 BLEU 및 METEOR와 같은 자동 평가 지표에 크게 의존합니다. 그러나 이러한 n-gram 기반 측정항목은 사람의 평가와 낮은 상관관계를 보이는 것으로 나타났으며, 이로 인해 영어용 SPICE와 같은 대체 측정항목이 제안되었습니다. 그러나 다른 언어에 대해서는 동등한 측정항목이 확립되지 않았습니다. 따라서 본 연구에서는 장면 그래프를 기반으로 일본어 캡션을 평가하는 JaSPICE라는 자동 평가 메트릭을 제안합니다. 제안하는 방법은 종속성과 술어-인수 구조로부터 장면 그래프를 생성하고 동의어를 사용하여 그래프를 확장한다. 우리는 STAIR Captions 및 PFN-PIC에 대해 훈련된 10개의 이미지 캡션 모델을 사용하여 실험을 수행했으며 103,170개의 인간 평가가 포함된 Shichimi 데이터 세트를 구축했습니다. 결과는 우리의 지표가 인간 평가와의 상관 계수에 대한 기본 지표보다 우수한 것으로 나타났습니다."
506,http://arxiv.org/abs/2311.02247 ,PRISM: Progressive Restoration for Scene Graph-based Image Manipulation,"Pavel Jahoda, Azade Farshad, Yousef Yeganeh, Ehsan Adeli, Nassir Navab",장면 그래프는 이미지 생성 및 조작 작업에 대한 정확한 설명 사전으로 등장했지만 데이터의 개체 모양과 관계의 복잡성과 다양성으로 인해 이를 모델에 통합하고 고품질 결과를 생성하는 것이 어렵습니다. 이러한 과제를 해결하기 위해 우리는 장면에서 조작된 영역의 정확성과 품질을 향상시키기 위한 새로운 진보적 다중 헤드 이미지 조작 접근 방식인 PRISM을 제안합니다. 우리의 이미지 조작 프레임워크는 마스크된 영역이 외부 영역에서 내부 부분으로 점진적으로 마스크 해제되는 엔드투엔드 노이즈 제거 마스크 재구성 프록시 작업을 사용하여 훈련되었습니다. 우리는 장면의 맥락과 직접적인 상관관계가 있는 마스크 영역의 바깥 부분을 활용합니다. 또한 멀티 헤드 아키텍처는 전체 이미지 외에 세부적인 개체별 영역을 동시에 생성하여 고품질 이미지를 생성합니다. 우리 모델은 CLEVR 및 Visual Genome 데이터 세트의 의미 이미지 조작 작업에서 최첨단 방법보다 성능이 뛰어납니다. 우리의 결과는 장면 그래프 기반 이미지 조작의 품질과 정밀도를 향상시키기 위한 우리 접근 방식의 잠재력을 보여줍니다.
505,http://arxiv.org/abs/2311.01755 ,Towards a Unified Transformer-based Framework for Scene Graph Generation and Human-object Interaction Detection,"Tao He, Lianli Gao, Jingkuan Song, Yuan-Fang Li","장면 그래프 생성(SGG)과 인간-객체 상호 작용(HOI) 감지는 각각 객체 간의 관계, 인간과 객체 간의 상호 작용을 파악하고 인식하는 것을 목표로 하는 두 가지 중요한 시각적 작업입니다.   널리 사용되는 작업은 이러한 작업을 별개의 작업으로 처리하여 개별 데이터 세트에 맞는 작업별 모델을 개발합니다. 그러나 우리는 시각적 관계의 존재가 인간-객체 상호 작용의 추론을 크게 증가시키는 중요한 상황적, 복잡한 관계적 단서를 제공할 수 있다고 가정합니다. 이는 장면 그래프가 인간-객체 상호 작용을 추론하기 위한 소스 역할을 할 수 있는 두 작업 사이에 자연스러운 내재적 관계가 있는지 생각하도록 동기를 부여합니다. 이를 고려하여 Transformer 아키텍처를 기반으로 한 통합 원스텝 모델인 SG2HOI+를 소개합니다. 우리의 접근 방식은 두 개의 대화형 계층적 변환기를 사용하여 SGG 및 HOI 감지 작업을 원활하게 통합합니다. 구체적으로, 우리는 일련의 시각적 특징으로부터 관계 트리플을 생성하는 임무를 맡은 Transformer 관계를 시작합니다. 그 후, 생성된 관계 트리플을 기반으로 인간-객체 상호 작용을 예측하기 위해 또 다른 변환기 기반 디코더를 사용합니다. Visual Genome, V-COCO 및 HICO-DET를 포함한 확립된 벤치마크 데이터 세트에서 수행된 포괄적인 일련의 실험은 널리 사용되는 1단계 SGG 모델과 비교하여 SG2HOI+ 모델의 뛰어난 성능을 보여줍니다. 놀랍게도, 우리의 접근 방식은 최첨단 HOI 방법과 비교할 때 경쟁력 있는 성능을 달성합니다. 또한 우리는 SGG와 HOI 작업 모두에 대해 엔드투엔드 방식으로 공동 교육한 SG2HOI+가 개별화된 교육 패러다임에 비해 두 작업 모두에서 상당한 개선을 가져온다는 것을 확인했습니다."
504,http://arxiv.org/abs/2311.01192 ,Semantic Scene Graph Generation Based on an Edge Dual Scene Graph and Message Passing Neural Network,"Hyeongjin Kim, Sangwon Kim, Jong Taek Lee, Byoung Chul Ko","제너레이티브 AI와 함께 이미지 속 객체 간의 관계와 상호작용을 종합적으로 포착해 구조화된 그래프 기반 표현을 생성하는 장면 그래프 생성(SGG)에 대한 관심이 최근 크게 높아졌습니다. 그러나 객체 중심 및 이분법적 관계에 의존하는 기존 SGG 방법은 세부적인 관계를 정확하게 예측하는 능력이 제한되어 있습니다. 이러한 문제를 해결하기 위해 EdgeSGG(Edge Dual Scene Graph Generation)라고 하는 다중 개체 관계 모델링에 대한 새로운 접근 방식이 여기에서 제안됩니다. EdgeSGG는 제한되지 않은 개체 간의 풍부한 상황별 상호 작용을 캡처할 수 있는 Edge 이중 장면 그래프 및 DualMPNN(Dual Message Passing Neural Network)을 기반으로 합니다. 대칭형 그래프 구조로 엣지 듀얼 장면 그래프의 학습을 용이하게 하기 위해 제안된 DualMPNN은 관계 인식 컨텍스트를 보다 정확하게 예측하기 위해 객체 중심 기능과 관계 중심 기능을 모두 학습하고 객체 간의 세밀한 관계 업데이트를 허용합니다. SGG 운영을 위한 2개의 공개 데이터 세트와 3개의 하위 작업에 대한 6개의 지표를 사용하여 최첨단(SoTA) 방법을 사용한 비교 실험이 수행되었습니다. SoTA 접근 방식과 비교하여 제안된 모델은 모든 SGG 하위 작업에서 상당한 성능 향상을 나타냈습니다. 또한, 롱테일 분포에 대한 실험을 통해 개체 간의 관계를 통합하면 기존 롱테일 문제가 효과적으로 완화되는 것으로 나타났습니다."
503,http://arxiv.org/abs/2310.20357 ,Enhancing the Spatial Awareness Capability of Multi-Modal Large Language Model,"Yongqiang Zhao, Zhenyu Li, Zhi Jin, Feng Zhang, Haiyan Zhao, Chengfeng Dou, Zhengwei Tao, Xinhai Xu, Donghong Liu","MLLM(Multi-Modal Large Language Model)은 다중 모드 데이터를 수신하고 추론하는 기능을 갖춘 LLM(Large Language Model)의 확장을 의미합니다. 공간 인식은 객체 간, 객체와 장면 영역 간의 공간 관계를 이해하는 것과 관련된 다양한 기술을 포괄하는 MLLM의 중요한 능력 중 하나입니다. 자율 주행, 스마트 헬스케어, 로봇공학, 가상 및 증강 현실과 같은 산업에서는 MLLM의 공간 인식 기능이 크게 요구됩니다. 그러나 현재 MLLM의 공간 인식 기능과 인간의 요구 사항에 따른 요구 사항 사이에는 눈에 띄는 차이가 있습니다. 이 문제를 해결하기 위해 본 논문에서는 MLLM이 사용자 관련 문의에 보다 정확한 응답을 제공할 수 있도록 안내하기 위해 개체 간 보다 정확한 공간 위치 정보를 사용할 것을 제안합니다. 특히 특정 다중 모드 작업의 경우 기하학적 공간 정보 및 장면 그래프를 획득하는 알고리즘을 활용하여 쿼리에 포함된 개체의 관련 기하학적 공간 정보 및 장면 세부 정보를 얻습니다. 그런 다음 이 정보를 기반으로 MLLM에 사용자가 제기한 공간 인식 관련 쿼리를 처리하도록 지시합니다. MME, MM-Vet 및 기타 다중 모드 대형 언어 모델과 같은 벤치마크에서 광범위한 실험이 수행되었습니다. 실험 결과는 MLLM의 공간 인식 작업 및 관련 작업을 향상시키는 데 제안된 방법의 효율성을 철저히 확인했습니다."
502,http://arxiv.org/abs/2310.20159 ,Language Guided Visual Question Answering: Elevate Your Multimodal Language Model Using Knowledge-Enriched Prompts,"Deepanway Ghosal, Navonil Majumder, Roy Ka-Wei Lee, Rada Mihalcea, Soujanya Poria","VQA(시각적 질문 응답)는 이미지에 대한 질문에 대답하는 작업입니다. 이 작업에서는 자연어 답변을 제공하기 위해 이미지와 질문을 모두 이해한다고 가정합니다. VQA는 로봇공학, 교육, 의료 등 광범위한 분야에서의 잠재적인 응용 가능성으로 인해 최근 몇 년 동안 인기를 얻었습니다. 본 논문에서는 질문에 답하려면 상식적인 지식, 세계 지식, 이미지에 없는 아이디어와 개념에 대한 추론이 필요한 지식 증강 VQA에 중점을 둡니다. 질문에 보다 정확하게 답변하기 위해 근거, 이미지 캡션, 장면 그래프 등의 형태로 언어 안내(LG)를 사용하는 다중 모드 프레임워크를 제안합니다. 우리는 CLIP 및 BLIP 모델을 사용하여 A-OKVQA, Science-QA, VSR 및 IconQA 데이터 세트의 객관식 질문 답변 작업에 대한 방법을 벤치마킹합니다. 우리는 언어 지도의 사용이 시각적 질문 답변을 위한 간단하지만 강력하고 효과적인 전략임을 보여줍니다. 우리의 언어 안내는 까다로운 A-OKVQA 데이터 세트에서 CLIP의 성능을 7.6%, BLIP-2의 성능을 4.8% 향상시킵니다. 또한 제안된 언어 지침을 사용할 때 Science-QA, VSR 및 IconQA 데이터 세트의 성능이 지속적으로 향상되는 것을 관찰했습니다. LG-VQA 구현은 https://github.com/declare-lab/LG-VQA에서 공개적으로 제공됩니다."
501,http://arxiv.org/abs/2310.18235 ,Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-to-Image Generation,"Jaemin Cho, Yushi Hu, Roopal Garg, Peter Anderson, Ranjay Krishna, Jason Baldridge, Mohit Bansal, Jordi Pont-Tuset, Su Wang","텍스트-이미지 모델을 평가하는 것은 매우 어렵습니다. 텍스트-이미지 충실도를 평가하기 위한 최근의 강력한 접근 방식은 QG/A(질문 생성 및 응답)를 기반으로 합니다. 이는 사전 훈련된 기본 모델을 사용하여 프롬프트에서 일련의 질문과 답변을 자동으로 생성하고, 시각적 질문 응답 모델로 추출된 답변이 프롬프트 기반 답변과 일치하는지 여부에 따라 출력 이미지에 점수가 매겨집니다. 이러한 종류의 평가는 당연히 기본 QG 및 VQA 모델의 품질에 따라 달라집니다. 우리는 기존 QG/A 작업에서 몇 가지 신뢰성 문제를 식별하고 해결합니다. (a) QG 질문은 프롬프트(환각, 중복 및 누락 방지)를 존중해야 하며 (b) VQA 답변은 일관되어야 합니다(오토바이가 파란색이라고 주장하면서 이미지에 오토바이가 없다고 주장하지 않음). 우리는 모든 QG/A 프레임워크에 적용할 수 있는 공식적인 의미론에서 영감을 받은 경험적 기반 평가 프레임워크인 DSG(Davidsonian Scene Graph)를 통해 이러한 문제를 해결합니다. DSG는 (i) 적절한 의미 적용 범위를 보장하고 (ii) 일관되지 않은 답변을 회피하는 종속성 그래프로 구성된 원자적이고 고유한 질문을 생성합니다. 다양한 모델 구성(LLM, VQA 및 T2I)에 대한 광범위한 실험과 인간 평가를 통해 DSG가 위에서 언급한 문제를 해결한다는 것을 경험적으로 입증합니다. 마지막으로, 균형 잡힌 분포로 광범위한 세분화된 의미 범주를 포괄하는 1,060개의 프롬프트를 포함하는 오픈 소스 평가 벤치마크인 DSG-1k를 제시합니다. DSG-1k 프롬프트와 해당 DSG 질문을 공개합니다."
500,http://arxiv.org/abs/2310.17493 ,A Hybrid Graph Network for Complex Activity Detection in Video,"Salman Khan, Izzeddin Teeti, Andrew Bradley, Mohamed Elhoseiny, Fabio Cuzzolin","비디오의 해석과 이해는 다양한 분야에서 어려운 컴퓨터 비전 작업을 제시합니다. 자율주행과 스포츠 분석. 비디오 클립 내에서 발생하는 동작을 해석하는 기존 접근 방식은 일반적으로 단기 동작을 식별하는 TAL(Temporal Action Localization)을 기반으로 합니다. CompAD(복합 활동 감지)라는 새로운 분야는 비디오 내에서 발생하는 복잡한 활동의 ​​내부 구조를 모델링하여 얻은 더 깊은 이해를 바탕으로 이 분석을 장기적인 활동으로 확장합니다. 우리는 로컬(단기) 동적 장면을 인코딩하는 그래프에 적용되는 주의와 전반적인 장기 활동을 모델링하는 시간 그래프를 결합하는 하이브리드 그래프 신경망을 사용하여 CompAD 문제를 해결합니다. 우리의 접근 방식은 다음과 같습니다. i) 첫째, 우리는 각 비디오 조각에 대해 개별 객체를 감지하고 추적한 다음 전체 장면뿐만 아니라 모든 에이전트 튜브에서 3D 특징을 추출하여 (로컬) 장면의 활성 요소('에이전트')에 대한 시공간 '튜브'를 생성하는 새로운 특징 추출 기술을 제안합니다. ii) 다음으로, 각 노드(에이전트 튜브 또는 장면을 나타냄)가 다른 모든 노드에 연결되는 로컬 장면 그래프를 구성합니다. 그런 다음 이 그래프에 주의를 기울여 로컬 동적 장면의 전체 표현을 얻습니다. iii) 마지막으로 모든 로컬 장면 그래프 표현은 시간 그래프를 통해 상호 연결되어 시작 및 종료 시간과 함께 복잡한 활동 클래스를 추정합니다. 제안된 프레임워크는 ActivityNet-1.3, Thumos-14 및 ROAD를 포함한 세 가지 데이터 세트 모두에서 이전의 모든 최첨단 방법보다 성능이 뛰어납니다."
499,http://arxiv.org/abs/2310.16737 ,Translating Universal Scene Descriptions into Knowledge Graphs for Robotic Environment,"Giang Hoang Nguyen, Daniel Bessler, Simon Stelter, Mihai Pomarlan, Michael Beetz","인간 규모의 조작 작업을 수행하는 로봇은 유능하고 인간과 유사한 작업을 수행하기 위해 주변 환경에 대한 광범위한 지식이 필요합니다. 본 연구에서는 로봇 환경 모델링을 구현하기 위해 가상 현실 기술을 사용하는 방법을 조사하고 장면 그래프를 지식 기반으로 변환하는 기술을 제시합니다. 이를 위해 우리는 복잡한 환경의 작성, 시각화 및 시뮬레이션을 위한 새로운 표준인 Universal Scene Description(USD) 형식을 활용합니다. 우리는 USD 기반 환경 모델을 의미론적 쿼리 및 추가 지식 소스와의 통합을 용이하게 하는 지식 그래프(KG) 표현으로 변환하는 방법을 조사합니다."
498,http://arxiv.org/abs/2310.16494 ,Lang3DSG: Language-based contrastive pre-training for 3D Scene Graph prediction,"Sebastian Koch, Pedro Hermosilla, Narunas Vaskevicius, Mirco Colosi, Timo Ropinski","D 장면 그래프는 장면에 존재하는 객체와 그 관계를 모두 모델링하는 새로운 3D 장면 표현입니다. 그러나 3D 장면 그래프를 학습하는 것은 개체 레이블뿐만 아니라 데이터 세트에서 매우 부족한 관계 주석도 필요하기 때문에 어려운 작업입니다. 사전 훈련이 낮은 데이터 체제에서 모델 성능을 향상시키는 효과적인 접근 방식이라는 것이 널리 받아들여지고 있지만, 본 논문에서는 기존 사전 훈련 방법이 3D 장면 그래프에 적합하지 않다는 것을 발견했습니다. 이 문제를 해결하기 위해 우리는 장면 그래프와 언어 간의 강력한 관계를 활용하는 3D 장면 그래프에 대한 최초의 언어 기반 사전 학습 접근 방식을 제시합니다. 이를 위해 우리는 널리 사용되는 비전 언어 모델인 CLIP의 언어 인코더를 활용하여 해당 지식을 그래프 기반 네트워크로 추출합니다. 우리는 관계의 텍스트 임베딩(주어-술어-객체 삼중항)과 예측된 3D 그래프 기능을 정렬하는 대조 사전 학습을 공식화합니다. 우리의 방법은 사전 훈련 기준에 비해 향상된 효율성을 보여주고 기존의 모든 완전 지도 장면 그래프 예측 방법보다 훨씬 뛰어난 성능을 보여 주 의미론적 3D 장면 그래프 벤치마크에서 최첨단 결과를 달성합니다. 또한 장면 그래프 기능은 언어 정렬되어 있으므로 제로샷 방식으로 기능의 언어 공간을 쿼리할 수 있습니다. 본 논문에서는 추가 훈련 없이 장면의 방 유형을 예측하기 위해 이러한 특징의 속성을 활용하는 예를 보여줍니다."
497,http://arxiv.org/abs/2310.16073 ,FloCoDe: Unbiased Dynamic Scene Graph Generation with Temporal Consistency and Correlation Debiasing,Anant Khandelwal,"비디오에서 동적 장면 그래프 생성(SGG)을 수행하려면 장면 전체의 객체에 대한 포괄적인 이해뿐만 아니라 시간적 움직임과 다양한 객체와의 상호 작용을 캡처하는 방법도 필요합니다. 더욱이, 시각적 관계의 긴 꼬리 분포는 대부분의 동적 SGG 방법에서 중요한 병목 현상입니다. 이는 많은 사람들이 복잡한 아키텍처를 사용하여 시공간적 맥락을 포착하는 데 중점을 두어 편향된 장면 그래프를 생성하기 때문입니다. 이러한 문제를 해결하기 위해 우리는 FloCoDe: 편향되지 않은 동적 장면 그래프에 대한 불확실성 감쇠 기능을 갖춘 흐름 인식 시간 일관성 및 상관 관계 제거를 제안합니다. FloCoDe는 흐름을 사용하여 기능 워핑을 사용하여 프레임 전체에서 시간적으로 일관된 개체를 감지합니다. 시각적 관계의 롱테일 문제를 해결하기 위해 우리는 롱테일 클래스에 대한 편향되지 않은 관계 표현을 학습하기 위해 상관 편향 제거 및 레이블 상관 기반 손실을 제안합니다. 특히 우리는 일반적으로 동시 발생하는 관계를 포착하기 위해 대조 손실을 사용하여 레이블 상관 관계를 통합할 것을 제안합니다. 이는 긴 꼬리 클래스에 대한 강력한 표현을 학습하는 데 도움이 됩니다. 또한 SGG 데이터의 노이즈 주석을 처리하기 위해 불확실성 감쇠 기반 분류 프레임워크를 채택합니다. 광범위한 실험 평가를 통해 4.1%의 높은 성능 향상을 보여 보다 편견 없는 장면 그래프 생성의 우수성을 입증했습니다."
496,http://arxiv.org/abs/2310.15504 ,Cross-view Self-localization from Synthesized Scene-graphs,"Ryogo Yamamoto, Kanji Tanaka","크로스 뷰 자체 위치 파악은 데이터베이스 이미지가 희박한 관점에서 제공되는 시각적 장소 인식의 어려운 시나리오입니다. 최근에는 NeRF(Neural Radiance Fields) 기술을 사용하여 보이지 않는 시점에서 데이터베이스 이미지를 합성하는 접근 방식이 인상적인 성능으로 등장했습니다. 그러나 이러한 기술을 통해 제공되는 합성 이미지는 원본 이미지에 비해 품질이 떨어지는 경우가 많으며, 더욱이 데이터베이스의 저장 비용을 크게 증가시킨다. 본 연구에서는 원시 이미지에서 계산된 뷰 불변 모양 특징과 합성 이미지에서 계산된 뷰 종속 공간 의미 특징의 장점을 결합한 새로운 하이브리드 장면 모델을 탐색합니다. 이 두 가지 유형의 특징은 장면 그래프로 융합되고 그래프 신경망에 의해 압축적으로 학습되고 인식됩니다. 제안된 방법의 효율성은 사실적인 Habitat 시뮬레이터를 사용하여 생성된 보이지 않는 많은 뷰가 포함된 새로운 횡단 뷰 자체 위치 파악 데이터 세트를 사용하여 검증되었습니다."
495,http://arxiv.org/abs/2310.14356 ,Semantic and Expressive Variation in Image Captions Across Languages,"Andre Ye, Sebastin Santy, Jena D. Hwang, Amy X. Zhang, Ranjay Krishna","컴퓨터 비전은 종종 인간의 인식을 동질적인 것으로 간주합니다. 이는 시각적 자극이 모든 사람에게 유사하게 인식된다는 암묵적인 가정입니다. 이 가정은 연구자가 데이터 세트를 수집하고 비전 모델을 훈련하는 방식에 반영됩니다. 대조적으로, 다문화 심리학 및 언어학 분야의 문헌은 서로 다른 문화적 배경을 가진 사람들이 동일한 시각적 자극을 볼 때에도 매우 다른 개념을 관찰한다는 증거를 제공했습니다. 본 논문에서는 언어를 문화의 프록시로 사용하여 이러한 차이가 비전 언어 데이터 세트 및 모델에서 어떻게 나타나는지 연구합니다. 동일한 이미지에 대해 7개 언어에 걸쳐 생성된 텍스트 설명을 비교함으로써 의미론적 내용과 언어적 표현에서 상당한 차이를 발견합니다. 데이터 세트가 단일 언어가 아닌 다국어인 경우 설명은 평균적으로 더 높은 의미 범위를 가지며 범위는 장면 그래프, 모델 임베딩 및 언어 분류를 사용하여 측정됩니다. 예를 들어, 다국어 설명은 단일 언어 캡션 세트보다 평균 29.9% 더 많은 개체, 24.5% 더 많은 관계, 46.0% 더 많은 속성을 갖습니다. 이미지를 다른 언어로 설명하라는 메시지가 표시되면 인기 ​​모델(예: LLaVA)은 이러한 편견을 물려받아 이미지의 다른 부분을 설명합니다. 또한 한 언어의 캡션에 대한 미세 조정 모델은 해당 언어의 해당 테스트 데이터에서 가장 잘 수행되는 반면, 다국어 데이터에 대한 미세 조정은 모든 테스트 데이터 구성에서 일관되게 잘 수행됩니다. 우리의 작업은 컴퓨터 비전 커뮤니티에서 인간 인식의 다양성을 설명하고 포용해야 할 필요성을 지향합니다."
494,http://arxiv.org/abs/2310.10586 ,VidCoM: Fast Video Comprehension through Large Language Models with Multimodal Tools,"Ji Qi, Kaixuan Ji, Jifan Yu, Duokang Wang, Bin Xu, Lei Hou, Juanzi Li","비디오를 이해하고 특정 사용자 지침에 응답하는 모델을 구축하는 것은 비전 이해와 지식 추론에 대한 숙달이 필요하기 때문에 실용적이고 어려운 주제입니다. 언어 및 이미지 양식과 비교할 때 기존 연구에서는 간략한 설명과 결합된 대규모 희소 비디오에 대한 모델을 교육하므로 교육 효율성은 여전히 ​​심각한 문제로 남아 있습니다. 이 문서에서는 LLM(대형 언어 모델)을 활용하여 가벼운 시각적 도구를 사용하여 비디오에 대해 추론하는 빠른 적응형 프레임워크인 \textbf{VidCoM}을 소개합니다. 구체적으로, 특정 지시에 응답하는 핵심은 관련 비디오 이벤트에 초점을 맞추고, 구조화된 장면 그래프 생성과 설명 이미지 캡션 생성이라는 두 가지 시각적 도구를 활용하여 이벤트 정보를 수집하고 표현하는 것임을 밝힙니다. 따라서 세계 지식이 풍부한 LLM은 특정 비디오 이벤트에 대해 여러 추론 단계를 수행하여 응답을 달성하기 위한 추론 에이전트로 채택됩니다. 비디오 이벤트를 식별하는 LLM의 어려움을 해결하기 위해 InsOVER(명령 중심 비디오 이벤트 인식) 알고리즘을 추가로 제안합니다. 이 알고리즘은 언어 지침 분해와 비디오 이벤트 간의 효율적인 헝가리어 일치를 기반으로 해당 비디오 이벤트를 찾아 LLM이 확장된 비디오와 효과적으로 상호 작용할 수 있도록 합니다. 두 가지 일반적인 비디오 이해 작업에 대한 광범위한 실험에서는 제안된 조정 없는 프레임워크가 Flamingo-80B를 포함한 사전 훈련된 모델보다 성능이 뛰어나며 최첨단 성능을 달성한다는 것을 보여줍니다. 우리의 소스 코드와 시스템은 공개적으로 이용 가능합니다."
493,http://arxiv.org/abs/2310.10404 ,LLM4SGG: Large Language Models for Weakly Supervised Scene Graph Generation,"Kibum Kim, Kanghoon Yoon, Jaehyeong Jeon, Yeonjun In, Jinyoung Moon, Donghyun Kim, Chanyoung Park","WSSGG(Weakly-Supervised Scene Graph Generation) 연구는 비용이 많이 드는 주석에 크게 의존하는 완전 감독 방식의 대안으로 최근 등장했습니다. 이와 관련하여 WSSGG에 대한 연구는 주로 이미지 영역에 대한 지역화되지 않은 삼중항을 접지하는 데 초점을 맞추면서 지역화되지 않은 삼중항을 얻기 위해 이미지 캡션을 활용했습니다. 그러나 그들은 캡션에서 삼중항 형성 과정과 관련된 두 가지 문제를 간과했습니다. 1) 캡션에서 삼중항을 추출할 때 의미론적 과도한 단순화 문제가 발생하며, 캡션의 세밀한 술어가 바람직하지 않게 성긴 술어로 변환되어 긴 꼬리 술어 분포가 발생하고, 2) 캡션의 삼중항을 다음 엔터티/술어 클래스와 정렬할 때 저밀도 장면 그래프 문제가 발생합니다. 많은 삼중항이 폐기되고 훈련에 사용되지 않아 감독이 충분하지 않은 경우가 있습니다. 두 가지 문제를 해결하기 위해 우리는 약한 감독 SGG(LLM4SGG)를 위한 대규모 언어 모델과 같은 새로운 접근 방식을 제안합니다. 여기서는 캡션에서 삼중항을 추출하고 엔터티/술어 클래스를 대상 데이터와 정렬하는 동안 LLM의 언어에 대한 심층적인 이해와 추론 능력을 활용하여 두 가지 문제를 완화합니다. 이러한 프로세스에 LLM을 더 많이 참여시키기 위해 우리는 사고의 사슬 아이디어와 상황에 맞는 소수 학습 전략을 채택합니다. LLM4SGG의 효과를 검증하기 위해 우리는 Visual Genome 및 GQA 데이터 세트에 대한 광범위한 실험을 수행하여 최첨단 WSSGG 방법에 비해 Recall@K와 Mean Recall@K 모두에서 상당한 개선을 보여줍니다. 또 다른 매력은 LLM4SGG가 데이터 효율적이어서 적은 양의 훈련 이미지로 효과적인 모델 훈련이 가능하다는 것입니다."
492,http://arxiv.org/abs/2310.10338 ,Scene Graph Conditioning in Latent Diffusion,Frank Fundel,확산 모델은 이미지 생성에 탁월하지만 텍스트 프롬프트를 사용한 세부적인 의미 제어가 부족합니다. 이러한 제한을 해결하기 위해 추가 기술이 개발되었습니다. 그러나 텍스트 기반 설명에만 컨디셔닝 확산 모델을 적용하는 것은 모호함과 구조 부족으로 인해 어렵습니다. 이와 대조적으로 장면 그래프는 이미지 내용을 보다 정확하게 표현하므로 이미지 생성 모델의 세밀한 제어 및 정확한 합성에 탁월합니다. 이미지와 장면 그래프 데이터의 양이 부족하여 대규모 확산 모델을 미세 조정하는 것이 어렵습니다. 우리는 ControlNet과 Gated Self-Attention을 사용하여 이 문제를 해결하기 위한 여러 가지 접근 방식을 제안합니다. 우리는 제안된 방법을 사용하면 장면 그래프에서 이전 방법보다 훨씬 더 높은 품질의 이미지를 생성할 수 있음을 보여줄 수 있었습니다. 우리의 소스 코드는 https://github.com/FrankFundel/SGCond에서 공개적으로 제공됩니다.
491,http://arxiv.org/abs/2310.07573 ,Relational Prior Knowledge Graphs for Detection and Instance Segmentation,"Osman Ülger, Yu Wang, Ysbrand Galama, Sezer Karaoglu, Theo Gevers, Martin R. Oswald",인간은 사물 간의 관계를 이해함으로써 주변 세계를 인지하고 추론하는 놀라운 능력을 가지고 있습니다. 본 논문에서는 객체 감지 및 인스턴스 분할을 위해 이러한 관계를 사용하는 효과를 조사합니다. 이를 위해 우리는 관계형 사전 기반 기능 향상 모델(RP-FEM)을 제안합니다. 이는 관계형 사전을 사용하여 객체 제안 기능을 향상시키는 그래프 변환기입니다. 제안된 아키텍처는 초기 제안에서 얻은 장면 그래프를 기반으로 작동하며 객체 감지 및 인스턴스 분할을 위한 관계형 컨텍스트 모델링을 동시에 학습하는 것을 목표로 합니다. COCO에 대한 실험적 평가는 관계형 사전 분석으로 강화된 장면 그래프의 활용이 객체 감지 및 인스턴스 분할에 이점을 제공한다는 것을 보여줍니다. RP-FEM은 이미지 내에서 있을 법하지 않은 클래스 예측을 억제하는 동시에 모델이 중복 예측을 생성하는 것을 방지하여 기본 모델보다 개선되는 능력을 보여줍니다.
490,http://arxiv.org/abs/2310.07056 ,TextPSG: Panoptic Scene Graph Generation from Textual Descriptions,"Chengyang Zhao, Yikang Shen, Zhenfang Chen, Mingyu Ding, Chuang Gan","최근에는 포괄적인 장면 이해를 위해 Panoptic Scene Graph가 제안되었습니다. 그러나 이전 연구에서는 완전 지도 학습 방식을 채택하여 픽셀 단위로 조밀하게 주석이 달린 대량의 데이터가 필요하며, 이를 얻는 데 항상 지루하고 비용이 많이 듭니다. 이러한 한계를 해결하기 위해 우리는 순수 텍스트 설명(Caption-to-PSG)에서 Panoptic 장면 그래프 생성의 새로운 문제를 연구합니다. 핵심 아이디어는 웹에 있는 무료 이미지 캡션 데이터의 대규모 컬렉션을 활용하여 파노라마 장면 그래프를 생성하는 것입니다. 이 문제는 세 가지 제약 조건으로 인해 매우 어렵습니다. 1) 사전 위치 없음; 2) 시각적 영역과 텍스트 엔터티 사이에 명시적인 링크가 없습니다. 3) 사전 정의된 개념 세트가 없습니다. 이 문제를 해결하기 위해 우리는 여러 가지 새로운 기술을 사용하여 영역 그룹화, 엔터티 접지, 세그먼트 병합 및 레이블 생성기 등 4개의 모듈로 구성된 새로운 프레임워크 TextPSG를 제안합니다. 영역 그룹화기는 먼저 이미지 픽셀을 서로 다른 세그먼트로 그룹화하고 엔터티 접지기는 참조되는 세그먼트의 텍스트 설명을 기반으로 시각적 세그먼트를 언어 엔터티와 정렬합니다. 따라서 접지 결과는 세그먼트 병합이 세그먼트 유사성을 학습할 수 있도록 하는 의사 레이블 역할을 할 수 있을 뿐만 아니라 레이블 생성기가 객체 의미 및 관계 조건을 학습하도록 안내하여 세밀하게 구조화된 장면을 이해할 수 있게 해줍니다. 우리의 프레임워크는 효과적이며 기준선보다 훨씬 뛰어난 성능을 발휘하고 강력한 배포 외 견고성을 달성합니다. 우리는 포괄적인 절제 연구를 수행하여 설계 선택의 효율성을 확증하고 향후 방향을 강조하기 위한 심층 분석을 제공합니다. 우리의 코드, 데이터 및 결과는 프로젝트 페이지(https://textpsg.github.io/)에서 확인할 수 있습니다."
489,http://arxiv.org/abs/2310.05867 ,Domain-wise Invariant Learning for Panoptic Scene Graph Generation,"Li Li, You Qin, Wei Ji, Yuxiao Zhou, Roger Zimmermann",PSG(Panoptic Scene Graph Generation)에는 객체 감지 및 해당 관계(술어) 예측이 포함됩니다. 그러나 편향된 조건부 주석의 존재는 PSG 모델에 중요한 과제를 제기합니다. 이는 서로 다른 조건부 사이에 명확한 결정 경계를 설정하는 능력을 방해하기 때문입니다. 이 문제는 PSG 모델의 실질적인 유용성과 실제 적용성을 실질적으로 방해합니다. 위의 본질적인 편향을 해결하기 위해 우리는 각 주제-객체 쌍(도메인) 내에서 조건자 예측 위험을 측정하여 잠재적으로 편향된 주석을 추론하고 불변 조건자 표현 임베딩을 학습하여 편향된 주석을 일관된 주석으로 적응적으로 전송하는 새로운 프레임워크를 제안합니다. 실험에 따르면 우리의 방법은 벤치마크 모델의 성능을 크게 향상시켜 새로운 최첨단 성능을 달성하고 PSG 데이터 세트에 대한 뛰어난 일반화 및 효율성을 보여줍니다.
488,http://arxiv.org/abs/2310.01842 ,SelfGraphVQA: A Self-Supervised Graph Neural Network for Scene-based Question Answering,"Bruno Souza, Marius Aasan, Helio Pedrini, Adín Ramírez Rivera","인식과 추론 사이의 원활한 통합에 대한 관심이 높아짐에 따라 비전과 언어의 교차점은 주요 관심사입니다. 장면 그래프(SG)는 다중 모드 이미지 분석을 위한 유용한 도구로 등장하여 VQA(시각적 질문 답변)와 같은 작업에서 인상적인 성능을 보여줍니다. 이 연구에서는 VQA 작업에서 장면 그래프의 효율성에도 불구하고 이상화된 주석이 달린 장면 그래프를 활용하는 현재 방법이 이미지에서 추출된 예측 장면 그래프를 사용할 때 일반화하는 데 어려움을 겪고 있음을 보여줍니다. 이 문제를 해결하기 위해 SelfGraphVQA 프레임워크를 소개합니다. 우리의 접근 방식은 사전 훈련된 장면 그래프 생성기를 사용하여 입력 이미지에서 장면 그래프를 추출하고 자기 감독 기술을 사용하여 의미 보존 증강을 사용합니다. 이 방법은 비용이 많이 들고 잠재적으로 편향된 주석이 달린 데이터의 필요성을 회피함으로써 VQA 작업에서 그래프 표현의 활용도를 향상시킵니다. 이미지 증대를 통해 추출된 그래프의 대체 뷰를 생성함으로써 정규화되지 않은 대조 접근 방식을 사용하여 표현의 정보 콘텐츠를 최적화함으로써 공동 임베딩을 학습할 수 있습니다. SG를 사용하면서 우리는 노드별, 그래프별, 순열 등변 정규화라는 세 가지 고유한 최대화 전략을 실험합니다. 우리는 VQA를 위해 추출된 장면 그래프의 효율성을 경험적으로 보여주고 이러한 접근 방식이 시각적 정보의 중요성을 강조하여 전반적인 성능을 향상시킨다는 것을 보여줍니다. 이는 복잡한 추론 질문에 대해 SG에 의존하는 VQA 작업에 대한 보다 실용적인 솔루션을 제공합니다."
487,http://arxiv.org/abs/2310.01636 ,Adaptive Visual Scene Understanding: Incremental Scene Graph Generation,"Naitik Khandelwal, Xiao Liu, Mengmi Zhang","SGG(장면 그래프 생성)는 이미지를 분석하여 개체와 개체 관계에 대한 의미 있는 정보를 추출합니다. 역동적인 시각적 세계에서는 AI 시스템이 지속적으로 새로운 객체를 감지하고 기존 객체와의 관계를 설정하는 것이 중요합니다. 최근에는 객체 감지 및 이미지 인식 영역 내에서 지속적인 학습에 중점을 둔 수많은 연구가 이루어졌습니다. 그러나 제한된 양의 연구는 SGG의 보다 어려운 지속적인 학습 문제에 초점을 맞추고 있습니다. 이러한 증가된 어려움은 개체 및 관련 컨텍스트 간의 복잡한 상호 작용과 동적 관계로 인해 발생합니다. 따라서 지속적인 학습에서 SGG 모델은 적응형 시각적 장면 이해 프로세스 내에서 장면 그래프를 확장, 수정, 유지 및 추론해야 하는 경우가 많습니다. 연속 장면 그래프 생성(CSEGG)을 체계적으로 탐색하기 위해 관계 증분, 장면 증분 및 관계 일반화의 세 가지 학습 방식으로 구성된 포괄적인 벤치마크를 제시합니다. 또한 RAS라는 ""합성 분석을 통한 재생"" 방법을 소개합니다. 이 접근 방식은 장면 그래프를 활용하고 이를 분해 및 재구성하여 다양한 장면을 표현하고 이러한 구성 장면 그래프를 기반으로 합성된 장면을 재생합니다. 재생된 합성 장면은 알려진 환경과 알려지지 않은 환경에서 SGG 숙련도를 연습하고 개선하는 수단 역할을 합니다. 우리의 실험 결과는 기존 연속 학습 방법을 SGG 백본과 직접 결합하는 과제를 강조할 뿐만 아니라 제안된 접근 방식의 효과를 통해 CSEGG 효율성을 향상시키는 동시에 개인 정보 보호 및 메모리 사용을 보존합니다. 모든 데이터와 소스 코드는 온라인에서 공개됩니다."
486,http://arxiv.org/abs/2310.01356 ,Less is More: Toward Zero-Shot Local Scene Graph Generation via Foundation Models,"Shu Zhao, Huijuan Xu","인간은 본질적으로 선택적 시각적 지각을 통해 사물을 인식하고, 시각 영역의 특정 영역을 구조화된 상징적 지식으로 변환하며, 인간의 목표에 맞춰 제한된 주의 자원 할당을 기반으로 영역 간의 관계를 추론합니다. 인간에게는 직관적이지만 현대의 인식 시스템은 복잡한 인지 능력과 필요한 상식적 지식으로 인해 구조적 정보를 추출하는 데 어려움을 겪습니다. 이러한 격차를 메우기 위해 로컬 장면 그래프 생성이라는 새로운 작업을 제시합니다. 이미지의 모든 객체와 관계를 생성하는 기존의 장면 그래프 생성 작업과 달리, 우리가 제안하는 작업은 고급 이해력과 추론 능력이 요구되는 다운스트림 작업을 강화하기 위해 부분 객체와 그 관계로 관련 구조 정보를 추상화하는 것을 목표로 합니다. 이에 따라 우리는 강력한 인식과 상식적 추론으로 유명한 기반 모델을 활용하는 프레임워크인 zEro-shot Local Scene Graph GeneraTion(ELEGANT)을 소개합니다. 여기서는 기반 모델 간의 협업과 정보 통신이 우수한 결과를 낳고 라벨링된 감독 없이 제로샷 로컬 장면 그래프 생성을 실현합니다. 또한 제한된 레이블 공간을 초월하여 이전의 폐쇄형 평가 메트릭을 능가하고 더 광범위한 평가를 제공하는 새로운 개방형 평가 메트릭인 엔터티 수준 CLIPScorE(ECLIPSE)를 제안합니다. 실험 결과에 따르면 우리의 접근 방식은 개방형 평가 설정에서 기준선보다 훨씬 뛰어난 성능을 보였고 근접 설정 설정에서 이전 방법에 비해 최대 24.58%의 상당한 성능 향상을 달성하여 제안된 프레임워크의 효율성과 강력한 추론 능력을 입증했습니다."
485,http://arxiv.org/abs/2310.00712 ,Logical Bias Learning for Object Relation Prediction,"Xinyu Zhou, Zihan Ji, Anna Zhu",장면 그래프 생성(SGG)은 장면을 더 잘 이해할 수 있도록 이미지를 의미 구조 그래프에 자동으로 매핑하는 것을 목표로 합니다. 개체 및 관계 정보를 제공하여 다운스트림 작업에 대한 그래프 추론을 가능하게 하는 기능으로 인해 상당한 주목을 받았습니다. 그러나 편향된 데이터와 훈련 방법으로 인해 실제로는 심각한 한계에 직면해 있습니다. 본 논문에서는 객체 관계 예측을 위한 인과 추론을 기반으로 하는 보다 합리적이고 효과적인 전략을 제시합니다. 우리 전략의 우월성을 추가로 평가하기 위해 절제 연구를 수행하기 위한 개체 향상 모듈을 제안합니다. Visual Gnome 150(VG-150) 데이터 세트에 대한 실험 결과는 제안된 방법의 효율성을 보여줍니다. 이러한 기여는 의사결정을 위한 기초 모델에 큰 잠재력을 제공할 수 있습니다.
484,http://arxiv.org/abs/2310.00670 ,A Hierarchical Graph-based Approach for Recognition and Description Generation of Bimanual Actions in Videos,"Fatemeh Ziaeetabar, Reza Safabakhsh, Saeedeh Momtazi, Minija Tamosiunaite, Florentin Wörgötter","비디오의 (양손) 조작 작업에 대한 미묘한 이해와 자세한 설명 콘텐츠 생성은 로봇 공학, 인간-컴퓨터 상호 작용 및 비디오 콘텐츠 분석과 같은 분야에 중요합니다. 이 연구는 그래프 기반 모델링과 계층적 계층 주의 메커니즘을 통합하여 비디오 설명의 정확성과 포괄성을 높이는 새로운 방법을 설명합니다. 이를 달성하기 위해 먼저 장면 그래프를 사용하여 객체와 작업 간의 시공간적 상호 종속성을 인코딩하고 두 번째 단계에서 이를 GAT(Graph Attention Network)를 사용하여 계층적 주의 메커니즘을 생성하는 새로운 3단계 아키텍처와 결합합니다. 3레벨 GAT 아키텍처를 사용하면 로컬 요소는 물론 글로벌 컨텍스트 요소도 인식할 수 있습니다. 이러한 방식으로 동일한 비디오 클립에 대해 의미론적 복잡성이 서로 다른 여러 설명을 병렬로 생성할 수 있어 동작 인식 및 동작 설명의 차별적 정확성이 향상됩니다. 우리 접근 방식의 성능은 여러 2D 및 3D 데이터 세트를 사용하여 경험적으로 테스트되었습니다. 우리의 방법을 최신 기술과 비교함으로써 우리는 동작 인식 및 설명 생성을 평가할 때 정확성, 정밀성 및 상황적 관련성과 관련하여 지속적으로 더 나은 성능을 얻습니다. 대규모 절제 실험에서 우리는 모델의 다양한 구성 요소의 역할도 평가합니다. 우리의 다단계 접근 방식을 통해 시스템은 다른 사람들이 작성한 설명에서도 종종 관찰되는 다양한 의미론적 설명 깊이를 얻습니다. 또한, 우리 모델을 통해 달성된 양손 손 개체 상호 작용에 대한 더 나은 통찰력은 로봇 공학 분야의 발전을 예고하여 복잡한 인간 행동을 향상된 정밀도로 에뮬레이션할 수 있게 해줍니다."
483,http://arxiv.org/abs/2309.16650 ,ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning,"Qiao Gu, Alihusein Kuwajerwala, Sacha Morin, Krishna Murthy Jatavallabhula, Bipasha Sen, Aditya Agarwal, Corban Rivera, William Paul, Kirsty Ellis, Rama Chellappa, Chuang Gan, Celso Miguel de Melo, Joshua B. Tenenbaum, Antonio Torralba, Florian Shkurti, Liam Paull",로봇이 다양한 작업을 수행하려면 의미가 풍부하면서도 작업 중심 인식 및 계획을 위해 간결하고 효율적인 세계의 3D 표현이 필요합니다. 최근 접근 방식에서는 대규모 비전 언어 모델의 기능을 활용하여 3D 표현의 의미를 인코딩하려고 시도했습니다. 그러나 이러한 접근 방식은 더 큰 환경에서 잘 확장되지 않는 포인트별 특징 벡터가 있는 맵을 생성하는 경향이 있으며 다운스트림 계획에 유용한 환경 내 엔터티 간의 의미론적 공간 관계를 포함하지도 않습니다. 본 연구에서는 3D 장면을 위한 개방형 어휘 그래프 구조 표현인 ConceptGraphs를 제안합니다. ConceptGraphs는 2D 기반 모델을 활용하고 다중 뷰 연결을 통해 출력을 3D로 융합하여 구축되었습니다. 결과 표현은 대규모 3D 데이터 세트를 수집하거나 모델을 미세 조정할 필요 없이 새로운 의미 클래스로 일반화됩니다. 우리는 추상(언어) 프롬프트를 통해 지정되고 공간 및 의미 개념에 대한 복잡한 추론이 필요한 여러 다운스트림 계획 작업을 통해 이 표현의 유용성을 보여줍니다. (프로젝트 페이지: https://concept-graphs.github.io/ 설명 영상: https://youtu.be/mRhNkQwRYnc )
482,http://arxiv.org/abs/2309.15940 ,Context-Aware Entity Grounding with Open-Vocabulary 3D Scene Graphs,"Haonan Chang, Kowndinya Boyalakuntla, Shiyang Lu, Siwei Cai, Eric Jing, Shreesh Keskar, Shijie Geng, Adeeb Abbas, Lifeng Zhou, Kostas Bekris, Abdeslam Boularias","우리는 자유 형식의 텍스트 기반 쿼리를 사용하여 객체 인스턴스, 에이전트 및 영역과 같은 다양한 엔터티를 기반으로 하는 공식 프레임워크인 Open-Vocabulary 3D 장면 그래프(OVSG)를 제시합니다. 기존의 의미 기반 객체 위치 파악 접근 방식과 달리, 우리 시스템은 상황 인식 엔터티 위치 파악을 용이하게 하여 '식탁 위의 컵 집기' 또는 '누군가 앉아 있는 소파로 이동'과 같은 쿼리를 허용합니다. 3D 장면 그래프에 대한 기존 연구와 달리 OVSG는 자유로운 형식의 텍스트 입력과 개방형 어휘 쿼리를 지원합니다. ScanNet 데이터 세트와 자체 수집 데이터 세트를 사용한 일련의 비교 실험을 통해 우리는 제안한 접근 방식이 이전 의미 기반 위치 파악 기술의 성능을 크게 능가한다는 것을 보여줍니다. 또한 실제 로봇 탐색 및 조작 실험에서 OVSG의 실제 적용을 강조합니다."
481,http://arxiv.org/abs/2309.15702 ,SGRec3D: Self-Supervised 3D Scene Graph Learning via Object-Level Scene Reconstruction,"Sebastian Koch, Pedro Hermosilla, Narunas Vaskevicius, Mirco Colosi, Timo Ropinski","3D 장면 이해 분야에서 3D 장면 그래프는 객체와 그 관계에 대한 기하학적, 의미적 정보를 결합한 새로운 장면 표현으로 등장했습니다. 그러나 완전한 감독 방식으로 의미론적 3D 장면 그래프를 학습하는 것은 객체 수준 주석뿐만 아니라 관계 레이블도 필요하기 때문에 본질적으로 어렵습니다. 사전 훈련 접근 방식은 다양한 분야에서 많은 방법의 성능을 향상시키는 데 도움이 되었지만, 3D 장면 그래프 예측을 위한 사전 훈련은 거의 주목을 받지 못했습니다. 또한, 이 논문에서는 고전적인 대비 포인트 클라우드 기반 사전 학습 접근 방식이 3D 장면 그래프 학습에 효과적이지 않다는 것을 발견했습니다. 이를 위해 우리는 3차원 장면 그래프 예측을 위한 새로운 자기 지도 사전 학습 방법인 SGRec3D를 제시합니다. 우리는 프리텍스트 작업으로 그래프 병목 현상으로부터 3D 입력 장면을 재구성하는 것을 제안합니다. SGRec3D 사전 훈련에는 객체 관계 레이블이 필요하지 않으므로 이전에는 3D 장면 그래프 학습이 불가능했던 대규모 3D 장면 이해 데이터 세트를 활용할 수 있습니다. 우리의 실험은 최근의 포인트 클라우드 기반 사전 훈련 접근 방식과 달리 제안된 사전 훈련이 3D 장면 그래프 예측을 상당히 향상시켜 SOTA 성능을 발휘하고 객체 예측에서 다른 3D 장면 그래프 모델보다 10%, 관계 예측에서 4% 더 나은 성능을 발휘한다는 것을 보여줍니다. 또한 미세 조정 중에 10% 레이블이 지정된 데이터의 작은 하위 집합만 사용하면 사전 학습 없이 동일한 모델의 성능을 능가하는 데 충분하다는 것을 보여줍니다."
480,http://arxiv.org/abs/2311.12820 ,MSG-BART: Multi-granularity Scene Graph-Enhanced Encoder-Decoder Language Model for Video-grounded Dialogue Generation,"Hongcheng Liu, Zhe Chen, Hui Li, Pingjie Wang, Yanfeng Wang, Yu Wang","영상을 기반으로 한 대화를 생성하려면 영상 속 시각적 장면에 대한 높은 수준의 이해와 추론이 필요합니다. 그러나 기존의 대규모 시각적 언어 모델은 특히 시공간 관계 추론과 관련하여 잠재 기능과 디코더 전용 구조로 인해 효과적이지 않습니다. 본 논문에서는 다중 입도 시공간 장면 그래프를 인코더-디코더 사전 훈련된 언어 모델에 통합하여 비디오 정보의 통합을 향상시키는 MSG-BART라는 새로운 접근 방식을 제안합니다. 특히, 우리는 전반적인 인식과 대상 추론 능력을 모두 향상시키기 위해 글로벌 및 로컬 장면 그래프를 각각 인코더와 디코더에 통합합니다. 정보 선택 기능을 더욱 향상시키기 위해 텍스트와 비디오 사이의 선택을 용이하게 하는 다중 포인터 네트워크를 제안합니다. 다양한 최첨단 접근 방식과 비교하여 제안된 MSG-BART의 상당한 우수성을 보여주는 세 가지 비디오 기반 대화 벤치마크에 대한 광범위한 실험이 수행되었습니다."
479,http://arxiv.org/abs/2309.14538 ,Dynamic Scene Graph Representation for Surgical Video,"Felix Holm, Ghazal Ghazaei, Tobias Czempiel, Ege Özsoy, Stefan Saur, Nassir Navab","현미경 또는 내시경 영상 장치에서 캡처한 수술 비디오는 풍부하지만 오랜 시간 동안 사용되는 다양한 도구와 해부학적 구조를 묘사하는 복잡한 정보 소스입니다. 중요한 작업 흐름 정보를 포함하고 많은 절차에서 일반적으로 녹화됨에도 불구하고 자동화된 수술 작업 흐름을 이해하기 위한 수술 비디오의 사용은 여전히 ​​제한적입니다.   이 작업에서 우리는 모든 해부학적 구조, 도구 및 상호 작용을 인코딩하는 동시에 수술 비디오를 표현하기 위해 보다 전체적이고 의미상 의미가 있으며 사람이 읽을 수 있는 방법으로 장면 그래프를 활용합니다. 솔루션의 영향을 적절하게 평가하기 위해 CaDIS 및 CATARACTS 데이터세트의 의미론적 분할을 통해 장면 그래프 데이터세트를 생성합니다. 우리는 그래프 컨벌루션 네트워크(GCN)를 사용하여 장면 그래프를 활용하여 경쟁력 있는 성능으로 수술 워크플로우 인식과 같은 수술 다운스트림 작업을 처리할 수 있음을 보여줍니다. 또한 임상 환경에서 중요한 모델 결정의 설명 가능성 및 견고성과 관련하여 수술 장면 그래프의 이점을 보여줍니다."
478,http://arxiv.org/abs/2309.13237 ,Spatial-Temporal Knowledge-Embedded Transformer for Video Scene Graph Generation,"Tao Pu, Tianshui Chen, Hefeng Wu, Yongyi Lu, Liang Lin","비디오 장면 그래프 생성(VidSGG)은 시각적 장면에서 객체를 식별하고 주어진 비디오에 대한 관계를 추론하는 것을 목표로 합니다. 전체 장면에 흩어져 있는 각 개체에 대한 포괄적인 이해뿐만 아니라 시간적 움직임과 상호 작용에 대한 심층적인 분석도 필요합니다. 본질적으로 개체 쌍과 그 관계는 각 이미지 내의 공간적 동시 발생 상관 관계와 다양한 이미지 간의 시간적 일관성/전환 상관 관계를 누리며, 이는 VidSGG 모델 학습 및 추론을 용이하게 하는 사전 지식 역할을 할 수 있습니다. 본 연구에서는 보다 대표적인 관계 표현을 학습하기 위해 이전의 시공간 지식을 다중 헤드 교차 주의 메커니즘에 통합하는 STKET(공간-시간 지식 내장 변환기)를 제안합니다. 구체적으로, 우리는 먼저 통계적 방식으로 공간적 동시발생과 시간적 전환 상관관계를 학습합니다. 그런 다음, 시각적 표현과 지식 사이의 상호 작용을 완전히 탐색하여 각각 공간 및 시간 내장 표현을 생성하기 위해 다중 헤드 교차 주의 메커니즘을 도입하는 공간 및 시간 지식 내장 레이어를 설계합니다. 마지막으로 각 주제-객체 쌍에 대해 이러한 표현을 집계하여 최종 의미 레이블과 그 관계를 예측합니다. 광범위한 실험에 따르면 STKET은 현재 경쟁 알고리즘보다 큰 차이로 성능이 뛰어난 것으로 나타났습니다. 예를 들어 현재 알고리즘에 비해 다양한 설정에서 mR@50을 8.1%, 4.7%, 2.1% 개선했습니다."
477,http://arxiv.org/abs/2309.12188 ,SG-Bot: Object Rearrangement via Coarse-to-Fine Robotic Imagination on Scene Graphs,"Guangyao Zhai, Xiaoni Cai, Dianye Huang, Yan Di, Fabian Manhardt, Federico Tombari, Nassir Navab, Benjamin Busam","객체 재배열은 로봇 환경 상호 작용에서 중추적인 역할을 하며, 구현된 AI의 중요한 기능을 나타냅니다. 본 논문에서는 장면 표현으로 장면 그래프를 사용하여 대략적에서 정밀한 방식을 활용하는 새로운 재배열 프레임워크인 SG-Bot을 제시합니다. 알려진 목표 사전 설정 또는 제로 샷 대형 모델에 의존하는 이전 방법과 달리 SG-Bot은 상식적인 지식 고려와 자동 생성 기능을 원활하게 혼합하여 경량, 실시간 및 사용자 제어 가능 특성을 보여줍니다. SG-Bot은 관찰, 상상, 실행이라는 세 가지 절차를 통해 작업을 적절하게 처리합니다. 처음에는 관찰하는 동안 어수선한 장면에서 객체를 식별하고 추출합니다. 이러한 객체는 먼저 상식이나 사용자 정의 기준에 따라 대략적으로 구성되고 장면 그래프 내에 표시됩니다. 그런 다음 이 장면 그래프는 초기 장면의 모양 정보와 객체 의미론을 고려하여 세분화된 골 장면을 형성하는 생성 모델을 순차적으로 알려줍니다. 마지막으로 실행을 위해 초기 목표 장면과 구상 목표 장면을 일치시켜 로봇 행동 정책을 수립합니다. 실험 결과는 SG-Bot이 경쟁사보다 훨씬 뛰어난 성능을 보인다는 것을 보여줍니다."
476,http://arxiv.org/abs/2309.10461 ,Vision-based Situational Graphs Exploiting Fiducial Markers for the Integration of Semantic Entities,"Ali Tourani, Hriday Bavle, Jose Luis Sanchez-Lopez, Deniz Isinsu Avsar, Rafael Munoz Salinas, Holger Voos",S-그래프(Situational Graphs)는 SLAM(Simultaneous Localization and Mapping) 접근 방식으로 생성된 환경의 기하학적 모델을 3D 장면 그래프와 함께 다층의 공동 최적화 가능한 요인 그래프로 병합합니다. S-Graph는 기하학적 지도와 계층적으로 구성된 다양한 의미 개체 및 이들의 위상적 관계를 하나의 그래프 내에서 결합하여 보다 포괄적인 로봇 상황 인식을 제공할 뿐만 아니라 의미 정보를 활용하여 SLAM 수준의 위치 파악 및 매핑 성능을 향상시킵니다. 본 논문에서는 낮은 수준의 특징 추적 및 매핑을 위해 기존의 \ac{VSLAM} 시스템을 사용하는 비전 기반 버전의 S-Graph를 소개합니다. 또한 프레임워크는 기준 마커(표시되는 마커와 최근 도입된 투명 또는 완전히 보이지 않는 마커 모두)의 잠재력을 활용하여 환경과 그 안에 있는 개체에 대한 포괄적인 정보를 인코딩합니다. 마커는 환경의 벽과 문을 포함한 구조적 수준 의미 개체를 전역 참조에서 신뢰할 수 있는 위치로 식별하고 매핑한 후 복도와 방을 포함한 상위 수준 개체와 의미 있는 연결을 설정하는 데 도움이 됩니다. 그러나 의미론적 개체를 포함하는 것 외에도 기준 마커에 의해 부과된 의미론적 및 기하학적 제약도 재구성된 지도의 품질을 향상시키고 위치 파악 오류를 줄이는 데 활용됩니다. 다리가 있는 로봇을 사용하여 수집된 실제 데이터 세트에 대한 실험 결과는 우리의 프레임워크가 더 풍부하고 다층적인 계층적 맵을 만드는 데 탁월하고 동시에 로봇 자세 정확도를 향상시키는 것을 보여줍니다.
475,http://arxiv.org/abs/2309.10430 ,Predicate Classification Using Optimal Transport Loss in Scene Graph Generation,"Sorachi Kurita, Satoshi Oyama, Itsuki Noda",장면 그래프 생성(SGG)에서 교차 엔트로피 손실을 사용한 학습은 데이터 세트의 관계 레이블 분포의 심각한 불균형으로 인해 편향된 예측을 생성합니다. 따라서 본 연구에서는 두 가지 확률 분포를 비교하기 위한 척도로 최적의 전송을 이용하여 장면 그래프를 생성하는 방법을 제안한다. SGG의 술어 분류를 위해 운송 비용 측면에서 라벨 간의 유사성을 반영하는 최적의 운송 손실로 학습을 적용합니다. 제안된 접근 방식에서는 사전 훈련된 모델에서 얻은 단어의 유사성을 사용하여 최적 운송의 운송 비용을 정의합니다. 효율성에 대한 실험적 평가는 제안된 방법이 평균 Recall@50 및 100 측면에서 기존 방법보다 성능이 우수하다는 것을 보여줍니다. 또한 데이터 세트에서 거의 사용할 수 없는 관계 레이블의 리콜을 향상시킵니다.
474,http://arxiv.org/abs/2309.09844 ,CC-SGG: Corner Case Scenario Generation using Learned Scene Graphs,"George Drayson, Efimia Panagiotaki, Daniel Omeiza, Lars Kunze",코너 케이스 시나리오는 자율주행차(AV)의 안전성을 테스트하고 검증하는 데 필수적인 도구입니다. 이러한 시나리오는 자연스러운 주행 데이터 세트에 불충분하게 존재하는 경우가 많기 때문에 합성 코너 케이스로 데이터를 보강하면 독특한 상황에서 AV의 안전한 작동이 크게 향상됩니다. 그러나 합성적이면서도 현실적인 코너 케이스를 생성하는 것은 상당한 과제를 안겨줍니다. 이 연구에서는 일반 운전 시나리오를 코너 케이스로 변환하기 위해 HGNN(이종 그래프 신경망)을 기반으로 하는 새로운 접근 방식을 소개합니다. 이를 달성하기 위해 먼저 구조와 속성을 최소한으로 조작하여 일반적인 운전 장면을 장면 그래프로 간결하게 표현합니다. 그런 다음 모델은 해당 그래프를 교란하여 Attention 및 Triple Embedding을 사용하여 코너 케이스를 생성하는 방법을 학습합니다. 그런 다음 입력 및 교란된 그래프를 시뮬레이션으로 다시 가져와 코너 케이스 시나리오를 생성합니다. 우리 모델은 입력 장면 그래프에서 코너 케이스를 생성하는 방법을 성공적으로 학습하여 테스트 데이터 세트에서 89.9%의 예측 정확도를 달성했습니다. 우리는 기본 자율 주행 방법에 대해 생성된 시나리오를 추가로 검증하여 기본에 대한 중요한 상황을 효과적으로 생성하는 모델의 능력을 보여줍니다.
473,http://arxiv.org/abs/2309.09311 ,Towards Debiasing Frame Length Bias in Text-Video Retrieval via Causal Intervention,"Burak Satar, Hongyuan Zhu, Hanwang Zhang, Joo Hwee Lim","많은 연구는 텍스트-비디오 검색에서 사전 훈련을 개선하거나 새로운 백본을 개발하는 데 중점을 둡니다. 그러나 최근 연구에서 다른 텍스트-비디오 관련 작업에서 시사된 것처럼 기존 방법은 학습 및 추론 편향 문제로 어려움을 겪을 수 있습니다. 예를 들어, 동작 인식의 공간적 외관 특징이나 비디오 장면 그래프 생성의 시간 객체 동시 발생은 가짜 상관 관계를 유도할 수 있습니다. 이 연구에서 우리는 우리가 아는 한 텍스트-비디오 검색 작업에 대한 최초의 시도인 잘린 비디오 클립의 훈련 세트와 테스트 세트 간의 프레임 길이 불일치로 인한 시간적 편향에 대한 독특하고 체계적인 연구를 제시합니다. 먼저 기본 연구를 통해 설명된 모델에 편향이 어떤 영향을 미칠지에 대한 가설을 세우고 검증합니다. 그런 다음 인과적 편향성 제거 접근 방식을 제안하고 Epic-Kitchens-100, YouCook2 및 MSR-VTT 데이터 세트에 대한 광범위한 실험 및 절제 연구를 수행합니다. 우리 모델은 편향이 완화되었음을 증명하는 의미 관련성 중심 평가 지표인 nDCG의 기준선과 SOTA 및 기타 기존 지표를 능가합니다."
472,http://arxiv.org/abs/2309.09182 ,Optimal Scene Graph Planning with Large Language Model Guidance,"Zhirui Dai, Arash Asgharivaskasi, Thai Duong, Shusen Lin, Maria-Elizabeth Tzes, George Pappas, Nikolay Atanasov","미터법, 의미론적, 위상적 매핑의 최근 발전으로 자율 로봇에는 자연어 작업을 해석할 수 있는 의미론적 개념 기반 기능이 탑재되었습니다. 이 작업은 계층적 메트릭-의미론적 모델을 위한 효율적인 작업 계획 알고리즘을 통해 이러한 새로운 기능을 활용하는 것을 목표로 합니다. 우리는 환경의 장면 그래프 표현을 고려하고 LLM(대형 언어 모델)을 활용하여 자연어 작업을 LTL(선형 시간 논리) 자동 장치로 변환합니다. 우리의 주요 기여는 장면 그래프에 대한 LLM 지침을 통해 최적의 계층적 LTL 계획을 가능하게 하는 것입니다. 효율성을 달성하기 위해 장면 그래프와 작업 자동화의 속성과 연결성을 캡처하고 LLM 휴리스틱 기능을 통해 의미론적 지침을 제공하는 계층적 계획 도메인을 구성합니다. 최적성을 보장하기 위해 우리는 일관성이 입증되고 다중 경험적 계획에서 잠재적으로 허용되지 않는 LLM 지침을 보완하는 LTL 경험적 기능을 설계합니다. 가상화된 실제 환경의 장면 그래프에서 복잡한 자연어 작업의 효율적인 계획을 보여줍니다."
471,http://arxiv.org/abs/2309.08914 ,Outram: One-shot Global Localization via Triangulated Scene Graph and Global Outlier Pruning,"Pengyu Yin, Haozhi Cao, Thien-Minh Nguyen, Shenghai Yuan, Shuyang Zhang, Kangcheng Liu, Lihua Xie","원샷 LiDAR 위치 파악은 하나의 단일 포인트 클라우드에서 로봇 자세를 추정하는 기능을 말하며, 이는 초기화 및 위치 재지정 프로세스에서 상당한 이점을 제공합니다. 포인트 클라우드 영역에서 이 주제는 전역 디스크립터 검색(즉, 루프 폐쇄 감지) 및 포즈 구체화(즉, 포인트 클라우드 등록) 문제로서 단독으로 또는 결합하여 광범위하게 연구되었습니다. 그러나 포즈 추정에서 후보 검색과 대응 생성 사이의 관계를 명시적으로 고려한 사람은 거의 없으므로 하위 구조 모호성에 취약합니다. 이를 위해 우리는 3D 장면 그래프의 하위 구조를 활용하여 지역적으로 일관된 대응 검색과 전역 하위 구조별 이상치 제거를 수행하는 Outram이라는 계층적 원샷 위치 파악 알고리즘을 제안합니다. 이러한 계층적 프로세스는 특징 검색과 대응 추출을 결합하여 로컬-글로벌 일관성 개선을 수행함으로써 하위 구조의 모호성을 해결합니다. 우리는 여러 대규모 야외 데이터 세트의 다양한 시나리오에서 Outram의 기능을 시연합니다. 우리의 구현은 오픈 소스입니다: https://github.com/Pamphlett/Outram."
470,http://arxiv.org/abs/2309.08179 ,STDG: Semi-Teacher-Student Training Paradigram for Depth-guided One-stage Scene Graph Generation,"Xukun Zhou, Zhenbo Song, Jun He, Hongyan Liu, Zhaoxin Fan","장면 그래프 생성은 자율 로봇 시스템의 환경 이해를 돕는 중요한 요소입니다. 그러나 대부분의 기존 방법은 배경 복잡성의 복잡한 역학으로 인해 방해를 받는 경우가 많으며, 이로 인해 환경에 내재된 토폴로지 정보를 완전히 디코딩하는 능력이 제한됩니다. 또한 깊이 단서 내에 캡슐화된 풍부한 상황 정보가 활용되지 않는 경우가 많아 기존 접근 방식의 효율성이 떨어집니다. 이러한 단점을 해결하기 위해 우리는 전위적인 Depth-Guided One-Stage 장면 그래프 생성 방법론인 STDG를 제시합니다. STDG의 혁신적인 아키텍처는 깊이 안내 HHA 표현 생성 모듈, 깊이 안내 세미 교육 네트워크 학습 모듈 및 깊이 안내 장면 그래프 생성 모듈이라는 맞춤형 모듈의 3가지 요소로 구성됩니다. 이 세 가지 모듈은 깊이 정보를 시너지 효과적으로 활용하여 깊이 신호 생성 및 깊이 특징 활용부터 최종 장면 그래프 예측까지 모든 측면을 포괄합니다. 중요한 것은 추론 단계에서 추가 계산 부담을 부과하지 않고 이를 달성한다는 것입니다. 실험 결과는 우리의 방법이 1단계 장면 그래프 생성 기준의 성능을 크게 향상시키는 것을 확인했습니다."
469,http://arxiv.org/abs/2309.07726 ,GRID: Scene-Graph-based Instruction-driven Robotic Task Planning,"Zhe Ni, Xiaoxin Deng, Cong Tai, Xinyue Zhu, Qinghongbing Xie, Weihang Huang, Xiang Wu, Long Zeng","최근 연구에서는 LLM(대형 언어 모델)이 로봇 작업 계획에 대한 지침의 기초를 용이하게 할 수 있음을 보여주었습니다. 이러한 진전에도 불구하고 대부분의 기존 작업은 주로 LLM이 환경 정보를 이해하는 데 도움이 되도록 원시 이미지를 활용하는 데 중점을 두었습니다. 그러나 이 접근 방식은 관찰 범위를 제한할 뿐만 아니라 일반적으로 광범위한 다중 모드 데이터 수집과 대규모 모델이 필요합니다. 본 논문에서는 이미지 대신 장면 그래프를 활용하여 전체 장면 정보를 인식하고 주어진 명령에 대한 하위 작업을 반복적으로 계획하는 그래프 기반 로봇 명령 분해기(GRID)라는 새로운 접근 방식을 제안합니다. 우리의 방법은 LLM 및 Graph Attention Networks를 통해 그래프의 개체 속성과 관계를 인코딩하고 사전 정의된 로봇 동작과 장면 그래프의 대상 개체로 구성된 하위 작업을 예측하는 명령 기능을 통합합니다. 이 전략을 통해 로봇은 장면 그래프를 통해 환경에서 널리 관찰되는 의미 지식을 획득할 수 있습니다. GRID를 교육하고 평가하기 위해 그래프 기반 로봇 작업 계획을 위한 합성 데이터 세트를 생성하는 데이터 세트 구성 파이프라인을 구축합니다. 실험에 따르면 우리의 방법은 하위 작업 정확도에서 25.4%, 작업 정확도에서 43.6% 이상 GPT-4보다 뛰어난 것으로 나타났습니다. 또한 우리의 방법은 추론당 0.11초의 실시간 속도를 달성합니다. 보이지 않는 장면과 다양한 수의 객체가 있는 장면의 데이터 세트에 대해 수행된 실험에서는 GRID의 작업 정확도가 최대 3.8% 감소하여 강력한 장면 간 일반화 능력을 보여줍니다. 우리는 물리적 시뮬레이션과 실제 세계 모두에서 우리의 방법을 검증합니다. 자세한 내용은 프로젝트 페이지 https://jackyzengl.github.io/GRID.github.io/에서 확인할 수 있습니다."
468,http://arxiv.org/abs/2309.06635 ,Collaborative Dynamic 3D Scene Graphs for Automated Driving,"Elias Greve, Martin Büchner, Niclas Vödisch, Wolfram Burgard, Abhinav Valada","지도는 안전하고 자동화된 운전을 가능하게 하는 데 없어서는 안 될 역할을 해왔습니다. SLAM에서 의미론에 이르기까지 다양한 분야에서 많은 발전이 있었지만 도시의 역동적인 장면에 대한 실행 가능한 계층적 의미론적 표현을 구축하고 여러 에이전트의 정보를 처리하는 것은 여전히 ​​어려운 문제입니다. 본 연구에서는 자율주행의 다양한 기능에 대해 고차 추론과 효율적인 쿼리를 가능하게 하는 Collaborative URBan Scene Graphs(CURB-SG)를 제시합니다. CURB-SG는 여러 에이전트의 Panoptic LiDAR 데이터를 활용하여 에이전트 간 루프 폐쇄를 감지하는 효과적인 그래프 기반 협업 SLAM 접근 방식을 사용하여 대규모 지도를 구축합니다. 획득한 3D 지도를 의미론적으로 분해하기 위해 Ego Agent의 경로와 다른 차량에 대한 Panoptic 관찰을 통해 차선 그래프를 구축합니다. 차선 그래프의 연결성을 기반으로 환경을 교차하는 도로 영역과 교차하지 않는 도로 영역으로 분리합니다. 그 후, 우리는 차선 정보, 정적 랜드마크의 위치 및 특정 지도 섹션에 대한 할당, 자아 에이전트가 관찰한 다른 차량, 3D 팬옵틱 포인트 클라우드를 포함하는 SLAM의 포즈 그래프를 포함하는 다층 장면 그래프를 구성합니다. 우리는 사실적인 시뮬레이터를 사용하여 도시 시나리오에서 CURB-SG를 광범위하게 평가합니다. 우리는 http://curb.cs.uni-freiburg.de에서 코드를 공개합니다."
467,http://arxiv.org/abs/2309.04077 ,SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments,"Abhinav Rajvanshi, Karan Sikka, Xiao Lin, Bhoram Lee, Han-Pang Chiu, Alvaro Velasquez","자율 에이전트가 알 수 없는 환경에서 복잡한 탐색 작업을 수행하려면 의미론적 추론과 동적 계획 기능이 중요합니다. 이러한 일을 성공적으로 수행하기 위해서는 인간이 갖고 있는 많은 양의 상식적 지식이 필요합니다. 알려지지 않은 대규모 환경에서 복잡한 탐색 작업에 대한 효율적인 일반화를 위해 LLM(대형 언어 모델)의 인간 지식을 활용하는 새로운 접근 방식인 SayNav를 소개합니다. SayNav는 탐색을 위한 실행 가능하고 상황에 맞는 상위 수준 계획을 생성하기 위해 LLM에 대한 입력으로 탐색된 환경의 3D 장면 그래프를 점진적으로 구축하는 새로운 접지 메커니즘을 사용합니다. 그런 다음 LLM에서 생성된 계획은 사전 훈련된 하위 수준 계획자에 의해 실행되며, 각 계획 단계를 단거리 지점-목표 탐색 하위 작업으로 처리합니다. SayNav는 탐색 중에 단계별 지침을 동적으로 생성하고 새로 인식된 정보를 기반으로 향후 단계를 지속적으로 개선합니다. 우리는 에이전트가 알 수 없는 환경에서 여러 다른 개체를 효율적으로 검색하기 위해 막대한 양의 인간 지식을 활용해야 하는 다중 개체 탐색(MultiON) 작업에 대해 SayNav를 평가합니다. 또한 다양한 물체로 사실적인 대규모 실내 환경을 제공하는 ProcTHOR 프레임워크를 사용하는 MultiON 작업을 위한 벤치마크 데이터세트를 소개합니다. SayNav는 최첨단 결과를 달성하고 성공률 측면에서 강력한 실측 가정을 통해 오라클 기반 기준을 8% 이상 능가하며 대규모의 새로운 환경에서 물체를 성공적으로 찾기 위한 동적 계획을 생성하는 능력을 강조합니다. 코드, 벤치마크 데이터 세트 및 데모 비디오는 https://www.sri.com/ics/computer-vision/saynav에서 액세스할 수 있습니다."
466,http://arxiv.org/abs/2309.03542 ,Zero-Shot Scene Graph Generation via Triplet Calibration and Reduction,"Jiankai Li, Yunhong Wang, Weixin Li","SGG(장면 그래프 생성)는 다운스트림 비전 언어 작업에서 중추적인 역할을 합니다. 기존 SGG 방법은 일반적으로 보이지 않는 삼중항에 대한 구성 일반화가 좋지 않습니다. 그들은 일반적으로 주요 삼중항을 포함하고 추론 중에 보이는 삼중항에 편향되는 경향이 있는 불완전하게 주석이 달린 장면 그래프에 대해 훈련됩니다. 이 문제를 해결하기 위해 본 논문에서는 T-CAR(Triplet Calibration and Reduction) 프레임워크를 제안합니다. 우리 프레임워크에서는 다양한 삼중항의 표현을 정규화하고 불완전하게 주석이 달린 훈련 장면 그래프에서 보이지 않는 삼중항을 동시에 발굴하기 위해 삼중항 교정 손실이 먼저 제시됩니다. 더욱이, 장면 그래프의 보이지 않는 공간은 보이는 공간보다 일반적으로 몇 배 더 큽니다. 그 이유는 여기에는 비현실적인 구성이 엄청나게 많이 포함되어 있기 때문입니다. 따라서 우리는 모델 훈련을 용이하게 하기 위해 굴착의 관심을 합리적인 보이지 않는 구성으로 전환하기 위해 보이지 않는 공간 감소 손실을 제안합니다. 마지막으로, 우리는 주체와 객체 사이의 상대적 공간 관계를 명시적으로 모델링하여 보이지 않는 삼중항의 구성 일반화를 개선하기 위한 상황별 인코더를 제안합니다. 광범위한 실험을 통해 우리의 접근 방식이 최첨단 방법에 비해 제로 샷 SGG에 대한 일관된 개선을 달성한다는 것을 보여줍니다. 코드는 https://github.com/jkli1998/T-CAR에서 확인할 수 있습니다."
465,http://arxiv.org/abs/2309.03240 ,RepSGG: Novel Representations of Entities and Relationships for Scene Graph Generation,"Hengyue Liu, Bir Bhanu","장면 그래프 생성(SGG)은 최근 상당한 진전을 이루었습니다. 그러나 대부분의 이전 작업은 경계 상자 제안, 앵커 또는 학습 가능한 쿼리를 기반으로 하는 고정 크기 엔터티 표현에 크게 의존합니다. 각 표현의 카디널리티는 성능과 계산 오버헤드 간에 서로 다른 균형점을 갖기 때문에 매우 대표성이 높은 특징을 효율적이고 동적으로 추출하는 것은 SGG에 있어 어렵고 중요합니다. 이 연구에서는 앞서 언급한 과제를 해결하기 위해 RepSGG라는 새로운 아키텍처가 제안되어 주제를 쿼리로, 객체를 키로, 관계를 쌍별 쿼리와 키 간의 최대 주의 가중치로 공식화합니다. RepSGG는 엔터티 및 관계에 대한 더욱 세밀하고 유연한 표현 능력을 통해 관계 추론을 위해 의미상 차별적이고 대표 포인트를 샘플링하는 방법을 학습합니다. 더욱이, 긴 꼬리 분포는 SGG의 일반화에 있어 중요한 과제를 제기합니다. 훈련 중 런타임 성능을 기반으로 한 아핀 변환을 통해 관계 로짓을 수정하는 런타임 성능 기반 로짓 조정(PGLA) 전략이 제안되었습니다. 이 전략은 지배적 클래스와 희귀 클래스 간의 보다 균형 잡힌 성능을 장려합니다. 실험 결과에 따르면 RepSGG는 빠른 추론 속도로 Visual Genome 및 Open Images V6 데이터 세트에서 최첨단 또는 이에 필적하는 성능을 달성하여 제안된 방법의 효능과 효율성을 보여줍니다."
464,http://arxiv.org/abs/2309.02286 ,Haystack: A Panoptic Scene Graph Dataset to Evaluate Rare Predicate Classes,"Julian Lorenz, Florian Barthel, Daniel Kienzle, Rainer Lienhart","현재 장면 그래프 데이터 세트는 조건자 클래스의 강력한 롱테일 분포로 인해 어려움을 겪고 있습니다. 테스트 세트의 일부 조건자 클래스 수가 매우 적기 때문에 가장 희귀한 클래스에 대해 신뢰할 수 있는 측정항목을 검색할 수 없습니다. 우리는 특히 희귀 조건자 클래스에 대한 예측 성능에 대한 벤치마크로 설계된 새로운 Panoptic 장면 그래프 데이터 세트와 메트릭 세트를 구성합니다. 새로운 데이터 세트를 구성하기 위해 우리는 건초 더미의 바늘과 같은 대규모 이미지 세트에 숨겨진 희귀 조건자 클래스를 효율적으로 찾는 모델 지원 주석 파이프라인을 제안합니다.   이전 장면 그래프 데이터세트와 달리 Haystack에는 명시적인 부정적인 주석, 즉 주어진 관계에 특정 조건자 클래스가 없다는 주석이 포함되어 있습니다. 네거티브 주석은 특히 장면 그래프 생성 분야에서 유용하며 현재 장면 그래프 생성 모델을 개선할 수 있는 완전히 새로운 가능성을 열어줍니다.   Haystack은 기존 Panoptic 장면 그래프 데이터세트와 100% 호환되며 기존 평가 파이프라인과 쉽게 통합될 수 있습니다. 우리의 데이터 세트와 코드는 https://lorjul.github.io/haystack/에서 찾을 수 있습니다. 여기에는 기존 작업에 데이터 세트를 통합하는 데 도움이 되는 주석 파일과 사용하기 쉬운 스크립트 및 유틸리티가 포함되어 있습니다."
463,http://arxiv.org/abs/2309.00215 ,Towards Addressing the Misalignment of Object Proposal Evaluation for Vision-Language Tasks via Semantic Grounding,"Joshua Feinglass, Yezhou Yang","객체 제안 생성은 VL(Vision-Language) 작업(이미지 캡션, 시각적 질문 답변 등)의 표준 전처리 단계로 사용됩니다. VL 작업에 대해 생성된 객체 제안의 성능은 현재 사용 가능한 모든 주석에서 평가되며, 우리가 표시하는 프로토콜은 잘못 정렬되어 있습니다. 점수가 높다고 반드시 다운스트림 VL 작업의 성능이 향상되는 것은 아닙니다. 우리의 작업은 이 현상에 대한 연구 역할을 하며 그 효과를 완화하기 위한 의미론적 기반의 효과를 탐구합니다. 이를 위해 우리는 주석 중요도 점수를 임계값으로 설정하여 선택된 사용 가능한 주석의 하위 집합에 대해서만 객체 제안을 평가할 것을 제안합니다. VL 작업에 대한 객체 주석의 중요성은 이미지를 설명하는 텍스트에서 관련 의미 정보를 추출하여 정량화됩니다. 우리는 우리의 방법이 일관적이며 기존 기술과 비교할 때 이미지 캡션 측정항목 및 사람 주석에 의해 선택된 주석과의 정렬이 크게 향상되었음을 보여줍니다. 마지막으로 SGG(Scene Graph Generation) 벤치마크에 사용되는 현재 감지기를 사용 사례로 비교합니다. 이는 기존 객체 제안 평가 기술이 잘못 정렬된 경우의 예가 됩니다."
462,http://arxiv.org/abs/2308.13812 ,Dysen-VDM: Empowering Dynamics-aware Text-to-Video Diffusion with LLMs,"Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Tat-Seng Chua","T2V(텍스트-비디오) 합성은 커뮤니티에서 점점 더 많은 관심을 받고 있으며, 최근 등장한 확산 모델(DM)은 과거 접근 방식보다 더 강력한 성능을 유망하게 보여주었습니다. 기존의 최첨단 DM은 고해상도 비디오 생성을 달성할 수 있지만 비디오 합성의 핵심 중 하나인 복잡한 시간 역학 모델링과 관련하여 주요 제한 사항(예: 동작 발생 장애, 조잡한 비디오 동작)으로 인해 크게 어려움을 겪을 수 있습니다. 이 작업에서는 고품질 T2V 생성을 위해 DM의 비디오 역학에 대한 인식 강화를 조사합니다. 인간의 직관에서 영감을 받아 혁신적인 동적 장면 관리자(Dysen이라고 함) 모듈을 설계합니다. 이 모듈에는 (1단계) 입력 텍스트에서 적절한 시간 순서 배열로 주요 동작을 추출하고, (2단계) 동작 일정을 동적 장면 그래프(DSG) 표현으로 변환하고, (3단계) 충분하고 합리적인 세부 정보로 DSG의 장면을 풍부하게 합니다. 상황 내 학습을 통해 기존의 강력한 LLM(예: ChatGPT)을 활용하여 Dysen은 (거의) 인간 수준의 시간 역학 이해를 실현합니다. 마지막으로, 풍부한 액션 장면 디테일을 갖춘 결과 비디오 DSG는 세밀한 시공간 특징으로 인코딩되어 비디오 생성을 위한 백본 T2V DM에 통합됩니다. 인기 있는 T2V 데이터 세트에 대한 실험에 따르면 Dysen-VDM은 특히 복잡한 작업이 포함된 시나리오에서 상당한 마진으로 이전 기술보다 지속적으로 뛰어난 성능을 발휘하는 것으로 나타났습니다. https://haofei.vip/Dysen-VDM의 코드"
461,http://arxiv.org/abs/2308.12910 ,SCoRD: Subject-Conditional Relation Detection with Text-Augmented Data,"Ziyan Yang, Kushal Kafle, Zhe Lin, Scott Cohen, Zhihong Ding, Vicente Ordonez","우리는 입력 주제를 조건으로 한 주제-조건부 관계 탐지 SCoRD를 제안합니다. 목표는 위치와 함께 장면의 다른 개체와의 모든 관계를 예측하는 것입니다. Open Images 데이터 세트를 기반으로 훈련 및 테스트 분할이 $\langle$subject, 관계, object$\rangle$ 삼중항의 발생 통계 측면에서 분포 이동을 갖도록 하는 까다로운 OIv6-SCoRD 벤치마크를 제안합니다. 이 문제를 해결하기 위해 우리는 주제가 주어지면 이 출력을 토큰 시퀀스로 캐스팅하여 관계, 개체 및 개체 위치를 예측하는 자동 회귀 모델을 제안합니다. 첫째, 우리는 이전 장면-그래프 예측 방법이 이 벤치마크에서 주제를 조건으로 할 때 관계-객체 쌍의 완전한 열거를 생성하지 못한다는 것을 보여줍니다. 특히 최근 장면 그래프 검출기로 얻은 49.75%와 비교하여 관계 객체 예측에 대해 83.8%의 회상@3을 얻었습니다. 그런 다음 텍스트 캡션에서 자동으로 얻은 관계 객체 쌍을 훈련하는 동안 객체 상자 주석을 사용할 수 없는 관계 객체 및 객체 상자 예측 모두에 대한 향상된 일반화를 보여줍니다. 특히, 학습 중에 객체 위치를 사용할 수 없는 $\langle$subject, 관계, object$\rangle$ 삼중항의 경우 관계-객체 쌍에 대해 33.80%, 상자 위치에 대해 26.75%의 Recall@3을 얻을 수 있습니다."
460,http://arxiv.org/abs/2308.12048 ,Head-Tail Cooperative Learning Network for Unbiased Scene Graph Generation,"Lei Wang, Zejian Yuan, Yao Lu, Badong Chen","SGG(장면 그래프 생성)는 이미지 이해의 중요한 작업으로, 술어의 롱테일 분포로 인해 발생하는 헤드 편향 예측 문제에 직면해 있습니다. 그러나 현재의 편향되지 않은 SGG 방법은 머리 술어 예측의 실질적인 희생을 무시하면서 꼬리 술어 예측 개선에 쉽게 우선순위를 둘 수 있어 머리 편향에서 꼬리 편향으로 전환됩니다. 이 문제를 해결하기 위해 우리는 머리와 꼬리 술어를 모두 정확하게 인식하기 위해 협력하는 머리 선호 및 꼬리 선호 기능 표현 분기를 포함하는 모델 독립적인 HTCL(Head-Tail Collaborative Learning) 네트워크를 제안합니다. 또한 꼬리 선호 조건자 특성을 제한하여 꼬리 선호 특성 표현 분기의 예측 능력을 향상시키는 자기 지도 학습 접근 방식을 제안합니다. 구체적으로 자기 지도 학습은 머리 술어 특성을 클래스 중심에 수렴하고, 대비 학습과 머리 중심 손실을 통해 꼬리 술어 특성을 최대한 분산시킵니다. 우리는 HTCL을 VG150, Open Images V6 및 GQA200 데이터 세트의 다양한 SGG 모델에 적용하여 HTCL의 효율성을 입증합니다. 결과는 우리 방법이 Recall의 희생을 최소화하면서 더 높은 평균 Recall을 달성하고 새로운 최첨단 전체 성능을 달성한다는 것을 보여줍니다. 우리 코드는 https://github.com/wanglei0618/HTCL에서 확인할 수 있습니다."
459,http://arxiv.org/abs/2308.11242 ,Faster Optimization in S-Graphs Exploiting Hierarchy,"Hriday Bavle, Jose Luis Sanchez-Lopez, Javier Civera, Holger Voos","3D 장면 그래프는 다양한 레이어에서 다양한 환경 개체를 적절하게 구성하여 환경을 계층적으로 표현합니다. 상황 그래프에 대한 이전 작업에서는 로봇 포즈를 장면 그래프 엔터티와 긴밀하게 결합하여 최첨단 결과를 달성함으로써 3D 장면 그래프의 개념을 SLAM으로 확장했습니다. 하지만 S-Graph의 한계 중 하나는 시간이 지남에 따라 그래프 크기가 ​​증가하고 계산 복잡성이 증가하기 때문에 대규모 환경에서의 확장성입니다.   이 작업에서 이러한 한계를 극복하기 위해 우리는 중복된 로봇 포즈와 동일한 구조적 실체의 관찰에 대한 연결을 소외시켜 그래프 크기를 줄이기 위해 계층 구조를 활용하는 개선된 버전의 S-그래프에 대한 초기 연구를 제시합니다. 첫째, 방과 같은 구조 내의 모든 그래프 엔터티를 포함하는 방-로컬 그래프의 생성 및 최적화를 제안합니다. 이러한 룸-로컬 그래프는 지정된 룸 내의 중복 로봇 키프레임을 소외시키는 S-그래프를 압축하는 데 사용됩니다. 그런 다음 정기적인 시간 간격으로 압축된 그래프의 창 로컬 최적화를 수행합니다. 루프 종료가 감지될 때마다 압축된 그래프의 전역 최적화가 수행됩니다. 기준선과 비교하여 유사한 정확도를 보였으며 기준선 대비 계산 시간은 39.81% 감소했습니다."
458,http://arxiv.org/abs/2308.09472 ,Vision Relation Transformer for Unbiased Scene Graph Generation,"Gopika Sudhakaran, Devendra Singh Dhami, Kristian Kersting, Stefan Roth",최근 몇 년 동안 객체 인코더-디코더 백본 위에 쌓인 관계 인코더-디코더 파이프라인을 사용하여 엔터티 관계를 예측하는 것을 목표로 하는 포괄적인 시각적 장면 이해 작업인 SGG(장면 그래프 생성)에 대한 관심이 높아지고 있습니다. 불행하게도 현재 SGG 방법은 관계 인코딩 프로세스 중에 엔터티 로컬 수준 단서와 관련된 정보 손실로 인해 어려움을 겪고 있습니다. 이를 완화하기 위해 새로운 로컬 수준 엔터티 관계 인코더로 구성된 VETO(Vision rElation TransfOrmer)를 소개합니다. 또한 우리는 기존의 많은 SGG 방법이 편견이 없다고 주장하지만 여전히 헤드 또는 테일 클래스에 편향되어 있음을 관찰합니다. 이러한 편견을 극복하기 위해 헤드 또는 테일 클래스에 대한 편견 없이 중요한 관계 기능을 포착하는 MEET(Mutually Exclusive ExperT) 학습 전략을 소개합니다. VG 및 GQA 데이터 세트에 대한 실험 결과는 VETO + MEET가 10배 더 작으면서도 최신 기술에 비해 예측 성능을 최대 47% 향상시키는 것으로 나타났습니다.
457,http://arxiv.org/abs/2308.09351 ,RLIPv2: Fast Scaling of Relational Language-Image Pre-training,"Hangjie Yuan, Shiwei Zhang, Xiang Wang, Samuel Albanie, Yining Pan, Tao Feng, Jianwen Jiang, Dong Ni, Yingya Zhang, Deli Zhao","관계형 언어-이미지 사전 훈련(RLIP)은 비전 표현을 관계형 텍스트와 정렬하여 컴퓨터 비전 작업에서 관계형 추론 기능을 향상시키는 것을 목표로 합니다. 그러나 RLIPv1 아키텍처의 느린 수렴과 기존 장면 그래프 데이터의 제한된 가용성으로 인해 RLIPv1을 확장하는 것은 어렵습니다. 본 논문에서는 관계형 사전 훈련을 대규모 의사 레이블이 지정된 장면 그래프 데이터로 확장할 수 있는 빠른 수렴 모델인 RLIPv2를 제안합니다. 빠른 확장을 가능하게 하기 위해 RLIPv2는 희소화된 언어 인코딩 레이어를 사용하여 더 빠르고 더 심층적인 게이트 교차 모달 융합을 촉진하는 메커니즘인 ALIF(Asymmetric Language-Image Fusion)를 도입합니다. ALIF는 사전 학습 및 미세 조정 시간을 단축하여 RLIPv1과 비슷하거나 더 나은 성능을 제공합니다. 대규모 장면 그래프 데이터를 얻기 위해 캡션 작성자(예: BLIP)와 설계된 관계 태거를 도입하여 자유 형식 관계 레이블로 객체 감지 데이터세트를 확장합니다. Relation Tagger는 BLIP에서 생성된 관계 텍스트를 영역 쌍에 할당하여 대규모 관계형 사전 학습을 가능하게 합니다. 인간-객체 상호 작용 감지 및 장면 그래프 생성에 대해 수행된 광범위한 실험을 통해 RLIPv2는 완전 미세 조정, 퓨샷 및 제로샷 설정에서 세 가지 벤치마크에서 최첨단 성능을 보여줍니다. 특히 가장 큰 RLIPv2는 미세 조정 없이 HICO-DET에서 23.29mAP를 달성하고 단 1% 데이터로 32.22mAP를 생성하며 100% 데이터로 45.09mAP를 생성합니다. 코드와 모델은 https://github.com/JacobYuan7/RLIPv2에서 공개적으로 제공됩니다."
456,http://arxiv.org/abs/2308.06719 ,3D Scene Graph Prediction on Point Clouds Using Knowledge Graphs,"Yiding Qiu, Henrik I. Christensen","3D 장면 그래프 예측은 3D 환경 내에서 객체 클래스와 그 관계를 동시에 예측하는 것을 목표로 하는 작업입니다. 이러한 환경은 주로 인간에 의해 그리고 인간을 위해 설계되었으므로 객체 및 객체 관계에 대한 상식적 지식을 통합하면 장면 그래프의 예측을 크게 제한하고 향상시킬 수 있습니다. 본 논문에서는 실내 장면의 포인트 클라우드에 대한 3차원 장면 그래프 예측을 위한 상식 지식 그래프의 적용을 조사한다. 실제 실내 데이터 세트에 대한 실험을 통해 메시지 전달 방법을 통해 외부 상식 지식을 통합하면 최신 알고리즘과 비교하여 장면 그래프 예측 정확도가 외부 지식의 경우 15.0%, 내부 지식의 경우 $7.96\%$ 향상된다는 것을 입증합니다. 또한 보다 현실적인 로봇 설정에서 모델의 사용법을 보여주기 위해 장면 그래프 생성을 위해 초당 10프레임으로 실제 세계에서 테스트했습니다."
455,http://arxiv.org/abs/2308.06712 ,Compositional Feature Augmentation for Unbiased Scene Graph Generation,"Lin Li, Guikun Chen, Jun Xiao, Yi Yang, Chunping Wang, Long Chen","장면 그래프 생성(SGG)은 주어진 이미지에서 모든 시각적 관계 삼중항 $<$\texttt{sub}, \texttt{pred}, \texttt{obj}$>$를 감지하는 것을 목표로 합니다. 각 관계 삼중항의 내재적 정보와 외재적 정보를 더 잘 활용하기 위한 다양한 고급 기술의 출현으로 SGG는 최근 몇 년간 큰 발전을 이루었습니다. 그러나 유비쿼터스된 긴 꼬리 술어 분포로 인해 오늘날의 SGG 모델은 여전히 ​​쉽게 머리 술어로 편향됩니다. 현재 SGG에 가장 널리 사용되는 편향성 제거 솔루션은 원래 훈련 샘플의 분포를 변경하는 등의 재균형 방법입니다. 이 논문에서 우리는 기존의 모든 재조정 전략이 강력한 SGG에 중요한 각 술어의 관계 삼중항 기능의 다양성을 증가시키지 못한다고 주장합니다. 이를 위해 우리는 삼중항 특성의 다양성을 증가시키는 관점에서 편향 문제를 완화하기 위한 최초의 편견 없는 SGG 작업인 새로운 Compositional Feature Augmentation(\textbf{CFA}) 전략을 제안합니다. 구체적으로, 우리는 먼저 각 관계 삼중항 특징을 내재적 특징과 외적 특징이라는 두 가지 구성요소로 분해합니다. 이는 각각 관계 삼중항의 내재적 특성과 외적 맥락에 해당합니다. 그런 다음 다른 샘플의 고유 또는 외부 기능을 대체하거나 혼합하여 원래 관계 삼중항의 기능 다양성을 강화하기 위해 두 가지 서로 다른 기능 확대 모듈을 설계합니다. 모델에 구애받지 않는 특성으로 인해 CFA는 다양한 SGG 프레임워크에 원활하게 통합될 수 있습니다. 광범위한 절제를 통해 CFA가 서로 다른 측정항목 간의 절충점에서 새로운 최첨단 성능을 달성하는 것으로 나타났습니다."
454,http://arxiv.org/abs/2308.05515 ,Mono-hydra: Real-time 3D scene graph construction from monocular camera input with IMU,"U. V. B. L. Udugama, G. Vosselman, F. Nex","3D 환경을 자율적으로 탐색하는 로봇의 능력은 낮은 수준의 기하학에서 높은 수준의 의미론(물체, 장소, 건물 등)에 이르는 공간 개념에 대한 이해에 달려 있습니다. 이러한 이해를 가능하게 하기 위해 3D 장면 그래프는 개념과 그 관계에 대한 계층화된 그래프로 환경을 표현하기 위한 강력한 도구로 등장했습니다. 그러나 실시간으로 단안 비전 시스템을 사용하여 이러한 표현을 구축하는 것은 깊이 탐구되지 않은 어려운 작업으로 남아 있습니다. 본 논문에서는 실내 시나리오에 초점을 맞춰 단안 카메라와 IMU 센서 설정을 결합한 실시간 공간 인식 시스템 Mono-Hydra를 제시합니다. 그러나 제안된 접근 방식은 실외 응용 프로그램에 적용할 수 있어 잠재적인 용도에 유연성을 제공합니다. 이 시스템은 깊이와 의미를 도출하기 위해 일련의 딥 러닝 알고리즘을 사용합니다. 제곱근 정보를 기반으로 하는 로봇 중심의 VIO(시각 관성 주행 거리 측정) 알고리즘을 사용하여 IMU 및 단안 카메라로 일관된 시각적 주행 거리 측정을 보장합니다. 이 시스템은 15fps의 실시간 처리에서 20cm 미만의 오류를 달성하여 노트북 GPU(NVIDIA 3080)를 사용하여 실시간 3D 장면 그래프 구성을 가능하게 합니다. 이를 통해 간단한 카메라 설정에서 의사 결정 효율성과 효과가 향상되어 로봇 시스템 민첩성이 향상됩니다. Mono-Hydra는 https://github.com/UAV-Centre-ITC/Mono_Hydra에서 공개적으로 제공됩니다."
453,http://arxiv.org/abs/2308.05286 ,Informative Scene Graph Generation via Debiasing,"Lianli Gao, Xinyu Lyu, Yuyu Guo, Yuxuan Hu, Yuan-Fang Li, Lu Xu, Heng Tao Shen, Jingkuan Song","장면 그래프 생성은 시각적 관계 삼중항(주어, 술어, 목적어)을 감지하는 것을 목표로 합니다. 데이터의 편향으로 인해 현재 모델은 일반적인 예측을 예측하는 경향이 있습니다. 정보를 제공하는 것 대신 ""on"" 및 ""at"" '서다'와 '바라보다'. 이러한 경향으로 인해 정확한 정보와 전반적인 성과가 손실됩니다. 모델이 '도로를 막고 있는 돌'이 아닌 '도로 위의 돌'만을 사용하여 이미지를 표현한다면 이는 심각한 오해일 수 있습니다. 우리는 이 현상이 의미 공간 수준 불균형과 훈련 샘플 수준 불균형이라는 두 가지 불균형으로 인해 발생한다고 주장합니다. 이 문제를 위해 우리는 기존의 분포 피팅이 아닌 편향성 제거에 기반한 효과적인 프레임워크인 DB-SGG를 제안합니다. 이러한 불균형을 위해 SD(Semantic Debiasing)와 BPL(Balanced Predicate Learning)이라는 두 가지 구성 요소를 통합합니다. SD는 혼동 행렬과 이분 그래프를 활용하여 술어 관계를 구성합니다. BPL은 유익한 조건자에 초점을 맞추기 위해 무작위 언더샘플링 전략과 모호성 제거 전략을 채택합니다. 모델에 구애받지 않는 프로세스의 이점을 활용하면 우리 방법을 SGG 모델에 쉽게 적용할 수 있으며 SGG-VG 데이터 세트의 세 가지 SGG 하위 작업에서 mR@20에 대해 Transformer보다 136.3%, 119.5%, 122.6% 성능이 뛰어납니다. 우리의 방법은 또 다른 복잡한 SGG 데이터세트(SGG-GQA)와 두 개의 다운스트림 작업(문장-그래프 검색 및 이미지 캡션 작성)에서 추가로 검증되었습니다."
452,http://arxiv.org/abs/2308.05274 ,Local-Global Information Interaction Debiasing for Dynamic Scene Graph Generation,"Xinyu Lyu, Jingwei Liu, Yuyu Guo, Lianli Gao","동적 장면 그래프 생성(DynSGG) 작업은 주어진 비디오에 대한 장면 그래프를 생성하는 것을 목표로 하며, 여기에는 비디오의 시공간 정보를 모델링하는 작업이 포함됩니다. 그러나 데이터 세트에서 샘플의 긴 꼬리 분포로 인해 이전 DynSGG 모델은 꼬리 조건자를 예측하지 못했습니다. 우리는 이러한 현상이 지역적인 시공간 정보에만 주의를 기울이고 여러 프레임의 일관성을 무시하는 이전 방법에 기인한다고 주장합니다. 이 문제를 해결하기 위해 우리는 로컬 상호 작용 정보와 전역 인간 행동 상호 작용 정보를 도입하는 다중 작업 학습 기반의 새로운 DynSGG 모델인 DynSGG-MTL을 제안합니다. 개체와 프레임 기능 간의 상호 작용을 통해 모델은 단일 이미지의 시각적 맥락을 더 완벽하게 이해할 수 있습니다. 장기간의 인간 행동은 모델을 감독하여 전역 제약 조건을 준수하는 여러 장면 그래프를 생성하고 모델이 꼬리 조건자를 학습할 수 없는 것을 방지합니다. Action Genome 데이터 세트에 대한 광범위한 실험은 동적 장면 그래프 생성을 향상시킬 뿐만 아니라 롱테일 문제를 완화하는 제안된 프레임워크의 효율성을 보여줍니다."
451,http://arxiv.org/abs/2308.05081 ,Constructing Holistic Spatio-Temporal Scene Graph for Video Semantic Role Labeling,"Yu Zhao, Hao Fei, Yixin Cao, Bobo Li, Meishan Zhang, Jianguo Wei, Min Zhang, Tat-Seng Chua","VidSRL(Video Semantic Role Labeling)은 예측 인수 이벤트 구조와 이벤트 간의 상호 관계를 인식하여 주어진 비디오에서 중요한 이벤트를 감지하는 것을 목표로 합니다. 최근 VidSRL에 대한 방법이 제시되었지만 세밀한 공간적 장면 인식이 부족하고 비디오 시간성에 대한 모델링이 불충분하다는 점을 포함하여 대부분 두 가지 주요 단점이 있을 수 있습니다. 이를 위해 이 작업은 VidSRL에 대한 비디오의 세밀한 공간 의미와 시간적 역학을 모두 잘 모델링하는 기존 동적 장면 그래프 구조를 기반으로 하는 새로운 전체적인 시공간 장면 그래프(즉, HostSG) 표현을 탐구합니다. HostSG를 기반으로 구축된 우리는 틈새 타겟팅 VidSRL 프레임워크를 제시합니다. 장면-이벤트 매핑 메커니즘은 먼저 기본 장면 구조와 상위 수준 이벤트 의미 구조 사이의 격차를 해소하여 전체 계층적 장면-이벤트(ICE라고 함) 그래프 구조를 생성하도록 설계되었습니다. 우리는 전체 구조 표현이 최종 작업 요구와 가장 잘 일치할 수 있도록 ICE 그래프를 최적화하기 위해 반복적인 구조 개선을 추가로 수행합니다. 마지막으로 VidSRL의 세 가지 하위 작업 예측이 공동으로 디코딩되며, 여기서 엔드투엔드 패러다임은 오류 전파를 효과적으로 방지합니다. 벤치마크 데이터세트에서 우리의 프레임워크는 현재 최고 성능의 모델보다 훨씬 향상되었습니다. 우리 방법의 발전을 더 잘 이해하기 위해 추가 분석이 표시됩니다."
450,http://arxiv.org/abs/2308.04802 ,Generalized Unbiased Scene Graph Generation,"Xinyu Lyu, Lianli Gao, Junlin Xie, Pengpeng Zeng, Yulu Tian, Jie Shao, Heng Tao Shen","기존 USGG(Unbiased Scene Graph Generation) 방법은 개념 수준 불균형을 간과하면서 고주파 클래스가 희귀 클래스 예측을 지배하는 조건자 수준 불균형을 해결하는 데만 중점을 둡니다. 실제로 술어 자체가 균형을 이룬다 하더라도 맥락의 긴 꼬리 분포(즉, 주체-객체 조합)로 인해 술어 내에는 여전히 심각한 개념-불균형이 존재합니다. 주체-객체 쌍은 본질적으로 조합이 복잡하기 때문에 이러한 개념 수준 불균형은 술어 수준 불균형에 비해 더 광범위하고 어려운 문제를 제기합니다. 따라서 우리는 술어 수준과 개념 수준의 불균형을 모두 고려하는 G-USGG(Generalized Unbiased Scene Graph Generation)라는 새로운 연구 문제를 소개합니다. 마지막으로 우리는 희귀/비일반적/공통 개념 전반에 걸쳐 균형 잡힌 학습 프로세스를 보장하는 MCL(Multi-Concept Learning) 프레임워크를 제안합니다. MCL은 먼저 동일한 클래스 내의 여러 개념 프로토타입으로 표현되는 서로 다른 양의 개념 측면에서 술어 전체의 개념 수준 불균형을 수량화합니다. 그런 다음 개념 정규화(CR) 기술을 적용하여 개념 프로토타입을 효과적으로 학습합니다. 또한 다양한 개념에 대한 균형 잡힌 학습을 달성하기 위해 SGG 모델이 개념 프로토타입에 대한 균형 잡힌 표현을 생성하도록 안내하는 BPM(균형 프로토타입 메모리)을 도입합니다. 광범위한 실험을 통해 VG-SGG 및 OI-SGG 데이터 세트 모두에서 벤치마크 모델의 성능을 향상시키는 데 있어 모델에 구애받지 않는 전략의 놀라운 효율성이 입증되어 술어 수준의 편견 없는 관계 인식과 개념 수준의 구성 생성성이라는 두 가지 주요 측면에서 새로운 최첨단 성과를 거두었습니다."
449,http://arxiv.org/abs/2308.04758 ,Bird's-Eye-View Scene Graph for Vision-Language Navigation,"Rui Liu, Xiaohan Wang, Wenguan Wang, Yi Yang","에이전트가 인간의 지시에 따라 3D 환경을 탐색하는 비전 언어 탐색(VLN)은 큰 발전을 보였습니다. 그러나 현재 에이전트는 파노라마 관찰을 기반으로 구축되어 있어 3D 장면 형상을 인식하는 능력을 방해하고 파노라마 뷰를 모호하게 선택하기 쉽습니다. 이러한 제한 사항을 해결하기 위해 우리는 다단계 BEV 표현을 활용하여 3D 감지 감독 하에 실내 환경의 장면 레이아웃과 기하학적 단서를 인코딩하는 BEV 장면 그래프(BSG)를 제시합니다. 탐색 중에 BSG는 각 단계에서 로컬 BEV 표현을 구축하고 BEV 기반 글로벌 장면 맵을 유지 관리합니다. 이 지도는 온라인으로 수집된 모든 로컬 BEV 표현을 토폴로지 관계에 따라 저장하고 구성합니다. 에이전트는 BSG를 기반으로 로컬 BEV 그리드 수준 결정 점수와 글로벌 그래프 수준 결정 점수를 파노라마 뷰의 하위 뷰 선택 점수와 결합하여 예측하여 보다 정확한 동작 예측을 수행합니다. 우리의 접근 방식은 REVERIE, R2R 및 R4R에 대한 최첨단 방법을 크게 능가하여 VLN에서 BEV 인식의 잠재력을 보여줍니다."
448,http://arxiv.org/abs/2308.04468 ,3D Scene Diffusion Guidance using Scene Graphs,"Mohammad Naanaa, Katharina Schmid, Yinyu Nie",고품질 3D 장면의 유도 합성은 어려운 작업입니다. 확산 모델은 3D 장면을 포함한 다양한 데이터 생성에 대한 가능성을 보여주었습니다. 그러나 현재 방법은 생성을 제어하기 위해 텍스트 임베딩에 직접 의존하므로 객체 간의 복잡한 공간 관계 통합이 제한됩니다. 우리는 장면 그래프를 활용한 3차원 장면 확산 안내를 위한 새로운 접근 방식을 제안합니다. 장면 그래프가 제공하는 상대적 공간 정보를 활용하기 위해 잡음 제거 네트워크 내에서 관계형 그래프 컨벌루션 블록을 사용합니다. 우리는 우리의 접근 방식이 장면 설명과 생성된 장면 간의 정렬을 크게 향상시킨다는 것을 보여줍니다.
447,http://arxiv.org/abs/2308.03282 ,Environment-Invariant Curriculum Relation Learning for Fine-Grained Scene Graph Generation,"Yukuan Min, Aming Wu, Cheng Deng","SGG(장면 그래프 생성) 작업은 주제-객체 쌍을 기반으로 조건자를 식별하도록 설계되었습니다. 그러나 기존 데이터 세트에는 일반적으로 두 가지 불균형 사례가 포함됩니다. 하나는 예측 조건자의 클래스 불균형이고 다른 하나는 주어진 주제-객체 쌍의 컨텍스트 불균형으로, 이는 SGG에 중요한 과제를 제시합니다. 대부분의 기존 방법은 예측 술어의 불균형에만 초점을 맞추고 주어-객체 쌍의 불균형을 무시하여 만족스러운 결과를 얻지 못했습니다. 두 가지 불균형 사례를 해결하기 위해 우리는 기존 SGG 방법에 플러그 앤 플레이 방식으로 적용할 수 있는 새로운 EICR(Environment Invariant Curriculum Relation Learning) 방법을 제안합니다. 구체적으로 주체-객체 쌍의 불균형을 제거하기 위해 먼저 주체-객체 쌍에 대해 서로 다른 분포 환경을 구축하고 환경 변화에 불변하는 모델을 학습합니다. 그런 다음, 전제적 불균형을 제거하기 위해 서로 다른 환경의 균형을 맞추는 클래스 균형 교육과정 학습 전략을 구축합니다. VG 및 GQA 데이터 세트에 대해 수행된 포괄적인 실험은 EICR 프레임워크가 다양한 SGG 모델에 대한 일반적인 전략으로 채택되어 상당한 개선을 달성할 수 있음을 보여줍니다."
446,http://arxiv.org/abs/2308.02339 ,Improving Scene Graph Generation with Superpixel-Based Interaction Learning,"Jingyi Wang, Can Zhang, Jinfa Huang, Botao Ren, Zhidong Deng","장면 그래프 생성(SGG)의 최근 발전은 일반적으로 사전 정의된 감지기의 상자 수준 기능을 활용하여 개체 간의 관계를 모델링합니다. 우리는 SGG에서 간과된 문제는 상자 간의 조잡한 상호작용으로, 관계 모델링에 대한 맥락적 의미를 부적절하게 포착하여 해당 분야의 발전을 실질적으로 제한한다고 주장합니다. 본 논문에서는 박스 수준에서 거친 상호작용을 해결하기 위해 슈퍼픽셀 기반 상호작용 학습(SIL)이라는 일반적인 패러다임을 탐색하고 제안하는 데 앞장서고 있습니다. 이를 통해 SGG의 슈퍼픽셀 수준에서 세분화된 상호 작용을 모델링할 수 있습니다. 구체적으로 (i) 장면을 점 집합으로 처리하고 장면의 하위 영역을 나타내는 슈퍼픽셀로 클러스터링합니다. (ii) 초기 단계에서 엔터티 간의 세분화된 상호 작용을 강화하기 위해 슈퍼픽셀 간의 엔터티 내 및 엔터티 간 상호 작용을 탐색합니다. 두 가지 까다로운 벤치마크(Visual Genome 및 Open Image V6)에 대한 광범위한 실험을 통해 우리의 SIL이 이전 박스 수준 방법보다 슈퍼픽셀 수준에서 세밀한 상호 작용을 가능하게 하고 모든 측정 항목에서 이전의 최첨단 방법보다 훨씬 뛰어난 성능을 발휘한다는 것이 입증되었습니다. 더욱 고무적인 점은 제안된 방법을 플러그 앤 플레이 방식으로 기존 박스 수준 접근 방식의 성능을 향상시키는 데 적용할 수 있다는 것입니다. 특히 SIL은 Visual Genome의 PredCls 작업 기준선을 평균 2.0% mR(최대 3.4%) 향상시켜 기존 박스 수준 방법과의 통합을 용이하게 합니다."
445,http://arxiv.org/abs/2308.01180 ,Interpretable End-to-End Driving Model for Implicit Scene Understanding,"Yiyang Sun, Xiaonian Wang, Yangyang Zhang, Jiagui Tang, Xiaqiang Tang, Jing Yao","주행 장면 이해는 센서 데이터를 통해 종합적인 장면 정보를 획득하고, 자율주행차의 안전을 위해 필수적인 후속 작업의 기반을 제공하는 것입니다. 객체 감지 및 장면 그래프 생성과 같은 특정 인식 작업이 일반적으로 사용됩니다. 그러나 이러한 작업의 결과는 시나리오를 표현하기에 충분하지 않은 고차원 장면 특징에서 샘플링하는 특성화와만 동일합니다. 또한, 지각 작업의 목표는 자아 궤적에 영향을 미칠 수 있는 것에만 초점을 맞추는 인간 운전과 일치하지 않습니다. 따라서 우리는 계획 모듈에 의해 안내되는 장면 이해 결과로서 암시적 고차원 장면 특징을 추출하고 시각화를 위한 보조 인식 작업을 사용하여 장면 이해의 타당성을 검증하기 위한 엔드투엔드 해석 가능한 암시적 운전 장면 이해(II-DSU) 모델을 제안합니다. CARLA 벤치마크에 대한 실험 결과는 우리의 접근 방식이 새로운 최첨단 기술을 달성하고 운전과 관련된 보다 풍부한 장면 정보를 구현하는 장면 기능을 얻을 수 있어 다운스트림 계획의 우수한 성능을 가능하게 한다는 것을 보여줍니다."
444,http://arxiv.org/abs/2307.16309 ,Triple Correlations-Guided Label Supplementation for Unbiased Video Scene Graph Generation,"Wenqing Wang, Kaifeng Gao, Yawei Luo, Tao Jiang, Fei Gao, Jian Shao, Jianwen Sun, Jun Xiao",비디오 기반 장면 그래프 생성(VidSGG)은 시각적 개체와 그 관계를 식별하여 비디오 콘텐츠를 동적 그래프로 표현하는 것을 목표로 하는 접근 방식입니다. 본질적으로 편향된 분포와 교육 데이터의 누락된 주석으로 인해 현재 VidSGG 방법은 덜 대표되는 조건자에서는 제대로 수행되지 않는 것으로 나타났습니다. 본 논문에서는 Ground-Truth 주석에 나타나야 하는 누락된 조건자를 보완하여 이 과소 탐구 문제를 해결하기 위한 명시적인 솔루션을 제안합니다. Trico라고 불리는 우리의 방법은 세 가지 보완적인 시공간 상관 관계를 탐색하여 누락된 술어를 보완하려고 합니다. 이러한 상관 관계에 따라 누락된 레이블을 효과적으로 보완하여 편견 없는 예측을 달성할 수 있습니다. 우리는 가장 널리 사용되는 VidSGG 데이터 세트(예: VidVRD 및 VidOR)에서 Trico의 효율성을 검증합니다. 광범위한 실험을 통해 특히 꼬리 조건부에서 Trico가 달성한 최첨단 성능이 입증되었습니다.
443,http://arxiv.org/abs/2307.16206 ,Synthesizing Event-centric Knowledge Graphs of Daily Activities Using Virtual Space,"Shusaku Egami, Takanori Ugai, Mikiko Oono, Koji Kitamura, Ken Fukuda","인공지능(AI)은 가정 환경에서 일상생활의 다양한 맥락 정보를 이해해 다양한 상황에서 인간의 행동과 의사결정을 지원할 수 있는 소프트웨어 에이전트, 로봇, 사이버물리시스템 등으로 구현될 것으로 예상된다. 이러한 기대에 부응하는 지식 기반의 구체화된 질문답변으로 장면 그래프(Scene Graph)와 지식 그래프(KG) 구축 기술이 많은 주목을 받고 있다. 그러나 물리적 공간에서 다양한 실험 조건 하에서 일상 활동에 대한 실제 데이터를 수집하고 관리하는 것은 상당한 비용이 들고, 의도와 맥락을 이해하는 AI를 개발하는 것은 어렵다. 앞으로는 조건을 쉽게 변경할 수 있는 가상 공간과 조건을 변경하기 어려운 물리적 공간의 데이터를 결합해 일상생활 활동을 분석할 수 있을 것으로 기대된다. 그러나 가상공간을 활용한 일상활동의 KG구축과 그 적용에 관한 연구는 아직까지 진전이 없다. 인간의 일상생활을 위한 AI 개발을 촉진하려면 잠재력과 과제가 여전히 명확해야 합니다. 따라서 본 연구에서는 가상 공간에서 일상생활 활동의 합성 KG를 생성하기 위한 VirtualHome2KG 프레임워크를 제안합니다. 이 프레임워크는 제안된 이벤트 중심 스키마와 가상 공간 시뮬레이션 결과를 기반으로 일상 활동의 합성 비디오 데이터와 비디오 콘텐츠에 해당하는 상황별 의미 데이터를 모두 강화합니다. 따라서 상황 인식 데이터를 분석할 수 있으며, 관련 데이터 및 의미 정보의 가용성이 부족하여 기존 개발이 어려웠던 다양한 애플리케이션 개발이 가능해졌습니다. 또한 쿼리, 임베딩 및 클러스터링을 통한 일상 활동 분석과 낙상 위험 감지를 포함한 여러 사용 사례를 통해 제안된 VirtualHome2KG 프레임워크의 유용성과 잠재력을 보여줍니다."
442,http://arxiv.org/abs/2307.15567 ,Panoptic Scene Graph Generation with Semantics-Prototype Learning,"Li Li, Wei Ji, Yiming Wu, Mengze Li, You Qin, Lina Wei, Roger Zimmermann","PSG(Panoptic Scene Graph Generation)는 객체를 구문 분석하고 객체의 관계(술어)를 예측하여 인간의 언어와 시각적 장면을 연결합니다. 그러나 주석자의 서로 다른 언어 선호도와 조건자 간의 의미론적 중복으로 인해 데이터세트에서 편향된 조건자 주석이 발생합니다. 즉, 동일한 개체 쌍에 대한 서로 다른 조건자입니다. 편향된 술어 주석으로 인해 PSG 모델은 술어 간의 명확한 결정 평면을 구성하는 데 어려움을 겪게 되며, 이는 PSG 모델의 실제 적용을 크게 방해합니다. 위의 본질적인 편견을 해결하기 위해 우리는 편향된 조건자 주석을 유익하고 통합된 주석으로 적응적으로 전송하는 ADTrans라는 새로운 프레임워크를 제안합니다. 전송 프로세스 동안 일관성과 정확성을 보장하기 위해 우리는 각 술어 클래스에서 표현의 불변성을 측정하고 다양한 강도를 가진 편견 없는 술어 프로토타입을 학습할 것을 제안합니다. 한편, 우리는 각 프레젠테이션과 프로토타입 간의 분포 변화를 지속적으로 측정하고 잠재적인 편향된 데이터를 지속적으로 선별합니다. 마지막으로 편향되지 않은 조건자 프로토타입 표현 삽입 공간을 사용하면 편향된 주석을 쉽게 식별할 수 있습니다. 실험에 따르면 ADTrans는 벤치마크 모델의 성능을 크게 향상시켜 새로운 최첨단 성능을 달성하고 여러 데이터 세트에 대한 뛰어난 일반화 및 효율성을 보여줍니다."
441,http://arxiv.org/abs/2307.15377 ,Co-attention Graph Pooling for Efficient Pairwise Graph Interaction Learning,"Junhyun Lee, Bumsoo Kim, Minji Jeon, Jaewoo Kang","그래프 신경망(GNN)은 그래프 구조 데이터를 처리하고 학습하는 데 효과적인 것으로 입증되었습니다. 그러나 이전 작업은 주로 단일 그래프 입력을 이해하는 데 중점을 두었지만 많은 실제 응용 프로그램에서는 그래프 구조 데이터(예: 장면 그래프 일치, 코드 검색 및 약물-약물 상호 작용 예측)에 대한 쌍별 분석이 필요합니다. 이를 위해 최근 연구에서는 그래프 쌍 간의 상호 작용을 학습하는 데 초점을 맞추고 있습니다. 향상된 성능에도 불구하고 이러한 작업은 상호 작용이 노드 수준에서 고려되어 높은 계산 비용과 차선의 성능을 초래한다는 점에서 여전히 제한적이었습니다. 이 문제를 해결하기 위해 우리는 그래프 풀링에서 Co-Attention을 사용하여 상호 작용 표현을 추출하기 위한 새롭고 효율적인 그래프 수준 접근 방식을 제안합니다. 우리의 방법인 CAGPool(Co-Attention Graph Pooling)은 실제 데이터 세트를 사용하는 분류 및 회귀 작업 모두에서 기존 방법에 비해 경쟁력 있는 성능을 나타내면서 계산 복잡성을 낮춥니다."
440,http://arxiv.org/abs/2307.14009 ,Car-Studio: Learning Car Radiance Fields from Single-View and Endless In-the-wild Images,"Tianyu Liu, Hao Zhao, Yang Yu, Guyue Zhou, Ming Liu",구성 신경 장면 그래프 연구에 따르면 복사 필드는 편집 가능한 자율 주행 시뮬레이터에서 효율적인 도구가 될 수 있습니다. 그러나 이전 연구에서는 일련의 자율 주행 데이터 세트 내에서 학습했기 때문에 시뮬레이터에서 자동차를 회전할 때 만족스럽지 못한 흐릿함이 발생했습니다. 이 편지에서 우리는 제약 없는 이미지를 학습하고 처리된 이미지로부터 데이터 세트를 구축하기 위한 파이프라인을 제안합니다. 관점이 바뀔 때 차량의 선명도를 유지하고 편집 시 아티팩트를 피하기 위해 윤곽이 배경에서 선명하게 유지되어야 하는 시뮬레이터의 요구 사항을 충족하기 위해 우리는 도시 장면 전경의 중요한 부분인 차량의 방사선 필드를 설계합니다. 실험을 통해 우리 모델이 기준선과 비교하여 경쟁력 있는 성능을 달성한다는 것을 보여줍니다. 실제 이미지로 구축된 데이터 세트를 사용하여 우리의 방법은 점차적으로 제어 가능한 모양 편집 기능을 제시합니다. 해당 분야의 추가 연구를 촉진하기 위해 https://lty2226262.github.io/car-studio/에 데이터세트와 코드를 공개할 예정입니다.
439,http://arxiv.org/abs/2307.08699 ,Pair then Relation: Pair-Net for Panoptic Scene Graph Generation,"Jinghao Wang, Zhengyu Wen, Xiangtai Li, Zujin Guo, Jingkang Yang, Ziwei Liu","PSG(Panoptic Scene Graph)는 상자 대신 Panoptic 분할을 사용하여 보다 포괄적인 장면 그래프 표현을 생성하는 것을 목표로 하는 SGG(장면 그래프 생성)의 어려운 작업입니다. SGG와 비교하여 PSG에는 픽셀 수준 세그먼트 출력 및 전체 관계 탐색(사물 관계도 고려함)이라는 몇 가지 어려운 문제가 있습니다. 따라서 현재 PSG 방법은 성능이 제한되어 있어 다운스트림 작업이나 애플리케이션을 방해합니다. 이 작업의 목표는 PSG를 위한 새롭고 강력한 기준선을 설계하는 것입니다. 이를 달성하기 위해 먼저 현재 PSG 모델의 병목 현상을 식별하기 위한 심층 분석을 수행하여 개체 간 쌍별 리콜이 이전 PSG 방법에서 무시되었던 중요한 요소임을 확인했습니다. 이 프레임워크와 최근의 쿼리 기반 프레임워크를 기반으로 우리는 PPN(Pair Proposal Network)을 사용하여 주체와 개체 간의 희소 쌍 관계를 학습하고 필터링하는 새로운 프레임워크인 pair then Relation(Pair-Net)을 제시합니다. 또한, 우리는 두 가지 모두에 대한 개체 쌍의 희박한 특성을 관찰했습니다. 이에 동기를 부여하여 쌍 제안 생성을 위한 쌍별 관계를 직접 학습하는 PPN 내에 경량 매트릭스 학습기를 설계했습니다. 광범위한 절제 및 분석을 통해 분할기의 견고한 기준을 활용하면 우리의 접근 방식이 크게 향상됩니다. 특히, 우리의 방법은 우리의 기준인 PSGFormer에 비해 10% 이상의 절대 이득을 달성합니다. 이 백서의 코드는 https://github.com/king159/Pair-Net에서 공개적으로 제공됩니다."
438,http://arxiv.org/abs/2307.07468 ,SGGNet$^2$: Speech-Scene Graph Grounding Network for Speech-guided Navigation,"Dohyun Kim, Yeseung Kim, Jaehwi Jang, Minjae Song, Woojin Choi, Daehyung Park",음성 언어는 비전문가와 장애인 사용자가 복잡한 보조 로봇과 상호 작용할 수 있도록 하는 접근 가능하고 효율적인 인터페이스 역할을 합니다. 그러나 언어 발화를 정확하게 접지하는 것은 화자의 목소리와 환경 소음의 음향적 가변성으로 인해 상당한 어려움을 안겨줍니다. 본 연구에서는 자동 음성 인식(ASR) 시스템에서 얻은 올바르게 인식된 단어와 잘못 인식된 단어 간의 음향 유사성을 활용하여 음성 발화를 강력하게 접지하는 새로운 음성 장면 그래프 접지 네트워크(SGGNet$^2$)를 제안합니다. 음향 유사성을 통합하기 위해 이전 접지 모델인 장면 그래프 기반 접지 네트워크(SGGNet)를 NVIDIA NeMo의 ASR 모델로 확장합니다. 우리는 음성 발음의 잠재 벡터를 SGGNet 내의 BERT 기반 접지 네트워크에 공급하여 이를 달성합니다. 우리는 질적 및 양적 연구를 통해 접지에 음성 명령의 잠재 벡터를 사용하는 효과를 평가합니다. 또한 Rainbow Robotics의 실제 4족 보행 로봇 RBQ-3을 사용하여 음성 기반 탐색 작업에서 SGGNet$^2$의 기능을 시연합니다.
437,http://arxiv.org/abs/2307.06135 ,SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning,"Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian Reid, Niko Suenderhauf","LLM(대형 언어 모델)은 다양한 작업을 위한 일반 계획 에이전트를 개발하는 데 인상적인 결과를 보여주었습니다. 그러나 이러한 계획을 광대한 다층 및 다중 공간 환경에 기반을 두는 것은 로봇 공학에 있어 중요한 과제를 제시합니다. 3D 장면 그래프(3DSG) 표현을 사용하여 로봇 공학을 위한 LLM 기반 대규모 작업 계획에 대한 확장 가능한 접근 방식인 SayPlan을 소개합니다. 접근 방식의 확장성을 보장하기 위해 (1) 3DSG의 계층적 특성을 활용하여 LLM이 전체 그래프의 더 작고 축소된 표현에서 작업 관련 하위 그래프에 대한 '의미론적 검색'을 수행할 수 있도록 합니다. (2) 클래식 경로 플래너를 통합하여 LLM의 계획 기간을 줄이고 (3) 장면 그래프 시뮬레이터의 피드백을 사용하여 초기 계획을 구체화하고 실행 불가능한 작업을 수정하며 계획 실패를 방지하는 '반복적 재계획' 파이프라인을 도입합니다. 우리는 140개의 자산과 개체가 포함된 최대 3층과 36개 방에 이르는 두 개의 대규모 환경에 대한 접근 방식을 평가하고, 우리의 접근 방식이 모바일 조작 로봇이 실행할 추상 및 자연어 명령을 통해 대규모의 장기적인 작업 계획을 기반으로 할 수 있음을 보여줍니다. 우리는 프로젝트 페이지 https://sayplan.github.io에서 실제 로봇 비디오 데모를 제공합니다."
436,http://arxiv.org/abs/2307.05276 ,Unbiased Scene Graph Generation via Two-stage Causal Modeling,"Shuzhou Sun, Shuaifeng Zhi, Qing Liao, Janne Heikkilä, Li Liu","최근 편향되지 않은 장면 그래프 생성(SGG) 방법의 인상적인 성능에도 불구하고 현재의 편향성 제거 문헌은 주로 긴 꼬리 분포 문제에 중점을 두는 반면 편향의 또 다른 원인, 즉 SGG 모델이 유사한 관계에 대해 잘못된 예측을 생성하기 쉬운 의미론적 혼란을 간과합니다. 본 논문에서는 인과 추론을 활용하는 SGG 작업에 대한 편향성 제거 절차를 살펴봅니다. 우리의 핵심 통찰력은 인과 관계의 SMS(Sparse Mechanism Shift)가 다중 편향에 대한 독립적인 개입을 허용함으로써 정보가 풍부한 꼬리 관계의 예측을 추구하는 동시에 헤드 카테고리 성능을 잠재적으로 보존한다는 것입니다. 그러나 시끄러운 데이터 세트는 SGG 작업에 대해 관찰되지 않은 혼란을 야기하므로 구성된 인과 모델은 항상 SMS의 이점을 얻기에는 인과 관계가 부족합니다. 이 문제를 해결하기 위해 우리는 SGG 작업에 대한 2단계 인과 모델링(TsCM)을 제안합니다. 이는 긴 꼬리 분포와 의미 혼란을 구조적 인과 모델(SCM)의 혼란 요인으로 삼은 다음 인과 개입을 두 단계로 분리합니다. 첫 번째 단계는 인과적 표현 학습으로, 의미 혼란 혼란에 개입하기 위해 새로운 P-Loss(Population Loss)를 사용합니다. 두 번째 단계에서는 긴 꼬리 분포 교란 요인을 제거하여 인과 교정 학습을 완료하는 적응형 로짓 조정(AL-Adjustment)을 도입합니다. 이 두 단계는 모델에 구애받지 않으므로 편견 없는 예측을 추구하는 모든 SGG 모델에서 사용할 수 있습니다. 인기 있는 SGG 백본과 벤치마크에서 수행된 포괄적인 실험은 우리의 TsCM이 평균 재현율 측면에서 최첨단 성능을 달성할 수 있음을 보여줍니다. 또한 TsCM은 다른 편향성 제거 방법보다 더 높은 재현율을 유지할 수 있으며 이는 우리의 방법이 머리와 꼬리 관계 사이에서 더 나은 균형을 달성할 수 있음을 나타냅니다."
435,http://arxiv.org/abs/2307.03339 ,Open-Vocabulary Object Detection via Scene Graph Discovery,"Hengcan Shi, Munawar Hayat, Jianfei Cai","최근 몇 년 동안 개방형 어휘(OV) 객체 감지에 대한 연구 관심이 높아지고 있습니다. 고정된 범주 개체만 인식하는 기존 탐지와 달리 OV 탐지는 열린 범주 집합의 개체를 탐지하는 것을 목표로 합니다. 이전 작업에서는 VL(시각 언어) 교육 데이터(예: 접지 데이터 참조)를 활용하여 OV 객체를 인식하는 경우가 많습니다. 그러나 VL 데이터에서는 명사 쌍과 개별 객체만 사용하는 반면, 이러한 데이터에는 일반적으로 OV 감지에 중요한 장면 그래프와 같은 훨씬 더 많은 정보가 포함됩니다. 본 논문에서는 OV 감지를 위해 장면 그래프 큐를 활용하는 새로운 SGDN(Scene-Graph-Based Discovery Network)을 제안합니다. 먼저 SSGA(sparse scene-graph-guided attention)를 포함한 장면 그래프 기반 디코더(SGDecoder)가 제시됩니다. 장면 그래프를 캡처하고 이를 활용하여 OV 개체를 검색합니다. 둘째, 장면 그래프 추출과 객체 위치 파악 간의 상호 향상을 가능하게 하는 장면 그래프 기반 오프셋 회귀(SGOR) 메커니즘을 구축하는 장면 그래프 기반 예측(SGPred)을 제안합니다. 셋째, SGPred에서 교차 모드 학습 메커니즘을 설계합니다. OV 객체 분류를 위한 크로스 모달 임베딩 간의 일관성을 향상시키기 위해 장면 그래프를 브리지로 사용합니다. COCO 및 LVIS에 대한 실험은 우리 접근 방식의 효율성을 보여줍니다. 또한 이전 OV 장면 그래프 생성 방법으로는 이 작업을 처리할 수 없었지만 OV 장면 그래프 감지를 위한 모델의 기능을 보여줍니다."
434,http://arxiv.org/abs/2306.17469 ,Manga109Dialog: A Large-scale Dialogue Dataset for Comics Speaker Detection,"Yingxuan Li, Kiyoharu Aizawa, Yusuke Matsui","전자 만화 시장이 확대되면서 만화를 분석하는 자동화된 방법 개발에 대한 관심이 촉발되었습니다. 만화를 더 깊이 이해하려면 만화의 텍스트를 단어를 말하는 캐릭터에 연결하는 자동화된 접근 방식이 필요합니다. 만화 화자 탐지 연구는 오디오북의 자동 캐릭터 할당, 캐릭터 성격에 따른 자동 번역, 캐릭터 관계 및 스토리 추론 등의 실용적인 응용 분야를 가지고 있습니다.   화자-텍스트 주석이 충분하지 않은 문제를 해결하기 위해 Manga109를 기반으로 하는 새로운 주석 데이터 세트 MANGA109Dialog를 만들었습니다. MANGA109Dialog는 132,692개의 화자-텍스트 쌍을 포함하는 세계 최대의 만화 화자 주석 데이터 세트입니다. 우리는 화자 감지 방법을 보다 적절하게 평가하기 위해 예측 난이도에 따라 데이터 세트를 다양한 수준으로 나누었습니다. 거리 중심의 기존 방식과 달리 장면 그래프 생성 모델을 활용한 딥러닝 기반의 방식을 제안한다. 만화의 독특한 특성으로 인해 프레임 읽기 순서를 고려하여 제안 모델의 성능을 향상시킵니다. MANGA109Dialog 및 기타 데이터 세트를 사용하여 실험을 수행했습니다. 실험 결과는 우리의 장면 그래프 기반 접근 방식이 기존 방법보다 성능이 뛰어나 75% 이상의 예측 정확도를 달성한다는 것을 보여줍니다."
433,http://arxiv.org/abs/2307.05356 ,VisText: A Benchmark for Semantically Rich Chart Captioning,"Benny J. Tang, Angie Boggust, Arvind Satyanarayan","차트를 설명하는 캡션은 표시된 데이터의 기억력과 이해력을 향상시키고 시각 장애가 있는 사람들에게 보다 접근하기 쉬운 매체를 제공하는 데 도움이 됩니다. 그러나 이러한 캡션을 자동으로 생성하기 위한 현재 접근 방식은 차트의 특징인 지각적 또는 인지적 특징(예: 복잡한 추세 및 패턴)을 명확하게 표현하는 데 어려움을 겪고 있습니다. 이에 대한 응답으로 우리는 차트 구성을 설명하고 주요 통계를 보고하며 지각 및 인지 현상을 식별하는 12,441쌍의 차트와 캡션으로 구성된 데이터 세트인 VisText를 소개합니다. VisText에서 차트는 래스터화된 이미지, 지원 데이터 테이블, 장면 그래프(웹 페이지의 DOM(문서 개체 모델)과 유사한 차트의 시각적 요소에 대한 계층적 표현)의 세 가지 표현으로 제공됩니다. VisText의 영향을 평가하기 위해 우리는 차트 캡션 작업에 대한 최첨단 언어 모델을 미세 조정하고 접두사 조정을 적용하여 전달하는 의미 체계 내용을 변경하는 캡션을 생성합니다. 우리의 모델은 일관되고 의미론적으로 풍부한 캡션을 생성하고 기계 번역 및 텍스트 생성 메트릭 전반에 걸쳐 최첨단 차트 캡션 모델과 동등한 성능을 발휘합니다. 질적 분석을 통해 우리는 모델에서 발생하는 향후 작업에 영향을 미칠 수 있는 6가지 광범위한 오류 범주를 식별합니다."
432,http://arxiv.org/abs/2306.13760 ,Task-Driven Graph Attention for Hierarchical Relational Object Navigation,"Michael Lingelbach, Chengshu Li, Minjune Hwang, Andrey Kurenkov, Alan Lou, Roberto Martín-Martín, Ruohan Zhang, Li Fei-Fei, Jiajun Wu",대규모 장면에 구현된 AI 에이전트는 객체를 찾기 위해 탐색해야 하는 경우가 많습니다. 이 작업에서 우리는 객체 탐색 작업의 자연스럽게 나타나는 변형인 계층적 관계형 객체 탐색(HRON)을 연구합니다. 여기서 목표는 부엌 테이블 위에 있는 사과를 찾는 것과 같이 계층 구조로 구성된 논리적 조건으로 지정된 객체(가구와 방에 관련된 객체)를 찾는 것입니다. 이러한 작업을 해결하려면 대상 관계를 추론하고 환경과 작업 목표의 관계를 연관시키는 효율적인 표현이 필요합니다. 대규모 장면(예: 집)의 HRON은 부분적인 관찰 가능성과 긴 지평선으로 인해 특히 어렵습니다. 따라서 장면을 효과적으로 탐색하면서 과거 정보를 콤팩트하게 저장할 수 있는 솔루션이 필요합니다. 우리는 장면 그래프가 이미지나 2D 지도와 같은 기존 표현에 비해 가장 적합한 표현임을 실험적으로 보여줍니다. 우리는 장면 그래프를 입력의 일부로 사용하고 통합 작업 중심 주의 메커니즘을 통해 그래프 신경망을 백본으로 통합하고 최첨단 기준보다 더 나은 확장성과 학습 효율성을 입증하는 솔루션을 제안합니다.
431,http://arxiv.org/abs/2306.13420 ,Towards Unseen Triples: Effective Text-Image-joint Learning for Scene Graph Generation,"Qianji Di, Wenxi Ma, Zhongang Qi, Tianxiang Hou, Ying Shan, Hanzi Wang","SGG(장면 그래프 생성)는 이미지에서 객체와 객체의 연결을 구조적이고 포괄적으로 표현하는 것을 목표로 하며 장면 이해 및 기타 관련 다운스트림 작업에 큰 도움이 될 수 있습니다. 기존 SGG 모델은 편향된 데이터 세트로 인해 발생하는 긴 꼬리 문제를 해결하는 데 종종 어려움을 겪습니다. 그러나 이러한 모델이 특정 데이터 세트에 더 잘 적합하더라도 훈련 세트에 포함되지 않은 보이지 않는 트리플을 해결하기 어려울 수 있습니다. 대부분의 방법은 전체 트리플을 제공하고 통계적 기계 학습을 기반으로 전체 기능을 학습하는 경향이 있습니다. 이러한 모델은 훈련 세트의 객체와 조건자가 테스트 세트의 새로운 트리플과 다르게 결합되기 때문에 보이지 않는 트리플을 예측하는 데 어려움을 겪습니다. 본 연구에서는 보이지 않는 트리플을 해결하고 SGG 모델의 일반화 기능을 향상시키기 위해 TISGG(텍스트-이미지-조인트 장면 그래프 생성) 모델을 제안합니다. 우리는 객체와 술어 범주를 특징 수준에서 별도로 학습하고 모델이 더 이상 트리플 매칭으로 제한되지 않도록 해당 시각적 특징과 정렬하는 JFL(Joint Fearture Learning) 모듈과 FKR(Factual Knowledge based Refinement) 모듈을 제안합니다. 또한, 긴 꼬리 문제가 일반화 능력에도 영향을 미치기 때문에 CGS(Charater Guided Sampling) 및 IR(Informative Re-weighting) 모듈을 포함한 새로운 균형 학습 전략을 설계하여 각 술어의 성격에 따른 맞춤형 학습 방법을 제공합니다. 광범위한 실험을 통해 우리 모델이 최첨단 성능을 달성하는 것으로 나타났습니다. 더 자세히 설명하면 TISGG는 Visual Genome 데이터 세트의 PredCls 하위 작업에서 성능을 zR@20(제로 샷 리콜)의 11.7% 향상시킵니다."
430,http://arxiv.org/abs/2306.10122 ,Multi-Label Meta Weighting for Long-Tailed Dynamic Scene Graph Generation,"Shuo Chen, Yingjun Du, Pascal Mettes, Cees G. M. Snoek","본 논문은 $\langle$subject, predicate, object$\rangle$ 세 쌍의 형태로 주어와 객체 사이의 의미론적 관계를 포착하는 것을 목표로 비디오에서 장면 그래프 생성 문제를 조사합니다. 주어와 목적어 쌍 사이의 술어를 인식하는 것은 본질적으로 공간 관계(\eg \emph{in front of})와 같은 유비쿼터스 상호 작용부터 \emph{twisting}과 같은 드문 상호 작용에 이르기까지 불균형하고 다중 레이블입니다. Action Genome, VidOR 등 널리 사용되는 벤치마크에서는 가장 빈도가 높은 술어와 가장 빈도가 낮은 술어 간의 불균형 비율이 각각 3,218 및 3,408에 이르며, 이는 롱테일 인식을 위해 특별히 설계된 벤치마크조차 능가합니다. 긴 꼬리 분포와 레이블 동시 발생으로 인해 최근 최첨단 방법은 주로 가장 자주 발생하는 술어 클래스에 초점을 맞추고 긴 꼬리에 있는 술어 클래스는 무시합니다. 본 논문에서는 비디오의 장면 그래프 생성에 대한 현재 접근 방식의 한계를 분석하고 술어 빈도와 회상 성능 간의 일대일 대응을 식별합니다. 비디오에서 편향되지 않은 장면 그래프 생성을 향한 단계를 만들기 위해 편향된 조건자 분포를 처리하는 다중 레이블 메타 학습 프레임워크를 소개합니다. 우리의 메타 학습 프레임워크는 가능한 모든 라벨 손실에 대해 각 훈련 샘플에 대한 메타 가중치 네트워크를 학습합니다. 우리는 각 벤치마크에 대한 두 가지 최신 방법을 기반으로 Action Genome 및 VidOR 벤치마크에 대한 접근 방식을 평가합니다. 실험은 다중 레이블 메타 가중치 네트워크가 헤드 클래스의 성능을 저하시키지 않으면서 롱테일의 술어에 대한 성능을 향상시켜 전반적인 성능이 향상되고 일반화 가능성이 좋아짐을 보여줍니다. 코드: \url{https://github.com/shanshuo/ML-MWN}."
429,http://arxiv.org/abs/2306.09112 ,On Certified Generalization in Structured Prediction,"Bastian Boll, Christoph Schnörr",구조화된 예측에서 대상 개체는 독립적인 구성 요소로 분해되지 않고 공통 i.i.d.를 위반하는 풍부한 내부 구조를 갖습니다. 가정. 이러한 과제는 이미지 분할이나 장면 그래프 생성과 같은 응용 프로그램에서 기하급수적으로 큰 출력 공간을 통해 분명해집니다. 우리는 일반화 비율이 구조화된 사례의 수뿐만 아니라 그 크기에 따라 조정되는 구조화된 예측을 위한 새로운 PAC-베이지안 위험 경계를 제시합니다. 생성 모델에 대한 지속적인 연구에 따른 기본 가정은 인수분해 참조 측정값의 Knothe-Rosenblatt 재배열에 의해 데이터가 생성된다는 것입니다. 이를 통해 무작위 출력 변수 간의 구조를 Wasserstein 종속성 매트릭스로 명시적으로 추출할 수 있습니다. 우리의 작업은 강력한 생성 모델을 활용하여 구조화된 예측이라는 어려운 환경에서 차별적인 다운스트림 작업에 대한 일반화 경계를 설정하기 위한 예비 단계입니다.
428,http://arxiv.org/abs/2306.05689 ,Single-Stage Visual Relationship Learning using Conditional Queries,"Alakh Desai, Tz-Ying Wu, Subarna Tripathi, Nuno Vasconcelos","장면 그래프 생성(SGG) 연구에서는 일반적으로 2단계 모델, 즉 개체 집합을 감지한 후 이를 결합하고 가능한 모든 관계에 레이블을 지정하는 모델을 고려합니다. 파이프라인 구조는 유망한 결과를 보여주지만 큰 매개변수 및 계산 오버헤드를 유발하고 일반적으로 엔드투엔드 최적화를 방해합니다. 이 문제를 해결하기 위해 최근 연구에서는 계산적으로 효율적인 단일 단계 모델을 훈련하려고 시도합니다. 집합 기반 탐지 모델인 DETR의 출현으로 1단계 모델은 단일 샷에서 주어-술어-객체 삼중항 집합을 직접 예측하려고 시도합니다. 그러나 SGG는 본질적으로 엔터티와 조건자 분포를 동시에 모델링해야 하는 다중 작업 학습 문제입니다. 본 논문에서는 SGG에 대한 조건부 쿼리가 있는 Transformer, 즉 다중 작업 학습 문제와 조합 엔터티 쌍 분포를 방지하는 SGG에 대한 새로운 공식을 갖춘 TraCQ를 제안합니다. 우리는 DETR 기반 인코더-디코더 설계를 사용하고 조건부 쿼리를 활용하여 엔터티 레이블 공간도 크게 줄입니다. 이는 최첨단 단일 단계 모델에 비해 매개변수가 20% 적습니다. 실험 결과에 따르면 TraCQ는 기존의 단일 단계 장면 그래프 생성 방법보다 성능이 뛰어날 뿐만 아니라 Visual Genome 데이터세트의 여러 최첨단 2단계 방법보다 뛰어날 뿐 아니라 엔드투엔드 훈련과 더 빠른 추론이 가능하다는 것을 보여줍니다."
427,http://arxiv.org/abs/2306.05419 ,TopoMask: Instance-Mask-Based Formulation for the Road Topology Problem via Transformer-Based Architecture,"M. Esat Kalfaoglu, Halil Ibrahim Ozturk, Ozsel Kilinc, Alptekin Temizel","운전 장면 이해 작업에는 차선, 교통 표지판, 신호등 등의 정적 요소와 이들 간의 관계를 감지하는 작업이 포함됩니다. 다중 카메라 뷰를 사용하는 포괄적인 장면 이해 솔루션 개발을 촉진하기 위해 Road Genome(OpenLane-V2)이라는 새로운 데이터 세트가 출시되었습니다. 이 데이터세트를 사용하면 복잡한 도로 연결과 차선 표시가 없을 수 있는 상황을 탐색할 수 있습니다. 전통적인 차선 표시를 사용하는 대신 이 데이터세트의 차선은 차선과 그 연결을 보다 적절하게 표현하는 중앙선으로 표시됩니다. 본 연구에서는 도로 지형의 중심선을 예측하기 위해 TopoMask라는 새로운 접근 방식을 도입했습니다. 키포인트 또는 파라메트릭 방법에 의존하는 문헌의 기존 접근 방식과 달리 TopoMask는 변환기 기반 아키텍처와 함께 인스턴스 마스크 기반 공식을 활용하고 흐름 정보로 마스크 인스턴스를 풍부하게 하기 위해 방향 레이블 표현이 제안됩니다. TopoMask는 OpenLane-V2 점수(OLS)에서 4위, OpenLane Topology Challenge 2023에서 중심선 예측 F1 점수에서 2위를 기록했습니다. 현재의 최첨단 방법인 TopoNet과 비교하여 제안하는 방법은 Frechet 기반 차선 탐지에서 유사한 성능을 달성했으며 장면 그래프 신경망을 활용하지 않고 Chamfer 기반 차선 탐지에서는 TopoNet을 능가했습니다."
426,http://arxiv.org/abs/2306.02651 ,Dynamic Interactive Relation Capturing via Scene Graph Learning for Robotic Surgical Report Generation,"Hongqiu Wang, Yueming Jin, Lei Zhu","로봇 보조 수술의 경우 정확한 수술 보고서는 수술 중 임상 수술을 반영하고 문서 입력 작업, 수술 후 분석 및 후속 치료에 도움이 됩니다. 수술 현장에서 기구와 조직 사이의 복잡하고 다양한 상호 작용으로 인해 이는 어려운 작업입니다. 딥러닝을 기반으로 한 기존 수술 보고서 ​​생성 방법은 큰 성공을 거두었지만, 조직과 도구 도구 간의 상호 작용 관계를 무시하는 경우가 많아 보고서 생성 성능이 저하됩니다. 이 논문은 조직과 수술 기구 사이의 상호작용 관계를 명시적으로 탐구함으로써 수술 보고서 ​​생성을 촉진하는 신경망을 제시합니다. 우리는 널리 사용되는 로봇 수술 벤치마크 데이터 세트에서 우리 방법의 효율성을 검증하고 실험 결과에 따르면 우리 네트워크는 기존의 최첨단 수술 보고서 ​​생성 방법(예: BLEU-1 및 ROUGE의 경우 7.48% 및 5.43% 더 높음)보다 훨씬 뛰어난 성능을 발휘할 수 있습니다."
425,http://arxiv.org/abs/2305.18668 ,Fine-Grained is Too Coarse: A Novel Data-Centric Approach for Efficient Scene Graph Generation,"Neau Maëlic, Paulo E. Santos, Anne-Gwenn Bosser, Cédric Buche","장면 그래프 형태의 원시 이미지에서 시각적 관계를 구성하는 방법을 배우는 것은 상황적 종속성으로 인해 매우 어려운 작업이지만 장면 이해에 의존하는 컴퓨터 비전 응용 프로그램에서는 필수적입니다. 그러나 장면 그래프 생성(SGG)의 현재 접근 방식은 다운스트림 작업에 유용한 그래프를 제공하는 것을 목표로 하지 않습니다. 대신, 보다 세분화된 관계를 예측하기 위해 데이터 분포를 편향화하는 작업에 주로 중점을 두었습니다. 즉, 모든 세분화된 관계가 똑같이 관련성이 있는 것은 아니며 적어도 그 중 일부는 실제 응용 프로그램에 사용되지 않습니다. 이 작업에서는 관련 관계 생성을 우선시하여 이미지 생성과 같은 다운스트림 작업에서 장면 그래프의 사용을 촉진하는 효율적인 SGG 작업을 소개합니다. 추가 접근 방식을 지원하기 위해 인기 있는 Visual Genome 데이터 세트의 주석을 기반으로 VG150 선별된 새로운 데이터 세트를 제시합니다. 우리는 일련의 실험을 통해 이 데이터세트에 SGG에서 일반적으로 사용하는 것보다 더 고품질의 다양한 주석이 포함되어 있음을 보여줍니다. 마지막으로 장면 그래프의 이미지 생성 작업에서 이 데이터 세트의 효율성을 보여줍니다."
424,http://arxiv.org/abs/2305.18391 ,MemeGraphs: Linking Memes to Knowledge Graphs,"Vasiliki Kougia, Simon Fetzel, Thomas Kirchmair, Erion Çano, Sina Moayed Baharlou, Sahand Sharifzadeh, Benjamin Roth","밈은 이미지와 텍스트의 형식을 결합하여 소셜 미디어와 인터넷 전반에서 트렌드와 아이디어를 전달하는 인기 있는 형식입니다. 유머와 빈정거림을 표현할 수 있지만 불쾌한 내용을 담고 있을 수도 있습니다. 밈의 해석은 시각적 요소, 언어 및 배경 지식에 대한 이해에 달려 있기 때문에 자동으로 밈을 분석하고 분류하는 것은 어렵습니다. 따라서 밈을 전체적으로 분류하기 위해서는 이러한 소스와 소스 간의 상호 작용을 의미 있게 표현하는 것이 중요합니다. 본 연구에서는 객체와 객체의 시각적 관계 측면에서 이미지를 표현하는 장면 그래프와 Transformer 기반 아키텍처를 사용하여 밈 분류를 위한 구조화된 표현으로 지식 그래프를 사용할 것을 제안합니다. 우리는 구조화된 밈 표현 대신 학습된 표현만 사용하는 다중 모드 모델인 ImgBERT와 우리의 접근 방식을 비교하고 일관된 개선을 관찰합니다. 또한 자동으로 생성된 그래프 및 엔터티 연결과 비교할 수 있는 휴먼 그래프 주석이 포함된 데이터 세트를 제공합니다. 분석에 따르면 자동 방법은 인간 주석자보다 더 많은 개체를 연결하고 자동으로 생성된 그래프가 밈의 증오심 분류에 더 적합하다는 사실이 밝혀졌습니다."
423,http://arxiv.org/abs/2305.17537 ,Modeling Dynamic Environments with Scene Graph Memory,"Andrey Kurenkov, Michael Lingelbach, Tanmay Agarwal, Emily Jin, Chengshu Li, Ruohan Zhang, Li Fei-Fei, Jiajun Wu, Silvio Savarese, Roberto Martín-Martín","가정 등 대규모 환경에서 객체를 검색하는 구체화된 AI 에이전트는 부분적인 정보를 기반으로 객체 위치를 예측하여 효율적인 결정을 내려야 하는 경우가 많습니다. 우리는 이것을 새로운 유형의 링크 예측 문제, 즉 부분적으로 관찰 가능한 동적 그래프에 대한 링크 예측으로 제시합니다. 우리의 그래프는 방과 물체가 노드이고 그 관계가 가장자리에 인코딩되어 있는 장면을 표현한 것입니다. 변화하는 그래프의 일부만 각 단계에서 에이전트에 알려집니다. 이러한 부분적인 관찰 가능성은 우리가 해결하는 기존 링크 예측 접근 방식에 문제를 제기합니다. 우리는 에이전트의 누적된 관찰 세트를 캡처하는 새로운 상태 표현인 장면 그래프 메모리(SGM)와 SGM에서 정보를 추출하여 효율적으로 검색하는 NEP(Node Edge Predictor)라는 신경망 아키텍처를 제안합니다. 우리는 집에서 일반적으로 볼 수 있는 의미론적 패턴에 따라 다양한 동적 그래프를 생성하는 새로운 벤치마크인 Dynamic House Simulator에서 방법을 평가하고 NEP가 다양한 개체 이동 역학을 통해 다양한 환경에서 개체의 위치를 ​​예측하도록 훈련될 수 있으며 새로운 장면 적응성과 전반적인 정확도 측면에서 기준선을 능가한다는 것을 보여줍니다. 코드베이스 등은 https://www.scenegraphmemory.com에서 확인할 수 있습니다."
422,http://arxiv.org/abs/2305.17497 ,FACTUAL: A Benchmark for Faithful and Consistent Textual Scene Graph Parsing,"Zhuang Li, Yuyang Chai, Terry Yue Zhuo, Lizhen Qu, Gholamreza Haffari, Fei Li, Donghong Ji, Quan Hung Tran","텍스트 장면 그래프 구문 분석은 이미지 캡션 평가 및 이미지 검색을 포함한 다양한 비전 언어 애플리케이션에서 점점 더 중요해지고 있습니다. 그러나 이미지 캡션을 장면 그래프로 변환하는 기존 장면 그래프 파서는 두 가지 유형의 오류가 발생하는 경우가 많습니다. 첫째, 생성된 장면 그래프는 캡션이나 해당 이미지의 진정한 의미를 포착하지 못하여 충실도가 부족합니다. 둘째, 생성된 장면 그래프는 서로 다른 주석으로 표현되는 동일한 의미로 인해 불일치가 높습니다.   이러한 문제를 해결하기 위해 우리는 FACTUAL-MR이라는 새로운 중간 표현을 사용하여 Visual Genome(VG)의 캡션에 다시 주석을 추가하는 새로운 데이터 세트를 제안합니다. FACTUAL-MR은 충실하고 일관된 장면 그래프 주석으로 직접 변환될 수 있습니다. 우리의 실험 결과는 우리의 데이터 세트에 대해 훈련된 파서가 충실도와 일관성 측면에서 기존 접근 방식보다 우수하다는 것을 분명히 보여줍니다. 이러한 개선으로 인해 이미지 캡션 평가와 제로샷 이미지 검색 작업 모두에서 성능이 크게 향상되었습니다. 또한 향상된 장면 그래프 파서와 결합하면 앞서 언급한 작업에 대한 여러 벤치마크 데이터세트에서 최첨단(SOTA) 결과를 달성하는 장면 그래프 유사성을 측정하기 위한 새로운 측정항목을 소개합니다. 코드와 데이터 세트는 https://github.com/zhuang-li/FACTUAL에서 확인할 수 있습니다."
421,http://arxiv.org/abs/2305.16925 ,How To Not Train Your Dragon: Training-free Embodied Object Goal Navigation with Semantic Frontiers,"Junting Chen, Guohao Li, Suryansh Kumar, Bernard Ghanem, Fisher Yu","개체 목표 탐색은 에이전트가 알 수 없는 환경(일반적으로 실내 장면)에서 개체 범주의 인스턴스로 이동하도록 안내하는 것과 관련된 Embodied AI의 중요한 문제입니다. 불행하게도 이 문제에 대한 현재의 최첨단 방법은 엔드투엔드 강화 학습, 모방 학습 등과 같은 데이터 기반 접근 방식에 크게 의존합니다. 더욱이, 이러한 방법은 일반적으로 훈련하는 데 비용이 많이 들고 디버깅이 어렵기 때문에 전달 가능성과 설명 가능성이 부족합니다. 고전적인 방법과 학습 방법을 결합한 최근의 성공에 영감을 받아 객체 목표 탐색 문제를 해결하기 위해 보다 고전적인 접근 방식을 수용하는 교육이 필요 없는 모듈식 솔루션을 제시합니다. 우리의 방법은 고전적인 V-SLAM(Visual Simultaneous Localization and Mapping) 프레임워크를 기반으로 구조화된 장면 표현을 구축합니다. 그런 다음 기하학적 기반 프론티어 탐색에 의미론을 주입하여 목표 객체를 검색할 유망한 영역을 추론합니다. 우리의 구조화된 장면 표현은 2D 점유 맵, 의미 포인트 클라우드 및 공간 장면 그래프로 구성됩니다.   우리의 방법은 기하학적 경계에 의미론적 지식을 도입하기 위해 언어 사전 및 장면 통계를 기반으로 장면 그래프에 의미론을 전파합니다. 주입된 의미론적 사전을 통해 에이전트는 탐색할 가장 유망한 경계에 대해 추론할 수 있습니다. 제안된 파이프라인은 Gibson 벤치마크 데이터 세트에서 객체 목표 탐색에 대한 강력한 실험 성능을 보여 이전 최첨단 기술을 능가합니다. 또한 객체 탐색 작업의 현재 병목 현상을 식별하기 위해 포괄적인 절제 연구를 수행합니다."
420,http://arxiv.org/abs/2305.16283 ,CommonScenes: Generating Commonsense 3D Indoor Scenes with Scene Graph Diffusion,"Guangyao Zhai, Evin Pınar Örnek, Shun-Cheng Wu, Yan Di, Federico Tombari, Nassir Navab, Benjamin Busam","제어 가능한 장면 합성은 다양한 산업 사용 사례를 위한 대화형 환경을 만드는 것을 목표로 합니다. 장면 그래프는 장면 컨텍스트를 간결한 방식으로 추상화하여 이러한 애플리케이션을 용이하게 하는 데 매우 적합한 인터페이스를 제공합니다. 광범위한 데이터베이스의 검색이나 사전 훈련된 모양 임베딩에 의존하는 기존 방법은 종종 장면-객체 및 객체-객체 관계를 간과하여 제한된 생성 용량으로 인해 일관되지 않은 결과를 초래합니다. 이 문제를 해결하기 위해 우리는 장면 그래프를 의미상 현실적이고 상식에 맞는 제어 가능한 3D 장면으로 변환하는 완전 생성 모델인 CommonScenes를 제시합니다. 우리의 파이프라인은 두 가지 가지로 구성됩니다. 하나는 변형 자동 인코더를 통해 전체 장면 레이아웃을 예측하고 다른 하나는 잠재 확산을 통해 호환 가능한 모양을 생성하고 모양 다양성을 유지하면서 장면 그래프에서 전역 장면-객체 및 로컬 객체 간 관계를 캡처합니다. 생성된 장면은 입력 장면 그래프를 편집하고 확산 모델에서 노이즈를 샘플링하여 조작할 수 있습니다. 관계가 포함된 고품질 객체 수준 메시를 제공하는 장면 그래프 데이터세트가 부족하기 때문에 SG-FRONT도 구성하여 추가 장면 그래프 레이블을 사용하여 기성품 실내 데이터세트 3D-FRONT를 강화합니다. CommonScenes가 생성 일관성, 품질 및 다양성과 관련하여 다른 방법에 비해 분명한 이점을 보여주는 SG-FRONT에서 광범위한 실험이 수행되었습니다. 코드와 데이터 세트는 수락 시 공개됩니다."
419,http://arxiv.org/abs/2305.14885 ,Towards View-invariant and Accurate Loop Detection Based on Scene Graph,"Chuhao Liu, Shaojie Shen","루프 감지는 누적된 포즈 드리프트를 수정하여 SLAM(Visual Simultaneous Localization and Mapping)에서 핵심 역할을 합니다. 실내 시나리오에서 풍부하게 분산된 의미론적 랜드마크는 시점 불변이며 루프 감지에서 강력한 설명력을 보유합니다. 현재 의미 기반 루프 감지는 루프를 검색하기 위해 의미 인스턴스 사이에 토폴로지를 내장합니다. 그러나 현재의 의미 기반 루프 탐지 방법은 문헌에서 완전히 다루어지지 않은 모호한 의미 인스턴스와 급격한 관점 차이를 처리하는 데 어려움을 겪고 있습니다. 본 논문에서는 실내 장면의 시각적 SLAM을 대상으로 점진적으로 생성된 장면 그래프를 기반으로 하는 새로운 루프 감지 방법을 소개합니다. 매크로 뷰 토폴로지, 마이크로 뷰 토폴로지, 의미 인스턴스 점유를 공동으로 고려하여 올바른 대응점을 찾습니다. 휴대용 RGB-D 시퀀스를 사용한 실험은 우리의 방법이 크게 변경된 시점에서 루프를 정확하게 감지할 수 있음을 보여줍니다. 유사한 토폴로지와 외관을 가진 물체를 관찰하는 데 높은 정밀도를 유지합니다. 우리의 방법은 또한 변화된 실내 장면에서도 견고하다는 것을 보여줍니다."
418,http://arxiv.org/abs/2305.14836 ,NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario,"Tianwen Qian, Jingjing Chen, Linhai Zhuo, Yang Jiao, Yu-Gang Jiang","스트리트 뷰 단서를 기반으로 자연어 질문에 답하는 것을 목표로 자율 주행의 맥락에서 새로운 시각적 질문 응답(VQA) 작업을 소개합니다. 기존 VQA 작업과 비교할 때 자율 주행 시나리오의 VQA는 더 많은 과제를 제시합니다. 첫째, 원시 시각적 데이터는 각각 카메라와 LiDAR로 캡처한 이미지와 포인트 클라우드를 포함하는 다중 모드입니다. 둘째, 데이터는 지속적인 실시간 수집으로 인해 다중 프레임입니다. 셋째, 야외 장면은 움직이는 전경과 정적인 배경을 모두 보여준다. 기존 VQA 벤치마크는 이러한 복잡성을 적절하게 해결하지 못합니다. 이러한 격차를 해소하기 위해 우리는 34K 시각적 장면과 460K 질문-답변 쌍을 포함하는 자율 주행 시나리오의 VQA에 대한 첫 번째 벤치마크인 NuScenes-QA를 제안합니다. 특히 기존 3D 감지 주석을 활용하여 장면 그래프를 생성하고 질문 템플릿을 수동으로 디자인합니다. 그 후, 이러한 템플릿을 기반으로 질문-답변 쌍이 프로그래밍 방식으로 생성됩니다. 종합적인 통계는 우리의 NuScenes-QA가 다양한 질문 형식을 갖춘 균형잡힌 대규모 벤치마크임을 증명합니다. 이를 기반으로 고급 3D 감지 및 VQA 기술을 사용하는 일련의 기준선을 개발합니다. 우리의 광범위한 실험은 이 새로운 작업으로 인한 과제를 강조합니다. 코드와 데이터 세트는 https://github.com/qiantianwen/NuScenes-QA에서 확인할 수 있습니다."
417,http://arxiv.org/abs/2305.14281 ,Weakly-Supervised Learning of Visual Relations in Multimodal Pretraining,"Emanuele Bugliarello, Aida Nematzadeh, Lisa Anne Hendricks","시각 및 언어 사전 훈련에 대한 최근 연구에서는 객체 감지 데이터의 감독 신호를 조사하여 더 세밀하고 세분화된 다중 모드 표현을 학습했습니다. 이 작업에서 우리는 한 단계 더 나아가 소규모 시각적 관계 데이터로부터 감독을 활용할 수 있는 방법을 탐구합니다. 특히, 우리는 다중 모드 설정에서 시각적 엔터티를 맥락화하기 위한 두 가지 사전 훈련 접근 방식을 제안합니다. 언어화된 장면 그래프를 사용하여 시각적 관계 삼중항을 구조화된 캡션으로 변환하고 이를 추가 이미지 설명으로 처리합니다. 마스킹된 관계 예측을 통해 시각적으로 마스킹된 컨텍스트가 있는 이미지 영역의 엔터티 관련을 더욱 권장합니다. 대량의 웹 데이터에 대해 사전 훈련된 강력한 기준선에 적용될 때, 대략적이고 세분화된 작업 모두에 대한 제로샷 평가는 약하게 지도되는 관계 데이터로부터 다중 모달 표현을 학습하는 방법의 효율성을 보여줍니다."
416,http://arxiv.org/abs/2305.13812 ,Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space for Improved Vision-Language Compositionality,"Harman Singh, Pengchuan Zhang, Qifan Wang, Mengjiao Wang, Wenhan Xiong, Jingfei Du, Yu Chen","대조적으로 훈련된 비전 언어 모델은 비전 및 언어 표현 학습에서 놀라운 발전을 이루었으며 다양한 다운스트림 다중 모달 작업을 위한 최첨단 모델로 이어졌습니다. 그러나 최근 연구에서는 객체, 속성 및 관계에 대한 구성 추론을 수행하는 능력에서 이러한 모델의 심각한 한계를 강조했습니다. 장면 그래프는 이미지를 구성적으로 이해하는 효과적인 방법으로 등장했습니다. 이는 객체, 객체의 속성 및 장면의 다른 객체와의 관계를 포함하는 이미지의 그래프 구조 의미론적 표현입니다. 본 연구에서는 텍스트에서 파싱된 장면 그래프를 이미지 장면 그래프의 프록시로 간주하고 다양한 복잡성의 문장을 동일한 이미지에 정렬하는 이미지와 텍스트 간의 대략적 대 미세 대조 학습 목표와 함께 그래프 분해 및 확대 프레임워크를 제안합니다. 이와 함께 속성 바인딩 및 관계 이해를 향상시키기 위해 장면 그래프 공간에서 새로운 네거티브 마이닝 기법을 제안합니다. 광범위한 실험을 통해 우리는 최근 제안된 여러 벤치마크에서 속성 바인딩, 관계 이해, 체계적인 일반화 및 생산성을 크게 향상시키는 동시에(예: 체계적인 일반화의 경우 최대 $18\%$ 개선, 강력한 기준에 대한 관계 이해의 경우 $16.5\%$ 개선) 동시에 다양한 일반 다중 모드 작업에서 CLIP과 유사하거나 더 나은 성능을 달성하는 접근 방식의 효과를 입증합니다."
415,http://arxiv.org/abs/2305.12260 ,Cross2StrA: Unpaired Cross-lingual Image Captioning with Cross-lingual Cross-modal Structure-pivoted Alignment,"Shengqiong Wu, Hao Fei, Wei Ji, Tat-Seng Chua","짝을 이루지 않은 교차 ​​언어 이미지 캡션은 전송 중 의미론적 장면과 구문 속성의 불일치로 인해 오랫동안 관련성과 불일치 문제로 어려움을 겪었습니다. 본 연구에서는 SG(Scene Graph) 구조와 SC(Syntactic Constuency) 트리를 통합하여 위의 문제를 해결하는 방법을 제안합니다. 캡션 작성기에는 의미 구조 기반 이미지-피벗 캡션과 구문 구조 기반 피벗-대상 번역이 포함되어 있으며, 그 중 두 개는 피벗 언어를 통해 결합됩니다. 그런 다음 SG 및 SC 구조를 피봇팅으로 사용하여 모달 간 의미 구조 정렬 및 언어 간 구문 구조 정렬 학습을 수행합니다. 또한 캡션 작성 및 번역 단계를 완벽하게 조정하기 위해 교차 언어 및 교차 모달 역번역 교육을 도입합니다. 영어-중국어 전송에 대한 실험은 우리 모델이 캡션 관련성과 유창성을 향상시키는 데 큰 우월성을 보여줍니다."
414,http://arxiv.org/abs/2305.12256 ,Scene Graph as Pivoting: Inference-time Image-free Unsupervised Multimodal Machine Translation with Visual Scene Hallucination,"Hao Fei, Qian Liu, Meishan Zhang, Min Zhang, Tat-Seng Chua","이 작업에서는 모델이 소스 텍스트 이미지 쌍으로 훈련되고 소스 텍스트 입력만으로 테스트되는 보다 현실적인 UMMT(Unsupervised Multimodal Machine Translation) 설정, 추론 시간 이미지 없는 UMMT를 조사합니다. 먼저 입력 이미지와 텍스트를 시각적 및 언어 장면 그래프(SG)로 표현합니다. 여기서 이러한 세밀한 비전 언어 기능은 의미론에 대한 전체적인 이해를 보장합니다. 추론 중에 순수 텍스트 입력을 활성화하기 위해 우리는 주어진 텍스트 SG로부터 의사 시각적 SG를 동적으로 생성하는 시각적 장면 환각 메커니즘을 고안했습니다. 비지도 번역 교육을 위해 여러 SG 피벗 기반 학습 목표가 도입되었습니다. 벤치마크 Multi30K 데이터에서 당사의 SG 기반 방법은 작업 및 설정에 대한 상당한 BLEU 점수로 최고 성능 기준을 능가하여 쌍을 이루는 이미지에 의존하지 않고도 더 나은 완성도, 관련성 및 유창성을 갖춘 번역을 생성하는 데 도움이 됩니다. 추가 심층 분석을 통해 작업 설정에서 우리 모델이 어떻게 발전하는지 확인할 수 있습니다."
413,http://arxiv.org/abs/2305.11768 ,Generating Visual Spatial Description via Holistic 3D Scene Understanding,"Yu Zhao, Hao Fei, Wei Ji, Jianguo Wei, Meishan Zhang, Min Zhang, Tat-Seng Chua","VSD(시각적 공간 설명)는 이미지 내에서 주어진 개체의 공간적 관계를 설명하는 텍스트를 생성하는 것을 목표로 합니다. 기존 VSD 작업은 2D 기하학적 비전 기능을 모델링할 뿐이므로 대상 개체에 대한 왜곡된 공간 이해 문제에 필연적으로 빠지게 됩니다. 본 연구에서는 VSD에 대한 3D 장면 기능 통합을 조사합니다. 외부 3D 장면 추출기를 이용하여 입력 영상에 대한 3D 객체와 장면 특징을 획득하고, 이를 기반으로 대상 객체 중심의 3D 공간 장면 그래프(Go3D-S2G)를 구축하여 전체적 3D 장면 내 대상 객체의 공간 의미를 모델링합니다. 게다가 우리는 Go3D-S2G에서 토폴로지적으로 다양한 하위 그래프를 샘플링하는 장면 하위 그래프 선택 메커니즘을 제안합니다. 여기서 다양한 로컬 구조 기능을 탐색하여 공간적으로 다양한 텍스트 생성을 생성합니다. 두 개의 VSD 데이터 세트에 대한 실험 결과는 우리의 프레임워크가 기준선보다 훨씬 뛰어난 성능을 보여주며 특히 복잡한 시각적 공간 관계가 있는 경우에 개선되었습니다. 한편, 우리의 방법은 보다 공간적으로 다양화된 발전을 생산할 수 있습니다. 코드는 https://github.com/zhaoyucs/VSD에서 확인할 수 있습니다."
412,http://arxiv.org/abs/2305.11719 ,Information Screening whilst Exploiting! Multimodal Relation Extraction with Feature Denoising and Multimodal Topic Modeling,"Shengqiong Wu, Hao Fei, Yixin Cao, Lidong Bing, Tat-Seng Chua","다중 모드 관계 추출(MRE)에 대한 기존 연구는 내부 정보 과잉 활용과 외부 정보 과소 활용이라는 두 가지 공존 과제에 직면해 있습니다. 이에 맞서기 위해 우리는 내부 정보 선별과 외부 정보 활용 아이디어를 동시에 구현하는 새로운 프레임워크를 제안합니다. 먼저, 입력 이미지와 텍스트의 세밀한 의미 구조를 시각적 및 텍스트 장면 그래프로 표현하고 이를 통합된 CMG(Cross-Modal Graph)로 융합합니다. CMG를 기반으로 그래프 정보 병목 원리에 따라 구조 개선을 수행하여 정보가 적은 기능에 대한 노이즈를 적극적으로 제거합니다. 다음으로, 입력 이미지와 텍스트에 대해 주제 모델링을 수행하고 잠재적인 다중 모달 주제 기능을 통합하여 컨텍스트를 풍부하게 합니다. 벤치마크 MRE 데이터 세트에서 우리 시스템은 현재 최고의 모델보다 훨씬 뛰어난 성능을 발휘합니다. 추가 심층 분석을 통해 MRE 작업에 대한 방법의 큰 잠재력을 밝힙니다. 우리 코드는 https://github.com/ChocoWu/MRE-ISE에서 공개됩니다."
411,http://arxiv.org/abs/2305.08522 ,Cross-Modality Time-Variant Relation Learning for Generating Dynamic Scene Graphs,"Jingyi Wang, Jinfa Huang, Can Zhang, Zhidong Deng","비디오 클립에서 생성된 동적 장면 그래프는 환경 인식, 자율 탐색, 자율 주행 차량 및 모바일 로봇의 작업 계획과 같은 광범위하고 까다로운 작업에서 의미론적 시각적 이해를 향상시키는 데 도움이 될 수 있습니다. 동적 장면 그래프 생성 중 시간적, 공간적 모델링 과정에서 프레임 간 동적 장면 그래프의 시변 관계를 학습하는 것은 특히 어렵습니다. 본 논문에서는 동적 장면 그래프에서 관계의 시간적 변화를 모델링하는 것을 목표로 하는 시변 관계 인식 TRansformer(TR$^2$)를 제안합니다. 명시적으로 우리는 관계 레이블에 대해 프롬프트된 문장의 텍스트 임베딩 차이를 관계에 대한 감독 신호로 활용합니다. 이러한 방식으로 시변 관계 학습을 위해 교차 양식 기능 안내가 실현됩니다. 암시적으로 우리는 변환기와 인접 프레임 간의 차이를 설명하는 추가 메시지 토큰을 사용하여 관계 기능 융합 모듈을 설계합니다. Action Genome 데이터 세트에 대한 광범위한 실험은 TR$^2$가 시변 관계를 효과적으로 모델링할 수 있음을 입증합니다. TR$^2$는 두 가지 다른 설정에서 이전의 최첨단 방법보다 각각 2.1%와 2.6% 더 뛰어난 성능을 발휘합니다."
410,http://arxiv.org/abs/2305.08190 ,TSGN: Temporal Scene Graph Neural Networks with Projected Vectorized Representation for Multi-Agent Motion Prediction,"Yunong Wu, Thomas Gilles, Bogdan Stanciulescu, Fabien Moutarde","자율주행차가 안전하고 효과적인 조치를 취하려면 근처 에이전트의 미래 움직임을 예측하는 것이 필수적입니다. 본 논문에서는 다중 에이전트 궤적 예측을 위해 투영된 벡터화 표현을 갖춘 임시 장면 그래프 신경망을 사용하는 프레임워크인 TSGN을 제안합니다. 투영된 벡터화된 표현은 교통 상황을 벡터 세트로 구성된 그래프로 모델링합니다. 이러한 벡터는 에이전트, 도로 네트워크 및 이들의 공간적 상대적 관계를 나타냅니다. 이 표현의 모든 상대적 특징은 이동 및 회전 불변입니다. 이 표현을 기반으로 TSGN은 에이전트, 도로 네트워크, 이들 간의 상호 작용 및 시간적 교통 장면의 시간적 종속성을 전반에 걸쳐 공간적-시간적 특징을 포착합니다. TSGN은 모든 에이전트의 다중 모드 미래 궤적을 동시에, 그럴듯하고 정확하게 예측할 수 있습니다. 한편, 우리는 주변 도로 네트워크를 필터링하고 대상 에이전트의 향후 행동에 영향을 미칠 수 있는 가장 가능성 있는 차선 세그먼트만 유지하는 에이전트와 도로 네트워크 간의 상호 작용을 캡처하기 위한 계층적 차선 변환기를 제안합니다. 이는 예측 성능을 저하시키지 않으면서 계산 부담을 크게 줄여줍니다. 실험에 따르면 TSGN은 Argoverse 모션 예측 벤치마에서 최첨단 성능을 달성했습니다."
409,http://arxiv.org/abs/2305.07716 ,Learning to Reason over Scene Graphs: A Case Study of Finetuning GPT-2 into a Robot Language Model for Grounded Task Planning,"Georgia Chalvatzaki, Ali Younes, Daljeet Nandha, An Le, Leonardo F. R. Ribeiro, Iryna Gurevych","지능형 보조 및 서비스 로봇 개발을 위해서는 장기적인 작업 계획이 필수적입니다. 이 작업에서는 계획자가 순차적으로 실행할 수 있도록 작업을 하위 목표 사양으로 분해하는 방법을 학습하여 로봇 작업 계획에서 소규모 클래스의 LLM(대형 언어 모델), 특히 GPT-2의 적용 가능성을 조사합니다. 우리의 방법은 장면 그래프로 표시되는 도메인에 대한 LLM 입력을 기반으로 인간의 요청을 실행 가능한 로봇 계획으로 변환하여 ALFRED 벤치마크에서 볼 수 있듯이 장기적인 작업에 대한 추론을 학습할 수 있도록 합니다. 우리는 LLM 기반 계획의 적용 가능성과 일반화 가능성을 조사하기 위해 우리의 접근 방식을 고전적인 계획 및 기본 방법과 비교합니다. 우리의 연구 결과는 LLM에 저장된 지식이 장거리 작업 계획을 수행하는 데 효과적으로 기반을 둘 수 있음을 시사하며 로봇 공학에서 신경 기호 계획 방법의 향후 적용에 대한 유망한 잠재력을 보여줍니다."
408,http://arxiv.org/abs/2305.07154 ,Foundations of Spatial Perception for Robotics: Hierarchical Representations and Real-time Systems,"Nathan Hughes, Yun Chang, Siyi Hu, Rajat Talak, Rumaisa Abdulhai, Jared Strader, Luca Carlone","3D 공간 인식은 센서 데이터와 사전 지식을 사용하여 실시간으로 실행 가능하고 지속적인 환경 표현을 구축하고 유지하는 문제입니다. 로봇 인식의 급속한 발전에도 불구하고 대부분의 기존 방법은 순수 기하학적 맵(기존 SLAM에서와 같이)을 구축하거나 대규모 환경이나 대규모 의미 레이블 사전으로 확장되지 않는 평면 메트릭 의미 맵을 구축합니다. 이 문서의 첫 번째 부분은 표현에 관한 것입니다. 공간 인식을 위한 확장 가능한 표현은 본질적으로 계층적이어야 함을 보여줍니다. 계층적 표현은 저장하기에 효율적이며 트리 너비가 작은 계층형 그래프로 이어져 효율적인 추론이 가능합니다. 그런 다음 실내 환경에 대한 계층적 표현의 예, 즉 3D 장면 그래프를 소개하고 그 구조와 속성에 대해 논의합니다. 논문의 두 번째 부분에서는 로봇이 환경을 탐색하면서 3D 장면 그래프를 점진적으로 구성하는 알고리즘에 중점을 둡니다. 우리의 알고리즘은 3D 기하학, 토폴로지(장소를 방으로 클러스터링) 및 기하학적 딥 러닝(예: 로봇이 이동하는 방 유형 분류)을 결합합니다. 논문의 세 번째 부분에서는 장기간 작동 중에 3D 장면 그래프를 유지하고 수정하는 알고리즘에 중점을 둡니다. 루프 폐쇄 검출을 위한 계층적 디스크립터를 제안하고, 3차원 장면 그래프 최적화 문제를 해결하여 루프 폐쇄에 대응하여 장면 그래프를 수정하는 방법을 설명합니다. 제안된 인식 알고리즘을 시각 관성 데이터로부터 실시간으로 3D 장면 그래프를 구축하는 실시간 공간 인식 시스템인 Hydra에 결합하여 논문을 마무리합니다. Clearpath Jackal 로봇과 Unitree A1 로봇이 수집한 실제 데이터와 사실적인 시뮬레이션을 통해 Hydra의 성능을 선보입니다. 우리는 https://github.com/MIT-SPARK/Hydra에서 Hydra의 오픈 소스 구현을 출시합니다."
407,http://arxiv.org/abs/2305.06343 ,Incorporating Structured Representations into Pretrained Vision & Language Models Using Scene Graphs,"Roei Herzig, Alon Mendelson, Leonid Karlinsky, Assaf Arbelle, Rogerio Feris, Trevor Darrell, Amir Globerson","VLM(비전 및 언어 모델)은 다양한 작업에서 놀라운 ZS(제로 샷) 성능을 보여주었습니다. 그러나 최근 연구에 따르면 최고의 VLM조차도 객체 속성, 관계 및 동작 상태와 같은 구성 장면 이해 측면을 포착하는 데 어려움을 겪고 있는 것으로 나타났습니다. 대조적으로, 이러한 모델을 개선할 수 있는 장면 그래프(SG)와 같은 구조화된 주석을 얻는 것은 시간이 많이 걸리고 비용이 많이 들기 때문에 대규모로 사용할 수 없습니다. 여기서는 소규모 SG 데이터 세트가 사전 훈련된 VLM의 구조적 이해를 향상시키는 데 충분한 정보를 제공할 수 있는지 묻습니다. 우리는 구조화된 정보를 시각적 및 텍스트 표현 모두에 통합하는 구성 요소를 통합하여 SG로부터 학습할 때 VLM을 개선하는 것이 실제로 가능하다는 것을 보여줍니다. 시각적 측면에서는 SG 정보를 예측하도록 훈련된 이미지 변환기에 특수 ""SG 구성요소""를 통합하고, 텍스트 측면에서는 SG를 활용하여 장면의 다양한 구성 측면을 강조하는 세분화된 캡션을 생성합니다. 우리의 방법은 ZS 기능의 약간의 저하만으로 여러 VL 데이터 세트에서 여러 인기 있는 VLM의 성능을 향상시킵니다."
406,http://arxiv.org/abs/2305.06141 ,Active Semantic Localization with Graph Neural Embedding,"Mitsuki Yoshida, Kanji Tanaka, Ryogo Yamamoto, Daiki Iwata","의미론적 위치 파악, 즉 의미론적 이미지 양식을 사용한 로봇 자체 위치 파악은 최근 새롭게 떠오르는 구현된 AI 애플리케이션(예: 포인트 목표 탐색, 개체 목표 탐색, 비전 언어 탐색) 및 위상 매핑 애플리케이션(예: 그래프 신경 SLAM, 자기 중심 위상 맵)에서 매우 중요합니다. 그러나 의미론적 현지화에 대한 대부분의 기존 작업은 관점 계획 없이 수동 비전 작업에 중점을 두거나 추가적인 풍부한 양식(예: 깊이 측정)에 의존합니다. 따라서 문제는 대부분 해결되지 않았습니다. 이 작업에서는 그래프 신경 로컬라이저라고 하는 경량의 완전히 CPU 기반 도메인 적응형 의미 지역화 프레임워크를 탐색합니다. 우리의 접근 방식은 최근에 떠오르는 두 가지 기술에서 영감을 얻었습니다. (1) 로컬 및 글로벌 특징의 관점 및 외관 불변성을 결합한 장면 그래프; (2) 그래프 데이터(즉, 비벡터 데이터)를 직접 학습/인식할 수 있는 그래프 신경망. 구체적으로 그래프 컨벌루션 신경망은 먼저 수동적 비전을 위한 장면 그래프 분류기로 훈련된 후 해당 지식이 능동적 비전을 위한 강화 학습 플래너로 전달됩니다. 사실적인 Habitat 시뮬레이터를 사용하여 자기 지도 학습과 비지도 도메인 적응의 두 가지 시나리오에 대한 실험을 통해 제안된 방법의 효율성을 검증합니다."
405,http://arxiv.org/abs/2305.06152 ,Structure-CLIP: Towards Scene Graph Knowledge to Enhance Multi-modal Structured Representations,"Yufeng Huang, Jiji Tang, Zhuo Chen, Rongsheng Zhang, Xinfeng Zhang, Weijie Chen, Zeng Zhao, Zhou Zhao, Tangjie Lv, Zhipeng Hu, Wen Zhang","대규모 비전 언어 사전 훈련은 다중 모드 이해 및 생성 작업에서 상당한 성능을 달성했습니다. 그러나 기존 방법은 구조화된 표현, 즉 객체, 속성 및 관계의 표현이 필요한 이미지-텍스트 매칭 작업에서 제대로 수행되지 않는 경우가 많습니다. Fig.~refig:case (a)에서 볼 수 있듯이 모델은 '우주비행사가 말을 탄다'와 '말이 우주비행사를 탄다'를 구분하지 못한다. 이는 다중 모드 시나리오에서 표현을 학습할 때 구조화된 지식을 완전히 활용하지 못하기 때문입니다. 본 논문에서는 장면 그래프 지식(SGK)을 통합하여 다중 모드 구조 표현을 향상시키는 엔드투엔드 프레임워크 Structure-CLIP을 제시합니다. 첫째, 우리는 장면 그래프를 사용하여 의미론적 부정 예의 구성을 안내하며, 이는 구조화된 표현 학습에 대한 강조를 증가시킵니다. 또한 구조화된 표현을 더욱 향상시키기 위해 SGK를 입력으로 활용하는 KEE(Knowledge-Enhance Encoder)가 제안되었습니다. 제안된 프레임워크의 효율성을 검증하기 위해 앞서 언급한 접근 방식으로 모델을 사전 훈련하고 다운스트림 작업에 대한 실험을 수행합니다. 실험 결과에 따르면 Structure-CLIP은 VG-Attribution 및 VG-Relation 데이터세트에서 다중 모드 SOTA 모델보다 각각 12.5% ​​및 4.1% 앞선 SOTA(최첨단) 성능을 달성하는 것으로 나타났습니다. 한편, MSCOCO의 결과는 Structure-CLIP이 일반 표현의 능력을 유지하면서 구조화된 표현을 크게 향상시키는 것으로 나타났습니다. 우리 코드는 https://github.com/zjukg/Structure-CLIP에서 확인할 수 있습니다."
404,http://arxiv.org/abs/2305.02743 ,Incremental 3D Semantic Scene Graph Prediction from RGB Sequences,"Shun-Cheng Wu, Keisuke Tateno, Nassir Navab, Federico Tombari",3D 의미론적 장면 그래프는 개별 개체를 설명하고 개체 간의 관계를 묘사하므로 강력하고 전체적인 표현입니다. 장면 추론이 필요한 많은 작업을 가능하게 하는 컴팩트한 고급 그래프입니다. 실제 환경에서 기존 3D 추정 방법은 대부분 조밀한 입력에 의존하는 강력한 예측을 생성합니다. 본 연구에서는 RGB 이미지 시퀀스가 ​​주어지면 장면의 일관된 3D 의미론적 장면 그래프를 점진적으로 구축하는 실시간 프레임워크를 제안합니다. 우리의 방법은 새로운 증분 엔터티 추정 파이프라인과 장면 그래프 예측 네트워크로 구성됩니다. 제안된 파이프라인은 희소 포인트 맵을 재구성하는 동시에 입력 이미지로부터 엔터티 추정을 융합합니다. 제안된 네트워크는 장면 엔터티로부터 추출된 기하학적 특징과 멀티뷰를 사용하여 반복적인 메시지 전달을 통해 3차원 의미론적 장면 그래프를 추정합니다. 3RScan 데이터 세트에 대한 광범위한 실험은 이 어려운 작업에서 제안된 방법의 효율성을 보여 주며 최첨단 접근 방식을 능가합니다.
403,http://arxiv.org/abs/2305.02519 ,ANetQA: A Large-scale Benchmark for Fine-grained Compositional Reasoning over Untrimmed Videos,"Zhou Yu, Lixiang Zheng, Zhou Zhao, Fei Wu, Jianping Fan, Kui Ren, Jun Yu","VideoQA(비디오 질문 답변) 모델의 다양한 기능을 체계적으로 분석하기 위한 벤치마크를 구축하는 것은 어렵지만 중요합니다. 기존 벤치마크는 비구성적 단순 질문을 사용하는 경우가 많고, 언어 편향으로 인해 모델의 약점을 예리하게 진단하기 어렵습니다. 최근 벤치마크 AGQA는 미리 주석이 달린 장면 그래프에서 자동으로 QA 쌍을 생성하여 세분화된 제어를 통해 다양한 추론 능력을 측정할 수 있는 유망한 패러다임을 제시합니다. 그러나 장면 그래프에는 그러한 정보가 없기 때문에 비디오의 세밀한 의미를 추론하는 데는 한계가 있습니다. 이를 위해 우리는 ActivityNet의 까다로운 자르지 않은 비디오에 대해 세밀한 구성 추론을 지원하는 대규모 벤치마크인 ANetQA를 제시합니다. AGQA와 마찬가지로 ANetQA의 QA 쌍은 주석이 달린 비디오 장면 그래프에서 자동으로 생성됩니다. ANetQA의 세밀한 속성은 다음에 반영됩니다. (i) 세밀한 의미를 지닌 다듬어지지 않은 비디오; (ii) 세밀한 분류가 포함된 시공간 장면 그래프; (iii) 세분화된 템플릿에서 생성된 다양한 질문. ANetQA는 14억 개의 불균형 QA 쌍과 1340만 개의 균형 QA 쌍을 달성합니다. 이는 비슷한 수의 동영상을 보유한 AGQA보다 훨씬 더 큰 규모입니다. 최첨단 방법에 대한 포괄적인 실험이 수행됩니다. 최고의 모델은 44.5%의 정확도를 달성하는 반면 인간의 성능은 84.5%로 최고 수준이므로 개선의 여지가 충분합니다."
402,http://arxiv.org/abs/2305.02177 ,Transforming Visual Scene Graphs to Image Captions,"Xu Yang, Jiawei Peng, Zihua Wang, Haiyang Xu, Qinghao Ye, Chenliang Li, Songfang Huang, Fei Huang, Zhangzikang Li, Yu Zhang","우리는 장면 그래프(TSG)를 보다 설명적인 캡션으로 변환할 것을 제안합니다. TSG에서는 MHA(Multi-Head Attention)를 적용하여 장면 그래프 삽입을 위한 GNN(Graph Neural Network)을 설계합니다. 임베딩 후 다양한 그래프 임베딩에는 다양한 품사를 가진 단어를 생성하기 위한 다양한 특정 지식이 포함됩니다. 예를 들어 객체/속성 임베딩은 명사/형용사 생성에 좋습니다. 이에 동기를 부여하여 우리는 그래프 임베딩을 구별하여 다양한 종류의 단어를 생성하기 위해 각 전문가가 MHA를 기반으로 구축된 MOE(Mixture-of-Expert) 기반 디코더를 설계합니다. 인코더와 디코더 모두 MHA를 기반으로 구축되므로, 결과적으로 일반적으로 Fully-Connected 기반 GNN 및 LSTM 기반 디코더를 적용하는 이전 이종 인코더와 달리 동종 인코더-디코더를 구성합니다. 동종 아키텍처를 사용하면 이종 파이프라인에서와 같이 다양한 하위 네트워크에 대해 서로 다른 훈련 전략을 지정하는 대신 전체 모델의 훈련 구성을 통합할 수 있어 훈련 어려움이 해소됩니다. MS-COCO 캡션 벤치마크에 대한 광범위한 실험을 통해 TSG의 효율성이 검증되었습니다. 코드는 https://github.com/GaryJiajia/TSG에 있습니다."
401,http://arxiv.org/abs/2304.14880 ,SGAligner : 3D Scene Alignment with Scene Graphs,"Sayan Deb Sarkar, Ondrej Miksik, Marc Pollefeys, Daniel Barath, Iro Armeni","3D 장면 그래프를 구축하는 것은 최근 구조화되고 풍부한 방식으로 세계를 표현하기 위해 구현된 여러 AI 애플리케이션의 장면 표현에서 주제로 부상했습니다. 다운스트림 작업(예: 탐색 및 공간 재배치)을 해결하는 데 사용이 증가함에 따라 에이전트 작업의 중추적인 단계인 환경의 3D 맵을 생성하는 데 활용하고 재활용할 수 있습니까? 우리는 중첩 범위가 0에서 부분까지이고 임의의 변경 사항을 포함할 수 있는 3D 장면 그래프 쌍을 정렬하는 근본적인 문제에 중점을 둡니다. 우리는 실제 시나리오(즉, 알 수 없는 중복(있는 경우) 및 환경 변화)에 견고한 3D 장면 그래프 쌍을 정렬하는 첫 번째 방법인 SGAligner를 제안합니다. 우리는 다중 모드 지식 그래프에서 영감을 얻고 대조 학습을 사용하여 공동 다중 모드 임베딩 공간을 학습합니다. 우리는 3RScan 데이터 세트를 평가하고 우리의 방법이 3D 장면 쌍 간의 변환을 추정하는 데 사용될 수 있음을 추가로 보여줍니다. 이러한 작업에 대한 벤치마크가 누락되었으므로 이 데이터 세트에서 벤치마크를 만듭니다. 코드, 벤치마크, 훈련된 모델은 프로젝트 웹사이트에서 확인할 수 있습니다."
400,http://arxiv.org/abs/2304.14573 ,SceneGenie: Scene Graph Guided Diffusion Models for Image Synthesis,"Azade Farshad, Yousef Yeganeh, Yu Chi, Chengzhi Shen, Björn Ommer, Nassir Navab","텍스트 조건에 따른 이미지 생성은 생성적 적대 네트워크와 최근에는 확산 모델을 통해 최근 몇 년간 상당한 발전을 이루었습니다. 텍스트 프롬프트를 기반으로 한 확산 모델은 인상적인 고품질 이미지를 생성했지만 특정 개체의 인스턴스 수와 같은 복잡한 텍스트 프롬프트를 정확하게 표현하는 것은 여전히 ​​​​어려운 일입니다.   이러한 한계를 해결하기 위해 우리는 추가 훈련 데이터 없이 추론 시 경계 상자 및 분할 맵 정보를 활용하는 확산 모델의 샘플링 프로세스에 대한 새로운 지침 접근 방식을 제안합니다. 샘플링 프로세스의 새로운 손실을 통해 우리의 접근 방식은 CLIP 임베딩의 의미론적 특징으로 모델을 안내하고 기하학적 제약 조건을 적용하여 장면을 정확하게 나타내는 고해상도 이미지를 생성합니다. 경계 상자 및 분할 맵 정보를 얻기 위해 텍스트 프롬프트를 장면 그래프로 구성하고 CLIP 임베딩으로 노드를 강화합니다. 우리가 제안한 모델은 장면 그래프에서 이미지 생성을 위한 두 가지 공개 벤치마크에서 최첨단 성능을 달성했으며, 다양한 측정 항목에서 장면 그래프에서 이미지로의 확산 모델과 텍스트 기반 확산 모델을 모두 능가했습니다. 우리의 결과는 보다 정확한 텍스트-이미지 생성을 위해 확산 모델 샘플링 프로세스에 경계 상자 및 분할 맵 지침을 통합하는 효과를 보여줍니다."
399,http://arxiv.org/abs/2304.13487 ,Hydra-Multi: Collaborative Online Construction of 3D Scene Graphs with Multi-Robot Teams,"Yun Chang, Nathan Hughes, Aaron Ray, Luca Carlone","3D 장면 그래프는 최근 노드가 여러 수준의 추상화(예: 객체, 방, 건물)에서 공간 개념을 나타내고 가장자리가 개념 간의 관계(예: 포함, 인접)를 나타내는 계층형 그래프로 3D 환경을 설명하는 표현적인 고급 지도 표현으로 등장했습니다. 본 논문에서는 팀 내 로봇이 수집한 센서 데이터로부터 온라인으로 다중 로봇 3D 장면 그래프를 구축할 수 있는 최초의 다중 로봇 공간 인식 시스템인 Hydra-Multi에 대해 설명합니다. 특히, 우리는 여러 로봇의 증분 입력을 받아 로봇 프레임 간의 상대적인 변환을 효과적으로 찾고 루프 폐쇄 감지를 통합하여 서로 다른 로봇의 장면 그래프 노드를 올바르게 조정함으로써 공동 3D 장면 그래프를 구성할 수 있는 중앙 집중식 시스템을 개발합니다. 우리는 시뮬레이션 및 실제 시나리오에서 Hydra-Multi를 평가하고 온라인에서 정확한 3D 장면 그래프를 재구성할 수 있음을 보여줍니다. 또한 로봇이 구축한 다양한 지도 표현을 다양한 센서 제품군과 융합하여 이질적인 팀을 지원하는 Hydra-Multi의 기능을 보여줍니다."
398,http://arxiv.org/abs/2304.13359 ,Scene Graph Lossless Compression with Adaptive Prediction for Objects and Relations,"Yufeng Zhang, Weiyao Lin, Wenrui Dai, Huabin Liu, Hongkai Xiong","장면 그래프는 이미지 장면 내 개체와 개체의 쌍 관계를 설명하는 새로운 데이터 구조입니다. 비전 애플리케이션에서 장면 그래프의 크기가 커짐에 따라 이러한 데이터를 디스크에 손실 없이 효율적으로 저장하거나 네트워크를 통해 전송하는 방법은 불가피한 문제가 됩니다. 그러나 장면 그래프의 압축은 데이터 구조와 분포가 복잡하기 때문에 이전에는 거의 연구되지 않았습니다. 기존 솔루션에는 일반적으로 범용 압축기나 그래프 구조 압축 방법이 사용되는데, 이는 장면 그래프 데이터의 중복성을 줄이는 데 취약합니다. 이 논문에서는 장면 그래프 데이터의 객체와 관계의 결합 압축을 위한 적응형 예측기를 갖춘 새로운 무손실 압축 프레임워크를 소개합니다. 제안된 프레임워크는 다양한 데이터 요소에 적응하기 위한 통합된 사전 추출기와 특수 요소 예측기로 구성됩니다. 또한 그래프 요소 내 및 그래프 요소 간 컨텍스트 정보를 활용하기 위해 다양한 그래프 요소에 대해 다양한 그래프 컨텍스트 모델링 방식을 지원하는 Graph Context Convolution이 제안됩니다. 마지막으로, 복잡한 조건부 제약 하에서 수치 데이터를 예측하기 위해 학습된 분포 모델이 고안되었습니다. 레이블이 지정되거나 생성된 장면 그래프에 대해 수행된 실험은 장면 그래프 무손실 압축 작업에서 제안된 프레임워크의 효율성을 입증합니다."
397,http://arxiv.org/abs/2304.09789 ,Automatic Interaction and Activity Recognition from Videos of Human Manual Demonstrations with Application to Anomaly Detection,"Elena Merlo, Marta Lagomarsino, Edoardo Lamon, Arash Ajoudani","이 논문은 수동 작업의 비디오 시연 내에서 상호 작용과 활동을 모두 인식하기 위해 물체와 손 사이의 시공간 관계를 설명하는 새로운 방법을 제시합니다. 이 접근 방식은 장면 그래프를 활용하여 이미지 시퀀스에서 주요 상호 작용 기능을 추출하는 동시에 모션 패턴과 컨텍스트를 인코딩합니다. 또한 이 방법은 유사한 이벤트를 그룹화하고 모니터링되는 활동이 올바르게 실행되는지 감지하는 이벤트 기반 자동 비디오 분할 및 클러스터링을 도입합니다. 이 접근 방식의 효과는 두 번의 다중 대상 실험에서 입증되었으며, 활동에 대한 사전 지식 없이 손-물체 및 물체-물체 상호 작용을 인식하고 클러스터링할 수 있을 뿐만 아니라 서로 다른 피험자가 수행한 동일한 활동을 일치시키는 능력을 보여줍니다."
396,http://arxiv.org/abs/2304.08600 ,RS2G: Data-Driven Scene-Graph Extraction and Embedding for Robust Autonomous Perception and Scenario Understanding,"Junyao Wang, Arnav Vaibhav Malawade, Junhong Zhou, Shih-Yuan Yu, Mohammad Abdullah Al Faruque","도로 사용자 간의 복잡한 상호 작용을 효과적으로 포착하는 것은 자율주행차의 안전한 탐색을 달성하는 데 매우 중요합니다. 그래프 학습(GL)이 이 문제를 해결하기 위한 유망한 접근 방식으로 등장했지만, 기존 GL 모델은 사전 정의된 도메인별 그래프 추출 규칙에 의존하는데, 이는 현실 세계의 급격하게 변화하는 시나리오에서 종종 실패합니다. 또한 이러한 그래프 추출 규칙은 도메인 전체에 걸쳐 지식을 일반화하는 기존 GL 방법의 기능을 심각하게 방해합니다. 이 문제를 해결하기 위해 우리는 도로 사용자 간의 다양한 관계를 동적으로 포착하는 새로운 데이터 기반 그래프 추출 및 모델링 접근 방식을 갖춘 혁신적인 자율 시나리오 이해 프레임워크인 RoadScene2Graph(RS2G)를 제안합니다. 우리의 평가에 따르면 RS2G는 주관적 위험 평가에서 평균적으로 RS2G가 최첨단(SOTA) 규칙 기반 그래프 추출 방법보다 4.47%, SOTA 딥 러닝 모델보다 22.19% 더 뛰어난 것으로 나타났습니다. 더 중요한 것은 RS2G가 시뮬레이션 환경에서 얻은 지식을 보이지 않는 실제 시나리오로 전달하는 데 있어 훨씬 더 나은 성능을 제공한다는 것입니다."
395,http://arxiv.org/abs/2304.07647 ,LASER: A Neuro-Symbolic Framework for Learning Spatial-Temporal Scene Graphs with Weak Supervision,"Jiani Huang, Ziyang Li, Mayur Naik, Ser-Nam Lim","비디오에서 STSG(시공간 장면 그래프)를 학습하기 위한 감독 방식은 대규모로 구성하는 데 노동 집약적인 STSG 주석이 달린 비디오에 의존하기 때문에 크게 방해를 받습니다. 대신 쉽게 사용할 수 있는 비디오 캡션을 약한 감독으로 사용하는 것이 가능합니까? 이 문제를 해결하기 위해 우리는 비디오 캡션만 사용하여 STSG 생성기를 훈련할 수 있는 신경 기호 프레임워크인 LASER를 제안합니다. LASER는 대규모 언어 모델을 사용하여 먼저 비디오 캡션에서 풍부한 시공간 의미 정보가 포함된 논리적 사양을 추출합니다. 그런 다음 LASER는 기본 STSG 생성기를 훈련하여 예측된 STSG를 사양에 맞춥니다. 정렬 알고리즘은 차별화 가능한 기호 추론을 활용하고 대조, 시간 및 의미 손실의 조합을 사용하여 약한 감독 문제를 극복합니다. 전반적인 접근 방식은 낮은 수준의 인식 모델을 효율적으로 훈련하여 비디오 캡션에 맞는 세분화된 STSG를 추출합니다. 이를 통해 지루한 주석 없이 STSG를 학습하는 새로운 방법론을 사용할 수 있습니다. 우리는 OpenPVSG, 20BN 및 MUGEN의 세 가지 비디오 데이터 세트에 대한 방법을 평가합니다. 우리의 접근 방식은 OpenPVSG에서 27.78%(+12.65%)의 단항 예측 정확도와 0.42(+0.22)의 이진 재현률@5을 달성하여 완전 감독 기준에 비해 상당한 개선을 보여줍니다. 또한 LASER는 전체 예측 예측 정확도 측면에서 기준선을 20BN에서 7%, MUGEN에서 5.2% 초과합니다."
394,http://arxiv.org/abs/2304.05402 ,Boosting Cross-task Transferability of Adversarial Patches with Visual Relations,"Tony Ma, Songze Li, Yisong Xiao, Shunchang Liu","적대적 사례의 전달 가능성은 특히 블랙박스 시나리오에서 딥 러닝 시스템의 견고성을 평가하는 데 중요한 측면입니다. 모델 간 전이성을 향상시키기 위해 여러 가지 방법이 제안되었지만, 서로 다른 작업에 걸친 적대적 사례의 전이성에 대해서는 거의 관심을 기울이지 않았습니다. 이 문제는 Visual ChatGPT와 같은 기본 다중 작업 AI 시스템의 출현과 관련성이 높아져 단일 작업에서 생성된 적대적 샘플의 유용성이 상대적으로 제한되었습니다. 더욱이 이러한 시스템은 단순한 인식과 같은 작업을 넘어서는 추론 기능을 수반하는 경우가 많습니다. 이러한 격차를 해소하기 위해 우리는 다양한 시각적 작업, 특히 시각적 질문 응답 및 이미지 캡션과 같은 시각적 추론과 관련된 작업의 견고성을 평가하는 것을 목표로 하는 VRAP라는 새로운 시각적 관계 기반 교차 작업 적대 패치 생성 방법을 제안합니다. VRAP는 장면 그래프를 사용하여 객체 인식 기반 속임수와 술어 기반 관계 제거를 결합함으로써 추론 작업 간에 공유되는 시각적 추론 정보를 방해합니다. 우리의 광범위한 실험은 VRAP가 다양한 시각적 추론 작업 전반에 걸쳐 블랙박스 전송 가능성 측면에서 이전 방법을 훨씬 능가한다는 것을 보여줍니다."
393,http://arxiv.org/abs/2304.05277 ,Graph-based Topology Reasoning for Driving Scenes,"Tianyu Li, Li Chen, Huijie Wang, Yang Li, Jiazhi Yang, Xiangwei Geng, Shengyin Jiang, Yuting Wang, Hang Xu, Chunjing Xu, Junchi Yan, Ping Luo, Hongyang Li","자율주행을 구현하기 위해서는 도로 게놈을 이해하는 것이 필수적이다. 이러한 고도로 지능적인 문제는 차선의 연결관계, 차선과 교통요소 간의 할당관계라는 두 가지 측면을 포함하고 있어 포괄적인 토폴로지 추론 방법이 비어 있다. 한편으로, 이전 지도 학습 기술은 분할 또는 차선 패러다임을 사용하여 차선 연결성을 도출하는 데 어려움을 겪고 있습니다. 또는 이전 차선 토폴로지 중심 접근 방식은 중앙선 감지에 초점을 맞추고 상호 작용 모델링을 무시합니다. 반면, 차선 할당 문제에 대한 교통 요소는 이미지 영역에 제한되어 두 가지 관점에서 대응을 구성하는 방법을 탐구되지 않은 과제로 남겨 둡니다. 이러한 문제를 해결하기 위해 우리는 기존 인식 작업을 넘어 교통 지식을 추상화할 수 있는 최초의 엔드투엔드 프레임워크인 TopoNet을 제시합니다. 운전 장면 토폴로지를 포착하기 위해 세 가지 주요 디자인을 소개합니다. (1) 2D 요소의 의미 지식을 통합 기능 공간에 통합하는 임베딩 모듈; (2) 관계를 모델링하고 네트워크 내부에서 기능 상호 작용을 가능하게 하는 선별된 장면 그래프 신경망; (3) 임의로 메시지를 전송하는 대신 다양한 유형의 도로 게놈과 사전 지식을 구별할 수 있도록 장면 지식 그래프를 고안합니다. 우리는 까다로운 장면 이해 벤치마크인 OpenLane-V2에서 TopoNet을 평가합니다. 여기서 우리의 접근 방식은 모든 지각 및 토폴로지 메트릭에서 큰 차이로 이전의 모든 작업보다 성능이 뛰어납니다. 코드는 https://github.com/OpenDriveLab/TopoNet에서 공개됩니다."
392,http://arxiv.org/abs/2304.05146 ,Loop Closure Detection Based on Object-level Spatial Layout and Semantic Consistency,"Xingwu Ji, Peilin Liu, Haochen Niu, Xiang Chen, Rendong Ying, Fei Wen","SLAM(시각적 동시 위치 파악 및 매핑) 시스템은 시점이 크게 변경되는 상황에서 루프 폐쇄를 감지하는 데 어려움을 겪습니다. 본 논문에서는 3D 장면 그래프의 공간적 레이아웃과 의미적 일관성을 기반으로 한 객체 기반 루프 폐쇄 탐지 방법을 제시합니다. 첫째, 의미론적 레이블, IoU(Intersection Over Union), 객체 색상 및 객체 임베딩의 의미 정보를 기반으로 객체 수준 데이터 연관 접근 방식을 제안합니다. 그 후, 연관된 객체와의 다중 뷰 번들 조정을 활용하여 객체와 카메라의 포즈를 공동으로 최적화합니다. 정제된 객체를 의미와 위상을 갖춘 3차원 공간 그래프로 표현합니다. 그런 다음, 정점 이웃의 구조 레이아웃과 의미적 속성 유사성을 기반으로 대응 객체를 선택하는 그래프 매칭 접근 방식을 제안합니다. 마지막으로, 객체 수준 포즈 그래프 최적화에서 카메라 궤적과 객체 포즈를 공동으로 최적화하여 전 세계적으로 일관된 맵을 생성합니다. 실험 결과는 우리가 제안한 데이터 연관 접근법이 보다 정확한 3D 의미 맵을 구성할 수 있으며 루프 폐쇄 방법이 시점 변경이 큰 상황에서 포인트 기반 및 객체 기반 방법보다 더 강력하다는 것을 보여줍니다."
391,http://arxiv.org/abs/2304.03495 ,Devil's on the Edges: Selective Quad Attention for Scene Graph Generation,"Deunsol Jung, Sanghyun Kim, Won Hwa Kim, Minsu Cho","장면 그래프 생성은 이미지의 노드와 가장자리가 각각 개체와 관계를 나타내도록 이미지로부터 의미 그래프 구조를 구성하는 것을 목표로 합니다. 작업의 주요 과제 중 하나는 이미지에 방해가 되는 개체와 관계가 있다는 것입니다. 맥락적 추론은 관련 없는 대상이나 배경, 그리고 더 중요하게는 수많은 관련 없는 후보자 관계에 의해 크게 산만해집니다. 이 문제를 해결하기 위해 우리는 관련 개체 쌍을 선택하고 다양한 상황별 상호 작용을 통해 이를 명확하게 하는 방법을 학습하는 SQUAT(Selective Quad Attention Network)를 제안합니다. SQUAT는 가장자리 선택과 쿼드 어텐션이라는 두 가지 주요 구성 요소로 구성됩니다. 가장자리 선택 모듈은 장면 그래프의 가장자리와 같은 관련 개체 쌍을 선택하여 상황적 추론을 돕고, 쿼드 어텐션 모듈은 개체와 개체 쌍 간의 상황별 정보를 캡처하기 위해 가장자리-노드 및 가장자리-엣지 교차 관심을 모두 사용하여 가장자리 특징을 업데이트합니다. 실험은 SQUAT의 강력한 성능과 견고성을 입증하여 Visual Genome 및 Open Images v6 벤치마크에서 최고 수준을 달성했습니다."
390,http://arxiv.org/abs/2304.00733 ,Unbiased Scene Graph Generation in Videos,"Sayak Nag, Kyle Min, Subarna Tripathi, Amit K. Roy Chowdhury","비디오에서 동적 장면 그래프 생성(SGG) 작업은 이미지 기반 SGG의 기존 문제 외에도 장면의 고유한 역학, 모델 예측의 시간적 변동, 시각적 관계의 긴 꼬리 분포로 인해 복잡하고 어렵습니다. 동적 SGG를 위한 기존 방법은 위에서 언급한 문제, 특히 장기적인 관계 분포를 해결하지 않고 복잡한 아키텍처를 사용하여 시공간적 컨텍스트를 캡처하는 데 주로 중점을 두었습니다. 이로 인해 편향된 장면 그래프가 생성되는 경우가 많습니다. 이러한 문제를 해결하기 위해 우리는 TEMPURA라는 새로운 프레임워크를 도입합니다. 편향되지 않은 동적 SGG를 위한 TEmporal 일관성 및 메모리 프로토타입 기반 UnceRtainty Attenuation입니다. TEMPURA는 변환기 기반 시퀀스 모델링을 통해 객체 수준의 시간적 일관성을 사용하고, 메모리 기반 교육을 사용하여 편향되지 않은 관계 표현을 합성하는 방법을 학습하고, GMM(Gaussian Mixture Model)을 사용하여 시각적 관계의 예측 불확실성을 줄입니다. 광범위한 실험을 통해 우리의 방법이 기존 방법에 비해 상당한(경우에 따라 최대 10%) 성능 향상을 달성하여 보다 편견 없는 장면 그래프를 생성하는 데 있어 우수함을 강조합니다."
389,http://arxiv.org/abs/2304.00590 ,SPAN: Learning Similarity between Scene Graphs and Images with Transformers,"Yuren Cong, Wentong Liao, Bodo Rosenhahn, Michael Ying Yang","장면 그래프와 이미지 간의 유사성 학습은 장면 그래프와 이미지가 주어졌을 때 유사성 점수를 추정하는 것을 목표로 합니다. 장면 그래프 생성 및 다운스트림 애플리케이션에 중요하지만 현재 이 작업에 대한 연구는 없습니다. 장면 그래프 생성은 일반적으로 Recall$@K$ 및 평균 Recall$@K$로 평가되며, 이는 사람이 라벨링한 삼중항 세트에 나타나는 예측 삼중항의 비율을 측정합니다. 그러나 이러한 삼중항 중심 측정항목은 장면 그래프와 이미지 간의 전체적인 의미적 차이를 보여주지 못하고 주석 편향과 노이즈에 민감합니다. 따라서 다운스트림 애플리케이션에서 생성된 장면 그래프를 사용하는 것은 제한됩니다. 이 문제를 해결하기 위해 장면 그래프와 이미지 간의 유사성을 측정할 수 있는 장면 그래프-이미지 대조 학습 프레임워크인 SPAN을 처음으로 제안합니다. 우리의 새로운 프레임워크는 공유 잠재 공간에서 장면 그래프와 해당 이미지를 정렬하기 위한 그래프 변환기와 이미지 변환기로 구성됩니다. 장면 그래프를 구조적 인코딩을 사용하여 시퀀스로 변환하는 새로운 그래프 직렬화 기술을 소개합니다. 우리 프레임워크를 기반으로 장면 그래프 생성을 위한 새로운 평가 지표로 이미지 검색 정확도를 측정하는 R-Precision을 제안합니다. 우리는 Visual Genome 및 Open Images 데이터 세트에 대한 새로운 벤치마크를 설정합니다. 장면 그래프 인코더로서 큰 잠재력을 보여주는 SPAN의 유효성을 검증하기 위해 광범위한 실험이 수행됩니다."
388,http://arxiv.org/abs/2303.15994 ,HiLo: Exploiting High Low Frequency Relations for Unbiased Panoptic Scene Graph Generation,"Zijian Zhou, Miaojing Shi, Holger Caesar","PSG(Panoptic Scene Graph Generation)는 이미지를 분할하고 피사체, 객체 및 관계의 삼중항을 추출하여 장면 그래프를 구축하는 것을 목표로 하는 이미지 장면 이해 분야에서 최근 제안된 작업입니다. 이 작업은 두 가지 이유로 특히 어렵습니다. 첫째, 관계 범주에서 롱테일 문제로 인해 순진한 편향된 방법이 고주파 관계에 더 기울어지게 됩니다. 기존의 편견 없는 방법은 저주파 관계를 선호하도록 데이터/손실 재조정을 통해 롱테일 문제를 해결합니다. 둘째, 주체-객체 쌍은 의미상 두 개 이상의 중첩 관계를 가질 수 있습니다. 기존 방법이 다른 방법보다 선호되는 반면, 우리가 제안한 HiLo 프레임워크는 다양한 네트워크 분기가 저주파 및 고주파 관계를 전문으로 하고 일관성을 강화하며 결과를 융합할 수 있도록 합니다. 우리가 아는 한, 우리는 명시적으로 편견이 없는 PSG 방법을 제안한 최초의 사람입니다. 광범위한 실험에서 우리는 HiLo 프레임워크가 PSG 작업에서 최첨단 결과를 달성한다는 것을 보여줍니다. 또한 마스크 대신 상자를 예측하는 장면 그래프 생성 작업에 방법을 적용하고 모든 기본 방법에 대한 개선 사항을 확인합니다. 코드는 https://github.com/franciszzj/HiLo에서 확인할 수 있습니다."
387,http://arxiv.org/abs/2303.14408 ,VL-SAT: Visual-Linguistic Semantics Assisted Training for 3D Semantic Scene Graph Prediction in Point Cloud,"Ziqin Wang, Bowen Cheng, Lichen Zhao, Dong Xu, Yang Tang, Lu Sheng","포인트 클라우드에서 3D 의미론적 장면 그래프(3DSSG) 예측 작업은 (1) 3D 포인트 클라우드가 2D 이미지에 비해 제한된 의미로 기하학적 구조만 캡처하고 (2) 긴 꼬리 관계 분포가 본질적으로 편견 없는 예측 학습을 방해하기 때문에 어렵습니다. 2D 이미지는 풍부한 의미를 제공하고 장면 그래프는 본질적으로 언어에 대처하므로 본 연구에서는 긴 꼬리와 모호한 의미 관계를 식별하여 3DSSG 예측 모델을 크게 강화할 수 있는 VL-SAT(Visual-Linguistic Semantics Assisted Training) 방식을 제안합니다. 핵심 아이디어는 3D 모델을 지원하기 위해 강력한 다중 모드 오라클 모델을 훈련시키는 것입니다. 이 오라클은 비전, 언어 및 3D 기하학의 의미론을 기반으로 신뢰할 수 있는 구조적 표현을 학습하며, 그 이점은 교육 단계에서 3D 모델에 이질적으로 전달될 수 있습니다. VL-SAT는 훈련에서 시각적 언어 의미론을 효과적으로 활용함으로써 특히 꼬리 관계 삼중 항을 처리할 때 추론 단계의 3D 입력만으로 SGFN 및 SGGpoint와 같은 일반적인 3DSSG 예측 모델을 크게 향상시킬 수 있습니다. 3DSSG 데이터 세트에 대한 포괄적인 평가 및 절제 연구를 통해 제안된 체계의 효율성이 검증되었습니다. 코드는 https://github.com/wz7in/CVPR2023-VLSAT에서 확인할 수 있습니다."
386,http://arxiv.org/abs/2304.10505 ,Video Pre-trained Transformer: A Multimodal Mixture of Pre-trained Experts,"Kastan Day, Daniel Christl, Rohan Salvi, Pranav Sriram","Video Pre-trained Transformer를 소개합니다. VPT는 이전 작업의 4가지 SOTA 인코더 모델을 사용하여 비디오를 컴팩트 임베딩 시퀀스로 변환합니다. 참조 Flan-T5-11B 아키텍처를 기반으로 하는 우리의 백본은 인코더 모델의 비선형 합인 비디오의 보편적인 표현을 학습합니다. YouTube 동영상에서 말하는 단어를 예측하여 자동 회귀 인과 언어 모델링 손실을 사용하여 학습합니다. 마지막으로 각 작업에 대해 완전히 연결된 예측 헤드를 훈련하여 표준 다운스트림 벤치마크를 평가합니다. 우리가 아는 한, 이는 ""임베딩 -> 백본 -> 예측 헤드"" 설계 패턴에서 인코더로 여러 고정 SOTA 모델을 처음으로 사용하는 것입니다. 다른 모든 모델은 자체 공동 인코더 모델을 훈련했습니다. 또한 명시적인 장면 그래프 정보를 추가하여 현재 SOTA, Merlot Reserve보다 더 많은 양식을 포함합니다. 이 두 가지 이유 때문에 우리는 세계 최고의 오픈 소스 모델을 결합하여 SOTA 성능을 달성할 수 있다고 믿습니다. 초기 실험에서는 모델이 적절하게 학습하고 있음을 보여 주지만 더 높은 목표를 실현하려면 더 많은 실험과 계산이 필요하며 이미 진행 중입니다. 이 작업과 함께 우리는 YT-20M 데이터세트를 기반으로 이를 재생산하고 개인적으로 선택한 25,000개의 YouTube 동영상을 코퍼스에 추가합니다. 모든 코드 및 모델 체크포인트는 표준 MIT 라이선스에 따라 오픈 소스로 제공됩니다."
385,http://arxiv.org/abs/2303.13293 ,LABRAD-OR: Lightweight Memory Scene Graphs for Accurate Bimodal Reasoning in Dynamic Operating Rooms,"Ege Özsoy, Tobias Czempiel, Felix Holm, Chantal Pellegrini, Nassir Navab","현대 수술은 의료진, 환자 및 장비 간의 끊임없이 변화하는 상호 작용을 포함하여 복잡하고 역동적인 환경에서 수행됩니다. 따라서 수술실(OR)의 전체적인 모델링은 수술팀의 성과를 최적화하고 환자 결과를 개선하기 위한 새로운 수술 기술 개발에 도움이 될 수 있는 도전적이면서도 필수적인 작업입니다. 엔터티가 노드로 표시되고 엔터티 간의 관계가 가장자리로 표시되는 SGG(의미론적 장면 그래프)로 수술 장면을 전체적으로 표현하는 것은 세밀한 의미론적 OR 이해를 위한 유망한 방향입니다. 우리는 보다 정확하고 일관된 전체적인 OR 모델링을 위해 시간 정보의 사용을 처음으로 제안합니다. 구체적으로, 이전 시간 단계의 장면 그래프가 현재 예측을 안내하는 시간적 표현 역할을 하는 메모리 장면 그래프를 소개합니다. 우리는 경량 메모리 장면 그래프의 시간 정보를 포인트 클라우드 및 이미지의 시각적 정보와 지능적으로 융합하는 엔드 투 엔드 아키텍처를 설계합니다. 우리는 4D-OR 데이터 세트에 대한 방법을 평가하고 시간성을 통합하면 매크로 F1에서 +5% 증가와 0.88의 새로운 SOTA를 달성하는 보다 정확하고 일관된 결과를 얻을 수 있음을 보여줍니다. 이 작업은 메모리 장면 그래프를 통해 전체 수술 이력을 표현하는 길을 열어주고 수술실에 대한 전체적인 이해를 향상시킵니다. 장면 그래프를 메모리 표현으로 도입하면 많은 시간적 이해 작업에 유용한 도구를 제공할 수 있습니다."
384,http://arxiv.org/abs/2303.13233 ,Visually-Prompted Language Model for Fine-Grained Scene Graph Generation in an Open World,"Qifan Yu, Juncheng Li, Yu Wu, Siliang Tang, Wei Ji, Yueting Zhuang","SGG(Scene Graph Generation)는 시각 이해를 위해 이미지 내 <주어, 술어, 객체> 관계를 추출하는 것을 목표로 합니다. 최근 SGG에 대한 연구가 꾸준히 진행되고 있지만 꼬리 술어는 훈련 비용이 더 많이 들고 빈번한 술어에 비해 주석이 달린 데이터가 적어 구별하기 어려운 롱테일 분포 문제를 여전히 겪고 있습니다. 기존 재조정 전략은 사전 규칙을 통해 이를 처리하려고 시도하지만 여전히 사전 정의된 조건에 국한되어 다양한 모델 및 데이터 세트에 대해 확장할 수 없습니다. 본 논문에서는 시각적으로 프롬프트되는 언어 모델을 학습하여 저자원 방식으로 다양한 세분화된 조건자를 생성하는 CaCao(Cross-modal prediCate Boosting) 프레임워크를 제안합니다. 제안된 CaCao는 플러그 앤 플레이 방식으로 적용할 수 있으며 기존 SGG를 자동으로 강화하여 롱테일 문제를 해결할 수 있습니다. 이를 바탕으로 모델이 보이지 않는 조건을 제로샷 방식으로 일반화할 수 있는 오픈 월드 조건자 장면 그래프 생성(Epic)을 위한 새로운 Entangled 교차 모달 프롬프트 접근 방식을 추가로 소개합니다. 세 가지 벤치마크 데이터세트에 대한 종합적인 실험에서는 CaCao가 모델에 구애받지 않는 방식으로 여러 장면 그래프 생성 모델의 성능을 지속적으로 향상시키는 것으로 나타났습니다. 또한, 우리 Epic은 오픈 월드 예측 예측에서 경쟁력 있는 성능을 달성합니다. 이 문서의 데이터와 코드는 공개적으로 사용 가능합니다."
383,http://arxiv.org/abs/2303.13209 ,Taking A Closer Look at Visual Relation: Unbiased Video Scene Graph Generation with Decoupled Label Learning,"Wenqing Wang, Yawei Luo, Zhiqing Chen, Tao Jiang, Lei Chen, Yi Yang, Jun Xiao","현재의 비디오 기반 장면 그래프 생성(VidSGG) 방법은 훈련 데이터의 고유한 편향된 분포로 인해 덜 표현되는 예측 조건에 대해 제대로 수행되지 않는 것으로 나타났습니다. 이 논문에서 우리는 술어를 자세히 살펴보고 대부분의 시각적 관계(예: sit_above)가 행동 패턴(sit)과 공간 패턴(위)을 모두 포함하는 반면, 분포 편향은 패턴 수준에서 훨씬 덜 심각하다는 것을 식별합니다. 이러한 통찰력을 바탕으로 우리는 패턴 수준 관점에서 다루기 힘든 시각적 관계 예측을 해결하기 위해 DLL(분리된 레이블 학습) 패러다임을 제안합니다. 특히 DLL은 조건자 레이블을 분리하고 별도의 분류자를 채택하여 동작 패턴과 공간 패턴을 각각 학습합니다. 그런 다음 패턴이 결합되어 조건자에 다시 매핑됩니다. 또한, 꼬리 클래스의 분포를 보정하기 위해 동일한 패턴 내에서 머리 술어에서 꼬리 술어로 비대상 지식을 전달하는 지식 수준 레이블 분리 방법을 제안합니다. 우리는 일반적으로 사용되는 VidSGG 벤치마크, 즉 VidVRD에서 DLL의 효율성을 검증합니다. 광범위한 실험을 통해 DLL이 최첨단 VidSGG 성능을 달성하여 롱테일 문제에 대해 매우 간단하면서도 매우 효과적인 솔루션을 제공한다는 사실이 입증되었습니다."
382,http://arxiv.org/abs/2303.11090 ,Scene Graph Based Fusion Network For Image-Text Retrieval,"Guoliang Wang, Yanlei Shang, Yong Chen","이미지-텍스트 검색의 중요한 과제는 이미지와 텍스트 간의 정확한 대응을 학습하는 방법입니다. 대부분의 기존 방법은 의미 객체의 동시 발생을 기반으로 한 대략적인 대응에 주로 초점을 맞추고 있으며, 세분화된 지역 대응을 구별하지 못합니다. 본 논문에서는 이미지-텍스트 검색을 위한 내부 및 교차 모달 융합을 통해 이미지/텍스트의 기능을 향상시키는 새로운 장면 그래프 기반 융합 네트워크(SGFN)를 제안합니다. 구체적으로, 우리는 장면 그래프를 통해 객체, 속성, 관계와 같은 의미적 맥락을 이미지/텍스트의 특징 벡터에 통합하는 모달 내 계층적 주의 융합과 맥락적 벡터를 통해 맥락적 의미론과 로컬 융합을 결합하는 교차 모달 주의 융합을 설계합니다. 공개 데이터 세트 Flickr30K 및 MSCOCO에 대한 광범위한 실험에서는 SGFN이 상당수의 SOTA 이미지-텍스트 검색 방법보다 성능이 더 우수하다는 것을 보여줍니다."
381,http://arxiv.org/abs/2303.11048 ,SGFormer: Semantic Graph Transformer for Point Cloud-based 3D Scene Graph Generation,"Changsheng Lv, Mengshi Qi, Xia Li, Zhengyuan Yang, Huadong Ma","본 논문에서는 포인트 클라우드 기반 3차원 장면 그래프 생성을 위한 새로운 모델인 SGFormer, Semantic Graph TransFormer를 제안한다. 이 작업은 복잡한 전역 구조를 모델링하는 핵심 과제와 함께 포인트 클라우드 기반 장면을 의미 구조 그래프로 구문 분석하는 것을 목표로 합니다. 그래프 컨벌루션 네트워크(GCN)를 기반으로 하는 기존 방법은 과도한 스무딩 딜레마를 겪고 있으며 제한된 이웃 노드에서만 정보를 전파할 수 있습니다. 이와 대조적으로 SGFormer는 Transformer 레이어를 기본 빌딩 블록으로 사용하여 전역 정보 전달을 허용하며, 3D 장면 그래프 생성 작업에 맞게 맞춤화된 두 가지 유형의 새로 설계된 레이어를 사용합니다. 특히, 비교 가능한 계산 비용을 유지하면서 그래프 가장자리의 전역 정보를 가장 잘 활용하기 위해 그래프 임베딩 레이어를 도입합니다. 또한 대규모 언어 모델(예: ChatGPT)의 언어 지식을 활용하여 객체의 시각적 특징을 향상시키는 의미 주입 계층을 제안합니다. 우리는 확립된 3DSSG 데이터 세트에서 SGFormer를 벤치마킹하고 관계 예측의 R@50에서 40.94%의 절대적 개선을 달성했으며 최첨단 기술에 비해 복잡한 장면이 있는 하위 집합에서 88.36%의 향상을 달성했습니다. 우리의 분석은 롱테일 및 제로샷 시나리오에서 SGFormer의 우월성을 더욱 보여줍니다. 우리의 소스 코드는 https://github.com/Andy20178/SGFormer에서 확인할 수 있습니다."
380,http://arxiv.org/abs/2303.10944 ,Location-Free Scene Graph Generation,"Ege Özsoy, Felix Holm, Mahdi Saleh, Tobias Czempiel, Chantal Pellegrini, Nassir Navab, Benjamin Busam","장면 그래프 생성(SGG)은 장면을 개체와 개체 간의 관계 그래프로 설명하는 것을 목표로 하는 시각적 이해 작업입니다. 기존 작업은 경계 상자 또는 분할 마스크 형태의 위치 레이블에 의존하므로 주석 비용이 증가하고 데이터 세트 확장이 제한됩니다. 많은 애플리케이션에 위치 데이터가 필요하지 않다는 점을 인식하여 이러한 종속성을 깨고 LF-SGG(위치 없는 장면 그래프 생성)를 도입했습니다. 이 새로운 작업은 공간적 위치를 명시적으로 계산하지 않고도 엔터티의 인스턴스와 그 관계를 예측하는 것을 목표로 합니다. 작업을 객관적으로 평가하려면 예측 장면 그래프와 실제 장면 그래프를 비교해야 합니다. 우리는 효율적인 분기 알고리즘을 통해 이 NP-hard 문제를 해결합니다. 또한 자동회귀 시퀀스 모델링을 사용하여 첫 번째 LF-SGG 방법인 Pix2SG를 설계합니다. 우리는 세 가지 장면 그래프 생성 데이터 세트와 두 가지 다운스트림 작업, 이미지 검색 및 시각적 질문 답변에 대한 방법의 효율성을 보여주고, 위치 단서에 의존하지 않으면서 우리의 접근 방식이 기존 방법에 비해 경쟁력이 있음을 보여줍니다."
379,http://arxiv.org/abs/2303.10863 ,Decomposed Prototype Learning for Few-Shot Scene Graph Generation,"Xingchen Li, Jun Xiao, Guikun Chen, Yinfu Feng, Yi Yang, An-an Liu, Long Chen","오늘날의 장면 그래프 생성(SGG) 모델은 일반적으로 새로운 조건자 유형을 학습하기 위해 풍부한 수동 주석이 필요합니다. 따라서 주석을 수집하기 어려운 대규모의 일반적이지 않은 술어 카테고리가 있는 실제 애플리케이션에 적용하기가 어렵습니다. 본 논문에서는 FSSGG(Few-Shot SGG)에 중점을 두는데, 이는 SGG 모델이 몇 가지 예만으로 이전 지식을 신속하게 전달하고 보이지 않는 조건자를 잘 인식할 수 있도록 지원합니다. 그러나 FSSGG에 대한 현재 방법은 SGG의 술어 카테고리의 높은 클래스 내 분산으로 인해 방해를 받습니다. 한편, 각 술어 카테고리는 일반적으로 서로 다른 컨텍스트에서 여러 의미론적 의미를 갖습니다. 반면, 동일한 술어를 갖는 세 쌍의 관계의 시각적 모양은 주어-객체 구성에 따라 크게 다릅니다. 입력의 이러한 큰 차이로 인해 현재 FSL(Few-Shot Learning) 방법으로 각 술어 범주에 대한 일반화 가능한 표현을 학습하기가 어렵습니다. 그러나 우리는 이러한 클래스 내 술어의 변동이 구성된 주어 및 객체와 밀접한 관련이 있음을 발견했습니다. 주체-객체 컨텍스트를 사용하여 술어의 클래스 내 분산을 모델링하기 위해 FSSGG를 위한 새로운 DPL(Decomposed Prototype Learning) 모델을 제안합니다. 구체적으로, 먼저 술어에 대한 주어와 객체의 다양한 의미와 시각적 패턴을 여러 프로토타입으로 분해하여 포착하기 위한 분해 가능한 프로토타입 공간을 구축합니다. 그런 다음 이러한 프로토타입을 서로 다른 가중치와 통합하여 각 쿼리 샘플에 대해 보다 신뢰할 수 있는 의미 체계를 갖춘 쿼리 적응형 조건자 표현을 생성합니다. 우리는 우리 방법의 효율성을 보여주기 위해 광범위한 실험을 수행하고 다양한 기본 방법과 비교합니다."
378,http://arxiv.org/abs/2303.10766 ,Multi-modal reward for visual relationships-based image captioning,"Ali Abedi, Hossein Karshenas, Peyman Adibi",심층 신경망은 효과적인 표현 학습 및 상황 기반 콘텐츠 생성 기능으로 인해 자동 이미지 캡션 작성에서 유망한 결과를 얻었습니다. 최근 많은 이미지 캡션 방법에 사용되는 주요 유형의 심층 기능으로 잘 알려진 상향식 기능은 원시 이미지에서 직접 추출된 기능 맵과 비교하여 이미지의 다양한 객체에 대한 자세한 표현을 제공합니다. 그러나 비용이 많이 들고 리소스를 많이 사용하는 추출 절차에도 불구하고 이러한 개체 간의 관계에 대한 높은 수준의 의미 정보가 부족하다는 점은 상향식 기능의 중요한 단점입니다. 캡션 생성 시 시각적 관계를 활용하기 위해 본 논문에서는 이미지의 장면 그래프에서 추출된 시각적 관계 정보와 이미지의 공간 특징 맵을 융합하는 이미지 캡션을 위한 심층 신경망 아키텍처를 제안합니다. 그런 다음 공통 임베딩 공간에서 언어와 비전 유사성의 조합을 사용하여 제안된 네트워크의 심층 강화 학습을 위해 다중 모드 보상 기능이 도입됩니다. MSCOCO 데이터 세트에 대한 광범위한 실험 결과는 제안된 캡션 방법에서 시각적 관계를 사용하는 효과를 보여줍니다. 또한 결과는 심층 강화 학습에서 제안된 다중 모드 보상이 더 나은 모델 최적화로 이어지며 여러 최첨단 이미지 캡션 알고리즘을 능가하는 동시에 가볍고 이미지 특징 추출이 용이하다는 것을 분명히 나타냅니다. 제안된 방법을 구성하는 구성 요소에 대한 자세한 실험적 연구도 제시됩니다.
377,http://arxiv.org/abs/2303.09410 ,Narrator: Towards Natural Control of Human-Scene Interaction Generation via Relationship Reasoning,"Haibiao Xuan, Xiongzheng Li, Jinsong Zhang, Hongwen Zhang, Yebin Liu, Kun Li","자연스럽게 제어 가능한 HSI(Human-Scene Interaction) 생성은 VR/AR 콘텐츠 제작, 인간 중심 AI 등 다양한 분야에서 중요한 역할을 합니다. 그러나 기존 방법은 제어 가능성이 부자연스럽고 직관적이지 않아 실제로 적용하는 데 큰 제한이 있습니다. 따라서 우리는 텍스트 설명으로부터 현실적이고 다양한 HSI를 자연스럽고 제어 가능하게 생성하는 어려운 작업에 중점을 둡니다. 인간의 인지를 바탕으로 이상적인 생성 모델은 공간 관계와 상호 작용 동작을 올바르게 추론해야 합니다. 이를 위해 우리는 3D 장면과 텍스트 설명이 주어지면 자연스럽게 제어 가능한 생성을 위해 조건부 변형 자동 인코더를 사용하는 새로운 관계 추론 기반 생성 접근 방식인 Narrator를 제안합니다. 또한 장면 그래프를 기반으로 각각 3D 장면과 텍스트 설명에서 전역 및 로컬 공간 관계를 모델링하고 상호 작용을 원자 신체 부분 상태로 표현하는 부분 수준 동작 메커니즘을 도입합니다. 특히, 관계 추론을 활용하여 제어 가능한 다중 인간 장면 상호 작용 생성에 대한 첫 번째 탐색인 간단하면서도 효과적인 다중 인간 생성 전략을 제안합니다. 우리의 광범위한 실험과 지각 연구에 따르면 내레이터는 다양한 상호 작용을 제어 가능하게 생성하고 기존 작업보다 훨씬 뛰어난 성능을 발휘할 수 있습니다. 코드와 데이터 세트는 연구 목적으로 사용할 수 있습니다."
376,http://arxiv.org/abs/2303.08998 ,Unified Visual Relationship Detection with Vision and Language Models,"Long Zhao, Liangzhe Yuan, Boqing Gong, Yin Cui, Florian Schroff, Ming-Hsuan Yang, Hartwig Adam, Ting Liu",이 작업은 여러 데이터 세트의 레이블 공간 결합을 예측하는 단일 시각적 관계 감지기를 교육하는 데 중점을 둡니다. 일관되지 않은 분류로 인해 다양한 데이터 세트에 걸쳐 있는 레이블을 병합하는 것이 어려울 수 있습니다. 개체 쌍 사이에 2차 시각적 의미 체계가 도입되면 시각적 관계 검색에서 문제가 더욱 악화됩니다. 이러한 과제를 해결하기 위해 우리는 VLM(비전 및 언어 모델)을 활용하여 통합 시각적 관계 감지를 위한 새로운 상향식 방법인 UniVRD를 제안합니다. VLM은 의미 체계 통합을 위해 유사한 관계가 서로 가까워지도록 최적화된 잘 정렬된 이미지 및 텍스트 임베딩을 제공합니다. 우리의 상향식 설계를 통해 모델은 객체 감지 및 시각적 관계 데이터 세트를 모두 사용하여 훈련의 이점을 누릴 수 있습니다. 인간-객체 상호 작용 감지 및 장면 그래프 생성에 대한 경험적 결과는 우리 모델의 경쟁력 있는 성능을 보여줍니다. UniVRD는 HICO-DET에서 38.07mAP를 달성하여 현재 최고의 상향식 HOI 검출기보다 14.26mAP 더 뛰어난 성능을 발휘합니다. 더 중요한 것은 통합 검출기가 mAP의 데이터 세트별 모델과 마찬가지로 성능을 발휘하고 모델을 확장할 때 추가 개선을 달성한다는 점입니다. 우리의 코드는 GitHub에서 공개적으로 제공될 예정입니다.
375,http://arxiv.org/abs/2303.08473 ,Unsupervised Traffic Scene Generation with Synthetic 3D Scene Graphs,"Artem Savkin, Rachid Ellouze, Nassir Navab, Federico Tombari","컴퓨터 그래픽에 의한 이미지 합성은 최근 놀라운 현실감을 얻었지만, 이렇게 생성된 합성 이미지 데이터는 실제 데이터와 상당한 영역 차이를 드러냅니다. 이는 신경망 훈련을 위해 합성 데이터 활용을 극복하는 데 중요한 측면을 나타내는 자율 주행 시나리오에서 특히 그렇습니다. 우리는 렌더링 없이 교통 장면 영상을 직접 합성하기 위해 도메인 불변 장면 표현 기반의 방법을 제안한다. 특히, 우리는 내부 표현으로 합성 장면 그래프를 사용하고 현실적인 교통 장면 합성을 위해 비지도 신경망 아키텍처를 도입합니다. 장면에 대한 공간 정보로 합성 장면 그래프를 향상시키고 장면 조작을 통해 접근 방식의 효율성을 보여줍니다."
374,http://arxiv.org/abs/2303.07096 ,Prototype-based Embedding Network for Scene Graph Generation,"Chaofan Zheng, Xinyu Lyu, Lianli Gao, Bo Dai, Jingkuan Song","SGG(현재 장면 그래프 생성) 방법은 상황별 정보를 탐색하여 엔터티 쌍 간의 관계를 예측합니다. 그러나 가능한 수많은 주체-객체 조합의 다양한 시각적 외관으로 인해 각 술어 범주 내에는 큰 클래스 내 변동(예: ""사람이 먹는 피자, 기린이 먹는 잎사귀"")이 있고, 모델의 잠재 공간에서는 ""사람이 들고 있는 접시, 사람이 먹는 피자""와 같이 서로 다른 클래스 간에 심각한 클래스 간 유사성이 있습니다. 위의 문제로 인해 현재 SGG 방법은 신뢰할 수 있는 관계 예측을 위한 강력한 기능을 획득하지 못합니다. 본 논문에서 우리는 술어의 범주 고유 의미론이 문제를 완화하기 위한 의미론적 공간에서 클래스별 프로토타입 역할을 할 수 있다고 주장합니다. 마지막으로 우리는 프로토타입에 맞춰 정렬된 컴팩트하고 고유한 표현으로 엔터티/술어를 모델링하고 이를 통해 관계 인식을 위한 공통 임베딩 공간에서 엔터티 쌍과 조건자 간의 매칭을 설정하는 프로토타입 기반 임베딩 네트워크(PE-Net)를 제안합니다. 또한 PE-Net이 이러한 엔터티 술어 매칭을 효율적으로 학습할 수 있도록 Prototype-guided Learning(PL)을 도입하고, 술어의 의미적 중복으로 인해 발생하는 모호한 엔터티-술어 매칭을 완화하기 위해 Prototype Regularization(PR)을 고안했습니다. 광범위한 실험을 통해 우리의 방법이 SGG에서 우수한 관계 인식 기능을 획득하여 Visual Genome 및 Open Images 데이터세트 모두에서 새로운 최첨단 성능을 달성했음을 보여줍니다."
373,http://arxiv.org/abs/2303.06842 ,Hierarchical Relationships: A New Perspective to Enhance Scene Graph Generation,"Bowen Jiang, Camillo J. Taylor","본 논문에서는 관계 및 개체에 대한 레이블 간의 계층 구조를 활용하면 장면 그래프 생성 시스템의 성능이 크게 향상될 수 있다는 사실을 제시합니다. 이 작업의 초점은 객체 및 관계 범주를 체계적으로 분리된 상위 범주로 나눌 수 있는 유익한 계층 구조를 만드는 것입니다. 구체적으로, 우리는 한 쌍의 객체 인스턴스 사이의 관계에 대한 슈퍼 카테고리와 해당 슈퍼 카테고리 내의 세부 관계를 동시에 예측하여 보다 유익한 예측을 촉진하는 베이지안 예측 헤드를 도입합니다. 결과 모델은 데이터 세트 주석을 넘어서 더 광범위한 조건자 세트를 생성하고 낮은 주석 품질이라는 일반적인 문제를 해결할 수 있는 기능을 보여줍니다. 우리의 논문은 예비 결과를 제시하는 반면, Visual Genome 데이터 세트에 대한 실험은 특히 조건자 분류 및 제로샷 설정에서 강력한 성능을 보여 우리 접근 방식의 가능성을 보여줍니다."
372,http://arxiv.org/abs/2303.04634 ,Transformer-based Image Generation from Scene Graphs,"Renato Sortino, Simone Palazzo, Concetto Spampinato","그래프 구조의 장면 설명은 생성 모델에서 효율적으로 사용되어 생성된 이미지의 구성을 제어할 수 있습니다. 이전 접근 방식은 각각 레이아웃 예측 및 이미지 생성을 위한 그래프 컨벌루션 네트워크와 적대적 방법의 조합을 기반으로 합니다. 이 작업에서는 다중 헤드 주의를 사용하여 그래프 정보를 인코딩하고 이미지 생성을 위해 잠재 공간에서 변환기 기반 모델을 사용하는 것이 학습 안정성 측면에서 후속 이점을 갖는 적대적 모델을 사용할 필요 없이 샘플링된 데이터의 품질을 향상시킬 수 있는 방법을 보여줍니다. 특히 제안된 접근 방식은 장면 그래프를 중간 개체 레이아웃으로 인코딩하고 이러한 레이아웃을 이미지로 디코딩하여 벡터 양자화된 변형 자동 인코더에 의해 학습된 저차원 공간을 통과하는 변환기 아키텍처를 전적으로 기반으로 합니다. 우리의 접근 방식은 최첨단 방법에 비해 향상된 이미지 품질과 동일한 장면 그래프의 여러 세대 간에 더 높은 수준의 다양성을 보여줍니다. 우리는 Visual Genome, COCO 및 CLEVR의 세 가지 공개 데이터 세트에 대한 접근 방식을 평가합니다. 우리는 COCO와 Visual Genome에서 각각 13.7과 12.8의 Inception Score와 52.3과 60.3의 FID를 달성했습니다. 우리는 각 구성요소의 영향을 평가하기 위해 기여도에 대한 절제 연구를 수행합니다. 코드는 https://github.com/perceivelab/trf-sg2im에서 확인할 수 있습니다."
371,http://arxiv.org/abs/2303.01080 ,LANDMARK: Language-guided Representation Enhancement Framework for Scene Graph Generation,"Xiaoguang Chang, Teng Wang, Shaowei Cai, Changyin Sun","장면 그래프 생성(SGG)은 복잡한 시각적 특징과 데이터 세트 롱테일 문제를 모두 겪는 정교한 작업입니다. 최근에는 새로운 손실 함수와 데이터 밸런싱 전략을 설계하여 다양한 unbiased 전략이 제안되었습니다. 불행하게도 이러한 편견 없는 방법은 기능 개선 관점에서 언어 우선 순위를 강조하지 못합니다. 술어는 주어-객체 쌍 및 전역적 맥락에 숨겨진 의미와 높은 상관관계가 있다는 사실에서 영감을 받아, 언어-비전 상호작용 패턴, 전역 언어 맥락 및 쌍-술어 상관관계로부터 술어 관련 표현을 학습하는 LANDMARK(LANguage-guiDed RepresentationenhanceMent frAmewoRK)를 제안합니다. 구체적으로, 우리는 먼저 다양한 표현 학습을 위해 객체 레이블을 세 가지 고유한 의미 임베딩에 투영합니다. 그런 다음 LAM(Language Attention Module) 및 EEM(Experience Estimation Module)은 주체-객체 단어 임베딩을 각각 주의 벡터 및 술어 분포로 처리합니다. LCM(Language Context Module)은 각 단어 삽입의 전역 컨텍스트를 인코딩하여 로컬 정보로부터 격리된 학습을 방지합니다. 마지막으로 모듈 출력은 시각적 표현과 SGG 모델의 예측을 업데이트하는 데 사용됩니다. 모든 언어 표현은 순수하게 개체 범주에서 생성되므로 추가 지식이 필요하지 않습니다. 이 프레임워크는 모델에 구애받지 않으며 기존 SGG 모델의 성능을 지속적으로 향상시킵니다. 게다가 표현 수준의 편견 없는 전략은 LANDMARK에 다른 방법과의 호환성이라는 이점을 부여합니다. 코드는 https://github.com/rafa-cxg/PySGG-cxg에서 확인할 수 있습니다."
370,http://arxiv.org/abs/2302.10425 ,Instance-incremental Scene Graph Generation from Real-world Point Clouds via Normalizing Flows,"Chao Qi, Jianqin Yin, Jinghang Xu, Pengxiang Ding","이 작업은 인스턴스 증분 장면 그래프 생성이라는 새로운 작업을 소개합니다. 포인트 클라우드 장면이 주어지면 이를 그래프로 표현하고 자동으로 새로운 인스턴스를 증가시킵니다. 최종적으로 장면의 객체 레이아웃을 나타내는 그래프가 생성됩니다. 증강 현실과 같은 비전 기반 애플리케이션에서 새로운 3D 개체를 실제 장면에 삽입하는 데 도움이 되므로 중요한 작업입니다. 또한 실제 포인트 클라우드의 복잡성으로 인해 관찰 데이터(레이블이 있는 의미 체계가 있는 비어 있지 않은 방)에서 객체 레이아웃 경험을 학습하는 데 어려움이 있기 때문에 어렵습니다. 우리는 이 작업을 조건부 생성 문제로 모델링하고 이를 해결하기 위해 정규화 흐름(3D-ANF)을 기반으로 하는 3D 자동 회귀 프레임워크를 제안합니다. 먼저, 라벨 의미와 맥락적 관계를 추출하여 포인트 클라우드를 그래프로 표현합니다. 다음으로, 그래픽 요소의 조건부 생성을 가우시안 프로세스에 매핑하기 위해 정규화 흐름을 기반으로 한 모델이 도입되었습니다. 매핑은 반전 가능합니다. 따라서 관찰 데이터에 표현된 실제 경험은 훈련 단계에서 모델링될 수 있으며, 테스트 단계에서는 가우시안 프로세스를 기반으로 새로운 인스턴스가 자동 회귀적으로 생성될 수 있습니다. 우리 방법의 성능을 충분히 평가하기 위해 우리는 실내 벤치마크 데이터세트 3DSSG-O27R16과 새로 제안된 실외 장면의 그래픽 데이터세트 GPL3D에 이 새로운 작업을 구현했습니다. 실험에 따르면 우리의 방법은 실제 포인트 클라우드에서 신뢰할 수 있는 새로운 그래프를 생성하고 데이터 세트에서 최첨단 성능을 달성하는 것으로 나타났습니다."
369,http://arxiv.org/abs/2302.07691 ,"Project Elements: A computational entity-component-system in a scene-graph pythonic framework, for a neural, geometric computer graphics curriculum","George Papagiannakis, Manos Kamarianakis, Antonis Protopsaltis, Dimitris Angelis, Paul Zikas","우리는 장면 그래프 기반 Python 프레임워크의 신속한 프로토타입 제작 편의성과 함께 ECS(엔티티 구성 요소 시스템)의 장점을 처음으로 제공하는 교육적 요구에 맞춰 제작된 경량 오픈 소스 전산 과학 및 컴퓨터 그래픽(CG) 프레임워크인 Elements 프로젝트를 소개합니다. 이 참신함은 이질적인 방향성 비순환 그래프와 깊이 우선 순회부터 독특한 시스템을 통해 렌더링된 애니메이션, 스키닝, 기하 대수 및 셰이더 기반 구성 요소, 3D 과학 시각화를 위한 그래프 신경망으로의 표현에 이르기까지 CG 교육의 발전을 가능하게 합니다. Scenegraph 기반 시스템의 고유한 ECS를 활용하는 이 프로젝트는 CG 커리큘럼과 동일한 접근 방식을 기반으로 하지만 종종 이러한 개념을 블랙박스 접근 방식으로 제시하는 최신 게임 엔진(MGE)을 연결하는 것을 목표로 합니다. 확장 가능한 오픈 소스 접근 방식으로 소프트웨어 디자인 패턴을 적극적으로 활용하도록 설계되었습니다. Elements는 Jupyter 노트북 및 단위 테스트를 통해 현대적이고(즉, 고정 기능 OpenGL이 아닌 셰이더 기반) 프로그램 접근 방식이 간단하지만 CG 파이프라인은 블랙박스가 아니며 처음으로 독특하고 도전적인 과학, 시각 및 신경 컴퓨팅 개념을 교육하기 위해 노출됩니다."
368,http://arxiv.org/abs/2302.06494 ,Explicit3D: Graph Network with Spatial Inference for Single Image 3D Object Detection,"Yanjun Liu, Wenming Yang","실내 3D 객체 감지는 단일 이미지 장면 이해에 필수적인 작업으로, 시각적 추론에서 근본적으로 공간 인식에 영향을 미칩니다. 단일 이미지에서 3D 개체 감지에 대한 기존 작업은 각 개체에 대한 독립적인 예측을 통해 이 목표를 추구하거나 가능한 모든 개체에 대해 암시적으로 추론하여 개체 간의 관계형 기하학적 정보를 활용하지 못합니다. 이 문제를 해결하기 위해 우리는 객체 기하학과 의미론적 특징을 기반으로 하는 Explicit3D라는 동적 희소 그래프 파이프라인을 제안합니다. 효율성을 고려하여 관련성 점수를 추가로 정의하고 새로운 동적 가지치기 알고리즘과 희소 장면 그래프 생성 및 업데이트를 위한 클러스터 샘플링 방법을 설계합니다. 또한 Explicit3D는 동종 행렬을 도입하고 새로운 상대 손실 및 코너 손실을 정의하여 대상 쌍 간의 공간 차이를 명시적으로 모델링합니다. 실측 라벨을 직접 감독으로 사용하는 대신 상대 및 코너 손실은 동종 변환에서 파생되며, 이는 모델이 객체 간의 기하학적 일관성을 학습하도록 렌더링합니다. SUN RGB-D 데이터 세트의 실험 결과는 Explicit3D가 최신 기술보다 더 나은 성능 균형을 달성했음을 보여줍니다."
367,http://arxiv.org/abs/2302.10276 ,See Your Heart: Psychological states Interpretation through Visual Creations,"Likun Yang, Xiaokun Feng, Xiaotang Chen, Shiyu Zhang, Kaiqi Huang",정신분석학에서는 시각적 창작을 통해 개인의 심리 상태에 대한 해석을 생성하는 것이 상당한 요구에 직면해 있습니다. 컴퓨터 비전 분야의 기존 연구의 두 가지 주요 과제인 정서/감정 분류와 정서적 캡션은 심리적 해석의 요구 사항을 거의 충족하지 못합니다. 정신분석에 대한 요구를 충족시키기 위해 \textbf{V}isual \textbf{E}motion \textbf{I}nterpretation \textbf{T}ask(VEIT)라는 어려운 작업을 소개합니다. VEIT에서는 AI가 시각적 창작물을 통해 제작자의 심리 상태에 대한 합리적인 해석을 생성하도록 요구합니다. 작업을 지원하기 위해 우리는 심리학 이론이 지원되고 전문적인 주석이 달린 SpyIn(\textbf{S}and\textbf{p}la\textbf{y} \textbf{In}terpretation Dataset)이라는 다중 모드 데이터 세트를 제시합니다. 데이터 세트 분석은 SpyIn이 VEIT를 지원할 수 있을 뿐만 아니라 다른 캡션 데이터 세트에 비해 더 어렵다는 것을 보여줍니다. SpyIn을 기반으로 여러 이미지 캡션 방법에 대한 실험을 수행하고 SpyIn에 대한 SOTA 결과를 얻는 시각적 의미 결합 모델을 제안합니다. 결과는 VEIT가 장면 그래프 정보와 심리적 지식이 필요한 더 어려운 작업임을 나타냅니다. 우리의 작업은 또한 AI가 시각적 창조물을 통해 인류의 내면 세계를 분석하고 설명할 수 있다는 가능성을 보여줍니다.
366,http://arxiv.org/abs/2302.03477 ,Explainable Action Prediction through Self-Supervision on Scene Graphs,"Pawit Kochakarn, Daniele De Martini, Daniel Omeiza, Lars Kunze","이 작업에서는 장면 그래프를 미래 운전자 행동 예측에 적용되는 자율 주행에 대한 상위 수준 정보의 증류된 표현으로 탐색합니다. 데이터 샘플의 희소성과 심각한 불균형을 고려하여 대표성과 잘 구분된 임베딩을 추론하기 위한 자체 감독 파이프라인을 제안합니다. 주요 측면은 해석 가능성과 설명 가능성입니다. 따라서 우리는 장면 그래프에 공간적, 시간적 히트맵을 생성할 수 있는 주의 메커니즘을 아키텍처에 포함시켰습니다. 우리는 완전히 감독되는 접근 방식과 비교하여 ROAD 데이터 세트에서 시스템을 평가하여 훈련 체제의 우수성을 보여줍니다."
365,http://arxiv.org/abs/2302.02651 ,1st Place Solution for PSG competition with ECCV'22 SenseHuman Workshop,"Qixun Wang, Xiaofeng Guo, Haofan Wang","PSG(Panoptic Scene Graph) 생성은 엄격한 경계 상자 대신 Panoptic 분할을 기반으로 장면 그래프 표현을 생성하는 것을 목표로 합니다. 기존 PSG 방법은 장면 그래프 생성과 의미론적 분할 마스크를 동시에 예측하는 1단계 패러다임을 활용하거나, 기성 팬옵틱 분할기를 먼저 채택한 후 예측된 개체 간의 쌍별 관계 예측을 적용하는 2단계 패러다임을 활용합니다. 단순화된 훈련 패러다임에도 불구하고 1단계 접근 방식은 분할 결과가 일반적으로 만족스럽지 못한 반면, 2단계 접근 방식은 전역 컨텍스트가 부족하고 관계 예측 성능이 낮습니다. 이러한 격차를 해소하기 위해 본 논문에서는 사전 추출된 로컬 객체 특징과 해당 마스크가 클래스 임베딩을 통해 변환기에 공급되는 2단계 패러다임의 글로벌 관계 네트워크인 GRNet을 제안합니다. 긴 꼬리 분포로 인한 관계 모호성과 술어 분류 편향을 처리하기 위해 두 번째 단계에서 관계 예측을 소프트 레이블이 있는 다중 클래스 분류 작업으로 공식화합니다. 우리는 OpenPSG 데이터 세트에 대한 포괄적인 실험을 수행하고 리드보드에서 최첨단 성능을 달성합니다. 또한 절제 연구에서 긴 꼬리 클래스에 대한 소프트 라벨 전략의 효율성을 보여줍니다. 우리 코드는 https://github.com/wangqixun/mfpsg에 공개되었습니다."
364,http://arxiv.org/abs/2302.01403 ,Self-Supervised Relation Alignment for Scene Graph Generation,"Bicheng Xu, Renjie Liao, Leonid Sigal","장면 그래프 생성의 목표는 입력 이미지에서 그래프를 예측하는 것입니다. 여기서 노드는 식별되고 지역화된 개체에 해당하고 가장자리는 해당 상호 작용 조건에 해당합니다. 기존 방법은 완전히 감독되는 방식으로 훈련되었으며 메시지 전달 메커니즘, 손실 기능 및/또는 편향 완화에 중점을 둡니다. 이 작업에서는 장면 그래프 생성 성능을 향상시키기 위해 설계된 간단하면서도 효과적인 자체 감독 관계형 정렬 정규화를 소개합니다. 제안된 정렬은 일반적이며 원래 모델의 목표와 함께 훈련되는 기존 장면 그래프 생성 프레임워크와 결합될 수 있습니다. 정렬은 감독 대상과 매개변수를 미러링하고 공유하는 보조 관계 예측 분기가 설계되는 증류를 통해 달성됩니다. 보조 분기에서 관계형 입력 기능은 메시지 전달 및 조건자 예측 전에 부분적으로 마스킹됩니다. 마스크된 관계에 대한 예측은 메시지 전달 후 감독 대상과 정렬됩니다. 우리는 두 가지 장면 그래프 생성 아키텍처인 SGTR 및 Neural Motif와 함께 이러한 자체 감독 관계형 정렬의 효율성을 설명하고 두 경우 모두 크게 향상된 성능을 달성한다는 것을 보여줍니다."
363,http://arxiv.org/abs/2302.08901 ,Exploring External Knowledge for Accurate modeling of Visual and Language Problems,Xuewen Yang,"인공지능(AI)과 그 응용에 대한 관심은 지난 몇 년 동안 전례 없이 증가했습니다. 이러한 성공은 부분적으로 컴퓨터 비전(CV) 및 자연어 처리(NLP)와 같은 AI 하위 분야에서 이루어진 심층 신경망의 발전에 기인합니다. 본 논문이 중점을 두고 있는 유망한 연구 분야는 분류, 탐지, 분할, 기계 번역, 캡션 등 많은 까다로운 작업을 수반하는 시각적 및 언어 이해입니다. 이러한 문제를 해결하기 위한 최첨단 방법은 일반적으로 소스 데이터와 대상 레이블의 두 부분만 포함하는데, 이는 특히 데이터 세트가 작을 때 다소 불충분합니다. 한편, 많은 외부 도구나 소스는 이러한 방법의 성능을 향상시키는 데 도움이 될 수 있는 추가 유용한 정보(외부 지식)를 제공할 수 있습니다. 예를 들어, 이미지 캡션 모델을 위한 최첨단 ResNet보다 더 나은 객체 기능을 제공하기 위해 감지 모델이 적용되었습니다. 이러한 관찰에서 영감을 받아 우리는 먼저 외부 지식을 추출한 다음 이를 원래 모델과 통합할 수 있는 방법론을 개발했습니다. 외부 지식은 데이터 세트에서 추출되어야 하거나 문법 규칙이나 장면 그래프와 같은 외부에서 직접 가져올 수 있습니다. 우리는 이 방법론을 기계 번역, 이미지 캡션 작성 등 다양한 AI 작업에 적용하고 원래의 최첨단 모델을 큰 폭으로 개선합니다."
362,http://arxiv.org/abs/2301.07666 ,DDS: Decoupled Dynamic Scene-Graph Generation Network,"A S M Iftekhar, Raphael Ruschel, Satish Kumar, Suya You, B. S. Manjunath",장면 그래프 생성에는 입력 데이터에서 주체-객체 관계 삼중항을 예측하여 장면 내 개체 간의 관계에 대한 구조적 표현을 만드는 작업이 포함됩니다. 기존 방법은 주로 종속 특성 학습에 의존하기 때문에 미리 정의된 세트 외부의 삼중항을 감지하는 데 성능이 좋지 않습니다. 이 문제를 해결하기 위해 우리는 추출된 특징을 분리할 수 있는 두 개의 독립적인 분기로 구성된 분리된 동적 장면 그래프 생성 네트워크인 DDS를 제안합니다. 현재 논문의 주요 혁신은 개체 관계를 나타내는 기능을 개체의 기능과 분리하여 새로운 개체 관계 조합을 감지할 수 있다는 것입니다. DDS 모델은 세 가지 데이터 세트에서 평가되었으며 특히 이전에 볼 수 없었던 삼중 항목을 감지하는 데 있어 이전 방법보다 훨씬 뛰어납니다.
361,http://arxiv.org/abs/2301.00929 ,EQUI-VOCAL: Synthesizing Queries for Compositional Video Events from Limited User Interactions [Technical Report],"Enhao Zhang, Maureen Daum, Dong He, Brandon Haynes, Ranjay Krishna, Magdalena Balazinska","제한된 사용자 상호 작용에서 비디오에 대한 쿼리를 자동으로 합성하는 새로운 시스템인 EQUI-VOCAL을 소개합니다. 사용자는 자신이 찾고 있는 것에 대한 몇 가지 긍정적인 예와 부정적인 예만 제공합니다. EQUI-VOCAL은 이러한 초기 예제와 능동 학습을 통해 수집된 추가 예제를 활용하여 복잡한 사용자 쿼리를 효율적으로 합성합니다. 우리의 접근 방식을 통해 사용자는 데이터베이스 전문 지식 없이도, 제한된 라벨링 노력으로, 선언적 사양이나 스케치 없이도 이벤트를 찾을 수 있습니다. EQUI-VOCAL 디자인의 핵심은 데이터 모델 및 쿼리 언어에 시공간 장면 그래프를 사용하고 크고 잡음이 많은 비디오 데이터에서 작동하는 새로운 쿼리 합성 접근 방식을 사용하는 것입니다. 우리 시스템은 F1 점수, 합성 시간, 노이즈에 대한 견고성 측면에서 두 가지 기준 시스템보다 성능이 뛰어나며 기준이 지원하지 않는 복잡한 쿼리를 유연하게 합성할 수 있습니다."
360,http://arxiv.org/abs/2301.00351 ,Skew Class-balanced Re-weighting for Unbiased Scene Graph Generation,"Haeyong Kang, Chang D. Yoo","긴 꼬리 분포로 인한 편견 없는 예측을 고려하기 위해 Skew Class-balanced Re-weighting(SCR)이라고 하는 편견 없는 장면 그래프 생성(SGG) 알고리즘이 제안되었습니다. 이전 연구들은 주로 소수 술어 예측의 성능 저하를 완화하는 데 중점을 두었으며, 회상 점수가 급격히 떨어지는, 즉 다수 술어 성능을 잃습니다. 제한된 SGG 데이터 세트에서 다수와 소수 조건자 성능 간의 균형을 아직 정확하게 분석하지 못했습니다. 본 논문에서는 이 문제를 완화하기 위해 편향되지 않은 SGG 모델에 대해 SCR(Skew Class-Balanced Re-weighting) 손실 함수를 고려합니다. 편향된 술어 예측의 왜곡도를 활용하여 SCR은 목표 술어 가중치 계수를 추정한 다음 다수 술어와 소수 술어 간의 더 나은 균형을 위해 편향된 술어에 더 많은 가중치를 부여합니다. 표준 Visual Genome 데이터 세트와 Open Image V4 \& V6에 대해 수행된 광범위한 실험은 기존 SGG 모델을 사용한 SCR의 성능과 일반성을 보여줍니다."
359,http://arxiv.org/abs/2301.00146 ,Peer Learning for Unbiased Scene Graph Generation,"Liguang Zhou, Junjie Hu, Yuhongze Zhou, Tin Lun Lam, Yangsheng Xu","USGG(편향되지 않은 장면 그래프 생성)는 이미지의 개체 간에 다양하고 불균형이 심한 조건을 예측해야 하는 어려운 작업입니다. 이 문제를 해결하기 위해 우리는 조건자 샘플링과 합의 투표(PSCV)를 사용하여 여러 동료가 서로 학습하도록 장려하는 새로운 프레임워크 동료 학습을 제안합니다. 조건자 샘플링은 조건자 클래스를 빈도에 따라 하위 분포로 나누고, 각 하위 분포 또는 이들의 조합을 처리할 다른 피어를 할당합니다. 합의 투표는 다수의 의견을 강조하고 소수의 의견을 축소함으로써 동료들의 보완적인 술어 지식을 앙상블합니다. Visual Genome에 대한 실험에서는 PSCV가 이전 방법보다 성능이 뛰어나고 평균 31.6으로 SGCls 작업에서 새로운 최첨단 기술을 달성했음을 보여줍니다."
358,http://arxiv.org/abs/2212.11770 ,S-Graphs+: Real-time Localization and Mapping leveraging Hierarchical Representations,"Hriday Bavle, Jose Luis Sanchez-Lopez, Muhammad Shaheer, Javier Civera, Holger Voos","본 논문에서는 최적화 가능한 단일 요소 그래프(1) 포즈 그래프(관련 측정 및 로봇 포즈로 구성된 로봇 키프레임 세트), (2) 의미론적 속성과 이들 간의 관계 정보를 사용하여 다양한 기하학적 요소를 인코딩하는 환경의 상위 수준 표현인 3D 장면 그래프로 공동 모델링하는 진화된 버전의 상황 그래프를 제시합니다.   특히, S-Graphs+는 (1) 로봇 포즈 추정이 포함된 키프레임 레이어, (2) 벽면을 나타내는 벽 레이어, (3) 벽면 세트를 둘러싸는 방 레이어, (4) 주어진 바닥 수준 내의 방을 모으는 바닥 레이어를 포함하는 새로운 4개 레이어로 구성된 요인 그래프입니다. 위의 그래프는 로봇의 자세와 맵에 대한 강력하고 정확한 추정치를 얻기 위해 실시간으로 최적화되었으며, 동시에 환경에 대한 높은 수준의 정보를 구성하고 활용합니다. 이러한 높은 수준의 정보를 추출하기 위해 매핑된 벽면과 자유 공간 클러스터를 활용하는 새로운 방 및 바닥 분할 알고리즘을 제시합니다.   우리는 다양한 건설 현장의 실내 환경에 대한 시뮬레이션 데이터와 실제 데이터를 포함한 여러 데이터세트와 여러 실내 사무실 공간의 실제 공개 데이터세트에서 S-Graphs+를 테스트했습니다. 평균적으로 우리 데이터세트에 비해 S-Graphs+는 차선책의 정확도보다 10.67% 더 뛰어나며, 더 풍부한 장면 모델을 통해 로봇 상황 인식을 확장합니다. 또한 소프트웨어를 도커 파일로 제공합니다."
357,http://arxiv.org/abs/2212.09329 ,SrTR: Self-reasoning Transformer with Visual-linguistic Knowledge for Scene Graph Generation,"Yuxiang Zhang, Zhenbo Liu, Shuai Wang","장면의 개체가 항상 관련되어 있는 것은 아닙니다. 희소 제안 세트와 몇 가지 쿼리를 사용하여 엔터티 쌍 간의 효과적인 관계를 추론하는 1단계 장면 그래프 생성 접근 방식의 실행 효율성은 상당히 높습니다. 그러나 삼중 집합의 주어 실체, 술어 실체, 목적어 실체에서는 주어와 목적어의 관계에만 초점을 맞추고, 주어와 술어 또는 술어와 목적어의 관계를 무시하고 자기추론 능력이 부족하다. 또한, 1단계 방식에서는 언어적 양상이 무시되어 왔다. 모델 추론 능력을 향상시키기 위해서는 언어 양식 지식을 채굴하는 것이 필요합니다. 위에서 언급한 단점을 해결하기 위해 모델에 유연한 자기 추론 능력을 추가하는 SrTR(Self-Resoning Transformer with Visual-linguistic Knowledge)이 제안되었습니다. SrTR에는 인코더-디코더 아키텍처가 채택되었으며, 삼중 집합, s+o-p, s+p-o 및 p+o-s의 세 가지 추론을 완료하기 위해 자체 추론 디코더가 개발되었습니다. 대규모 사전 학습 이미지-텍스트 기반 모델에서 영감을 받아 시각적 언어 사전 지식이 도입되고 시각적 언어 정렬 전략은 관계형 추론을 돕기 위해 사전 지식이 있는 의미 공간에 시각적 표현을 투영하도록 설계되었습니다. Visual Genome 데이터 세트에 대한 실험은 제안된 방법의 우수성과 빠른 추론 능력을 보여줍니다."
356,http://arxiv.org/abs/2212.08283 ,SceneGATE: Scene-Graph based co-Attention networks for TExt visual question answering,"Feiqi Cao, Siwen Luo, Felipe Nunez, Zean Wen, Josiah Poon, Caren Han","대부분의 TextVQA 접근 방식은 간단한 변환기 인코더를 통한 객체, 장면 텍스트 및 질문 단어의 통합에 중점을 둡니다. 그러나 이는 서로 다른 양식 간의 의미론적 관계를 포착하는 데 실패합니다. 본 논문에서는 객체, OCR(Optical Character Recognition) 토큰 및 질문 단어 간의 의미 관계를 밝히는 TextVQA용 장면 그래프 기반 co-Attention Network(SceneGATE)를 제안합니다. 이는 이미지의 기본 의미를 발견하는 TextVQA 기반 장면 그래프를 통해 달성됩니다. 우리는 모달 간 상호 작용을 위한 지침으로 언어와 비전 간의 모달 내 상호 작용을 포착하기 위해 안내 주의 모듈을 만들었습니다. 두 양식 사이의 관계를 명시적으로 가르치기 위해 우리는 두 개의 주의 모듈, 즉 장면 그래프 기반 의미론적 관계 인식 주의와 위치 관계 인식 주의를 제안하고 통합했습니다. 우리는 Text-VQA와 ST-VQA라는 두 가지 벤치마크 데이터 세트에 대해 광범위한 실험을 수행했습니다. 우리의 SceneGATE 방법은 장면 그래프와 주의 모듈로 인해 기존 방법보다 성능이 뛰어난 것으로 나타났습니다."
355,http://arxiv.org/abs/2212.07796 ,CREPE: Can Vision-Language Foundation Models Reason Compositionally?,"Zixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi, Irena Gao, Ranjay Krishna","인간의 시각과 자연어 모두에 공통된 기본 특성은 구성적 특성입니다. 그러나 대규모 비전 및 언어 사전 훈련으로 인한 성능 향상에도 불구하고 대규모 데이터 세트에 대한 4가지 알고리즘으로 훈련된 7개 아키텍처에서 구성성에 어려움을 겪고 있음을 발견했습니다. 이 결론에 도달하기 위해 우리는 인지 과학 문헌에서 식별된 구성성의 두 가지 중요한 측면인 체계성과 생산성을 측정하는 새로운 구성성 평가 벤치마크인 CREPE를 소개합니다. 체계성을 측정하기 위해 CREPE는 $370K$ 이상의 이미지-텍스트 쌍과 3가지 서로 다른 보이는-보이지 않는 분할을 포함하는 테스트 데이터 세트로 구성됩니다. 세 가지 분할은 CC-12M, YFCC-15M 및 LAION-400M이라는 세 가지 인기 있는 훈련 데이터 세트에서 훈련된 모델을 테스트하도록 설계되었습니다. 또한 쌍의 하위 집합에 대해 $325K$, $316K$, $309K$ 하드 네거티브 캡션을 생성합니다. 생산성을 테스트하기 위해 CREPE에는 9개의 서로 다른 복잡성을 지닌 $17K$ 이미지-텍스트 쌍과 원자, 스와핑 및 부정 포일을 포함한 $183K$ 하드 네거티브 캡션이 포함되어 있습니다. 데이터 세트는 Visual Genome 장면 그래프와 영역 설명을 용도 변경하고 손으로 제작한 템플릿과 GPT-3을 적용하여 생성됩니다. 체계성을 위해 새로운 구성이 검색 세트를 지배할 때 모델 성능이 지속적으로 감소하며 Recall@1이 최대 $12\%$까지 떨어지는 것을 발견했습니다. 생산성을 위해 모델의 검색 성공은 복잡성이 증가함에 따라 감소하며 높은 복잡성에서는 종종 무작위 기회에 가까워집니다. 이러한 결과는 모델 및 교육 데이터 세트 크기에 관계없이 유지됩니다."
354,http://arxiv.org/abs/2212.04155 ,Latent Graph Representations for Critical View of Safety Assessment,"Aditya Murali, Deepak Alapatt, Pietro Mascagni, Armine Vardazaryan, Alain Garcia, Nariaki Okamoto, Didier Mutter, Nicolas Padoy","복강경 담낭절제술에서 안전에 대한 중요한 관점을 평가하려면 주요 해부학적 구조의 정확한 식별과 위치 파악, 서로의 기하학적 관계에 대한 추론, 노출의 질 결정이 필요합니다. 이전 연구에서는 의미론적 분할을 중간 단계로 포함하고 예측 분할 마스크를 사용하여 CVS를 예측함으로써 이 작업에 접근했습니다. 이러한 방법은 효과적이지만 매우 비용이 많이 드는 실측 분할 주석에 의존하고 예측 분할이 올바르지 않으면 실패하는 경향이 있어 일반화가 제한됩니다. 본 연구에서는 먼저 풀린 잠재 장면 그래프를 사용하여 수술 이미지를 표현한 다음 그래프 신경망을 사용하여 이 표현을 처리하는 CVS 예측 방법을 제안합니다. 우리의 그래프 표현은 객체 위치, 클래스 정보, 기하학적 관계와 같은 의미론적 정보를 명시적으로 인코딩하여 해부학 중심 추론을 개선할 뿐만 아니라 시각적 특징을 식별하여 차별화 가능성을 유지함으로써 의미론적 오류에 대한 견고성을 제공합니다. 마지막으로, 주석 비용을 해결하기 위해 경계 상자 주석만을 사용하여 세밀한 객체 경계를 학습하기 위한 보조 이미지 재구성 목표를 통합하여 방법을 학습할 것을 제안합니다. 우리는 우리의 방법이 경계 상자 주석으로 훈련할 때 여러 기본 방법보다 성능이 뛰어날 뿐만 아니라 분할 마스크로 훈련할 때 효과적으로 확장되어 최첨단 성능을 유지한다는 것을 보여줍니다."
353,http://arxiv.org/abs/2212.03866 ,Learning Action-Effect Dynamics for Hypothetical Vision-Language Reasoning Task,"Shailaja Keyur Sampat, Pratyay Banerjee, Yezhou Yang, Chitta Baral","'행동'은 인간이 세상과 상호작용하는 방식에 중요한 역할을 합니다. 따라서 일상적인 작업을 지원하는 자율 에이전트에는 '행동 및 변경에 대한 추론'(RAC)을 수행하는 기능도 필요합니다. 이는 일반적으로 인공지능(AI) 분야에서 중요한 연구 방향이었지만, 시각적, 언어적 입력을 갖춘 RAC에 대한 연구는 비교적 최근에 이루어졌습니다. CLEVR_HYP(Sampat et. al., 2021)는 행동을 핵심 초점으로 하는 가상 비전 언어 추론을 위한 테스트베드 중 하나입니다. 본 연구에서는 행동의 효과에 대한 추론을 향상시킬 수 있는 새로운 학습 전략을 제안합니다. 우리는 동작을 벡터로 표현하는 방법을 배우기 위해 인코더-디코더 아키텍처를 구현합니다. 우리는 앞서 언급한 인코더-디코더 아키텍처를 기존 모달리티 파서 및 장면 그래프 질문 응답 모델과 결합하여 CLEVR_HYP 데이터 세트에서 제안된 시스템을 평가합니다. 우리는 제안된 접근 방식의 효율성을 입증하기 위해 철저한 실험을 수행하고 성능, 데이터 효율성 및 일반화 기능 측면에서 이전 기준에 비해 장점을 논의합니다."
352,http://arxiv.org/abs/2212.03433 ,Learning Action-Effect Dynamics from Pairs of Scene-graphs,"Shailaja Keyur Sampat, Pratyay Banerjee, Yezhou Yang, Chitta Baral","'행동'은 인간이 세상과 상호작용하는 방식에 중요한 역할을 합니다. 따라서 일상적인 작업을 지원하는 자율 에이전트에는 '행동 및 변경에 대한 추론'(RAC)을 수행하는 기능도 필요합니다. 최근 시각적, 언어적 입력을 활용한 RAC 연구에 대한 관심이 높아지고 있습니다. 그래프는 일반적으로 장면 그래프라고 하는 시각적 콘텐츠(예: 개체, 해당 속성 및 개체 간의 관계)의 의미 구조를 나타내는 데 사용되는 경우가 많습니다. 본 연구에서는 이미지의 장면 그래프 표현을 활용하여 자연어로 설명된 행동의 효과를 추론하는 새로운 방법을 제안합니다. 기존 CLEVR_HYP(Sampat et. al, 2021) 데이터 세트를 실험하고 제안된 접근 방식이 기존 모델에 비해 성능, 데이터 효율성 및 일반화 기능 측면에서 효과적이라는 것을 보여줍니다."
351,http://arxiv.org/abs/2212.02875 ,Multi-Task Edge Prediction in Temporally-Dynamic Video Graphs,"Osman Ülger, Julian Wiederer, Mohsen Ghafoorian, Vasileios Belagiannis, Pascal Mettes","그래프 신경망은 효과적인 노드 표현을 학습하여 노드, 링크 및 그래프 수준 추론을 가능하게 하는 것으로 나타났습니다. 기존 그래프 네트워크는 노드 간의 정적 관계를 가정하는 반면, 비디오의 개체 간의 관계는 노드가 동적으로 들어오고 나가면서 시간이 지남에 따라 진화하는 경우가 많습니다. 이러한 시간-동적 그래프에서 핵심 문제는 여러 유형의 관계를 구성할 수 있는 시공간 가장자리의 미래 상태를 추론하는 것입니다. 이 문제를 해결하기 위해 우리는 여러 유형의 관계에 대한 시간적 동적 에지를 예측하기 위한 그래프 네트워크인 MTD-GNN을 제안합니다. 우리는 동적 노드 표현을 학습하고 여러 관계를 동시에 모델링하는 다중 작업 에지 예측 손실을 제시하기 위해 인수분해된 시공간 그래프 주의 레이어를 제안합니다. 제안된 아키텍처는 객체 감지 및 시공간 연결을 통해 비디오에서 얻은 장면 그래프 위에서 작동합니다. ActionGenome 및 CLEVRER에 대한 실험적 평가는 시간적 동적 그래프 네트워크에서 여러 관계를 모델링하는 것이 상호 이익이 될 수 있으며 기존 정적 및 시공간적 그래프 신경망은 물론 최첨단 조건자 분류 방법보다 성능이 뛰어날 수 있음을 보여줍니다."
350,http://arxiv.org/abs/2212.01186 ,A General Purpose Supervisory Signal for Embodied Agents,"Kunal Pratap Singh, Jordi Salvador, Luca Weihs, Aniruddha Kembhavi","효과적인 내장 AI 에이전트 교육에는 수동 보상 엔지니어링, 전문가 모방, 지도와 같은 특수 구성 요소 또는 깊이 및 현지화를 위한 추가 센서 활용이 포함되는 경우가 많습니다. 또 다른 접근 방식은 더 나은 표현 학습을 장려하는 자기 감독 목표와 함께 신경 아키텍처를 사용하는 것입니다. 실제로 이러한 자기 감독 목표가 작업 관련 정보를 인코딩한다는 보장은 거의 없습니다. 우리는 장면 그래프를 범용, 훈련 전용 감독 신호로 사용하는 장면 그래프 대비(SGC) 손실을 제안합니다. SGC 손실은 명시적인 그래프 디코딩을 없애고 대신 대조 학습을 사용하여 에이전트의 표현을 해당 환경의 풍부한 그래픽 인코딩과 정렬합니다. SGC 손실은 일반적으로 적용 가능하고 구현이 간단하며 객체의 의미, 관계 및 기록을 인코딩하는 표현을 권장합니다. SGC 손실을 사용하여 객체 탐색, 다중 객체 탐색 및 Arm Point 탐색의 세 가지 구현 작업에서 상당한 이점을 얻습니다. 마지막으로, 우리는 훈련된 표현이 환경에 대한 의미론적 단서를 인코딩하는 능력을 보여주는 연구와 분석을 제시합니다."
349,http://arxiv.org/abs/2212.00443 ,Unbiased Heterogeneous Scene Graph Generation with Relation-aware Message Passing Neural Network,"Kanghoon Yoon, Kibum Kim, Jinyoung Moon, Chanyoung Park","최근 SGG(장면 그래프 생성) 프레임워크는 이미지 내 여러 개체 간의 복잡한 관계를 학습하는 데 중점을 두었습니다. 객체와 인접 객체 간의 고차 상호 작용을 모델링하는 메시지 전달 신경망(MPNN)의 특성 덕분에 MPNN은 SGG의 주요 표현 학습 모듈입니다. 그러나 기존 MPNN 기반 프레임워크는 장면 그래프를 동종 그래프로 가정하므로 객체 간의 시각적 관계에 대한 상황 인식이 제한됩니다. 즉, 그들은 관계가 연관된 대상에 크게 의존하는 경향이 있다는 사실을 간과합니다. 본 논문에서는 메시지 전달 신경망을 사용하여 관계 인식 컨텍스트를 캡처하는 HetSGG(편향되지 않은 이종 장면 그래프 생성) 프레임워크를 제안합니다. 우리는 객체 간 술어 유형을 고려하여 이미지의 상황별 정보를 집계하는 관계 인식 메시지 전달 신경망(RMP)이라는 새로운 메시지 전달 계층을 고안했습니다. 우리의 광범위한 평가는 HetSGG가 최첨단 방법보다 성능이 뛰어나며, 특히 꼬리 조건자 클래스에서 성능이 뛰어남을 보여줍니다."
348,http://arxiv.org/abs/2212.00338 ,3D-Aware Object Goal Navigation via Simultaneous Exploration and Identification,"Jiazhao Zhang, Liu Dai, Fanpeng Meng, Qingnan Fan, Xuelin Chen, Kai Xu, He Wang","보이지 않는 환경에서의 객체 목표 탐색(ObjectNav)은 Embodied AI의 기본 작업입니다. 기존 작업의 에이전트는 2D 지도, 장면 그래프 또는 이미지 시퀀스를 기반으로 ObjectNav 정책을 학습합니다. 이 작업이 3D 공간에서 발생한다는 점을 고려하면 3D 인식 에이전트는 세밀한 공간 정보 학습을 통해 ObjectNav 기능을 향상시킬 수 있습니다. 그러나 3D 장면 표현을 활용하는 것은 낮은 샘플 효율성과 값비싼 계산 비용으로 인해 이 바닥 수준 작업의 정책 학습에 엄청나게 비실용적일 수 있습니다. 이 작업에서 우리는 두 가지 간단한 하위 정책을 기반으로 하는 까다로운 3D 인식 ObjectNav를 위한 프레임워크를 제안합니다. 코너 유도 탐색 정책과 카테고리 인식 식별 정책이라는 두 가지 하위 경찰은 온라인 융합 3D 포인트를 관찰로 활용하여 동시에 수행됩니다. 광범위한 실험을 통해 우리는 이 프레임워크가 3D 장면 표현 학습을 통해 ObjectNav의 성능을 획기적으로 향상시킬 수 있음을 보여줍니다. 우리의 프레임워크는 Matterport3D 및 Gibson 데이터 세트의 모든 모듈 기반 방법 중에서 최고의 성능을 달성하는 동시에 교육에 필요한 계산 비용을 최대 30배까지 절감합니다."
347,http://arxiv.org/abs/2211.16697 ,SGDraw: Scene Graph Drawing Interface Using Object-Oriented Representation,"Tianyu Zhang, Xusheng Du, Chia-Ming Chang, Xi Yang, Haoran Xie","장면 이해는 컴퓨터 비전에서 필수적이고 어려운 작업입니다. 이미지의 시각적으로 기본적인 그래픽 구조를 제공하기 위해 장면 그래프는 강력한 의미 표현으로 인해 많은 주목을 받았습니다. 그러나 이미지 검색, 이미지 생성, 멀티모달 애플리케이션에 적합한 장면 그래프를 그리는 것은 어렵습니다. 기존의 장면 그래프 주석 인터페이스는 이미지 주석에 사용하기 쉽지 않으며, 심층 신경망을 사용한 자동 장면 그래프 생성 방식은 세부 사항을 무시한 채 중복된 콘텐츠를 생성하는 경향이 있습니다. 본 연구에서는 사용자가 대화형으로 장면 그래프를 그리고 편집할 수 있도록 객체지향 장면 그래프 표현을 이용한 장면 그래프 그리기 인터페이스인 SGDraw를 제안합니다. 제안된 객체지향 표현을 위해 우리는 객체, 속성, 객체의 관계를 구조적 단위로 간주합니다. SGDraw는 장면 이해 애플리케이션을 위한 웹 기반 장면 그래프 주석 및 생성 도구를 제공합니다. 제안된 인터페이스의 유효성을 검증하기 위해 기존 도구와의 비교 연구와 사용자 경험 연구를 수행하였다. 결과는 SGDraw가 더 풍부한 세부 정보로 장면 그래프를 생성하고 기존 경계 상자 주석보다 이미지를 더 정확하게 설명하는 데 도움이 될 수 있음을 보여줍니다. 우리는 제안된 SGDraw가 이미지 검색 및 생성과 같은 다양한 비전 작업에 유용할 수 있다고 믿습니다."
346,http://arxiv.org/abs/2211.16636 ,Iterative Scene Graph Generation with Generative Transformers,"Sanjoy Kundu, Sathyanarayanan N. Aakur","장면 그래프는 엔터티(객체)와 해당 공간 관계를 그래픽 형식으로 인코딩하여 장면을 풍부하고 구조적으로 표현합니다. 이 표현은 질문 답변, 캡션 작성, 개체 감지 등 여러 작업에 유용한 것으로 입증되었습니다. 현재 접근 방식은 장면 내 객체 사이의 가능한 모든 가장자리에 대한 레이블을 지정하여 장면 그래프가 생성되는 분류별 생성 접근 방식을 취하며, 이는 접근 방식에 계산 오버헤드를 추가합니다. 이 작업은 링크 예측을 넘어 장면 그래프를 생성하기 위한 생성적 변환기 기반 접근 방식을 소개합니다. 두 개의 변환기 기반 구성 요소를 사용하여 먼저 감지된 개체와 해당 시각적 특징에서 가능한 장면 그래프 구조를 샘플링합니다. 그런 다음 샘플링된 가장자리에 대해 조건자 분류를 수행하여 최종 장면 그래프를 생성합니다. 이 접근 방식을 사용하면 추론 오버헤드를 최소화하면서 이미지에서 장면 그래프를 효율적으로 생성할 수 있습니다. Visual Genome 데이터 세트에 대한 광범위한 실험은 제안된 접근 방식의 효율성을 보여줍니다. 추가 기능 없이 장면 그래프 생성(SGG)을 위한 다양한 설정에서 평균 20.7%의 평균 재현율(mR@100)을 얻었으며, 이는 편견 없는 SGG 접근 방식에 경쟁력 있는 성능을 제공하는 동시에 최첨단 SGG 접근 방식을 능가합니다."
345,http://arxiv.org/abs/2212.02503 ,Relation-based Motion Prediction using Traffic Scene Graphs,"Maximilian Zipfl, Felix Hertlein, Achim Rettinger, Steffen Thoma, Lavdim Halilaj, Juergen Luettin, Stefan Schmid, Cory Henson","자율주행의 성공을 위해서는 교통 현장의 관련 정보를 표현하고 주변 환경을 이해하는 것이 중요합니다. 의미론적 관계, 즉 교통 규칙 기반 행동의 맥락에서 다양한 교통 참가자가 어떻게 관련되는지를 사용하여 자율주행차 주변을 모델링하는 것은 이전 연구에서는 거의 고려되지 않았습니다. 이는 실제 교통 상황에서 이러한 관계를 추출하기 어렵다는 사실에서 비롯됩니다. 본 연구에서는 교통 참여자에 대한 다양한 예측(예: 가속 및 감속)을 위해 공간 의미론적 장면 그래프 형태로 교통 장면을 모델링합니다. 우리의 학습 및 추론 접근 방식은 그래프 신경망(GNN)을 사용하며 교통 참가자 간의 공간 의미 관계에 대한 명시적인 정보를 통합하면 예측 결과가 향상된다는 것을 보여줍니다. 특히, 교통 참가자의 가속도 예측은 이 명시적인 정보를 활용하지 않는 기준선에 비해 최대 12% 향상됩니다. 또한 이전 장면에 대한 추가 정보를 포함함으로써 73%의 개선을 달성했습니다."
344,http://arxiv.org/abs/2211.15508 ,Self Supervised Clustering of Traffic Scenes using Graph Representations,"Maximilian Zipfl, Moritz Jarosch, J. Marius Zöllner",그래프의 유사성을 검사하는 것은 잘 알려진 과제이지만 그래프를 그룹화하려면 필수 과제입니다. 우리는 수동 라벨링 없이 자체 감독되는 교통 상황을 클러스터링하는 데이터 기반 방법을 제시합니다. 우리는 의미론적 장면 그래프 모델을 활용하여 교통 장면의 일반 그래프 임베딩을 생성한 다음 클러스터링이 수행되는 Siamese 네트워크를 사용하여 저차원 임베딩 공간에 매핑됩니다. 새로운 접근 방식의 훈련 과정에서 우리는 데카르트 공간의 기존 교통 장면을 강화하여 긍정적인 유사성 샘플을 생성합니다. 이를 통해 그래프를 재구성하는 과제를 극복하는 동시에 교통 장면의 유사성을 설명하는 표현을 얻을 수 있습니다. 결과 클러스터가 공통 의미론적 특성을 가지고 있음을 보여줄 수 있습니다. 이 접근 방식은 INTERACTION 데이터 세트에서 평가되었습니다.
343,http://arxiv.org/abs/2211.12649 ,Predicting Topological Maps for Visual Navigation in Unexplored Environments,"Huangying Zhan, Hamid Rezatofighi, Ian Reid","우리는 미탐사 환경에서 자율 탐색 및 탐색을 위한 로봇 학습 시스템을 제안합니다. 우리는 눈에 보이지 않는 환경이라도 이전에 비슷한 환경에서 겪었던 경험을 통해 익숙할 수 있다는 생각에 동기를 부여받습니다. 따라서 우리 방법의 핵심은 목표 기반 시각적 탐색을 지원하기 위해 확률적 레이아웃 그래프를 구축, 예측 및 사용하는 프로세스입니다. 우리는 레이아웃 예측을 사용하여 이전 기술보다 더 빠르고 정확하게 높은 수준의 목표(예: ""부엌으로 가세요"")를 충족시키는 네비게이션 시스템을 설명합니다. 제안된 내비게이션 프레임워크는 세 가지 단계로 구성됩니다. (1) 인식 및 매핑: 다중 레벨 3D 장면 그래프 구축; (2) 예측: 미탐사 환경에 대한 확률적 3D 장면 그래프를 예측합니다. (3) 탐색: 그래프를 사용하여 탐색을 지원합니다. Matterport3D에서 프레임워크를 테스트하고 보이지 않는 환경에서 더 많은 성공과 효율적인 탐색을 보여줍니다."
342,http://arxiv.org/abs/2211.11138 ,Diffusion-Based Scene Graph to Image Generation with Masked Contrastive Pre-Training,"Ling Yang, Zhilin Huang, Yang Song, Shenda Hong, Guohao Li, Wentao Zhang, Bin Cui, Bernard Ghanem, Ming-Hsuan Yang","장면 그래프와 같은 그래프 구조 입력에서 이미지를 생성하는 것은 그래프의 노드 및 연결을 객체 및 이미지의 관계와 정렬하는 것이 어렵기 때문에 매우 어렵습니다. 대부분의 기존 방법은 장면 이미지의 대략적인 구조를 캡처하도록 설계된 장면 그래프의 이미지와 유사한 표현인 장면 레이아웃을 사용하여 이러한 문제를 해결합니다. 장면 레이아웃은 수동으로 제작되므로 이미지 정렬이 완전히 최적화되지 않아 생성된 이미지와 원본 장면 그래프 간의 적합성이 저하될 수 있습니다. 이 문제를 해결하기 위해 우리는 이미지와의 정렬을 직접 최적화하여 장면 그래프 임베딩을 학습할 것을 제안합니다. 구체적으로 우리는 마스크된 자동 인코딩 손실과 대조 손실이라는 두 가지 손실 함수를 사용하여 해당 이미지를 예측하는 장면 그래프에서 전역 및 로컬 정보를 모두 추출하도록 인코더를 사전 훈련합니다. 전자는 무작위로 마스크된 이미지 영역을 재구성하여 임베딩을 훈련하는 반면, 후자는 장면 그래프에 따라 준수 이미지와 비준수 이미지를 구별하도록 임베딩을 훈련합니다. 이러한 임베딩을 바탕으로 장면 그래프에서 이미지를 생성하는 잠재 확산 모델을 구축합니다. SGDiff라고 불리는 결과 메서드를 사용하면 장면 그래프 노드와 연결을 수정하여 생성된 이미지의 의미론적 조작이 가능합니다. Visual Genome 및 COCO-Stuff 데이터 세트에서 Inception Score 및 FID(Fréchet Inception Distance) 측정 기준으로 측정한 바와 같이 SGDiff가 최첨단 방법보다 성능이 우수하다는 것을 보여줍니다. 우리는 https://github.com/YangLing0818/SGDiff에서 소스 코드와 훈련된 모델을 공개할 예정입니다."
341,http://arxiv.org/abs/2211.08086 ,Visually Grounded VQA by Lattice-based Retrieval,"Daniel Reich, Felix Putze, Tanja Schultz","VQA(Visual Question Answering) 시스템의 VG(Visual Grounding)는 시스템이 질문과 답변을 관련 이미지 영역에 얼마나 잘 연결하는지 설명합니다. VG가 강한 시스템은 직관적으로 해석할 수 있는 것으로 간주되며 향상된 장면 이해를 제안합니다. VQA 정확도 성능은 지난 몇 년 동안 인상적인 향상을 보였지만 VG 성능 및 평가에 대한 명시적인 개선은 종종 전체 정확도 향상에 뒷전으로 밀렸습니다. 이에 대한 원인은 VQA 시스템에 대한 학습 패러다임의 우세한 선택에서 비롯됩니다. 이는 미리 결정된 일련의 답변 옵션에 대해 차별적인 분류기를 훈련하는 것으로 구성됩니다.   이 작업에서 우리는 분류의 지배적인 VQA 모델링 패러다임을 깨고 정보 검색 작업의 관점에서 VQA를 조사합니다. 이처럼 개발된 시스템은 VG를 핵심 검색 절차에 직접 연결합니다. 우리 시스템은 질문에서 추출된 영역 참조 표현과 함께 주어진 이미지의 장면 그래프에서 파생된 가중치가 있는 방향성 비순환 그래프(일명 ""격자"")를 통해 작동합니다.   우리는 우리의 접근 방식을 자세히 분석하고 그 고유한 속성과 한계에 대해 논의합니다. 우리의 접근 방식은 조사된 시스템 중에서 가장 강력한 VG 성능을 달성하고 여러 시나리오에서 탁월한 일반화 기능을 보여줍니다."
340,http://arxiv.org/abs/2211.07504 ,On Analyzing the Role of Image for Visual-enhanced Relation Extraction,"Lei Li, Xiang Chen, Shuofei Qiao, Feiyu Xiong, Huajun Chen, Ningyu Zhang","다중모달 관계 추출은 지식 그래프 구축에 필수적인 작업입니다. 본 논문에서는 시각적 장면 그래프의 부정확한 정보로 인해 모달 정렬 가중치가 낮아져 성능이 더욱 저하된다는 심층적인 경험적 분석을 수행합니다. 더욱이, 시각적 셔플 실험은 현재 접근 방식이 시각적 정보를 최대한 활용하지 못할 수 있음을 보여줍니다. 위의 관찰을 바탕으로 우리는 다중 모드 관계 추출을 위한 Transformer 기반의 암시적 세분화된 다중 모드 정렬을 사용하여 강력한 기준선을 추가로 제안합니다. 실험 결과는 우리 방법의 더 나은 성능을 보여줍니다. 코드는 https://github.com/zjunlp/DeepKE/tree/main/example/re/multimodal에서 확인할 수 있습니다."
339,http://arxiv.org/abs/2211.06444 ,Probabilistic Debiasing of Scene Graphs,"Bashirul Azam Biswas, Qiang Ji","최첨단(SOTA) 모델에서 생성된 장면 그래프의 품질은 관계와 해당 상위 개체 쌍의 롱테일 특성으로 인해 저하됩니다. 장면 그래프의 훈련은 다수 쌍의 다수 관계에 의해 지배되므로 훈련이 수렴된 후에는 소수 쌍의 관계의 객체 조건부 분포가 유지되지 않습니다. 결과적으로, 편향된 모델은 'on' 및 'wearing'과 같은 관계의 주변 분포에서 더 빈번한 관계에서 잘 수행되고, 'eating' 또는 'hanging from'과 같이 덜 빈번한 관계에서는 제대로 수행되지 않습니다. 이 연구에서 우리는 관계 레이블의 객체 조건부 분포를 보존하고 관계의 한계 확률에 의해 생성된 편향을 근절하기 위해 삼중 베이지안 네트워크(BN) 내에 통합된 가상 증거를 제안합니다. 소수 클래스의 관계 수가 부족하면 삼중 베이지안 네트워크를 학습하는 데 심각한 문제가 발생합니다. 우리는 의미 공간의 이웃 삼중항에서 소수 삼중항 클래스의 샘플을 빌려 삼중항의 삽입 기반 확대를 통해 이러한 부족함을 해결합니다. 우리는 두 개의 서로 다른 데이터 세트에 대한 실험을 수행하고 관계의 평균 회상에서 상당한 개선을 달성했습니다. 또한 장면 그래프 모델의 SOTA 편향성 제거 기술에 비해 재현율과 평균 재현율 성능 간의 균형이 더 잘 이루어졌습니다."
338,http://arxiv.org/abs/2211.06119 ,SSGVS: Semantic Scene Graph-to-Video Synthesis,"Yuren Cong, Jinhui Yi, Bodo Rosenhahn, Michael Ying Yang","이미지 합성 작업의 자연스러운 확장으로 최근 비디오 합성이 많은 관심을 끌고 있습니다. 많은 이미지 합성 작업에서는 클래스 레이블이나 텍스트를 지침으로 활용합니다. 그러나 레이블이나 텍스트 모두 작업이 시작되거나 끝나는 시점과 같은 명시적인 시간적 지침을 제공할 수 없습니다. 이러한 한계를 극복하기 위해 장면 내 개체 간의 공간적, 시간적 관계를 나타내는 시맨틱 비디오 장면 그래프를 비디오 합성의 입력으로 도입합니다. 비디오 장면 그래프는 일반적으로 시간적으로 이산적인 주석이므로 기존 비디오 장면 그래프를 인코딩할 뿐만 아니라 레이블이 없는 프레임에 대한 그래프 표현을 예측하는 VSG(비디오 장면 그래프) 인코더를 제안합니다. VSG 인코더는 다양한 대비 다중 모드 손실로 사전 학습되었습니다. 사전 훈련된 VSG 인코더, VQ-VAE 및 자동 회귀 변환기를 기반으로 하는 SSGVS(의미론적 장면 그래프-비디오 합성 프레임워크)는 초기 장면 이미지와 고정되지 않은 개수의 의미론적 장면 그래프가 주어지면 비디오를 합성하기 위해 제안됩니다. 우리는 Action Genome 데이터 세트에서 SSGVS 및 기타 최첨단 비디오 합성 모델을 평가하고 비디오 합성에서 비디오 장면 그래프의 긍정적인 중요성을 보여줍니다. 소스코드가 공개됩니다."
337,http://arxiv.org/abs/2211.05499 ,DisPositioNet: Disentangled Pose and Identity in Semantic Image Manipulation,"Azade Farshad, Yousef Yeganeh, Helisa Dhamo, Federico Tombari, Nassir Navab","장면 그래프라고 알려진 장면의 개체 및 해당 관계에 대한 그래프 표현은 그래프의 노드 또는 가장자리를 수정하여 장면을 조작할 수 있는 정확하고 식별 가능한 인터페이스를 제공합니다. 기존 작품들은 사물의 배치와 자세를 수정하는 측면에서 좋은 결과를 보여주었지만, 장면 조작으로 인해 사물의 외형이나 정체성과 같은 일부 시각적 특성이 상실되는 경우가 많습니다. 본 연구에서는 자기주도 방식으로 장면 그래프를 이용한 이미지 조작 작업을 위해 각 객체에 대한 분리된 표현을 학습하는 모델인 DisPositioNet을 제안합니다. 우리 프레임워크를 사용하면 변형 잠재 임베딩과 그래프의 특징 표현을 분리할 수 있습니다. 포즈, 아이덴티티 등의 특징을 분해하여 보다 사실적인 이미지를 생성하는 것 외에도, 우리의 방법은 중간 특징의 확률적 샘플링을 활용하여 객체 교체 또는 추가 작업에서 보다 다양한 이미지를 생성합니다. 우리의 실험 결과는 모델의 잠재 다양체에서 특징 표현을 분리하는 것이 두 가지 공개 벤치마크에서 질적, 양적으로 이전 작업보다 성능이 우수하다는 것을 보여줍니다. 프로젝트 페이지: https://scenegenie.github.io/DispositioNet/"
336,http://arxiv.org/abs/2211.04773 ,SG-Shuffle: Multi-aspect Shuffle Transformer for Scene Graph Generation,"Anh Duc Bui, Soyeon Caren Han, Josiah Poon","SGG(장면 그래프 생성)는 인간의 이해는 물론 시각적 이해 작업을 위한 이미지의 포괄적인 표현을 제공합니다. 사용 가능한 주석이 달린 데이터의 객체 및 술어 레이블의 롱테일 편향 문제로 인해 현재 방법론에서 생성된 장면 그래프는 일반적이고 정보가 없는 관계 레이블 쪽으로 편향될 수 있습니다. 관계는 때때로 상호 배타적이지 않을 수 있으며, 이는 기하학적 관계 또는 의미론적 관계와 같은 다양한 관점에서 설명될 수 있으므로 가장 적합한 관계 레이블을 예측하는 것이 더욱 어렵습니다. 이 작업에서 우리는 3가지 구성 요소로 장면 그래프 생성을 위한 SG-Shuffle 파이프라인을 제안했습니다. 1) 관계 레이블을 유사한 목적의 그룹으로 그룹화하여 보다 배타적인 방식으로 개체 관계를 예측하는 방법을 학습하는 병렬 변환기 인코더; 2) 이전 단계에서 생성된 카테고리별 기능에서 최종 관계 레이블을 선택하는 방법을 학습하는 Shuffle Transformer 3) 불균형 데이터 세트로 인한 훈련 편향을 완화하는 데 사용되는 가중 CE 손실."
335,http://arxiv.org/abs/2211.01969 ,Grounding Scene Graphs on Natural Images via Visio-Lingual Message Passing,"Aditay Tripathi, Anand Mishra, Anirban Chakraborty","이 논문은 장면 그래프에 주어진 특정 의미론적 관계 제약을 따르는 객체를 공동으로 접지하기 위한 프레임워크를 제시합니다. 일반적인 자연 장면에는 여러 개체가 포함되어 있으며, 종종 개체 간의 다양한 복잡성의 시각적 관계를 나타냅니다. 이러한 개체 간 관계는 기존 개체 쿼리 전용 기반 위치 파악 작업에 비해 접지 성능을 향상시키기 위한 강력한 상황별 단서를 제공합니다. 장면 그래프는 이미지의 모든 객체와 의미 관계를 나타내는 효율적이고 구조화된 방법입니다. 장면을 표현하는 이 두 가지 양식을 연결하고 객체 위치 파악을 개선하기 위한 상황 정보를 활용하기 위해 장면 그래프를 자연 이미지에 접지하는 문제를 엄격하게 연구합니다. 이를 위해 우리는 VL-MPAG Net(Visio-Lingual Message PAssing Graph Neural Network)이라는 새로운 그래프 신경망 기반 접근 방식을 제안합니다. VL-MPAG Net에서는 먼저 개체 제안을 노드로 사용하고 노드 간의 그럴듯한 관계를 나타내는 노드 쌍 사이의 간선을 사용하여 방향성 그래프를 구성합니다. 그런 다음 제안 및 쿼리 개체의 상황에 따른 표현을 학습하기 위해 3단계 그래프 간 및 그래프 내 메시지 전달이 수행됩니다. 이러한 객체 표현은 객체 위치 파악을 생성하기 위한 제안의 점수를 매기는 데 사용됩니다. 제안된 방법은 4개 공개 데이터 세트의 기준선보다 훨씬 뛰어난 성능을 발휘합니다."
334,http://arxiv.org/abs/2211.00848 ,Heterogeneous Trajectory Forecasting via Risk and Scene Graph Learning,"Jianwu Fang, Chen Zhu, Pu Zhang, Hongkai Yu, Jianru Xue","이종 궤적 예측은 지능형 교통 시스템에 매우 중요하지만 이종 도로 에이전트 간의 복잡한 상호 작용 관계와 에이전트-환경 제약 조건을 모델링하는 것이 어렵기 때문에 어렵습니다. 본 연구에서는 에이전트 카테고리와 이동 가능한 의미 영역 측면에서 HRG(Heterogeneous Risk Graph)와 HSG(Hierarchical Scene Graph)로 구성된 이종 도로 에이전트의 궤적 예측을 위한 위험 및 장면 그래프 학습 방법을 제안합니다. HRG는 각 종류의 도로 요원을 그룹화하고 효과적인 충돌 위험 지표를 기반으로 상호 작용 인접성 매트릭스를 계산합니다. 운전 장면의 HSG는 도로 장면 문법에 맞춰 도로 에이전트와 도로 의미 레이아웃 간의 관계를 추론하여 모델링됩니다. 이 공식을 기반으로 운전 상황에서 효과적인 궤적 예측을 얻을 수 있으며, nuScenes, ApolloScape 및 Argoverse 데이터 세트에 대한 철저한 실험을 통해 다른 최첨단 접근 방식보다 우수한 성능이 입증되었습니다."
333,http://arxiv.org/abs/2211.00562 ,Leveraging commonsense for object localisation in partial scenes,"Francesco Giuliari, Geri Skenderi, Marco Cristani, Alessio Del Bue, Yiming Wang",우리는 장면의 부분 3D 스캔만으로 알 수 없는 영역에 있는 물체의 위치를 ​​추정하는 것을 목표로 하는 부분 장면의 물체 위치 파악 문제를 해결하기 위한 엔드투엔드 솔루션을 제안합니다. 우리는 상식 지식 기반의 추가 개념 노드로 강화된 공간 장면 그래프인 D-SCG(Directed Spatial Commonsense Graph)라는 기하학적 추론을 용이하게 하는 새로운 장면 표현을 제안합니다. 구체적으로 D-SCG의 노드는 장면 객체를 나타내고 가장자리는 상대적인 위치를 나타냅니다. 그런 다음 각 개체 노드는 서로 다른 상식 관계를 통해 개념 노드 집합에 연결됩니다. 제안된 그래프 기반 장면 표현을 통해 새로운 주의 메시지 전달 메커니즘을 구현한 그래프 신경망을 사용하여 대상 객체의 알려지지 않은 위치를 추정합니다. 네트워크는 먼저 D-SCG의 객체 노드와 개념 노드를 모두 집계하여 객체의 풍부한 표현을 학습함으로써 대상 객체와 각 표시 객체 간의 상대적 위치를 예측합니다. 그런 다음 이러한 상대 위치가 병합되어 최종 위치를 얻습니다. 우리는 Partial ScanNet을 사용하여 방법을 평가하여 8배 빠른 훈련 속도로 위치 파악 정확도를 5.9% 향상시켰습니다.
332,http://arxiv.org/abs/2210.16472 ,Learning Audio-Visual Dynamics Using Scene Graphs for Audio Source Separation,"Moitreya Chatterjee, Narendra Ahuja, Anoop Cherian","정적 소스에서 생성되는 사운드와 움직이는 소스에서 생성되는 사운드 사이에는 분명한 차이가 있습니다. 특히 소스가 마이크를 향해 또는 마이크에서 멀어질 때 더욱 그렇습니다. 본 논문에서는 (i) 시각적 단서를 사용하여 혼합에서 오디오 소스를 분리하고 (ii) 분리된 오디오를 사용하여 음원의 3D 시각적 동작을 예측하는 두 가지 어려운 작업을 동시에 해결하기 위해 오디오와 시각적 역학 간의 이러한 연결을 사용할 것을 제안합니다. 이를 위해 우리는 더 나은 오디오 소스 분리를 ​​위해 장면의 3D 구조와 음원의 모션을 활용하는 딥 러닝 프레임워크인 ASMP(Audio Separator and Motion Predictor)를 제시합니다. ASMP의 핵심은 비디오의 다양한 개체와 해당 개체의 유사 3D 공간 근접성을 캡처하는 2.5D 장면 그래프입니다. 이 그래프는 2D 비디오 프레임의 2.5D 단안 깊이 예측을 함께 등록하고 2.5D 장면 영역을 해당 프레임에 적용된 개체 감지기의 출력과 연결하여 구성됩니다. 그런 다음 ASMP 작업은 (i) 2.5D 장면 그래프를 여러 하위 그래프로 재귀적으로 분할하고, 각 하위 그래프는 입력 오디오 혼합물의 구성 사운드(이후 분리됨)와 연결되고, (ii) 분리된 오디오에서 해당 음원의 3D 모션을 예측하는 공동 문제로 수학적으로 모델링됩니다. ASMP를 경험적으로 평가하기 위해 두 가지 까다로운 시청각 데이터 세트에 대한 실험을 제시합니다. ASIW(야생 오디오 분리) 및 AVE(오디오 비주얼 이벤트). 우리의 결과는 ASMP가 소스 분리 품질을 명확하게 향상시켜 두 데이터 세트에 대한 이전 작업을 능가하는 동시에 다른 방법보다 음원의 동작 방향을 더 잘 추정한다는 것을 보여줍니다."
331,http://arxiv.org/abs/2210.14862 ,Visual Semantic Parsing: From Images to Abstract Meaning Representation,"Mohamed Ashraf Abdelsalam, Zhan Shi, Federico Fancellu, Kalliopi Basioti, Dhaivat J. Bhatt, Vladimir Pavlovic, Afsaneh Fazly",시각적 장면 이해를 위한 장면 그래프의 성공은 시각적 입력(예: 이미지)을 구조화된 표현으로 추상화하는 이점에 주목하게 되었습니다. 여기서 엔터티(사람 및 개체)는 관계를 지정하는 가장자리로 연결된 노드입니다. 그러나 이러한 표현을 구축하려면 장면 그래프 또는 프레임과 쌍을 이루는 이미지 형태의 값비싼 수동 주석이 필요합니다. 이러한 형식주의는 포착할 수 있는 실체와 관계의 성격이 제한되어 있습니다. 본 논문에서는 이러한 단점을 해결하기 위해 자연어 처리 분야에서 널리 사용되는 의미 표현인 AMR(Abstract Meaning Representation)을 활용할 것을 제안합니다. 공간적 관계를 주로 강조하는 장면 그래프와 비교하여 시각적 AMR 그래프는 시각적 입력에서 추정된 상위 수준 의미 개념에 중점을 두고 언어적으로 더 많은 정보를 제공합니다. 또한 이를 통해 메타-AMR 그래프를 생성하여 여러 이미지 설명에 포함된 정보를 하나의 표현으로 통합할 수 있습니다. 광범위한 실험과 분석을 통해 기존 텍스트-AMR 파서의 용도를 변경하여 이미지를 AMR로 구문 분석할 수 있음을 보여줍니다. 우리의 연구 결과는 향상된 장면 이해를 위한 중요한 미래 연구 방향을 지적합니다.
330,http://arxiv.org/abs/2210.14162 ,Commonsense Knowledge from Scene Graphs for Textual Environments,"Tsunehiko Tanaka, Daiki Kimura, Michiaki Tatsubori",텍스트 기반 게임은 실제 시뮬레이션 환경으로 강화 학습에 널리 사용되고 있습니다. 일반적으로 불완전한 정보 게임이며 상호 작용은 텍스트 형식으로만 이루어집니다. 이러한 게임에 도전하기 위해서는 인간의 상식 등 게임 밖의 지식을 제공하여 누락된 정보를 보완하는 것이 효과적이다. 그러나 그러한 지식은 이전 작품의 텍스트 정보에서만 얻을 수 있었습니다. 본 논문에서는 장면 그래프 데이터 세트와 같은 시각적 데이터 세트에서 얻은 상식 추론을 사용하는 이점을 조사합니다. 일반적으로 이미지는 인간의 텍스트에 비해 더 포괄적인 정보를 전달합니다. 이 속성을 통해 게임에서 효과적으로 행동하는 데 더욱 유용한 상식적 관계 지식을 추출할 수 있습니다. Visual Genome(장면 그래프 데이터 세트)과 ConceptNet(텍스트 기반 지식)에서 제공되는 공간 관계 통계를 비교하여 장면 그래프 데이터 세트 도입의 효율성을 분석합니다. 또한 상식적 추론이 필요한 텍스트 기반 게임 과제에 대한 실험도 진행했습니다. 우리의 실험 결과는 우리가 제안한 방법이 기존의 최신 방법보다 더 높고 경쟁력 있는 성능을 가지고 있음을 보여주었습니다.
329,http://arxiv.org/abs/2210.11253 ,Image Semantic Relation Generation,Mingzhe Du,"장면 그래프는 이미지 이상의 구조화된 의미론적 이해를 제공합니다. 이미지 검색, 시각적 질문 답변, 시각적 관계 감지, 심지어 자율주행차 기술과 같은 다운스트림 작업의 경우 장면 그래프는 복잡한 이미지 정보를 추출할 수 있을 뿐만 아니라 의미론적 수준 관계를 사용하여 시각적 모델의 편향을 교정할 수 있어 적용 가능성이 넓습니다. 그러나 그래프 주석을 구성하는 데 드는 과도한 인건비로 인해 실제 시나리오에서 PSG를 적용하는 데 방해가 될 수 있습니다. 사람들은 일반적으로 피사체와 객체를 먼저 식별한 다음 이들 사이의 관계를 결정한다는 관찰에서 영감을 받아 장면 그래프 생성 작업을 두 개의 하위 작업, 즉 1) 적격 객체를 선택하는 이미지 분할 작업으로 분리할 것을 제안했습니다. 2) 주어진 객체 간의 관계를 생성하기 위한 제한된 자동 회귀 텍스트 생성 작업. 따라서 이 작업에서는 간단하지만 효과적인 이미지-텍스트 모델인 ISRG(이미지 의미 관계 생성)를 소개합니다. 이 모델은 OpenPSG 데이터 세트에서 31포인트를 달성하고 강력한 기준선보다 각각 16포인트(ResNet-50) 및 5포인트(CLIP) 성능을 능가합니다."
328,http://arxiv.org/abs/2210.09549 ,Swinv2-Imagen: Hierarchical Vision Transformer Diffusion Models for Text-to-Image Generation,"Ruijun Li, Weihua Li, Yi Yang, Hanyu Wei, Jianhua Jiang, Quan Bai","최근 확산 모델은 여러 연구에서 텍스트-이미지 합성 작업에서 탁월한 성능을 발휘하는 것으로 입증되었으며, 이미지 생성을 위한 새로운 연구 기회를 즉시 제시합니다. Google의 Imagen은 이러한 연구 추세를 따르고 있으며 텍스트-이미지 생성을 위한 최고의 모델로서 DALLE2를 능가합니다. 그러나 Imagen은 텍스트 처리를 위해 T5 언어 모델만 사용하므로 텍스트의 의미 정보 학습을 보장할 수 없습니다. 또한 Imagen이 활용하는 Efficient UNet은 이미지 처리에 있어 최선의 선택이 아닙니다. 이러한 문제를 해결하기 위해 우리는 Hierarchical Visual Transformer와 의미 레이아웃을 통합한 장면 그래프를 기반으로 하는 새로운 텍스트-이미지 확산 모델인 Swinv2-Imagen을 제안합니다. 제안된 모델에서는 개체와 관계의 특징 벡터를 추출하고 확산 모델에 포함시켜 생성된 이미지의 품질을 효과적으로 향상시킵니다. 또한 CNN 컨볼루션 작업에서 발생하는 문제를 해결할 수 있는 Swinv2-Unet이라는 Swin-Transformer 기반 UNet 아키텍처도 소개합니다. 제안된 모델의 성능을 평가하기 위해 MSCOCO, CUB 및 MM-CelebA-HQ의 세 가지 실제 데이터 세트를 사용하여 광범위한 실험을 수행했습니다. 실험 결과는 제안된 Swinv2-Imagen 모델이 여러 가지 인기 있는 최첨단 방법보다 성능이 우수하다는 것을 보여줍니다."
327,http://arxiv.org/abs/2210.08908 ,Cross-modal Semantic Enhanced Interaction for Image-Sentence Retrieval,"Xuri Ge, Fuhai Chen, Songpei Xu, Fuxiang Tao, Joemon M. Jose","이미지 문장 검색은 유망한 응용으로 인해 멀티미디어 및 컴퓨터 비전 분야에서 광범위한 연구 관심을 끌었습니다. 핵심 문제는 시각적 표현과 텍스트 표현을 공동으로 학습하여 유사성을 정확하게 추정하는 데 있습니다. 이를 위해 주류 스키마는 관련성 점수를 계산하고 주의 기능으로 대화형 표현을 개선하기 위해 객체-단어 기반 주의를 채택하지만, 문장의 술어와 일치하는 객체 간 관계에 대한 객체 표현의 맥락을 무시합니다. 본 논문에서는 객체와 단어 사이의 내부 및 내부 의미를 연관시키는 이미지-문장 검색을 위한 CMSEI라는 교차 모드 의미 강화 상호 작용 방법을 제안합니다. 특히, 우리는 먼저 객체의 공간 위치와 장면 그래프의 명시적인 관계에 따라 객체의 의미 표현을 향상시키기 위해 추론 기반 내부 모달 공간 및 의미 그래프를 설계합니다. 그런 다음 시각적 및 텍스트 의미론적 표현은 모달 간 대화형 주의 및 모달 간 정렬을 통해 공동으로 개선됩니다. 개체의 컨텍스트를 텍스트 컨텍스트와 연관시키기 위해 교차 수준 개체 문장 및 단어 이미지 기반 대화형 주의를 통해 시각적 의미 표현을 더욱 구체화합니다. 7가지 표준 평가 지표에 대한 실험 결과는 제안된 CMSEI가 MS-COCO 및 Flickr30K 벤치마크에 대한 최첨단 및 대체 접근 방식보다 성능이 우수하다는 것을 보여줍니다."
326,http://arxiv.org/abs/2210.08675 ,SGRAM: Improving Scene Graph Parsing via Abstract Meaning Representation,"Woo Suk Choi, Yu-Jung Heo, Byoung-Tak Zhang","장면 그래프는 이미지와 텍스트를 그래프 형태로 모델링할 수 있는 구조화된 의미론적 표현입니다. 최근까지 이미지 기반의 장면 그래프 생성 연구가 활발하게 진행되어 온 반면, 텍스트 기반의 장면 그래프 생성 연구는 활발하지 않다. 본 논문에서는 시각적 장면의 텍스트 설명에서 장면 그래프를 구문 분석하는 문제에 중점을 둡니다. 핵심 아이디어는 이전 연구에서 주로 사용했던 종속성 구문 분석 대신 AMR(Abstract Meaning Representation)을 사용하는 것입니다. AMR은 문장 내 모든 단어에 대한 종속 관계를 고려하는 종속성 분석과 달리 문장 내 단어의 개념을 추상화하는 그래프 기반의 자연어 의미 형식입니다. 이를 위해 우리는 추상적 의미 표현을 활용하는 간단하면서도 효과적인 2단계 장면 그래프 구문 분석 프레임워크인 SGRAM(Scene GRaph parsing via Abstract Meaning Representation)을 설계합니다. 1) 이미지의 텍스트 설명을 AMR 그래프로 변환(Text-to-AMR)하고 2) AMR 그래프를 Transformer 기반 언어 모델로 인코딩하여 장면 그래프(AMR-to-SG)를 생성합니다. 실험 결과에 따르면 프레임워크에서 생성된 장면 그래프는 종속성 구문 분석 기반 모델보다 11.61\%, 사전 훈련된 Transformer 언어 모델을 사용하는 이전 최첨단 모델보다 3.78\% 성능이 뛰어납니다. 또한 장면 그래프의 다운스트림 작업 중 하나인 이미지 검색 작업에 SGRAM을 적용하고 프레임워크에서 생성된 장면 그래프의 효율성을 확인합니다."
325,http://arxiv.org/abs/2210.06240 ,Explore Contextual Information for 3D Scene Graph Generation,"Yuanyuan Liu, Chengjiang Long, Zhaoxuan Zhang, Bokai Liu, Qiang Zhang, Baocai Yin, Xin Yang","3D 장면 그래프 생성(SGG)은 컴퓨터 비전에서 높은 관심을 받아 왔습니다. 거친 분류 및 단일 관계 레이블에 대한 3D SGG의 정확도가 점차 향상되었지만 기존 작업의 성능은 여전히 ​​세분화된 다중 레이블 상황에 완벽하지 않습니다. 본 논문에서는 세분화된 엔터티 클래스, 다중 관계 레이블 및 높은 정확도 요구 사항을 동시에 만족시키려는 3D SGG 작업에 대한 상황 정보를 완전히 탐색하는 프레임워크를 제안합니다. 우리가 제안하는 접근 방식은 그래프 특징 추출 모듈과 그래프 상황 추론 모듈로 구성되어 적절한 정보 중복 특징 추출, 구조화된 구성 및 계층적 추론을 달성합니다. 우리의 접근 방식은 3DSSG 데이터 세트, 특히 관계 예측 하위 작업에서 이전 방법보다 우수하거나 경쟁력 있는 성능을 달성합니다."
324,http://arxiv.org/abs/2210.04127 ,Towards Efficient Neural Scene Graphs by Learning Consistency Fields,"Yeji Song, Chaerin Kong, Seoyoung Lee, Nojun Kwak, Joonseok Lee","NeRF(Neural Radiance Fields)는 새로운 관점에서 사실적인 이미지 렌더링을 달성하고 NSG(Neural Scene Graphs) \cite{ost2021neural}는 이를 여러 개체가 있는 동적 장면(비디오)으로 확장합니다. 그럼에도 불구하고, 모든 이미지 프레임에 대해 계산적으로 무거운 레이 행진은 큰 부담이 됩니다. 본 논문에서는 비디오의 인접한 프레임 간의 상당한 중복성을 활용하여 기능 재사용 프레임워크를 제안합니다. 그러나 NSG 기능을 순진하게 재사용하려는 첫 번째 시도에서 우리는 임시 프레임의 프레임 전반에 걸쳐 일관된 개체 고유 속성을 분리하는 것이 중요하다는 것을 알게 되었습니다. 우리가 제안한 방법인 \textit{Consistency-Field-based NSG (CF-NSG)}는 신경 복사 필드를 다시 공식화하여 \textit{일관성 필드}를 추가로 고려합니다. 얽힌 표현을 통해 CF-NSG는 기능 재사용 방식을 최대한 활용하고 보다 제어 가능한 방식으로 확장된 수준의 장면 조작을 수행합니다. 우리는 CF-NSG가 렌더링 품질의 눈에 띄는 저하 없이 NSG보다 85% 적은 쿼리를 사용하여 추론 효율성을 크게 향상시키는 것을 경험적으로 확인합니다. 코드는 https://github.com/ldynx/CF-NSG에서 확인할 수 있습니다."
323,http://arxiv.org/abs/2210.02735 ,What Should the System Do Next?: Operative Action Captioning for Estimating System Actions,"Taiki Nakamura, Seiya Kawano, Akishige Yuguchi, Yasutomo Kawanishi, Koichiro Yoshino","로봇과 같은 인간 보조 시스템은 관찰을 바탕으로 주변 상황을 정확하게 이해하고 인간에게 필요한 지원 동작을 출력해야 합니다. 언어는 인간과 소통하는 중요한 채널 중 하나이며, 로봇은 자신의 이해와 행동 계획 결과를 표현하는 능력이 필요합니다. 본 연구에서는 인간 보조 영역에서 시스템이 취해야 할 행동을 추정하고 언어화하는 새로운 작업 행동 캡션 작업을 제안합니다. 우리는 현재 상태를 주어진 목표 상태로 변경하는 가능한 작동 조치에 대한 구두 설명을 출력하는 시스템을 구축했습니다. 우리는 일상생활 상황에서 크라우드소싱을 통해 현재 상태와 행동에 의해 변화되는 상태를 표현하는 관찰 이미지 2개와 현재 상태를 목표 상태로 바꾸는 행동을 설명하는 캡션으로 구성된 데이터 세트를 수집했습니다. 그런 다음 캡션을 통해 수술 동작을 추정하는 시스템을 구축했습니다. 작동 작업의 캡션에는 상태 변경 작업이 포함될 것으로 예상되므로 장면 그래프에 기록된 이벤트가 상태 변경에 해당하므로 장면 그래프 예측을 보조 작업으로 사용합니다. 실험 결과는 우리 시스템이 현재 상태와 목표 상태 사이에서 수행되어야 하는 작업 동작을 성공적으로 설명했음을 보여주었습니다. 장면 그래프를 예측하는 보조 작업을 통해 추정 결과의 품질이 향상되었습니다."
322,http://arxiv.org/abs/2210.00920 ,Unbiased Scene Graph Generation using Predicate Similarities,"Misaki Ohashi, Yusuke Matsui","장면 그래프는 이미지에 표시된 개체 간의 관계를 그래픽으로 표현하여 컴퓨터 비전에 널리 적용됩니다. 그러나 이러한 응용 프로그램은 긴 꼬리 예측 분포로 인한 편향된 학습으로 인해 아직 실제 개발 단계에 도달하지 못했습니다. 최근 몇 년 동안 많은 연구가 이 문제를 다루었습니다. 대조적으로, 편향된 예측으로 이어지는 고유한 데이터 세트 기능으로 술어 유사성을 고려한 연구는 상대적으로 적습니다. 이 기능으로 인해 빈도가 낮은 술어(예: parked on, Covered in)는 밀접하게 관련된 빈발 술어(예: on, in)로 쉽게 잘못 분류됩니다. 술어 유사성을 활용하여 유사한 술어 그룹에 대한 여러 세분화된 분류기로 프로세스를 분기하는 새로운 분류 체계를 제안합니다. 분류자는 유사한 술어 간의 차이점을 자세히 포착하는 것을 목표로 합니다. 또한 설명적 표현을 학습하기에 충분한 훈련 샘플이 부족한 술어의 기능을 향상시키기 위한 전이 학습 아이디어를 소개합니다. Visual Genome 데이터 세트에 대한 광범위한 실험 결과에 따르면 우리의 방법과 기존 편향성 제거 접근 방식을 결합하면 까다로운 SGCls/SGDet 작업에서 꼬리 조건자 성능이 크게 향상되는 것으로 나타났습니다. 그럼에도 불구하고 제안된 접근 방식의 전반적인 성능은 현재 기술 수준에 도달하지 못하므로 향후 작업으로 추가 분석이 필요합니다."
321,http://arxiv.org/abs/2209.14026 ,Human-in-the-loop Robotic Grasping using BERT Scene Representation,"Yaoxian Song, Penglei Sun, Pengfei Fang, Linyi Yang, Yanghua Xiao, Yue Zhang","현재 NLP 기술은 다양한 영역에 크게 적용되었습니다. 본 논문에서는 어수선한 장면에서 로봇이 파악하기 위한 Human-In-The-Loop 프레임워크를 제안하고, 사용자가 자연어 명령으로 개입할 수 있는 파악 프로세스에 대한 언어 인터페이스를 조사합니다. 이 프레임워크는 최첨단 래핑 기준선을 기반으로 구성되었으며, 여기서 BERT를 사용하여 장면 그래프 표현을 장면의 텍스트 표현으로 대체합니다. 시뮬레이션과 물리적 로봇에 대한 실험은 제안된 방법이 기존의 객체 불가지론 및 장면 그래프 기반 방법보다 성능이 우수하다는 것을 보여줍니다. 또한 사람의 개입을 통해 성능이 크게 향상될 수 있다는 사실도 발견했습니다."
320,http://arxiv.org/abs/2209.12274 ,Personalized Saliency in Task-Oriented Semantic Communications: Image Transmission and Performance Analysis,"Jiawen Kang, Hongyang Du, Zonghang Li, Zehui Xiong, Shiyao Ma, Dusit Niyato, Yuan Li","의미론적 통신은 미래 6G 네트워크 및 애플리케이션(예: 스마트 헬스케어)을 위한 핵심 지원자이자 기본 패러다임으로 구상되는 섀넌 한계를 돌파하기 위해 유망한 기술로 등장했습니다. 본 논문에서는 UAV 이미지 감지 기반 작업 지향 의미론적 통신 시나리오에 중점을 둡니다. 기존 작업의 대부분은 고성능 의미론적 통신을 위한 고급 알고리즘을 설계하는 데 중점을 두었습니다. 그러나 에너지 소모적이고 효율성이 제한된 이미지 검색 방식, 사용자 개성을 고려하지 않은 의미론적 인코딩과 같은 과제는 아직 탐색되지 않았습니다. 이러한 문제는 의미론적 의사소통의 광범위한 채택을 방해했습니다. 위의 문제를 해결하기 위해 의미론적 수준에서 먼저 이미지 정보에 대한 트리플 기반 {\color{black}scene graph}를 사용하여 에너지 효율적인 작업 지향 의미론적 통신 프레임워크를 설계합니다. 그런 다음 개인화된 중요도 요구 사항을 충족하기 위해 사용자 관심을 기반으로 새로운 개인화된 의미론적 인코더를 설계합니다. 또한, 통신 수준에서는 동적 무선 페이딩 채널이 의미 전달에 미치는 영향을 수학적으로 연구하고, 게임 이론을 이용하여 최적의 다중 사용자 자원 할당 방식을 설계한다. 실제 데이터 세트를 기반으로 한 수치 결과는 제안된 프레임워크와 방식이 의미 통신의 개인화 및 간섭 방지 성능을 크게 향상시키고 의미 통신 서비스의 통신 품질을 향상시키는 데에도 효율적이라는 것을 명확하게 나타냅니다."
319,http://arxiv.org/abs/2209.07896 ,3D VSG: Long-term Semantic Scene Change Prediction through 3D Variable Scene Graphs,"Samuel Looper, Javier Rodriguez-Puigvert, Roland Siegwart, Cesar Cadena, Lukas Schmid","수많은 애플리케이션에서는 인간이나 다른 로봇과 같은 다른 에이전트와 공유되는 환경에서 로봇이 작동해야 합니다. 그러나 이러한 공유 장면은 일반적으로 다양한 종류의 장기적인 의미론적 장면 변경의 영향을 받습니다. 따라서 이러한 변화를 모델링하고 예측하는 능력은 로봇 자율성에 매우 중요합니다. 이 연구에서 우리는 의미론적 장면 가변성 추정 작업을 공식화하고 의미론적 장면 변화의 세 가지 주요 종류, 즉 객체의 위치 변경, 의미론적 상태 또는 전체 장면 구성을 식별합니다. 이러한 가변성을 표현하기 위해 우리는 기존 3D 장면 그래프(SG) 표현을 가변성 속성으로 확장하여 개별 장기 변경 이벤트의 가능성을 나타내는 가변 장면 그래프(VSG)를 제안합니다. 우리는 감독 방식으로 VSG의 변동성을 추정하기 위한 새로운 방법인 DeltaVSG를 제시합니다. 우리는 3RScan 장기 데이터 세트에 대한 방법을 평가하여 기존 접근 방식에 비해 이 새로운 작업의 주목할만한 개선을 보여줍니다. 우리의 방법 DeltaVSG는 77.1%의 정확도와 72.3%의 재현율을 달성하여 시간이 지남에 따라 실내 장면이 어떻게 변하는지에 대한 인간의 직관을 모방하는 경우가 많습니다. 우리는 능동적인 로봇 변화 감지 작업에서 VSG 예측의 유용성을 보여 주며, 장면 변경을 인식하지 못하는 플래너에 비해 작업 완료 속도를 66.0% 높입니다. 우리는 코드를 오픈 소스로 제공합니다."
318,http://arxiv.org/abs/2209.09093 ,Scene Graph Modification as Incremental Structure Expanding,"Xuming Hu, Zhijiang Guo, Yu Fu, Lijie Wen, Philip S. Yu","장면 그래프는 장면 내 객체, 속성, 객체 간 관계를 표현하는 의미론적 표현입니다. 장면 그래프는 이미지와 텍스트 간의 상호 작용을 캡처할 수 있으므로 다양한 교차 양식 작업에서 중요한 역할을 합니다. 본 논문에서는 시스템이 자연어 쿼리를 기반으로 기존 장면 그래프를 업데이트하는 방법을 학습하는 데 필요한 장면 그래프 수정(SGM)에 중점을 둡니다. 전체 장면 그래프를 재구성하는 이전 접근 방식과 달리 ISE(증분 구조 확장)를 도입하여 SGM을 그래프 확장 작업으로 프레임합니다. ISE는 수정되지 않은 구조를 변경하지 않고 소스 그래프를 점진적으로 확장하여 대상 그래프를 구성합니다. ISE를 기반으로 노드 예측과 에지 예측을 반복하여 보다 정확하고 조화로운 확장 결정을 점진적으로 추론하는 모델을 추가로 제안합니다. 또한 기존 데이터 세트보다 더 복잡한 쿼리와 더 큰 장면 그래프를 포함하는 까다로운 데이터 세트를 구성합니다. 4가지 벤치마크에 대한 실험은 이전의 최첨단 모델을 훨씬 능가하는 우리 접근 방식의 효율성을 보여줍니다."
317,http://arxiv.org/abs/2209.07118 ,"Align, Reason and Learn: Enhancing Medical Vision-and-Language Pre-training with Knowledge","Zhihong Chen, Guanbin Li, Xiang Wan","의료 비전 및 언어 사전 훈련(Med-VLP)은 의료 이미지 및 텍스트에서 일반적인 비전 및 언어 표현을 추출하는 데 적용할 수 있기 때문에 상당한 주목을 받았습니다. 대부분의 기존 방법에는 주로 단일 모드 인코더(예: 비전 인코더 및 언어 인코더), 다중 모드 융합 모듈 및 프리텍스트 작업의 세 가지 요소가 포함되어 있으며, 의료 도메인 전문 지식의 중요성을 고려하고 이러한 지식을 Med-VLP를 촉진하기 위해 명시적으로 활용하는 연구는 거의 없습니다. 일반 영역에는 지식이 강화된 비전 및 언어 사전 훈련(VLP) 방법이 있지만 대부분은 의료 영역에서 사용할 수 없는 기성 툴킷(예: 개체 감지기 및 장면 그래프 파서)이 필요합니다. 본 논문에서는 세 가지 관점에서 구조화된 의학 지식을 통해 Med-VLP를 향상시키는 체계적이고 효과적인 접근 방식을 제안합니다. 첫째, 지식은 시각과 언어 사이의 중간 매개체로 간주될 수 있다는 점을 고려하여, 지식을 통해 시각 인코더와 언어 인코더의 표현을 정렬합니다. 둘째, 다중 모드 융합 모델에 지식을 주입하여 모델이 입력 이미지와 텍스트의 보완으로 지식을 활용한 추론을 수행할 수 있도록 합니다. 셋째, 지식 유도 프리텍스트 작업을 설계하여 이미지와 텍스트에서 가장 중요한 정보에 중점을 두도록 모델을 안내합니다. 포괄적인 평가를 수행하고 추가 연구를 촉진하기 위해 우리는 세 가지 작업을 포함하는 의료 비전 및 언어 벤치마크를 구성합니다. 실험 결과는 모든 다운스트림 작업에서 최첨단 성능을 달성하는 접근 방식의 효율성을 보여줍니다. 추가 분석에서는 접근 방식의 다양한 구성 요소와 다양한 사전 훈련 설정의 효과를 탐색합니다."
316,http://arxiv.org/abs/2209.06111 ,D-Lite: Navigation-Oriented Compression of 3D Scene Graphs for Multi-Robot Collaboration,"Yun Chang, Luca Ballotta, Luca Carlone",알려지지 않은 환경을 공동으로 탐색하는 다중 로봇 팀의 경우 탐색 및 탐색 작업을 지원하기 위해 수집된 정보를 로봇 간에 효율적으로 공유하는 것이 매우 중요합니다. 제한된 대역폭과 같은 무선 채널의 실질적인 제약으로 인해 로봇은 전송할 정보를 신중하게 선택해야 합니다. 본 논문에서는 환경의 기하학적 측면과 의미론적 측면을 모두 설명하는 계층적 지도 표현인 3D 장면 그래프를 사용하여 환경 정보를 모델링하는 경우를 고려합니다. 그런 다음 그래프 스패너라는 그래프 이론 도구를 활용하여 대역폭 제약 하에서 로봇 간의 통신을 가능하게 한다는 목표로 3D 장면 그래프를 효율적으로 압축하는 그리디 알고리즘을 설계합니다. 우리의 압축 알고리즘은 사용자가 지정한 통신 예산 제약을 충족시키면서 관심 위치 간의 최단 경로를 대략적으로 유지하도록 설계되었다는 점에서 탐색 중심입니다. 제안된 알고리즘의 효율성은 실제 시뮬레이터에서 합성 로봇 탐색 실험을 통해 입증되었습니다. 동영상 요약은 https://youtu.be/nKYXU5VC6A8에서 확인하실 수 있습니다.
315,http://arxiv.org/abs/2209.02981 ,VGStore: A Multimodal Extension to SPARQL for Querying RDF Scene Graph,"Yanzeng Li, Zilong Zheng, Wenjuan Han, Lei Zou","시맨틱 웹 기술은 풍부한 데이터 표현 방법을 통해 많은 RDF 모델을 성공적으로 촉진했습니다. 또한 다중 모드 장면 그래프와 같은 다중 모드 지식 기반을 표현하고 저장할 수 있는 잠재적인 기능도 있습니다. 그러나 대부분의 기존 쿼리 언어, 특히 SPARQL은 의미 유사성, 공간 관계 등과 같은 암시적 다중 모드 관계를 거의 탐색하지 않습니다. 우리는 먼저 RDF 그래프 데이터베이스에 대규모 장면 그래프 데이터 세트, 즉 Visual Genome을 구성하여 이 문제를 탐색했습니다. 제안된 RDF 저장 다중 모드 장면 그래프를 기반으로 SPARQL 쿼리를 확장하여 색상, 공간 등에 대한 관계 추론이 포함된 질문에 답했습니다. 추가 데모(예: VGStore)에서는 사용자 정의 쿼리의 효율성과 다중 모드 데이터 표시를 보여줍니다."
314,http://arxiv.org/abs/2209.02749 ,Scalable Regularization of Scene Graph Generation Models using Symbolic Theories,"Davide Buffelli, Efthymia Tsamoura","최근에는 배경 지식을 통합하여 장면 그래프 생성(SGG)을 위한 딥 러닝 모델의 성능을 향상시키는 것을 목표로 하는 여러 기술이 있습니다. 최첨단 기술은 두 가지 계열로 나눌 수 있습니다. 하나는 배경 지식이 하위 기호 방식으로 모델에 통합되는 것이고, 다른 하나는 배경 지식이 상징적 형태로 유지되는 것입니다. 유망한 결과에도 불구하고 두 기술 계열 모두 몇 가지 단점에 직면해 있습니다. 첫 번째 기술은 임시적이고 더 복잡한 신경 아키텍처가 필요하여 훈련 또는 추론 비용이 증가합니다. 두 번째는 제한된 확장성으로 인해 어려움을 겪습니다. 배경 지식의 크기. 우리의 작업은 선행 기술의 한계를 극복하는 신경 SGG 모델에 상징적 배경 지식을 주입하기 위한 정규화 기술을 소개합니다. 우리의 기술은 모델에 구애받지 않고 추론 시 비용이 발생하지 않으며 이전에 관리할 수 없었던 배경 지식 크기로 확장됩니다. 우리는 우리 기술이 최첨단 SGG 모델의 정확도를 최대 33%까지 향상시킬 수 있음을 보여줍니다."
313,http://arxiv.org/abs/2209.02567 ,Capsule Networks as Generative Models,"Alex B. Kiefer, Beren Millidge, Alexander Tschantz, Christopher L. Buckley","캡슐 네트워크는 시각적 장면 인식에 특화된 신경망 아키텍처입니다. 특징과 포즈 정보는 장면에서 추출된 다음 '캡슐'이라는 벡터 값 노드의 계층 구조를 통해 동적으로 라우팅되어 암시적 장면 그래프를 생성하며, 궁극적인 목표는 역 그래픽으로 비전을 직접 학습하는 것입니다. 그러나 이러한 직관에도 불구하고 캡슐 네트워크는 명시적인 확률 생성 모델로 공식화되지 않습니다. 게다가 일반적으로 사용되는 라우팅 알고리즘은 임시적이며 주로 알고리즘 직관에 의해 동기가 부여됩니다. 본 논문에서는 희소성 제약 하에서 반복 추론을 활용한 대체 캡슐 라우팅 알고리즘을 도출합니다. 그런 다음 변환기 네트워크의 self-attention 작업을 기반으로 하는 캡슐 네트워크에 대한 명시적 확률 생성 모델을 소개하고 VMF(Von-Mises-Fisher) 원형 가우스 분포를 사용하는 예측 코딩 네트워크의 변형과 어떻게 관련되는지 보여줍니다."
312,http://arxiv.org/abs/2208.13753 ,Frido: Feature Pyramid Diffusion for Complex Scene Image Synthesis,"Wan-Cyuan Fan, Yen-Chun Chen, Dongdong Chen, Yu Cheng, Lu Yuan, Yu-Chiang Frank Wang","확산 모델(DM)은 고품질 이미지 합성에 대한 큰 잠재력을 보여주었습니다. 그러나 복잡한 장면이 포함된 이미지를 생성하는 경우 이미지 전체 구조와 개체 세부정보를 모두 적절하게 설명하는 방법은 여전히 ​​어려운 작업으로 남아 있습니다. 본 논문에서는 이미지 합성을 위해 다중 규모의 거친 노이즈 제거 프로세스를 수행하는 Feature Pyramid Diffusion 모델인 Frido를 제시합니다. 우리 모델은 입력 이미지를 스케일 종속 벡터 양자화 특징으로 분해한 다음 이미지 출력을 생성하기 위한 대략적인 게이팅을 수행합니다. 위의 다중 스케일 표현 학습 단계에서는 텍스트, 장면 그래프 또는 이미지 레이아웃과 같은 추가 입력 조건을 더욱 활용할 수 있습니다. 따라서 Frido는 조건부 또는 교차 양식 이미지 합성에도 적용될 수 있습니다. 우리는 텍스트-이미지 합성, 레이아웃-이미지, 장면-그래프-이미지, 라벨-이미지에 이르기까지 다양한 무조건적 및 조건부 이미지 생성 작업에 대해 광범위한 실험을 수행합니다. 보다 구체적으로, 우리는 COCO 및 OpenImages의 레이아웃-이미지, COCO 및 Visual Genome의 장면-그래프-이미지, COCO의 라벨-이미지 등 5개 벤치마크에서 최첨단 FID 점수를 획득했습니다. 코드는 https://github.com/davidhalladay/Frido에서 확인할 수 있습니다."
311,http://arxiv.org/abs/2208.12037 ,Symbolic Replay: Scene Graph as Prompt for Continual Learning on VQA Task,"Stan Weixian Lei, Difei Gao, Jay Zhangjie Wu, Yuxuan Wang, Wei Liu, Mengmi Zhang, Mike Zheng Shou",VQA는 이미지와 관련된 모든 질문에 답하는 것을 목표로 하는 야심찬 작업입니다. 그러나 현실적으로 사용자의 요구가 지속적으로 업데이트되고 시스템에 새로운 기능을 구현해야 하기 때문에 이러한 시스템을 한번에 구축하기는 어렵습니다. 따라서 고급 VQA 시스템을 개발하려면 지속적인 학습(CL) 능력이 필수입니다. 최근 한 선구적인 작업에서는 이 주제를 연구하기 위해 VQA 데이터 세트를 분리된 답변 세트로 분할했습니다. 그러나 VQA의 CL에는 레이블 세트(새로운 답변 세트)의 확장만 포함되는 것은 아닙니다. VQA 시스템을 새로운 환경(새로운 시각적 장면)에 배포할 때 질문에 대답하는 방법과 새로운 기능이 필요한 질문에 대답하는 방법(새로운 질문 유형)을 연구하는 것이 중요합니다. 따라서 우리는 앞서 언급한 두 가지 CL 시나리오에 대한 장면 및 기능 증분 설정을 포함하는 시각적 질문 응답에 대한 지속적인 학습을 위한 벤치마크인 CLOVE를 제안합니다. 방법론 측면에서 VQA에 대한 CL과 분류의 주요 차이점은 전자는 추론 메커니즘을 확장하고 망각하는 것을 방지하는 반면 후자는 클래스 표현에 중점을 둔다는 것입니다. 따라서 우리는 VQA의 CL에 맞춰진 실제 데이터 없는 재생 기반 방법인 장면 그래프를 기호 재생 프롬프트로 명명하는 방법을 제안합니다. 장면 그래프 조각을 프롬프트로 사용하여 의사 장면 그래프를 재생하여 상관된 QA 쌍과 함께 과거 이미지를 나타냅니다. QA 능력을 향상시키기 위해 현재 데이터와 재생 데이터를 활용하는 통합 VQA 모델도 제안되었습니다. 마지막으로 실험 결과는 CLOVE의 문제점을 드러내고 우리 방법의 효율성을 보여줍니다. 데이터세트와 코드는 https://github.com/showlab/CLVQA에서 확인할 수 있습니다.
310,http://arxiv.org/abs/2208.09417 ,Target-oriented Sentiment Classification with Sequential Cross-modal Semantic Graph,"Yufeng Huang, Zhuo Chen, Jiaoyan Chen, Jeff Z. Pan, Zhen Yao, Wen Zhang",MABSC(Multi-modal Aspect-Based Sentiment Classification)는 문장과 이미지에 언급된 대상 개체의 감성을 분류하는 작업입니다. 그러나 이전의 방법들은 이미지와 텍스트 사이의 세밀한 의미론적 연관성을 설명하지 못하여 세밀한 이미지 측면과 의견을 식별하는 데 제한이 있었습니다. 이러한 제한 사항을 해결하기 위해 이 논문에서는 순차적 교차 모달 의미 그래프를 사용하여 인코더-디코더 감정 분류 프레임워크를 향상시키는 SeqCSG라는 새로운 접근 방식을 제안합니다. SeqCSG는 이미지 캡션과 장면 그래프를 활용하여 전역 및 로컬 세분화된 이미지 정보를 모두 추출하고 이를 트윗의 토큰과 함께 교차 모달 의미 그래프의 요소로 간주합니다. 순차 교차 모드 의미 그래프는 요소 간의 관계를 나타내는 다중 모드 인접 행렬이 있는 시퀀스로 표시됩니다. 실험 결과는 이 접근 방식이 기존 방법보다 성능이 뛰어나고 두 가지 표준 데이터 세트에서 최첨단 성능을 달성한다는 것을 보여줍니다. 추가 분석에 따르면 모델은 이미지의 세밀한 정보와 주어진 목표에 대한 텍스트 간의 상관 관계를 암묵적으로 학습할 수 있음이 입증되었습니다. 우리 코드는 https://github.com/zjukg/SeqCSG에서 확인할 수 있습니다.
309,http://arxiv.org/abs/2208.08165 ,Towards Open-vocabulary Scene Graph Generation with Prompt-based Finetuning,"Tao He, Lianli Gao, Jingkuan Song, Yuan-Fang Li","장면 그래프 생성(SGG)은 이미지 내 개체 간의 시각적 관계를 감지하는 것을 목표로 하는 기본 작업입니다. 일반적인 SGG 방법에서는 모든 객체 클래스가 훈련 세트에 제공되어야 합니다. 이러한 폐쇄적인 설정은 SGG의 실제 적용을 제한합니다. 본 논문에서는 모델이 일련의 기본 개체 클래스에 대해 훈련되지만 보이지 않는 대상 개체 클래스에 대한 관계를 추론해야 하는 새롭고 현실적이며 도전적인 설정인 개방형 어휘 장면 그래프 생성을 소개합니다. 이를 위해 우리는 먼저 대량의 대략적인 지역 캡션 데이터를 사전 훈련한 다음 두 가지 프롬프트 기반 기술을 활용하여 매개 변수를 업데이트하지 않고 사전 훈련된 모델을 미세 조정하는 2단계 방법을 제안합니다. 게다가 우리의 방법은 기존 방법으로는 처리할 수 없는 완전히 보이지 않는 객체 클래스에 대한 추론을 지원할 수 있습니다. 세 가지 벤치마크 데이터 세트인 Visual Genome, GQA 및 Open-Image에 대한 광범위한 실험에서 우리의 방법은 Ov-SGG 설정에 대한 최근의 강력한 SGG 방법과 기존의 폐쇄형 SGG보다 훨씬 뛰어난 성능을 보였습니다."
308,http://arxiv.org/abs/2208.07109 ,Context-aware Mixture-of-Experts for Unbiased Scene Graph Generation,"Liguang Zhou, Yuhongze Zhou, Tin Lun Lam, Yangsheng Xu","장면 그래프 생성(SGG)은 최근 몇 년 동안 엄청난 발전을 이루었습니다. 그러나 술어 클래스의 기본 긴 꼬리 분포는 어려운 문제입니다. 극도로 불균형한 술어 분포의 경우 기존 접근 방식은 일반적으로 복잡한 컨텍스트 인코더를 구성하여 술어 및 복잡한 네트워크에 대한 장면 컨텍스트의 본질적인 관련성을 추출하여 고도로 불균형한 술어 분포에 대한 네트워크 모델의 학습 능력을 향상시킵니다. 편향되지 않은 SGG 문제를 해결하기 위해 복잡한 설계 없이 모델 다양성을 개선하고 편향된 SGG를 완화할 수 있는 CAME(Context-Aware Mixture-of-Experts)이라는 간단하면서도 효과적인 방법을 소개합니다. 특히, 우리는 대부분의 편견 없는 장면 그래프 생성기에 적용할 수 있는 술어 클래스의 심각한 긴 꼬리 분포를 해결하기 위해 전문가 혼합을 분할 및 앙상블 전략과 통합할 것을 제안합니다. 이로써 편향된 SGG가 줄어들고 모델은 보다 균등하게 분산된 예측 예측을 예상하는 경향이 있습니다. 다양한 술어 분포 수준을 구별하기 위해 동일한 가중치를 가진 전문가는 충분히 다양하지 않습니다. 네트워크가 풍부한 장면 컨텍스트를 동적으로 활용하고 모델의 다양성을 더욱 강화하기 위해 내장 모듈을 사용하여 컨텍스트 인코더를 생성하기만 하면 됩니다. 장면 컨텍스트에 대한 각 전문가의 중요성과 각 전문가에 대한 각 술어의 중요성은 전문가 가중치(EW) 및 술어 가중치(PW) 전략과 동적으로 연관됩니다. 우리는 Visual Genome 데이터 세트를 사용하여 세 가지 작업에 대한 광범위한 실험을 수행했으며 CAME가 최근 방법보다 성능이 뛰어나고 최첨단 성능을 달성한다는 것을 보여주었습니다. 우리의 코드는 공개적으로 제공될 것입니다."
307,http://arxiv.org/abs/2208.03763 ,Label Semantic Knowledge Distillation for Unbiased Scene Graph Generation,"Lin Li, Long Chen, Hanrong Shi, Wenxiao Wang, Jian Shao, Yi Yang, Jun Xiao","SGG(장면 그래프 생성) 작업의 목표는 주어진 이미지에서 모든 객체와 객체의 쌍별 시각적 관계를 감지하는 것입니다. SGG가 지난 몇 년 동안 눈에 띄는 발전을 이루었지만 거의 모든 기존 SGG 모델은 동일한 학습 패러다임을 따릅니다. 즉, SGG의 객체 분류와 조건자 분류를 모두 단일 레이블 분류 문제로 처리하고 실제 정보는 원-핫 대상 레이블입니다. 그러나 이 널리 퍼진 훈련 패러다임은 현재 SGG 데이터 세트의 두 가지 특성을 간과했습니다. 1) 긍정적인 샘플의 경우 일부 특정 주체-객체 인스턴스에는 여러 개의 합리적인 조건자가 있을 수 있습니다. 2) 음성 샘플의 경우 주석이 누락된 경우가 많습니다. 두 가지 특성에 관계없이 SGG 모델은 혼동되기 쉽고 잘못된 예측을 하기 쉽습니다. 이를 위해 우리는 편견 없는 SGG를 위한 새로운 모델 독립적인 레이블 의미 지식 증류(LS-KD)를 제안합니다. 특히 LS-KD는 예측된 LSD(Label Semantic Distribution)를 원래의 원-핫 대상 레이블과 융합하여 각 주체-객체 인스턴스에 대한 소프트 레이블을 동적으로 생성합니다. LSD는 이 인스턴스와 여러 조건자 범주 간의 상관 관계를 반영합니다. 한편, 우리는 LSD를 예측하기 위해 반복적인 self-KD와 동기식 self-KD라는 두 가지 전략을 제안합니다. 세 가지 SGG 작업에 대한 광범위한 절제 및 결과는 제안된 LS-KD의 우월성과 일반성을 입증했으며, 이는 서로 다른 조건부 범주 간에 적절한 절충 성능을 일관되게 달성할 수 있습니다."
306,http://arxiv.org/abs/2208.01909 ,Rethinking the Evaluation of Unbiased Scene Graph Generation,"Xingchen Li, Long Chen, Jian Shao, Shaoning Xiao, Songyang Zhang, Jun Xiao","현재 장면 그래프 생성(SGG) 방법은 술어의 심각한 불균형 분포로 인해 빈도가 높은 술어 범주를 예측하고 희귀한 범주를 인식하지 못하는 경향이 있습니다. 다양한 조건자 범주에서 SGG 모델의 견고성을 향상시키기 위해 최근 연구에서는 편향되지 않은 SGG에 초점을 맞추고 평균 Recall@K(mR@K)를 주요 평가 지표로 채택했습니다. 그러나 우리는 현재의 편견 없는 SGG 평가를 취약하고 불공평하게 만드는 이 사실상의 표준 메트릭에 대해 간과된 두 가지 문제를 발견했습니다. 1) mR@K는 술어 카테고리에 관계없이 모든 삼중 예측의 순위를 매길 때 술어 간의 상관 관계를 무시하고 의도치 않게 카테고리 독립성을 깨뜨립니다. 2) mR@K는 서로 다른 술어의 구성적 다양성을 무시하고 구성 가능한 관계 삼중항 유형이 제한된 일부 지나치게 단순한 범주 샘플에 지나치게 높은 가중치를 할당합니다. 또한, 편견 없는 SGG를 위한 간단하지만 강력한 기준선 역할을 할 수 있는 개체와 조건자 간의 미탐색 상관 관계를 조사합니다. 본 논문에서는 mR@K를 개선하고 편향되지 않은 SGG에 대한 두 가지 보완적인 평가 지표인 MR(독립적 평균 재현율)과 wIMR(가중 IMR)을 제안합니다. 이 두 측정항목은 구성 가능한 관계 삼중항의 카테고리 독립성과 다양성을 각각 고려하여 설계되었습니다. 우리는 광범위한 실험을 통해 제안된 측정항목과 사실상의 표준 측정항목을 비교하고, 보다 신뢰할 수 있는 방식으로 편견이 없는 SGG를 평가할 수 있는 솔루션에 대해 논의합니다."
305,http://arxiv.org/abs/2208.01834 ,Integrating Object-aware and Interaction-aware Knowledge for Weakly Supervised Scene Graph Generation,"Xingchen Li, Long Chen, Wenbo Ma, Yi Yang, Jun Xiao","최근 WSSGG(Weakly Supervised Scene Graph Generation)에 대한 노력이 증가하고 있습니다. WSSGG의 주류 솔루션은 일반적으로 동일한 파이프라인을 따릅니다. 먼저 약한 이미지 수준 감독(예: 지역화되지 않은 관계 삼중항 또는 캡션)의 텍스트 엔터티를 이미지 영역과 정렬한 다음 정렬된 인스턴스 수준 ""의사"" 레이블을 사용하여 완전히 감독되는 방식으로 SGG 모델을 교육합니다. 그러나 우리는 대부분의 기존 WSSGG 작업이 객체 일관성에만 초점을 맞추고 있다고 주장합니다. 즉, 기반 영역은 텍스트 엔터티와 동일한 객체 범주 레이블을 가져야 함을 의미합니다. 그들은 이상적인 정렬을 위한 또 다른 기본 요구 사항인 상호 작용 일관성을 무시합니다. 이는 고정 영역 쌍이 텍스트 엔터티 쌍과 동일한 상호 작용(예: 시각적 관계)을 가져야 함을 의미합니다. 따라서 본 논문에서는 보다 신뢰할 수 있는 의사 레이블을 획득하기 위해 객체 인식 및 상호 작용 인식 지식을 모두 갖춘 간단한 접지 모듈을 향상시킬 것을 제안합니다. 이 두 가지 유형의 지식을 더 잘 활용하기 위해 우리는 그들을 두 명의 교사로 간주하고 생성된 목표를 융합하여 접지 모듈의 교육 프로세스를 안내합니다. 구체적으로, 우리는 각 교육 샘플에 대한 신뢰도를 평가하여 다양한 교사에게 가중치를 적응적으로 할당하는 두 가지 다른 전략을 설계합니다. 광범위한 실험을 통해 우리의 방법이 다양한 종류의 약한 감독에 대해 WSSGG 성능을 지속적으로 향상시키는 것으로 나타났습니다."
304,http://arxiv.org/abs/2207.13440 ,Iterative Scene Graph Generation,"Siddhesh Khandelwal, Leonid Sigal",장면 그래프 생성 작업에는 주어진 이미지(또는 비디오)에서 개체 엔터티와 해당 상호 작용 조건을 식별하는 작업이 수반됩니다. 조합적으로 큰 솔루션 공간으로 인해 장면 그래프 생성에 대한 기존 접근 방식은 추정이 가능하도록 결합 분포의 특정 인수분해를 가정합니다(예: 객체가 조건부 예측과 조건부 독립이라고 가정). 그러나 이 고정 인수분해는 모든 시나리오에서 이상적인 것은 아닙니다(예: 상호 작용에 수반되는 객체가 작고 그 자체로 식별할 수 없는 이미지의 경우). 본 연구에서는 Markov Random Field의 메시지 전달을 사용하여 이러한 한계를 해결하고 이미지에 동적 조건을 도입하는 장면 그래프 생성을 위한 새로운 프레임워크를 제안합니다. 이는 각 수정 사항이 이전 반복에서 생성된 그래프에 따라 조정되는 반복적인 개선 절차로 구현됩니다. 개선 단계 전반에 걸쳐 이러한 조건을 지정하면 엔터티와 관계에 대한 공동 추론이 가능해집니다. 이 프레임워크는 새롭고 종단 간 훈련 가능한 변환기 기반 아키텍처를 통해 실현됩니다. 또한 제안된 프레임워크는 기존 접근 방식 성능을 향상시킬 수 있습니다. Visual Genome 및 Action Genome 벤치마크 데이터 세트에 대한 광범위한 실험을 통해 장면 그래프 생성에서 향상된 성능을 보여줍니다.
303,http://arxiv.org/abs/2207.13316 ,NICEST: Noisy Label Correction and Training for Robust Scene Graph Generation,"Lin Li, Jun Xiao, Hanrong Shi, Hanwang Zhang, Yi Yang, Wei Liu, Long Chen","거의 모든 기존 장면 그래프 생성(SGG) 모델은 주류 SGG 데이터 세트의 실측 주석 품질을 간과했습니다. 즉, 다음을 가정합니다. 1) 수동으로 주석이 달린 모든 양성 샘플이 동일하게 정확합니다. 2) 주석이 추가되지 않은 모든 부정적인 샘플은 절대적으로 배경입니다. 이 논문에서 우리는 두 가지 가정 중 어느 것도 SGG에 적용되지 않는다고 주장합니다. 이 두 가지 가정을 깨고 편향되지 않은 SGG 모델의 훈련에 해를 끼치는 수많은 시끄러운 지상 진실 예측 레이블이 있습니다. 이를 위해 우리는 SGG: NICEST에 대한 새로운 NoIsy 레이블 CorrEction 및 샘플 교육 전략을 제안합니다. 구체적으로 NICE와 NIST의 두 부분으로 구성되어 있으며, 각각 고품질 샘플과 효과적인 훈련 전략을 생성하여 이러한 시끄러운 라벨 문제를 배제합니다. NICE는 먼저 잡음이 있는 샘플을 감지한 다음 더 높은 품질의 소프트 조건자 레이블을 다시 할당합니다. NIST는 모델이 편견 없는 융합 지식을 학습할 수 있도록 하는 다중 교사 지식 증류 기반 교육 전략입니다. 그리고 NIST의 동적 균형 가중치 전략은 다양한 교사의 편견에 불이익을 주기 위해 설계되었습니다. NICE와 NIST의 모델에 구애받지 않는 특성으로 인해 NICEST는 모든 SGG 아키텍처에 원활하게 통합되어 다양한 조건자 범주에서 성능을 향상시킬 수 있습니다. 또한 SGG 모델의 일반화를 더 잘 평가하기 위해 널리 사용되는 VG 데이터 세트를 재구성하고 의도적으로 각 주제-객체 범주 쌍에 대해 훈련 및 테스트 세트의 예측 분포를 최대한 다르게 만들어 새로운 벤치마크 VG-OOD를 제안합니다. 이 새로운 벤치마크는 주체-객체 범주 기반 주파수 편향의 영향을 분리하는 데 도움이 됩니다. 다양한 백본과 작업에 대한 광범위한 제거와 결과는 NICEST의 각 구성 요소의 효율성과 일반화 능력을 입증했습니다."
302,http://arxiv.org/abs/2207.11441 ,Meta Spatio-Temporal Debiasing for Video Scene Graph Generation,"Li Xu, Haoxuan Qu, Jason Kuen, Jiuxiang Gu, Jun Liu","비디오 장면 그래프 생성(VidSGG)은 비디오 콘텐츠를 장면 그래프로 구문 분석하는 것을 목표로 하며, 여기에는 비디오의 시공간적 맥락 정보 모델링이 포함됩니다. 그러나 데이터 세트의 긴 꼬리 훈련 데이터로 인해 기존 VidSGG 모델의 일반화 성능은 시공간 조건 편향 문제의 영향을 받을 수 있습니다. 본 연구에서는 이러한 편향 문제를 해결하기 위해 메타러닝 관점에서 새로운 MVSGG(Meta Video Scene Graph Generation) 프레임워크를 제안합니다. 구체적으로, 다양한 유형의 시공간 조건 편향을 처리하기 위해 우리 프레임워크는 먼저 훈련 데이터에서 지원 세트와 쿼리 세트 그룹을 구성합니다. 여기서 각 쿼리 세트의 데이터 분포는 지원 세트 w.r.t의 데이터 분포와 다릅니다. 조건부 편향의 일종. 그런 다음 지원 세트에 대한 교육 후 이러한 쿼리 세트에 대한 우수한 테스트 성능을 얻기 위해 모델을 최적화하기 위한 새로운 메타 교육 및 테스트 프로세스를 수행함으로써 프레임워크는 모델이 편향에 대해 잘 일반화하는 방법을 학습하도록 효과적으로 안내할 수 있습니다. 광범위한 실험을 통해 우리가 제안한 프레임워크의 효율성이 입증되었습니다."
301,http://arxiv.org/abs/2207.11247 ,Panoptic Scene Graph Generation,"Jingkang Yang, Yi Zhe Ang, Zujin Guo, Kaiyang Zhou, Wayne Zhang, Ziwei Liu","기존 연구에서는 감지 관점에서 이미지의 장면 이해를 위한 중요한 기술인 장면 그래프 생성(SGG)을 다루고 있습니다. 즉, 경계 상자를 사용하여 객체를 감지한 후 쌍 관계를 예측합니다. 우리는 그러한 패러다임이 해당 분야의 발전을 방해하는 몇 가지 문제를 야기한다고 주장합니다. 예를 들어 현재 데이터 세트의 경계 상자 기반 레이블에는 일반적으로 머리카락과 같은 중복 클래스가 포함되어 있으며 컨텍스트를 이해하는 데 중요한 배경 정보는 생략됩니다. 이 연구에서는 모델이 고정된 경계 상자가 아닌 팬옵틱 분할을 기반으로 보다 포괄적인 장면 그래프 표현을 생성하도록 요구하는 새로운 문제 작업인 팬옵틱 장면 그래프 생성(PSG)을 소개합니다. 커뮤니티가 진행 상황을 추적할 수 있도록 COCO 및 Visual Genome의 잘 주석 처리된 49k 중첩 이미지가 포함된 고품질 PSG 데이터 세트가 생성되었습니다. 벤치마킹을 위해 SGG의 고전적인 방법에서 수정된 4개의 2단계 기준선과 효율적인 Transformer 기반 탐지기, 즉 DETR을 기반으로 하는 PSGTR 및 PSGFormer라는 2개의 1단계 기준선을 구축합니다. PSGTR은 일련의 쿼리를 사용하여 트리플렛을 직접 학습하는 반면, PSGFormer는 두 개의 Transformer 디코더의 쿼리 형식으로 객체와 관계를 별도로 모델링한 후 프롬프트와 같은 관계-객체 매칭 메커니즘을 따릅니다. 마지막으로 열린 도전과 앞으로의 방향에 대한 인사이트를 공유합니다."
300,http://arxiv.org/abs/2207.12321 ,RSG-Net: Towards Rich Sematic Relationship Prediction for Intelligent Vehicle in Complex Environments,"Yafu Tian, Alexander Carballo, Ruifeng Li, Kazuya Takeda","행동 및 의미론적 관계는 지능형 자율 주행 차량 및 ADAS 시스템에서 중요한 역할을 합니다. 궤적, 위치 및 경계 상자에 초점을 맞춘 다른 연구와 달리 관계 데이터는 개체의 동작에 대해 사람이 이해할 수 있는 설명을 제공하며 개체의 과거와 미래 상태를 놀랍도록 간단한 방식으로 설명할 수 있습니다. 따라서 위험 감지, 환경 이해, 의사결정 등의 업무를 수행하기 위한 기본적인 방법입니다. 본 논문에서는 객체 제안으로부터 잠재적인 의미 관계를 예측하고 ""Road Scene Graph""라는 그래프 구조의 결과를 생성하도록 설계된 그래프 컨벌루션 네트워크인 RSG-Net(Road Scene Graph Net)을 제안합니다. 실험 결과는 도로 장면 그래프 데이터세트로 훈련된 이 네트워크가 자율주행차 주변 객체 간의 잠재적 의미론적 관계를 효율적으로 예측할 수 있음을 나타냅니다."
299,http://arxiv.org/abs/2207.07913 ,Dual-branch Hybrid Learning Network for Unbiased Scene Graph Generation,"Chaofan Zheng, Lianli Gao, Xinyu Lyu, Pengpeng Zeng, Abdulmotaleb El Saddik, Heng Tao Shen","장면 그래프 생성(SGG)에 대한 현재 연구는 편향되지 않은 장면 그래프를 생성하기 위한 긴 꼬리 문제를 해결하는 데 중점을 두고 있습니다. 그러나 대부분의 편향성 제거 방법은 훈련 전반에 걸쳐 꼬리 술어를 과도하게 강조하고 머리 술어를 과소평가하여 머리 술어 특징의 표현 능력을 손상시킵니다. 더욱이, 머리 술어의 이러한 손상된 특징은 꼬리 술어의 학습에 해를 끼칩니다. 실제로 꼬리 술어의 추론은 머리 술어에서 학습된 일반적인 패턴에 크게 의존합니다. 예를 들어 ""stand on""은 ""on""에 따라 다릅니다. 따라서 이러한 편향성 제거 SGG 방법은 꼬리 술어에서 우수한 성능을 달성하지 못하거나 머리 술어에서 만족스러운 동작을 달성할 수 없습니다. 이 문제를 해결하기 위해 우리는 CLB(Coarse-grained Learning Branch) 및 FLB(Fine-grained Learning Branch)를 포함하여 SGG의 헤드 조건자와 테일 조건을 모두 처리하는 이중 분기 하이브리드 학습 네트워크(DHL)를 제안합니다. 구체적으로 CLB는 머리 술어의 전문 지식과 강력한 기능을 학습하는 역할을 담당하고, FLB는 유익한 꼬리 술어를 예측하는 역할을 담당합니다. 또한 DHL은 두 지점이 서로 잘 협력할 수 있도록 BCS(Branch Curriculum Schedule)를 갖추고 있습니다. 실험에 따르면 우리의 접근 방식은 VG 및 GQA 데이터 세트에서 새로운 최첨단 성능을 달성하고 꼬리 술어와 머리 술어의 성능 간에 균형을 이룹니다. 또한 두 가지 다운스트림 작업(예: 이미지 캡션 및 문장-그래프 검색)에 대한 광범위한 실험을 통해 우리 방법의 일반화 및 실용성을 추가로 검증합니다."
298,http://arxiv.org/abs/2207.07870 ,Scene Graph for Embodied Exploration in Cluttered Scenario,"Yuhong Deng, Qie Sima, Di Guo, Huaping Liu, Yi Wang, Fuchun Sun","어수선한 환경에서 물체를 처리하는 능력은 로봇 커뮤니티에서 오랫동안 기대해 왔습니다. 그러나 대부분의 작품은 어수선한 사물에 숨겨진 의미 정보를 렌더링하는 대신 조작에만 중점을 둡니다. 본 연구에서는 이 문제를 해결하기 위해 복잡한 시나리오에서 구체화된 탐색을 위한 장면 그래프를 소개합니다. 복잡한 시나리오에서 우리 방법을 검증하기 위해 우리는 MQA(Manipulation Question Answering) 작업을 테스트 벤치마크로 채택했습니다. 이를 위해서는 구체화된 로봇이 시각과 언어에 대한 능동적 탐색 능력과 의미론적 이해 능력을 갖추어야 합니다. 작업에 대한 일반적인 솔루션 프레임워크로 탐색을 위한 조작을 생성하는 모방 학습 방법을 제안합니다. 한편, 조작자의 손목 카메라에서 일련의 RGB 프레임을 이해하기 위해 동적 장면 그래프를 기반으로 하는 VQA 모델이 채택되었으며 조작의 모든 단계는 프레임워크의 질문에 답하기 위해 수행되었습니다. 서로 다른 상호 작용 요구 사항을 가진 MQA 데이터 세트에 대한 실험은 제안된 프레임워크가 복잡한 시나리오의 작업을 대표하는 MQA 작업에 효과적이라는 것을 보여줍니다."
297,http://arxiv.org/abs/2207.05006 ,TASKOGRAPHY: Evaluating robot task planning over large 3D scene graphs,"Christopher Agia, Krishna Murthy Jatavallabhula, Mohamed Khodeir, Ondrej Miksik, Vibhav Vineet, Mustafa Mukadam, Liam Paull, Florian Shkurti","3D 장면 그래프(3DSG)는 새로운 설명입니다. 상징적, 위상적, 미터법적 장면 표현을 통합합니다. 그러나 일반적인 3DSG에는 소규모 환경에서도 수백 개의 개체와 기호가 포함되어 있습니다. 전체 그래프에 대한 작업 계획을 비현실적으로 만듭니다. 우리는 3DSG를 통한 최초의 대규모 로봇 작업 계획 벤치마크인 TASKOGRAPHY를 구축합니다. 이 분야의 대부분의 벤치마킹 노력은 비전 기반 계획에 중점을 두는 반면, 우리는 시각적 표현 학습에서 계획 성과를 분리하기 위해 상징적 계획을 체계적으로 연구합니다. 우리는 기존 방법 중에서 고전적 계획이나 학습 기반 계획자 모두 전체 3DSG에 대한 실시간 계획을 세울 수 없다는 것을 관찰했습니다. 실시간 계획을 활성화하려면 (a) 다루기 쉬운 계획을 위해 3DSG를 분산시키고 (b) 3DSG 계층을 더 잘 활용하는 계획자를 설계하는 과정이 필요합니다. 이전 목표를 위해 우리는 작업 조건부 3DSG 희소화 방법인 SCRUB를 제안합니다. 고전적인 계획자가 최첨단 학습 기반 계획자와 일치하거나 경우에 따라 능가할 수 있도록 합니다. 후자의 목표를 위해 우리는 학습 기반 계획자가 3DSG 구조를 활용하여 현재 최상의 접근 방식에 필요한 재계획 쿼리 수를 몇 배나 줄일 수 있는 절차인 SEEK를 제안합니다. 우리는 로봇 작업 계획, 학습 및 3DSG의 교차점을 따라 추가 연구를 촉진하기 위해 모든 코드와 기준선을 오픈 소스로 제공할 것입니다."
296,http://arxiv.org/abs/2207.04602 ,Adaptive Fine-Grained Predicates Learning for Scene Graph Generation,"Xinyu Lyu, Lianli Gao, Pengpeng Zeng, Heng Tao Shen, Jingkuan Song","현재 SGG(장면 그래프 생성) 모델의 성능은 구별하기 어려운 조건(예: 여성이 위에 서 있음/해변에 서 있음/해변에서 걷기)으로 인해 심각하게 저하됩니다. 일반적인 SGG 모델은 머리 술어를 예측하는 경향이 있고 재조정 전략은 꼬리 범주를 선호하므로 둘 중 어느 것도 구별하기 어려운 술어를 적절하게 처리할 수 없습니다. 이 문제를 해결하기 위해 구별하기 어려운 객체를 구별하는 데 초점을 맞춘 세밀한 이미지 분류에서 영감을 받아 SGG에 대해 구별하기 어려운 조건자를 구별하는 것을 목표로 하는 적응형 세밀한 조건자 학습(FGPL-A)을 제안합니다. 먼저, 구별하기 어려운 조건자를 파악하기 위해 PL-A(Adaptive Predicate Lattice)를 도입합니다. 이는 모델의 동적 학습 속도에 맞춰 조건부 상관 관계를 적응적으로 탐색합니다. 실제로 PL-A는 SGG 데이터 세트에서 초기화되고 현재 미니 배치에 대한 모델 예측을 탐색하여 개선됩니다. PL-A를 활용하여 모델의 동적 학습 상태에 대한 세밀한 감독을 통해 모델의 식별 프로세스를 점진적으로 정규화하여 균형 있고 효율적인 학습 프로세스를 보장하는 CDL-A(Adaptive Category Discriminating Loss) 및 EDL-A(Adaptive Entity Discriminating Loss)를 제안합니다. 광범위한 실험 결과는 우리가 제안한 모델 독립적 전략이 VG-SGG 및 GQA-SGG 데이터 세트에 대한 벤치마크 모델의 성능을 Mean Recall@100에서 최대 175% 및 76%까지 크게 향상시켜 새로운 최첨단 성능을 달성한다는 것을 보여줍니다. 또한 문장-그래프 검색 및 이미지 캡션 작업에 대한 실험은 우리 방법의 실용성을 더욱 입증합니다."
295,http://arxiv.org/abs/2207.04364 ,Sequential Manipulation Planning on Scene Graph,"Ziyuan Jiao, Yida Niu, Zeyu Zhang, Song-Chun Zhu, Yixin Zhu, Hangxin Liu","효율적인 순차적 작업 계획을 위해 3D 장면 그래프 표현인 접촉 그래프+(cg+)를 고안합니다. 술어와 같은 속성이 추가된 이 접촉 그래프 기반 표현은 간결한 기하학적 정보와 유효한 로봇 장면 상호 작용으로 장면 레이아웃을 추상화합니다. 접촉 그래프에 자연스럽게 지정된 목표 구성은 확률론적 최적화 방법을 사용하는 유전 알고리즘을 통해 생성될 수 있습니다. 그런 다음 초기 접촉 그래프와 목표 구성 사이의 그래프 편집 거리(GED)를 계산하여 작업 계획이 초기화되며, 이는 가능한 로봇 동작에 해당하는 그래프 편집 작업을 생성합니다. 그래프 편집 작업의 시간적 타당성을 규제하고 유효한 작업과 모션 대응을 보장하기 위해 제약 조건을 적용하여 작업 계획을 마무리합니다. 일련의 시뮬레이션과 실험에서 로봇은 PDDL(Planning Domain Definition Language)과 같은 기존 계획 언어를 사용하여 지정하기 어려운 복잡한 순차적 객체 재배열 작업을 성공적으로 완료하여 접촉 그래프에서 로봇 순차적 작업 계획의 높은 타당성과 잠재력을 입증했습니다."
294,http://arxiv.org/abs/2207.03729 ,GEMS: Scene Expansion using Generative Models of Graphs,"Rishi Agarwal, Tirupati Saketh Chandra, Vaidehi Patil, Aniruddha Mahapatra, Kuldeep Kulkarni, Vishwa Vinay",이미지 검색을 기반으로 하는 애플리케이션은 RGB 이미지나 의미 라벨 맵과 같은 조밀한 픽셀 수준 표현보다는 객체 및 그 관계와 같은 상위 수준 개념을 나타내는 중간 공간에서 편집하고 연결해야 합니다. 우리는 그러한 표현 중 하나인 장면 그래프에 초점을 맞추고 새로운 노드(객체)와 해당 관계를 추가하여 입력 시드 그래프를 강화하는 새로운 장면 확장 작업을 제안합니다. 이를 위해 장면 그래프 확장을 먼저 새 노드를 예측한 다음 그래프에서 새로 예측된 ​​노드와 이전 노드 간의 관계 집합을 예측하는 여러 단계를 포함하는 순차적 예측 작업으로 공식화합니다. 우리는 노드 간 클러스터링 패턴을 유지하는 관찰된 그래프에 대한 시퀀싱 전략을 제안합니다. 또한 외부 지식을 활용하여 그래프 생성 모델을 교육함으로써 노드 예측을 더욱 일반화할 수 있습니다. 노드(객체) 간의 예측 관계를 평가할 때 그래프 생성 문제에 대한 기존 최대 평균 불일치(MMD) 기반 메트릭의 비효율성으로 인해 예측 관계의 다양한 측면을 종합적으로 평가하는 새로운 메트릭을 설계합니다. 우리는 표준 MMD 기반 메트릭과 제안된 메트릭을 사용하여 확장된 장면 그래프를 평가하기 위해 Visual Genome 및 VRD 데이터 세트에 대한 광범위한 실험을 수행합니다. 우리는 GEMS 방법으로 생성된 그래프가 GraphRNN과 같은 기본 방법보다 장면 그래프의 실제 분포를 더 잘 표현한다는 것을 관찰했습니다.
293,http://arxiv.org/abs/2207.00545 ,Transforming Image Generation from Scene Graphs,"Renato Sortino, Simone Palazzo, Concetto Spampinato","의미론적 시각적 지식에서 이미지를 생성하는 것은 클래스 레이블이나 텍스트 설명과 같은 대안에 비해 복잡하고 미묘하며 명확한 방식으로 합성 프로세스를 조절하는 데 유용할 수 있는 어려운 작업입니다. 의미론적 표현에 의해 조건화된 생성 방법이 존재하더라도 객체 간의 제약 조건 지정 외에 생성 프로세스를 제어할 수 있는 방법을 제공하지 않습니다. 예를 들어, 특정 항목을 수동으로 추가하여 이미지를 반복적으로 생성하거나 수정하는 가능성은 우리가 아는 한 문헌에서 완전히 조사되지 않은 원하는 속성입니다. 이 작업에서 우리는 최근의 변환기 기반 방법과 반대로 디코더를 사용하여 이미지를 자동 회귀적으로 구성하여 합성 프로세스를 보다 효과적이고 제어 가능하게 만드는 장면 그래프를 조건으로 하는 변환기 기반 접근 방식을 제안합니다. 제안된 아키텍처는 세 가지 모듈로 구성됩니다. 1) 입력 그래프의 관계를 인코딩하기 위한 그래프 컨벌루션 네트워크; 2) 출력 이미지를 자동 회귀적으로 구성하는 인코더-디코더 변환기; 3) 변환기에 의한 각 생성 단계의 입력/출력으로 사용되는 표현을 생성하는 데 사용되는 자동 인코더. CIFAR10 및 MNIST 이미지에서 얻은 결과는 우리 모델이 장면 그래프로 정의된 의미론적 제약 조건을 충족하고 원하는 대상의 사용자 제공 부분 렌더링을 고려하여 장면의 시각적 객체 간의 관계를 모델링할 수 있음을 보여줍니다."
292,http://arxiv.org/abs/2206.11653 ,Learning To Generate Scene Graph from Head to Tail,"Chaofan Zheng, Xinyu Lyu, Yuyu Guo, Pengpeng Zeng, Jingkuan Song, Lianli Gao",SGG(장면 그래프 생성)는 개체 및 개체와 그래프 구조의 상호 작용을 나타냅니다. 최근 SGG의 불균형 문제를 해결하기 위해 많은 연구가 진행되고 있습니다. 그러나 전체 훈련 과정에서 머리 술어를 과소평가하면 꼬리 술어에 일반적인 기능을 제공하는 머리 술어의 기능이 손상됩니다. 게다가 꼬리 술어에 지나치게 주의를 기울이면 의미적 편차가 발생합니다. 이를 기반으로 CRM(Curriculum Re-weight Mechanism)과 SCM(Semantic Context Module)을 포함하는 SGG-HT(Head to Tail) 장면 그래프 생성을 학습하는 새로운 SGG 프레임워크를 제안합니다. CRM은 먼저 헤드 술어의 강력한 기능에 대해 헤드/쉬운 샘플을 학습한 다음 점차적으로 테일/하드 샘플에 초점을 맞춥니다. SCM은 생성된 장면 그래프와 전역 및 로컬 표현의 Ground Truth 간의 의미적 일관성을 보장하여 의미적 편차를 완화하기 위해 제안되었습니다. 실험에 따르면 SGG-HT는 편향된 문제를 크게 완화하고 Visual Genome에서 최첨단 성능을 달성하는 것으로 나타났습니다.
291,http://arxiv.org/abs/2206.11352 ,Doubly Reparameterized Importance Weighted Structure Learning for Scene Graph Generation,"Daqi Liu, Miroslaw Bober, Josef Kittler","구조화된 예측 작업인 장면 그래프 생성은 입력 이미지가 주어지면 시각적으로 기반이 되는 장면 그래프를 구성하여 객체와 객체의 관계를 명시적으로 모델링하는 것을 목표로 합니다. 현재 문헌에서 이러한 작업은 메시지 전달 신경망 기반 평균 필드 변이 베이지안 방법론을 통해 보편적으로 해결됩니다. 고전적인 느슨한 증거 하한은 일반적으로 변이 추론 목표로 선택되며, 이는 과도하게 단순화된 변이 근사를 유도하여 기본 복합 사후를 과소평가할 수 있습니다. 본 논문에서는 더 엄격한 중요도 가중치 하한을 변이 추론 목표로 사용하는 새로운 이중 재매개변수화 중요도 가중 구조 학습 방법을 제안합니다. 이는 재매개변수화 가능한 Gumbel-Softmax 샘플러에서 가져온 여러 샘플에서 계산되며 결과적으로 제한된 변형 추론 작업은 일반 엔트로피 미러 하강 알고리즘으로 해결됩니다. 결과적으로 이중으로 다시 매개변수화된 그래디언트 추정기는 학습에 유익한 영향을 미치면서 해당 도함수의 분산을 줄입니다. 제안된 방법은 널리 알려진 다양한 장면 그래프 생성 벤치마크에서 최고의 성능을 달성했습니다."
290,http://arxiv.org/abs/2206.04863 ,Symbolic image detection using scene and knowledge graphs,"Nasrin Kalanat, Adriana Kovashka",때로는 이미지가 전달하는 의미가 이미지에 포함된 대상의 목록을 넘어서는 경우도 있습니다. 대신 이미지는 보는 사람의 마음에 영향을 미치는 강력한 메시지를 표현할 수 있습니다. 이 메시지를 추론하려면 객체 간의 관계에 대한 추론과 구성 요소에 대한 일반적인 상식 지식이 필요합니다. 본 논문에서는 시각적 구성 요소를 포착하기 위해 이미지를 그래프로 표현한 장면 그래프(Scene Graph)를 사용합니다. 또한 ConceptNet에서 추출한 사실을 활용하여 객체와 속성에 대한 추론을 위한 지식 그래프를 생성합니다. 기호를 감지하기 위해 SKG-Sym이라는 신경망 프레임워크를 제안합니다. 프레임워크는 먼저 Graph Convolution Network를 사용하여 이미지의 장면 그래프 표현과 지식 그래프를 생성합니다. 그런 다음 프레임워크는 표현을 융합하고 MLP를 사용하여 분류합니다. 우리는 그래프 표현의 중요성을 학습하는 Attention 메커니즘을 사용하기 위해 네트워크를 더욱 확장합니다. 우리는 광고 데이터 세트에 대한 방법을 평가하고 이를 기본 상징 분류 방법(ResNet 및 VGG)과 비교합니다. 결과는 우리의 방법이 F-점수 측면에서 ResNet보다 성능이 뛰어나고 주의 기반 메커니즘이 VGG와 경쟁적이지만 모델 복잡성이 훨씬 낮다는 것을 보여줍니다.
289,http://arxiv.org/abs/2206.05051 ,Temporal Inductive Logic Reasoning over Hypergraphs,"Yuan Yang, Siheng Xiong, Ali Payani, James C Kerce, Faramarz Fekri","귀납적 논리 추론은 데이터의 패턴을 일반화하는 것을 목표로 하는 그래프 분석의 기본 작업입니다. 이 작업은 유도 논리 프로그래밍(ILP)과 같은 기술을 사용하여 지식 그래프(KG)와 같은 전통적인 그래프 표현에 대해 광범위하게 연구되었습니다. 기존 ILP 방법은 정적 사실과 이진 관계를 통해 KG로부터 학습한다고 가정합니다. KG 외에도 그래프 구조는 절차 지침, 장면 그래프 및 프로그램 실행과 같은 다른 응용 프로그램에 널리 존재합니다. ILP는 이러한 애플리케이션에 유용하지만 이를 그래프에 적용하는 것은 사소한 일이 아닙니다. ILP는 일반적으로 타임스탬프와 n-ary 관계를 포함하는 KG보다 더 복잡하며 사실상 시간적 이벤트가 있는 일종의 하이퍼그래프입니다. 본 연구에서는 시간 초곡선을 추론하는 ILP 방법인 시간 귀납 논리 추론(TILR)을 제안합니다. 하이퍼그래프 추론을 활성화하기 위해 하이퍼그래프를 위한 새로운 그래프 순회 방법인 다중 시작 무작위 B-walk를 도입합니다. 이를 경로 일관성 알고리즘과 결합함으로써 TILR은 시간 데이터와 관계형 데이터 모두에서 일반화하여 논리 규칙을 학습합니다. 하이퍼그래프 벤치마크의 부족을 해결하기 위해 우리는 YouCook2-HG 및 nuScenes-HG라는 두 가지 시간 하이퍼그래프 데이터 세트를 생성하고 출시합니다. 이러한 벤치마크에 대한 실험은 TILR이 다양한 강력한 기준에 비해 뛰어난 추론 기능을 달성한다는 것을 보여줍니다."
288,http://arxiv.org/abs/2206.04585 ,Extracting Zero-shot Common Sense from Large Language Models for Robot 3D Scene Understanding,"William Chen, Siyi Hu, Rajat Talak, Luca Carlone","의미론적 3D 장면 이해는 로봇 공학에서 매우 중요한 문제입니다. 동시 위치 파악 및 매핑 알고리즘이 크게 발전했지만, 로봇은 여전히 ​​가정의 물건과 일반 인간의 위치에 대한 상식적인 지식을 갖고 있지 않습니다. 우리는 내부에 포함된 개체를 고려하여 방에 라벨을 지정하기 위해 대규모 언어 모델에 포함된 상식을 활용하는 새로운 방법을 소개합니다. 이 알고리즘은 (i) 작업별 사전 훈련이 필요하지 않고(완전히 제로샷 방식으로 작동) (ii) 이전에 볼 수 없었던 항목을 포함하여 임의의 공간 및 개체 레이블을 일반화하는 추가 이점이 있습니다. 두 가지 모두 로봇 장면 이해 알고리즘에서 매우 바람직한 특성입니다. 제안된 알고리즘은 현대 공간 인식 시스템에 의해 생성된 3D 장면 그래프에서 작동하며, 로봇공학에 대한 보다 일반화되고 확장 가능한 높은 수준의 3D 장면 이해를 위한 길을 열 수 있기를 바랍니다."
287,http://arxiv.org/abs/2206.03014 ,The Devil is in the Labels: Noisy Label Correction for Robust Scene Graph Generation,"Lin Li, Long Chen, Yifeng Huang, Zhimeng Zhang, Songyang Zhang, Jun Xiao","Unbiased SGG는 최근 몇 년 동안 상당한 진전을 이루었습니다. 그러나 거의 모든 기존 SGG 모델은 일반적인 SGG 데이터 세트의 실제 주석 품질을 간과했습니다. 즉, 항상 다음을 가정합니다. 1) 수동으로 주석이 달린 모든 양성 샘플이 동일하게 정확합니다. 2) 주석이 추가되지 않은 모든 부정적인 샘플은 절대적으로 배경입니다. 이 문서에서 우리는 두 가지 가정이 SGG에 적용되지 않는다고 주장합니다. 이 두 가지 가정을 깨뜨리는 수많은 ""잡음이 있는"" 실측 조건자 레이블이 있으며 이러한 잡음이 있는 샘플은 실제로 편향되지 않은 SGG 모델의 교육에 해를 끼칩니다. 이를 위해 우리는 SGG: NICE에 대한 새로운 모델 독립적인 NoIsy 레이블 CorrEction 전략을 제안합니다. NICE는 잡음이 있는 샘플을 감지할 수 있을 뿐만 아니라 더 높은 품질의 조건부 레이블을 해당 샘플에 재할당할 수도 있습니다. NICE 훈련 후에는 모델 훈련을 위한 보다 깨끗한 버전의 SGG 데이터 세트를 얻을 수 있습니다. 구체적으로 NICE는 네거티브 노이즈 샘플 감지(Neg-NSD), 포지티브 NSD(Pos-NSD) 및 노이즈 샘플 수정(NSC)의 세 가지 구성 요소로 구성됩니다. 첫째, Neg-NSD에서는 이 작업을 분포를 벗어난 탐지 문제로 공식화하고 탐지된 모든 잡음이 있는 네거티브 샘플에 의사 레이블을 할당합니다. 그런 다음 Pos-NSD에서는 클러스터링 기반 알고리즘을 사용하여 모든 포지티브 샘플을 여러 세트로 나누고 노이즈가 가장 많은 세트의 샘플을 노이즈 포지티브 샘플로 처리합니다. 마지막으로 NSC에서는 간단하지만 효과적인 가중치 KNN을 사용하여 새로운 예측 레이블을 잡음이 있는 양성 샘플에 재할당합니다. 다양한 백본과 작업에 대한 광범위한 결과는 NICE의 각 구성 요소의 효율성과 일반화 능력을 입증했습니다."
286,http://arxiv.org/abs/2205.12089 ,Sim-To-Real Transfer of Visual Grounding for Human-Aided Ambiguity Resolution,"Georgios Tziafas, Hamidreza Kasaei","서비스 로봇은 비전문 인간 사용자와 자연스럽게 상호 작용할 수 있어야 하며, 다양한 작업을 도와줄 뿐만 아니라 지시에 존재할 수 있는 모호함을 해결하기 위한 안내도 받을 수 있어야 합니다. 우리는 에이전트가 자연어 설명을 바탕으로 붐비는 장면에서 객체를 분할하는 시각적 접지 작업을 고려합니다. 시각적 기초에 대한 현대의 전체적인 접근 방식은 일반적으로 언어 구조를 무시하고 일반적인 도메인을 다루기 위해 노력하므로 대규모 데이터 세트에 크게 의존합니다. 또한 벤치마크와 대상 도메인 간의 높은 시각적 불일치로 인해 RGB-D 데이터 세트의 전송 성능이 저하됩니다. 모듈식 접근 방식은 학습과 도메인 모델링을 결합하고 언어의 구성적 특성을 활용하여 시각적 표현을 언어 구문 분석에서 분리하지만 외부 파서에 의존하거나 강력한 감독이 부족하여 엔드투엔드 방식으로 훈련됩니다. 이 작업에서 우리는 엔터티, 속성 및 공간 관계의 구성적 시각적 기반을 위한 완전히 분리된 모듈식 프레임워크를 도입하여 이러한 제한 사항을 해결하려고 합니다. 우리는 합성 도메인에서 생성된 풍부한 장면 그래프 주석을 활용하고 각 모듈을 독립적으로 교육합니다. 우리의 접근 방식은 시뮬레이션과 두 개의 실제 RGB-D 장면 데이터세트 모두에서 평가됩니다. 실험 결과에 따르면 프레임워크의 분리된 특성을 통해 Sim-To-Real 시각적 인식을 위한 도메인 적응 접근 방식과 쉽게 통합할 수 있으며 로봇 애플리케이션의 시각적 접지에 대한 데이터 효율적이고 강력하며 해석 가능한 솔루션을 제공할 수 있습니다."
285,http://arxiv.org/abs/2205.11501 ,VQA-GNN: Reasoning with Multimodal Knowledge via Graph Neural Networks for Visual Question Answering,"Yanan Wang, Michihiro Yasunaga, Hongyu Ren, Shinya Wada, Jure Leskovec","시각적 질문 답변(VQA)에서는 시스템이 비구조적(예: 질문과 답변의 컨텍스트, ""QA 컨텍스트"") 및 구조화된(예: QA 컨텍스트 및 장면에 대한 지식 그래프, ""개념 그래프"") 다중 모드 지식을 통합하여 개념 수준 추론을 수행해야 합니다. 기존 작업은 일반적으로 해당 시각적 노드와 개념 노드를 연결하여 장면 그래프와 장면의 개념 그래프를 결합한 다음 QA 컨텍스트 표현을 통합하여 질문 답변을 수행합니다. 그러나 이러한 방법은 구조화되지 않은 지식에서 구조화된 지식으로 단방향 융합만 수행하므로 이질적인 지식 양식에 대한 공동 추론을 포착할 수 있는 잠재력이 제한됩니다. 보다 표현적인 추론을 수행하기 위해 우리는 통합된 지식 표현을 얻기 위해 비구조화된 다중 모드 지식과 구조화된 다중 모드 지식 간의 양방향 융합을 수행하는 새로운 VQA 모델인 VQA-GNN을 제안합니다. 구체적으로 QA 컨텍스트를 표현하는 슈퍼 노드를 통해 장면 그래프와 개념 그래프를 상호 연결하고, 양식 간 표현 격차를 완화하는 추론을 위한 모드 간 메시지 전달을 수행하는 새로운 다중 모드 GNN 기술을 도입합니다. 두 가지 까다로운 VQA 작업(VCR 및 GQA)에서 우리의 방법은 강력한 기준 VQA 방법보다 VCR(Q-AR)에서 3.2%, GQA에서 4.6% 더 우수하여 개념 수준 추론 수행에 강점이 있음을 나타냅니다. 절제 연구는 구조화되지 않은 다중 모드 지식과 구조화된 다중 모드 지식을 통합하는 데 있어 양방향 융합 및 다중 모드 GNN 방법의 효율성을 추가로 보여줍니다."
284,http://arxiv.org/abs/2205.08325 ,GraphMapper: Efficient Visual Navigation by Scene Graph Generation,"Zachary Seymour, Niluthpol Chowdhury Mithun, Han-Pang Chiu, Supun Samarasekera, Rakesh Kumar","장면에 있는 객체 간의 기하학적 관계를 이해하는 것은 인간과 자율 에이전트 모두 새로운 환경에서 탐색할 수 있도록 하는 핵심 기능입니다. 장면 토폴로지의 희박하고 통합된 표현을 통해 에이전트는 효율적으로 작업하여 환경을 이동하고, 환경 상태를 다른 사람과 전달하고, 다양한 다운스트림 작업에 표현을 활용할 수 있습니다. 이를 위해, 우리는 해당 환경을 탐색하는 방법을 동시에 학습함으로써 해당 환경의 3D 장면 그래프 표현을 축적하는 방법을 학습하도록 자율 에이전트를 훈련시키는 방법을 제안합니다. 우리는 우리의 접근 방식인 GraphMapper를 사용하면 비전 기반 시스템보다 환경과의 더 적은 상호 작용을 통해 효과적인 탐색 정책을 학습할 수 있음을 보여줍니다. 또한 GraphMapper는 기존 학습 기반 솔루션과 함께 작동하여 탐색 효율성을 높일 뿐만 아니라 향후 다른 작업에 유용한 중간 장면 표현을 생성하는 모듈식 장면 인코더 역할을 할 수 있음을 보여줍니다."
283,http://arxiv.org/abs/2205.07017 ,Importance Weighted Structure Learning for Scene Graph Generation,"Daqi Liu, Miroslaw Bober, Josef Kittler","장면 그래프 생성은 입력 이미지에 대해 시각적으로 기반이 되는 장면 그래프를 구성하여 객체와 그 관계를 명시적으로 모델링하는 것을 목표로 하는 구조화된 예측 작업입니다. 현재 메시지 전달 신경망 기반 평균 필드 변형 베이지안 방법론은 이러한 작업에 대한 유비쿼터스 솔루션이며, 여기서 변형 추론 목표는 종종 고전적인 증거 하한으로 가정됩니다. 그러나 이러한 느슨한 목적에서 추론된 변형 근사는 일반적으로 기본 사후를 과소평가하여 종종 열등한 생성 성능으로 이어집니다. 본 논문에서는 재매개변수화 가능한 Gumbel-Softmax 샘플러에서 추출한 여러 샘플에서 계산된 보다 엄격한 중요도 가중 하한을 사용하여 기본 로그 분할 함수를 근사화하는 것을 목표로 하는 새로운 중요도 가중 구조 학습 방법을 제안합니다. 결과적으로 제한된 변형 추론 작업을 해결하기 위해 일반적인 엔트로피 거울 하강 알고리즘이 적용됩니다. 제안된 방법은 널리 알려진 다양한 장면 그래프 생성 벤치마크에서 최고의 성능을 달성했습니다."
282,http://arxiv.org/abs/2205.04188 ,Joint learning of object graph and relation graph for visual question answering,"Hao Li, Xu Li, Belhal Karimi, Jie Chen, Mingming Sun","장면 그래프를 통해 시각적 질의응답(VQA)을 모델링하면 추론의 정확성과 해석성을 크게 향상시킬 수 있습니다. 그러나 기존 모델은 속성이나 관계가 포함된 복잡한 추론 질문에 대해 제대로 답변하지 못하여 그림 1(a)에서 잘못된 속성 선택 또는 관계 누락이 발생합니다. 이는 이들 모델이 관계 및 속성 정보를 무시하고 장면 그래프의 모든 정보의 균형을 맞추지 못하기 때문입니다. 본 논문에서는 다중 스케일 장면 그래프 정보를 적절하게 인코딩하여 균형 잡힌 표현을 얻을 수 있는 새로운 DM-GNN(Dual Message-Passing Enhanced Graph Neural Network)을 소개합니다. 구체적으로, 우리는 (i) 장면 그래프를 객체와 관계에 다양한 초점을 맞춘 두 개의 그래프로 변환합니다. 그런 다음 이를 인코딩하기 위한 이중 구조를 설계하여 관계의 가중치를 증가시킵니다. (ii) 인코더 출력을 속성 기능과 융합하여 속성의 가중치를 증가시킵니다. (iii) 객체, 관계 및 속성 간의 정보 전달을 향상시키기 위한 메시지 전달 메커니즘을 제안합니다. 우리는 GQA, VG, Motif-VG를 포함한 데이터 세트에 대한 광범위한 실험을 수행하고 새로운 수준의 기술을 달성합니다."
281,http://arxiv.org/abs/2205.02958 ,Scene Graph Expansion for Semantics-Guided Image Outpainting,"Chiao-An Yang, Cheng-Yo Tan, Wan-Cyuan Fan, Cheng-Fu Yang, Meng-Lin Wu, Yu-Chiang Frank Wang","본 논문에서는 의미론적으로 실용적인 콘텐츠를 생성하여 이미지를 완성하는 의미론 기반 이미지 아웃페인팅 작업을 다룹니다. 대부분의 기존 이미지 아웃페인팅 작업과 달리 장면 그래프 수준에서 이미지 의미를 이해하고 완성하여 위 작업에 접근합니다. 특히, 우리는 연관된 구조 정보를 모델링하기 위한 입력으로 노드와 에지 특징을 취하도록 설계된 SGT(Scene Graph Transformer)의 새로운 네트워크를 제안합니다. 그래프 기반 입력을 더 잘 이해하고 처리하기 위해 SGT는 노드 및 에지 수준 모두에서 기능 주의를 고유하게 수행합니다. 전자는 에지를 관계 정규화로 보는 반면, 후자는 주의 프로세스를 안내하기 위해 노드의 동시 발생을 관찰합니다. 우리는 레이아웃과 장면 그래프가 포함된 부분 입력 이미지가 주어지면 장면 그래프 확장과 전체 레이아웃으로의 변환에 SGT를 적용할 수 있음을 보여줍니다. 최첨단 레이아웃-이미지 변환 작업에 이어, 이미지 아웃페인팅 작업은 충분하고 실용적인 의미론을 도입하여 완료될 수 있습니다. MS-COCO 및 Visual Genome의 데이터 세트에 대해 광범위한 실험이 수행되어 제안된 SGT 및 아웃페인팅 프레임워크의 효율성을 정량적, 질적으로 확인합니다."
280,http://arxiv.org/abs/2205.01316 ,HL-Net: Heterophily Learning Network for Scene Graph Generation,"Xin Lin, Changxing Ding, Yibing Zhan, Zijian Li, Dacheng Tao","SGG(장면 그래프 생성)는 객체를 감지하고 이미지 내 쌍별 관계를 예측하는 것을 목표로 합니다. 현재 SGG 방법은 일반적으로 그래프 신경망(GNN)을 활용하여 객체/관계 간의 컨텍스트 정보를 획득합니다. 그러나 효율성에도 불구하고 현재 SGG 방법은 이종성을 무시하고 장면 그래프의 동질성만을 가정합니다. 이에 본 논문에서는 장면 그래프에서 객체/관계 간의 동질성과 이질성을 종합적으로 탐색하기 위한 새로운 이종성 학습 네트워크(HL-Net)를 제안합니다. 보다 구체적으로 HL-Net은 다음과 같이 구성됩니다. 1) 객체의 이종성과 동질성을 모두 활용하기 위해 다양한 계층의 정보를 적응적으로 통합하는 적응형 재가중 변환기 모듈; 2) 관계 표현을 개선하기 위해 이종애를 고려하여 관계 간의 연결을 효율적으로 탐색하는 관계 특성 전파 모듈; 3) 객체/관계 간의 이종성과 동질성을 더욱 구별하여 그래프에서 개선된 메시지 전달을 촉진하는 이종성을 인식하는 메시지 전달 방식. 우리는 VG(Visual Genome)와 OI(Open Images)라는 두 가지 공개 데이터 세트에 대해 광범위한 실험을 수행했습니다. 실험 결과는 우리가 제안한 HL-Net이 기존의 최첨단 접근 방식보다 우수함을 보여줍니다. 좀 더 자세히 살펴보면 HL-Net은 장면 그래프 분류에 대한 VG 데이터 세트에서 2.1$\%$, 최종 점수에 대한 IO 데이터 세트에서 1.2$\%$만큼 2위 경쟁사보다 우수한 성능을 보입니다. 코드는 https://github.com/siml3/HL-Net에서 확인할 수 있습니다."
279,http://arxiv.org/abs/2205.01297 ,RU-Net: Regularized Unrolling Network for Scene Graph Generation,"Xin Lin, Changxing Ding, Jing Zhang, Yibing Zhan, Dacheng Tao","SGG(장면 그래프 생성)는 객체를 감지하고 각 객체 쌍 간의 관계를 예측하는 것을 목표로 합니다. 기존 SGG 방법은 일반적으로 1) 그래프 신경망 기반 메시지 전달(GMP) 모듈이 가짜 노드 간 상관 관계에 민감하기 때문에 모호한 개체 표현, 2) 심각한 클래스 불균형 및 많은 수의 누락된 주석으로 인해 관계 예측의 다양성이 낮은 등 여러 문제를 겪고 있습니다. 두 가지 문제를 모두 해결하기 위해 본 논문에서는 정규화된 언롤링 네트워크(RU-Net)를 제안합니다. 먼저 언롤링 기법의 관점에서 GMP와 GLD(Graph Laplacian Denoising) 사이의 관계를 연구하여 GMP가 GLD의 솔버로 공식화될 수 있음을 결정합니다. 이러한 관찰을 바탕으로 우리는 언롤링된 메시지 전달 모듈을 제안하고 $\ell_p$ 기반 그래프 정규화를 도입하여 노드 간의 가짜 연결을 억제합니다. 둘째, 순위 최대화를 통해 관계의 예측 다양성을 촉진하는 그룹 다양성 향상 모듈을 제안합니다. 체계적인 실험은 RU-Net이 다양한 설정 및 측정 기준에서 효과적이라는 것을 보여줍니다. 또한 RU-Net은 VG, VRD 및 OI의 세 가지 인기 데이터베이스에서 새로운 최첨단 기술을 달성합니다. 코드는 https://github.com/siml3/RU-Net에서 확인할 수 있습니다."
278,http://arxiv.org/abs/2204.11143 ,Supplementing Missing Visions via Dialog for Scene Graph Generations,"Zhenghao Zhao, Ye Zhu, Xiaoguang Zhu, Yuzhang Shang, Yan Yan","대부분의 최신 AI 시스템은 입력된 시각적 데이터가 다양한 컴퓨터 비전 작업에서 경쟁력 있는 성능을 달성하기에 충분하다는 전제에 의존합니다. 그러나 고전적인 작업 설정은 다양한 이유(예: 제한된 보기 범위 및 폐색)로 인해 완전한 시각적 데이터에 액세스할 수 없는 까다롭지만 일반적인 실제 상황을 거의 고려하지 않습니다. 이를 위해 불완전한 시각적 입력 데이터가 있는 컴퓨터 비전 작업 설정을 조사합니다. 특히 다양한 수준의 시각적 데이터 누락이 있는 장면 그래프 생성(SGG) 작업을 입력으로 활용합니다. 시각적 입력이 부족하면 직관적으로 성능 저하가 발생하지만, 작업 목표를 더 잘 달성하기 위해 자연어 대화 상호 작용을 통해 누락된 비전을 보완할 것을 제안합니다. 우리는 대부분의 기존 모델과 공동으로 학습할 수 있는 모델 독립적인 SI-Dial(보충 대화형 대화 상자) 프레임워크를 설계하여 현재 AI 시스템에 자연 언어로 질문 답변 상호 작용 기능을 부여합니다. 우리는 여러 기준에 걸쳐 유망한 성능 향상을 달성함으로써 광범위한 실험과 분석을 통해 시각적 입력이 누락된 작업 설정의 타당성과 보충 정보 소스로서 제안된 대화 모듈의 효율성을 보여줍니다."
277,http://arxiv.org/abs/2204.08123 ,Non-Parallel Text Style Transfer with Self-Parallel Supervision,"Ruibo Liu, Chongyang Gao, Chenyan Jia, Guangxuan Xu, Soroush Vosoughi","기존 텍스트 스타일 전송 모델의 성능은 모델이 훈련되는 비병렬 데이터세트로 인해 심각하게 제한됩니다. 비병렬 데이터세트에서는 소스 스타일과 대상 스타일의 문장 사이에 직접적인 매핑이 존재하지 않습니다. 따라서 스타일 전달 모델은 훈련 중에 대상 문장에 대한 약한 감독만 받게 되며, 이로 인해 모델이 스타일 독립적인 정보를 너무 많이 버리거나 스타일 전달에 완전히 실패하게 되는 경우가 많습니다. 본 연구에서는 대규모 언어 모델을 기반으로 한 새로운 텍스트 스타일 전송 프레임워크인 LaMer를 제안합니다. LaMer는 먼저 장면 그래프를 사용하여 비병렬 데이터세트에서 대략적인 병렬식을 마이닝한 다음 MLE 교육을 사용하고 모방 학습 개선을 통해 데이터 내의 본질적인 병렬성을 활용합니다. 두 가지 벤치마크 작업(감정 및 형식 전송)과 새로 제안된 도전적인 작업(정치적 입장 전송)에서 우리 모델은 전송 정확성, 콘텐츠 보존 및 유창성에서 질적 향상을 달성합니다. 추가적인 경험적 및 인간적 평가는 우리 모델이 훈련을 더욱 효율적으로 만들 뿐만 아니라 이전 모델보다 더 읽기 쉽고 다양한 표현을 생성한다는 것을 보여줍니다."
276,http://arxiv.org/abs/2204.02597 ,Fine-Grained Predicates Learning for Scene Graph Generation,"Xinyu Lyu, Lianli Gao, Yuyu Guo, Zhou Zhao, Hao Huang, Heng Tao Shen, Jingkuan Song","현재 장면 그래프 생성 모델의 성능은 구별하기 어려운 일부 조건자(예: ""여자가 위에 서 있음/해변을 걷고 있음"" 또는 ""여자가 근처에 있음/아이를 바라보고 있음"")로 인해 심각하게 저하됩니다. 일반 SGG 모델은 머리 술어를 예측하는 경향이 있고 기존 재조정 전략은 꼬리 범주를 선호하지만, 이들 중 어느 것도 구별하기 어려운 술어를 적절하게 처리할 수 없습니다. 이 문제를 해결하기 위해 구별하기 어려운 객체 클래스를 구별하는 데 초점을 맞춘 세분화된 이미지 분류에서 영감을 받아 장면 그래프 생성 작업을 위해 구별하기 어려운 조건자를 구별하는 것을 목표로 하는 FGPL(Fine-Grained Predicates Learning)이라는 방법을 제안합니다. 특히 SGG 모델이 세분화된 조건자 쌍을 파악하는 데 도움이 되는 조건자 격자(Predicate Lattice)를 먼저 소개합니다. 그런 다음 Predicate Lattice를 활용하여 인식 가능한 항목에 대해 학습된 판별력을 유지하면서 세분화된 술어를 구별하는 데 기여하는 Category Discriminating Loss 및 Entity Discriminating Loss를 제안합니다. 제안된 모델 독립적 전략은 술어 분류 하위 작업에서 평균 재현율(mR@100)의 22.8\%, 24.1\% 및 21.7\%만큼 세 가지 벤치마크 모델(Transformer, VCTree 및 Motif)의 성능을 각각 크게 향상시킵니다. 또한 우리 모델은 Visual Genome 데이터 세트에서 최첨단 방법보다 큰 차이(예: 평균 재현율(mR@100)의 6.1\%, 4.6\% 및 3.2\%)로 성능이 뛰어납니다."
275,http://arxiv.org/abs/2204.02380 ,CLEVR-X: A Visual Reasoning Dataset for Natural Language Explanations,"Leonard Salewski, A. Sophia Koepke, Hendrik P. A. Lensch, Zeynep Akata",VQA(Visual Question Answering)의 맥락에서 설명을 제공하는 것은 머신러닝의 근본적인 문제를 제시합니다. VQA에 대한 자연어 설명을 생성하는 프로세스에 대한 자세한 통찰력을 얻기 위해 CLEVR 데이터세트를 자연어 설명으로 확장하는 대규모 CLEVR-X 데이터세트를 소개합니다. CLEVR 데이터 세트의 각 이미지-질문 쌍에 대해 CLEVR-X에는 원본 장면 그래프에서 파생된 여러 구조화된 텍스트 설명이 포함되어 있습니다. 구성상 CLEVR-X 설명은 정확하며 주어진 질문에 대답하는 데 필요한 추론과 시각적 정보를 설명합니다. 우리는 제안된 데이터 세트의 실측 설명이 실제로 완전하고 관련성이 있는지 확인하기 위해 사용자 연구를 수행했습니다. 우리는 CLEVR-X 데이터세트의 두 가지 최첨단 프레임워크를 사용하여 VQA 맥락에서 자연어 설명을 생성하기 위한 기본 결과를 제시합니다. 또한 다양한 질문 및 답변 유형에 대한 설명 생성 품질에 대한 자세한 분석을 제공합니다. 또한 자연어 생성(NLG) 측정항목의 수렴에 대한 다양한 수의 실측 설명 사용이 미치는 영향을 연구합니다. CLEVR-X 데이터 세트는 \url{https://explainableml.github.io/CLEVR-X/}에서 공개적으로 제공됩니다.
274,http://arxiv.org/abs/2203.14260 ,Unsupervised Vision-Language Parsing: Seamlessly Bridging Visual Scene Graphs with Language Structures via Dependency Relationships,"Chao Lou, Wenjuan Han, Yuhuan Lin, Zilong Zheng","언어 설명과 함께 사실적인 시각적 장면 이미지를 이해하는 것은 일반적인 시각적 이해를 위한 기본 작업입니다. 이전 연구에서는 시각적 장면(예: 장면 그래프)과 자연어(예: 종속성 트리)에 대한 계층 구조를 개별적으로 구축하여 강력하고 포괄적인 결과를 보여주었습니다. 그러나 공동 비전 언어(VL) 구조를 구성하는 방법은 거의 연구되지 않았습니다. 더 도전적이지만 가치 있는, ​​우리는 감독되지 않은 방식으로 이러한 공동 VL 구조를 유도하는 것을 목표로 하는 새로운 작업을 소개합니다. 우리의 목표는 시각적 장면 그래프와 언어적 종속성 트리를 원활하게 연결하는 것입니다. VL 구조 데이터가 부족하기 때문에 새로운 데이터 세트 VLParse를 구축하는 것부터 시작합니다. 노동 집약적인 라벨링을 처음부터 사용하는 대신, 우리는 거친 구조를 생성하기 위한 자동 정렬 절차와 인간의 정제를 통해 고품질 구조를 생성하는 방법을 제안합니다. 또한 Vision-Language Graph Autoencoder의 약자인 대조 학습(CL) 기반 프레임워크 VLGAE를 제안하여 데이터 세트를 벤치마킹합니다. 우리 모델은 두 가지 파생 작업, 즉 언어 문법 유도 및 VL 구문 접지에서 우수한 성능을 얻습니다. 절제는 세분화된 VL 구조 구성에 대한 시각적 단서와 종속 관계 모두의 효율성을 보여줍니다."
273,http://arxiv.org/abs/2203.12849 ,Complex Scene Image Editing by Scene Graph Comprehension,"Zhongping Zhang, Huiwen He, Bryan A. Plummer, Zhenyu Liao, Huayan Wang","조건부 확산 모델은 텍스트 기반 의미 이미지 편집과 같은 다양한 작업에서 인상적인 성능을 보여주었습니다. 이전 작업에서는 이미지 영역을 인간 사용자가 수동으로 식별하거나 객체 중심 조작에만 잘 수행되는 객체 감지기를 사용해야 합니다. 예를 들어, 입력 이미지에 의미론적 의미가 동일한 여러 개체(예: 새 그룹)가 포함된 경우 개체 감지기는 대상 개체를 정확하게 조작하는 것은 물론이고 대상 개체를 인식하고 위치를 파악하는 데 어려움을 겪을 수 있습니다. 이러한 문제를 해결하기 위해 우리는 SGC-Net(Scene Graph Comprehension)을 통해 복잡한 장면 이미지 편집을 달성하기 위한 2단계 방법을 제안합니다. 첫 번째 단계에서는 장면 그래프를 사용하고 대상 객체의 위치를 ​​예측하는 RoI(관심 영역) 예측 네트워크를 훈련합니다. 객체 카테고리에만 기반한 객체 감지 방법과 달리, 우리의 방법은 복잡한 장면 내에서 객체와 의미 관계를 이해하여 대상 객체를 정확하게 인식할 수 있습니다. 두 번째 단계에서는 조건부 확산 모델을 사용하여 RoI 예측을 기반으로 이미지를 편집합니다. 우리는 CLEVR 및 Visual Genome 데이터 세트에 대한 접근 방식의 효율성을 평가합니다. 우리는 CLEVR의 SSIM에서 8포인트 개선을 보고했으며 편집된 이미지는 Visual Genome의 이전 작업보다 인간 사용자가 9~33% 더 선호하여 제안된 방법의 효율성을 검증했습니다. 코드는 github.com/Zhongping-Zhang/SGC_Net에서 확인할 수 있습니다."
272,http://arxiv.org/abs/2203.11937 ,4D-OR: Semantic Scene Graphs for OR Domain Modeling,"Ege Özsoy, Evin Pınar Örnek, Ulrich Eck, Tobias Czempiel, Federico Tombari, Nassir Navab","수술 절차는 다양한 행위자, 장치 및 상호 작용으로 구성된 매우 복잡한 수술실(OR)에서 수행됩니다. 현재까지 의학적으로 훈련받은 인간 전문가만이 이러한 까다로운 환경에서 모든 연결과 상호 작용을 이해할 수 있습니다. 이 문서는 커뮤니티가 OR 도메인의 자동화되고 전체적이며 의미론적인 이해와 모델링에 한 단계 더 가까워지도록 하는 것을 목표로 합니다. 이 목표를 위해 처음으로 SSG(의미론적 장면 그래프)를 사용하여 수술 장면을 설명하고 요약할 것을 제안합니다. 장면 그래프의 노드는 의료진, 환자, 의료 장비 등 방에 있는 다양한 행위자와 개체를 나타내고, 가장자리는 이들 간의 관계를 나타냅니다. 제안된 표현의 가능성을 검증하기 위해 우리는 실제 OR 시뮬레이션 센터에서 6개의 RGB-D 센서로 기록된 10개의 시뮬레이션된 전체 무릎 교체 수술을 포함하는 최초의 공개적으로 사용 가능한 4D 수술 SSG 데이터 세트인 4D-OR을 생성했습니다. 4D-OR에는 6734개의 프레임이 포함되어 있으며 SSG, 인간 및 물체 포즈, 임상 역할에 대한 풍부한 주석이 달려 있습니다. 우리는 실제로 OR에서 의미론적 추론을 추론할 수 있는 0.75 매크로 F1의 성공률을 갖춘 종단 간 신경망 기반 SSG 생성 파이프라인을 제안합니다. 우리는 0.85 매크로 F1을 달성하는 임상 역할 예측 문제에 장면 그래프를 사용하여 장면 그래프의 표현력을 추가로 보여줍니다. 코드와 데이터 세트는 수락 시 제공됩니다."
271,http://arxiv.org/abs/2203.11654 ,Fine-Grained Scene Graph Generation with Data Transfer,"Ao Zhang, Yuan Yao, Qianyu Chen, Wei Ji, Zhiyuan Liu, Maosong Sun, Tat-Seng Chua","SGG(장면 그래프 생성)는 이미지에서 삼중항(주어, 술어, 목적어)을 추출하도록 설계되었습니다. 최근 작업은 SGG에서 꾸준한 진전을 이루었으며 높은 수준의 시각 및 언어 이해를 위한 유용한 도구를 제공합니다. 그러나 롱테일 분포 및 의미 모호성을 포함한 데이터 분포 문제로 인해 현재 SGG 모델의 예측은 자주 발생하지만 정보가 없는 몇 가지 조건부(예: on, at)로 붕괴되는 경향이 있으며, 이는 다운스트림 작업에서 이러한 모델의 실제 적용을 제한합니다. 위의 문제를 해결하기 위해 플러그 앤 플레이 방식으로 적용할 수 있고 1,807개의 조건자 클래스가 있는 대규모 SGG로 확장할 수 있는 새로운 내부 및 외부 데이터 전송(IETrans) 방법을 제안합니다. 우리의 IETrans는 모든 조건자에 대해 더 충분하고 일관된 주석을 제공하는 향상된 데이터 세트를 자동으로 생성하여 데이터 배포 문제를 완화하려고 합니다. Neural Motif 모델은 강화된 데이터 세트에 대한 교육을 통해 경쟁력 있는 마이크로 성능을 유지하면서 매크로 성능을 두 배로 늘립니다. 코드와 데이터는 https://github.com/waxnkw/IETrans-SGG.pytorch에서 공개적으로 제공됩니다."
270,http://arxiv.org/abs/2203.10569 ,Self-supervised Point Cloud Completion on Real Traffic Scenes via Scene-concerned Bottom-up Mechanism,"Yiming Ren, Peishan Cong, Xinge Zhu, Yuexin Ma","실제 스캔은 자체 폐색, 외부 폐색 및 제한된 센서 해상도로 인해 항상 개체의 부분적인 형상을 놓칩니다. 포인트 클라우드 완성은 객체의 불완전한 3D 스캔을 위해 완전한 모양을 참조하는 것을 목표로 합니다. 현재의 딥 러닝 기반 접근 방식은 일반적으로 합성 데이터 세트에서 얻은 훈련 프로세스의 대규모 완전한 형태에 의존합니다. 도메인 차이로 인해 실제 스캔에는 적용할 수 없습니다. 본 논문에서는 완전한 데이터가 없는 실제 교통 상황의 차량에 대한 자체 감독 포인트 클라우드 완성 방법(TraPCC)을 제안합니다. 차량의 대칭성과 유사성을 기반으로 연속적인 포인트 클라우드 프레임을 활용하여 차량 메모리 뱅크를 참조로 구성합니다. 우리는 입력의 로컬 형상 세부 사항과 전역 모양 기능 모두에 초점을 맞춘 상향식 메커니즘을 설계합니다. 또한, 주변 차량의 도움을 받아 누락된 부품에 주의할 수 있도록 네트워크에 장면 그래프를 설계합니다. 실험에 따르면 TraPCC는 훈련 시 완전한 데이터가 없어도 KITTI 및 nuScenes 트래픽 데이터 세트에서 실제 스캔 완료에 대해 우수한 성능을 달성하는 것으로 나타났습니다. 또한 완성 접근 방식의 이점을 활용하는 3D 감지의 다운스트림 애플리케이션도 보여줍니다."
269,http://arxiv.org/abs/2203.10424 ,MetaOnce: A Metaverse Framework Based on Multi-scene Relations and Entity-relation-event Game,Hongyin Zhu,"기존 메타버스 시스템에는 엔터티와 이벤트 간의 풍부한 관계 유형이 부족합니다. 문제는 풍부한 개념, 관계, 이벤트를 메타버스에 도입할 수 있는 이식 가능한 프레임워크가 없다는 것입니다. 본 논문에서는 새로운 메타버스 프레임워크인 MetaOnce를 소개합니다. 이 프레임워크는 다중 장면 그래프 구축을 제안합니다. 이 프레임워크는 단일 장면의 풍부한 관계를 설명할 뿐만 아니라 보다 포괄적인 분석 및 추론을 위해 여러 장면 그래프를 완전한 그래프로 결합합니다. 이전 소셜 네트워크 시스템은 주로 친구 관계를 설명했습니다. 그들은 메타버스 시스템과 기존 규칙 제약에 대한 엔터티-관계-이벤트 게임의 효과를 무시합니다. 우리는 규칙 컨트롤러를 제안하고 프레임워크가 규정을 준수하는 방식으로 작동할 수 있도록 관계에 제약을 가합니다. 우리는 프레임워크의 기능을 테스트하기 위해 메타버스 시스템을 구축했으며 실험 결과는 우리 프레임워크가 메모리 및 규칙 제약 조건을 갖춘 다중 장면 메타버스를 구축할 수 있음을 보여줍니다."
268,http://arxiv.org/abs/2203.10202 ,Relationformer: A Unified Framework for Image-to-Graph Generation,"Suprosanna Shit, Rajat Koner, Bastian Wittmann, Johannes Paetzold, Ivan Ezhov, Hongwei Li, Jiazhen Pan, Sahand Sharifzadeh, Georgios Kaissis, Volker Tresp, Bjoern Menze","이미지를 포괄적으로 표현하려면 특히 도로 네트워크 추출, 혈관 네트워크 추출 또는 장면 그래프 생성과 같은 이미지-그래프 생성에서 객체와 객체의 상호 관계에 대한 이해가 필요합니다. 전통적으로 이미지-그래프 생성은 객체 감지와 별도의 관계 예측으로 구성된 2단계 접근 방식으로 처리되어 동시 객체 관계 상호 작용을 방지합니다. 본 연구에서는 객체와 객체의 관계를 공동으로 예측하는 통합된 1단계 변환기 기반 프레임워크, 즉 Relationformer를 제안합니다. 우리는 직접적인 집합 기반 객체 예측을 활용하고 객체 간의 상호 작용을 통합하여 객체 관계 표현을 공동으로 학습합니다. 기존 [obj]-토큰 외에도 새로운 학습 가능 토큰, 즉 [rln]-토큰을 제안합니다. [obj]-토큰과 함께 [rln]-토큰은 일련의 상호 연관을 통해 이미지의 로컬 및 글로벌 의미 추론을 활용합니다. 쌍별 [obj] 토큰과 함께 [rln] 토큰은 계산적으로 효율적인 관계 예측에 기여합니다. 우리는 접근 방식의 효율성과 일반화 가능성을 입증하는 다양하고 다양한 다중 도메인 데이터 세트에서 최첨단 성능을 달성합니다."
267,http://arxiv.org/abs/2203.09811 ,Stacked Hybrid-Attention and Group Collaborative Learning for Unbiased Scene Graph Generation,"Xingning Dong, Tian Gan, Xuemeng Song, Jianlong Wu, Yuan Cheng, Liqiang Nie","일반적으로 일반 인코더-디코더 파이프라인을 따르는 장면 그래프 생성은 먼저 주어진 이미지 내의 시각적 콘텐츠를 인코딩한 다음 이를 간단한 요약 그래프로 구문 분석하는 것을 목표로 합니다. 기존 SGG 접근 방식은 일반적으로 시각과 언어 사이의 불충분한 양식 융합을 무시할 뿐만 아니라 편향된 관계 예측으로 인해 유용한 술어를 제공하지 못하여 SGG의 실용성과는 거리가 멀습니다. 이를 위해 본 논문에서는 먼저 모드 내 개선과 모드 간 상호 작용을 촉진하여 인코더 역할을 하는 새로운 Stacked Hybrid-Attention 네트워크를 제시합니다. 그런 다음 디코더를 최적화하기 위한 혁신적인 그룹 협업 학습 전략을 고안합니다. 특히, 하나의 분류기의 인식 기능이 극도로 불균형한 데이터 세트로 제한된다는 관찰을 기반으로, 우리는 먼저 클래스의 다양한 하위 집합을 구별하는 데 전문적인 분류기 그룹을 배치한 다음 편향되지 않은 SGG를 촉진하기 위해 두 가지 측면에서 협력적으로 최적화합니다. VG 및 GQA 데이터 세트에 대해 수행된 실험은 편견 없는 측정 항목에서 새로운 최첨단 기술을 확립했을 뿐만 아니라 두 기준에 비해 성능이 거의 두 배 향상되었음을 보여줍니다."
266,http://arxiv.org/abs/2203.09160 ,Biasing Like Human: A Cognitive Bias Framework for Scene Graph Generation,"Xiaoguang Chang, Teng Wang, Changyin Sun, Wenzhe Cai","장면 그래프 생성은 특정 인식 패턴이 없기 때문에 정교한 작업입니다(예: ""바라보는""과 ""가까운""은 시각과 관련하여 눈에 띄는 차이가 없는 반면 ""가까운""은 형태가 다른 엔터티 간에 발생할 수 있습니다). 따라서 일부 장면 그래프 생성 방법은 변덕스러운 시각적 특징과 사소한 데이터 세트 주석으로 인해 가장 빈번하게 발생하는 관계 예측에 갇혀 있습니다. 따라서 최근 연구에서는 보다 유익한 장면 그래프를 위해 균형 예측에 대한 ""편향되지 않은"" 접근 방식을 강조했습니다. 그러나 수많은 대상들 사이의 관계에 대한 인간의 빠르고 정확한 판단은 순수한 시각보다는 '편향'(즉, 경험과 언어적 지식)에 기인해야 한다. ""인지 편향"" 메커니즘에서 영감을 받은 모델 기능을 향상시키기 위해 우리는 인간이 숨겨진 관계 패턴을 더 잘 발굴하고 시끄러운 시각적 전파를 완화하기 위해 비전 기반 표현의 지침으로 레이블 언어 기능을 통합하는 방법을 시뮬레이션하는 새로운 3-패러다임 프레임워크를 제안합니다. 우리의 프레임워크는 모든 장면 그래프 모델에 대해 모델에 구애받지 않습니다. 포괄적인 실험을 통해 프레임워크가 최소 매개변수 증분으로 여러 측정항목에서 기본 모듈보다 성능이 우수하고 Visual Genome 데이터세트에서 새로운 SOTA 성능을 달성했음을 입증했습니다."
265,http://arxiv.org/abs/2203.07600 ,Procedural Text Understanding via Scene-Wise Evolution,"Jialong Tang, Hongyu Lin, Meng Liao, Yaojie Lu, Xianpei Han, Le Sun, Weijian Xie, Jin Xu","절차적 텍스트 이해를 위해서는 기계가 동적 내러티브 내에서 엔터티 상태를 추론해야 합니다. 현재의 절차적 텍스트 이해 접근 방식은 일반적으로 \textbf{entity-wise}이며, 이는 각 엔터티를 개별적으로 추적하고 각 엔터티의 다양한 상태를 독립적으로 예측합니다. 이러한 엔터티별 패러다임은 엔터티와 해당 상태 간의 상호 작용을 고려하지 않습니다. 본 논문에서는 모든 개체의 상태를 장면별로 공동 추적하는 절차적 텍스트 이해를 위한 새로운 \textbf{scene-wise} 패러다임을 제안합니다. 이 패러다임을 기반으로 우리는 \textbf{S}cene \textbf{G}raph \textbf{R}easoner(\textbf{SGR})를 제안합니다. 이는 동적으로 진화하는 일련의 장면 그래프를 도입하여 내러티브 전반에 걸쳐 개체, 상태 및 해당 연관성의 진화를 공동으로 공식화합니다. 이러한 방식으로 모든 엔터티와 상태 간의 깊은 상호 작용을 공동으로 캡처하고 장면 그래프에서 동시에 파생할 수 있습니다. 실험에 따르면 SGR은 새로운 최첨단 성능을 달성할 뿐만 아니라 추론 속도도 크게 가속화하는 것으로 나타났습니다."
264,http://arxiv.org/abs/2203.06907 ,Hierarchical Memory Learning for Fine-Grained Scene Graph Generation,"Youming Deng, Yansheng Li, Yongjun Zhang, Xiang Xiang, Jian Wang, Jingdong Chen, Jiayi Ma","장면 그래프 생성(SGG)의 경우 크라우드 소스 라벨링으로 인해 데이터 세트에 거친 조건과 미세한 조건이 혼합되고 롱테일 문제도 뚜렷합니다. 이러한 까다로운 상황을 감안할 때 기존의 많은 SGG 방법은 술어를 동일하게 취급하고 한 단계에서 혼합 세분성 술어의 감독하에 모델을 학습하므로 상대적으로 거친 예측이 이루어집니다. 차선의 혼합 입도 주석 및 롱테일 효과 문제의 부정적인 영향을 완화하기 위해 본 논문에서는 인간의 계층적 기억 학습 과정과 유사한 단순한 것부터 복잡한 것까지 모델을 학습하는 새로운 계층적 기억 학습(HML) 프레임워크를 제안합니다. 거친 술어와 미세한 술어를 자율적으로 분할한 후 모델은 먼저 거친 술어에 대해 학습한 다음 미세한 술어를 학습합니다. 이러한 계층적 학습 패턴을 실현하기 위해 본 논문에서는 처음으로 새로운 개념 재구성(CR) 및 모델 재구성(MR) 제약 조건을 사용하여 HML 프레임워크를 공식화합니다. HML 프레임워크는 다양한 SGG 모델을 개선하기 위한 하나의 일반적인 최적화 전략으로 사용될 수 있으며 SGG 벤치마크(즉, Visual Genome)에서 상당한 개선이 달성될 수 있다는 점은 주목할 가치가 있습니다."
263,http://arxiv.org/abs/2203.05380 ,Spatial Commonsense Graph for Object Localisation in Partial Scenes,"Francesco Giuliari, Geri Skenderi, Marco Cristani, Yiming Wang, Alessio Del Bue","우리는 장면의 부분 3D 스캔을 통해 물체의 알 수 없는 위치(예: 가방은 어디에 있습니까?)를 추정하는 새로운 문제인 부분 장면의 물체 위치 파악을 해결합니다. 제안된 솔루션은 새로운 장면 그래프 모델인 공간 상식 그래프(SCG)를 기반으로 합니다. 여기서 객체는 노드이고 가장자리는 상식 지식 베이스의 개념 노드와 관계로 강화되어 객체 사이의 쌍별 거리를 정의합니다. 이를 통해 SCG는 알려지지 않은 3D 장면에 대한 공간 추론을 더 잘 일반화할 수 있습니다. SCG는 두 단계로 대상 객체의 알려지지 않은 위치를 추정하는 데 사용됩니다. 먼저 SCG를 새로운 근접 예측 네트워크(Proximity Prediction Network)에 공급합니다. 이 그래프 신경망은 주의를 사용하여 대상 객체를 나타내는 노드와 SCG에서 관찰된 객체를 나타내는 노드 사이의 거리 예측을 수행합니다. 둘째, 우리는 참조 시스템으로부터 독립하기 위해 예측된 모든 쌍별 거리를 사용하여 객체 위치를 추정하기 위해 원형 교차점을 기반으로 하는 위치 파악 모듈을 제안합니다. 우리는 제안된 방법이 최고의 위치 파악 성능을 달성하는 부분 장면에서 객체 위치 파악을 위한 방법과 기준선을 벤치마킹하기 위해 부분적으로 재구성된 장면의 새로운 데이터 세트를 생성합니다."
262,http://arxiv.org/abs/2203.00600 ,Dual Embodied-Symbolic Concept Representations for Deep Learning,Daniel T. Chang,"인지 신경 과학의 최근 연구 결과에 힘입어 우리는 개념 표현을 위한 이중 수준 모델의 사용을 옹호합니다. 구체화된 수준은 개념 지향 특징 표현으로 구성되고 기호 수준은 개념 그래프로 구성됩니다. 구체화된 개념 표현은 양식에 따라 다르며 특징 공간에서 특징 벡터의 형태로 존재합니다. 반면에 상징적 개념 표현은 모달적이고 언어별로 다르며 개념/지식 공간에 단어/지식-그래프 임베딩 형태로 존재합니다. 인간의 개념 체계는 구체화된 표현과 상징적 표현으로 구성되며, 이는 일반적으로 개념 처리를 추진하기 위해 상호 작용합니다. 따라서 우리는 딥러닝을 위한 이중 구현 기호 개념 표현의 사용을 더욱 옹호합니다. 그 사용법과 가치를 보여주기 위해 우리는 두 가지 중요한 사용 사례, 즉 소수 클래스 증분 학습을 위한 구체화된 기호 지식 증류와 이미지-텍스트 매칭을 위한 구체화된 기호 융합 표현을 논의합니다. 이중 구체화된 기호 개념 표현은 딥 러닝 및 기호 AI 통합의 기초입니다. 우리는 그러한 통합의 두 가지 중요한 예, 즉 지식 그래프 브리징을 통한 장면 그래프 생성과 다중 모드 지식 그래프를 논의합니다."
261,http://arxiv.org/abs/2202.12912 ,"SGL: Symbolic Goal Learning in a Hybrid, Modular Framework for Human Instruction Following","Ruinian Xu, Hongyi Chen, Yunzhi Lin, Patricio A. Vela","본 논문에서는 모호한 요청을 통해 인간의 지시에 기반한 로봇 조작을 연구합니다. 시각적 관찰을 통해 불완전한 자연어를 보완하려는 의도입니다. 수동으로 정의된 기호를 기반으로 하는 초기 기호 방법은 자연어 요청에서 일련의 작업을 생성하기 위한 의미론적 구문 분석 및 작업 계획으로 구성된 모듈식 프레임워크를 구축했습니다. 현대의 연결주의 방법은 심층 신경망을 사용하여 시각적, 언어적 특징을 자동으로 학습하고 엔드투엔드 방식으로 일련의 하위 수준 작업에 매핑합니다. 이 두 가지 접근 방식을 혼합하여 하이브리드 모듈식 프레임워크를 만듭니다. 이는 심층 신경망을 통한 상징적 목표 학습과 기호 계획자를 통한 작업 계획으로 지침을 공식화합니다. 연결주의자 및 기호 모듈은 계획 도메인 정의 언어와 연결됩니다. 비전 및 언어 학습 네트워크는 작업 완료 동작 시퀀스를 생성하기 위해 플래너에게 전송되는 목표 표현을 예측합니다. 자연어의 유연성을 향상시키기 위해 우리는 명시적인 인간 지시와 함께 암시적인 인간 의도를 추가로 통합합니다. 비전과 언어에 대한 일반적인 기능을 학습하기 위해 장면 그래프 구문 분석 및 의미론적 텍스트 유사성 작업에 대한 비전 및 언어 인코더를 별도로 사전 학습할 것을 제안합니다. 벤치마킹은 비전 및 언어 학습 모델의 다양한 구성 요소 또는 옵션의 영향을 평가하고 사전 훈련 전략의 효과를 보여줍니다. 시뮬레이터 AI2THOR에서 수행된 조작 실험은 새로운 시나리오에 대한 프레임워크의 견고성을 보여줍니다."
260,http://arxiv.org/abs/2202.12197 ,Situational Graphs for Robot Navigation in Structured Indoor Environments,"Hriday Bavle, Jose Luis Sanchez-Lopez, Muhammad Shaheer, Javier Civera, Holger Voos","모바일 로봇은 실제 환경에서 지능적인 결정을 내리고 자율적으로 작업을 성공적으로 수행하기 위해 주변 환경에 대한 깊은 이해와 자체 상태 추정으로 구성된 상황을 인식해야 합니다. 3D 장면 그래프는 기하학적, 의미적, 관계적/위상적 차원으로 구성된 공동 모델에서 환경을 표현하도록 제안하는 새로운 연구 분야입니다. 3D 장면 그래프는 이미 SLAM 기술과 결합되어 로봇에게 상황 이해를 제공하지만 이를 모바일 로봇에 효과적으로 배치하려면 여전히 추가 연구가 필요합니다.   이를 위해 우리는 이 논문에서 앞서 언급한 3차원 환경 표현과 로봇 자세를 최적화 가능한 단일 그래프로 결합한 새로운 실시간 온라인 구축 상황 그래프(S-Graph)를 제시합니다. 우리의 방법은 3D LiDAR 스캔에서 추출된 주행 거리 판독값과 평면 표면을 활용하여 (1) 로봇 포즈가 등록되는 로봇 추적 레이어, (2) 평면 벽과 같은 기능이 포함된 미터법 의미 레이어, (3) 복도 및 방과 같은 상위 수준 기능을 사용하여 평면 벽을 제한하는 새로운 토폴로지 레이어를 포함하는 3개 계층 S-Graph를 실시간으로 구성하고 최적화합니다. 우리의 제안은 로봇의 자세 추정을 위한 최첨단 결과를 보여줄 뿐만 아니라 환경의 미터법-의미론-토폴로지 모델에도 기여합니다."
259,http://arxiv.org/abs/2202.11337 ,Commonsense Reasoning for Identifying and Understanding the Implicit Need of Help and Synthesizing Assistive Actions,"Maëlic Neau, Paulo Santos, Anne-Gwenn Bosser, Nathan Beu, Cédric Buche","인간-로봇 상호작용(HRI)은 서비스 로봇공학의 새로운 하위 분야입니다. 대부분의 기존 접근 방식은 참여를 위해 명시적인 신호(예: 음성, 제스처)에 의존하지만, 현재 문헌에는 암시적인 사용자 요구 사항을 해결하는 솔루션이 부족합니다. 본 논문에서는 (a) 사용자의 암시적 도움 요구를 감지하고 (b) 사전 학습 없이 일련의 보조 동작을 생성하는 아키텍처를 제시합니다. 작업 (a)는 상식 지식의 사용과 결합된 장면 그래프 생성을 위한 최첨단 솔루션을 사용하여 수행됩니다. 반면, 작업 (b)는 추가적인 상식 지식과 그래프 구조에 대한 감정 분석을 사용하여 수행됩니다. 마지막으로 우리는 인간 실험과 함께 확립된 벤치마크(예: ActionGenome 데이터 세트)를 사용하여 솔루션 평가를 제안합니다. 우리 접근 방식의 주된 동기는 단일 아키텍처에 인식-결정-행동 루프를 내장하는 것입니다."
258,http://arxiv.org/abs/2202.10826 ,Relation Regularized Scene Graph Generation,"Yuyu Guo, Lianli Gao, Jingkuan Song, Peng Wang, Nicu Sebe, Heng Tao Shen, Xuelong Li","장면 그래프 생성(SGG)은 감지된 객체 위에 구축되어 이미지 콘텐츠 추상화를 설명하기 위한 객체 쌍별 시각적 관계를 예측합니다. 기존 연구에서는 객체 간의 링크를 사전 지식으로 제공하면 SGG의 성능이 크게 향상되는 것으로 나타났습니다. 이러한 관찰에 영감을 받아 이 기사에서는 두 객체 사이에 관계가 있는지 예측하고 이 관계를 객체 특징 개선 및 더 나은 SGG로 인코딩할 수 있는 관계 정규화 네트워크(R2-Net)를 제안합니다. 구체적으로, 우리는 먼저 두 객체 사이의 관계 확률을 나타내기 위해 감지된 객체들 사이에 선호도 매트릭스를 구성합니다. 그런 다음 이 관계 친화도 행렬에 대한 그래프 컨볼루션 네트워크(GCN)가 객체 인코더로 사용되어 객체의 관계 정규 표현을 생성합니다. 이러한 관계 정규화 기능을 통해 R2-Net은 객체 레이블을 효과적으로 개선하고 장면 그래프를 생성할 수 있습니다. 세 가지 SGG 작업(즉, 조건자 분류, 장면 그래프 분류 및 장면 그래프 감지)에 대한 시각적 게놈 데이터 세트에 대한 광범위한 실험이 수행되어 제안된 방법의 효율성을 입증합니다. 절제 연구는 또한 성능 개선에서 제안된 구성 요소의 핵심 역할을 검증합니다."
257,http://arxiv.org/abs/2202.10824 ,One-shot Scene Graph Generation,"Yuyu Guo, Jingkuan Song, Lianli Gao, Heng Tao Shen","이미지 콘텐츠의 구조화된 표현으로서 시각적 장면 그래프(시각적 관계)는 컴퓨터 비전과 자연어 처리 사이의 가교 역할을 합니다. 장면 그래프 생성 작업에 대한 기존 모델에는 수십 또는 수백 개의 레이블이 지정된 샘플이 필요한 것으로 악명이 높습니다. 대조적으로, 인간은 몇 가지 또는 심지어 하나의 예로부터 시각적 관계를 배울 수 있습니다. 이에 영감을 받아 우리는 One-Shot Scene Graph Generation이라는 작업을 설계합니다. 여기서 각 관계 삼중항(예: ""dog-has-head"")은 단 하나의 레이블이 지정된 예에서만 나옵니다. 핵심 통찰력은 처음부터 배우는 것보다 풍부한 사전 지식을 활용할 수 있다는 것입니다. 본 논문에서는 원샷 장면 그래프 생성 작업을 위한 다중 구조 지식(관계 지식과 상식 지식)을 제안합니다. 구체적으로, 관계형 지식은 시각적 콘텐츠에서 추출된 개체 간의 관계에 대한 사전 지식을 나타냅니다. 예를 들어 ""개""와 ""마당"" 사이에 ""서 있는"", ""앉아 있는"", ""눕는"" 시각적 관계가 존재할 수 있는 반면, 상식 지식은 ""개가 마당을 지킬 수 있다""와 같은 ""의미 만들기"" 지식을 인코딩합니다. 이 두 종류의 지식을 그래프 구조로 정리하여 그래프 컨볼루션 네트워크(GCN)를 사용하여 개체의 지식이 내장된 의미 특징을 추출합니다. 게다가 Faster R-CNN에 의해 ​​생성된 각 엔터티에서 격리된 시각적 특징을 추출하는 대신 Instance Relation Transformer 인코더를 활용하여 컨텍스트 정보를 완전히 탐색합니다. 구성된 일회성 데이터세트를 기반으로 한 실험 결과는 우리의 방법이 기존의 최첨단 방법보다 훨씬 뛰어난 성능을 보인다는 것을 보여줍니다. 절제 연구는 또한 인스턴스 관계 변환기 인코더와 다중 구조 지식의 효율성을 검증합니다."
256,http://arxiv.org/abs/2202.10432 ,Reasoning with Scene Graphs for Robot Planning under Partial Observability,"Saeid Amiri, Kishan Chandan, Shiqi Zhang","부분적으로 관찰 가능한 영역에서의 로봇 계획은 로봇이 현재 상태를 추정하고 동시에 조치를 계획해야 하기 때문에 어렵습니다. 도메인에 많은 객체가 포함되어 있으면 객체와 객체의 관계에 대한 추론으로 인해 로봇 계획이 더욱 어려워집니다. 본 논문에서는 로봇이 불확실성 하에서 장기적인 목표를 달성하기 위해 시각적 상황 정보를 바탕으로 추론할 수 있도록 하는 SARP(로봇 계획을 위한 장면 분석)라는 알고리즘을 개발합니다. SARP는 다양한 위치에서 캡처한 이미지와 이유를 사용하여 객체와 객체의 관계를 팩터링된 표현인 장면 그래프를 구성하여 부분 관찰 가능성 하에서 상황 인식 로봇 계획을 가능하게 합니다. 시뮬레이션에서는 여러 3D 환경을 활용하고, 실제 로봇이 수집한 데이터세트를 이용해 실험을 진행했습니다. 표준 로봇 계획 및 장면 분석 방법과 비교하여 대상 검색 영역에서 SARP는 작업 완료의 효율성과 정확성을 모두 향상시킵니다. 보충 자료는 https://tinyurl.com/sarp22에서 찾을 수 있습니다."
255,http://arxiv.org/abs/2202.10201 ,OG-SGG: Ontology-Guided Scene Graph Generation. A Case Study in Transfer Learning for Telepresence Robotics,"Fernando Amodeo, Fernando Caballero, Natalia Díaz-Rodríguez, Luis Merino","이미지에서 장면 그래프를 생성하는 것은 로봇공학과 같은 응용 분야에서 큰 관심을 끄는 작업입니다. 왜냐하면 그래프는 세상에 대한 지식을 표현하고 시각적 질문 응답(VQA)과 같은 작업에서 인간과 로봇의 상호 작용을 규제하는 주요 방법이기 때문입니다. 불행하게도 해당 기계 학습 영역은 아직 초기 단계에 있으며 현재 제공되는 솔루션은 구체적인 사용 시나리오에 특화되어 있지 않습니다. 특히 그들은 도메인 세계에 대한 기존의 ""전문"" 지식을 고려하지 않습니다. 그리고 이는 사용 사례 시나리오에서 요구하는 수준의 안정성을 제공하기 위해 실제로 필요할 수도 있습니다. 본 논문에서는 온톨로지 형태로 제공되는 사전 지식(구체적으로 정의된 공리 사용)을 사용하여 기존 기계 학습 기반 장면 그래프 생성기의 성능을 향상시킬 수 있는 온톨로지 기반 장면 그래프 생성(OG-SGG)이라는 프레임워크에 대한 초기 근사치를 제안합니다. 그리고 우리는 텔레프레즌스 로봇공학에 기반을 둔 특정 시나리오에 대해 평가된 결과를 제시합니다. 이러한 결과는 생성된 장면 그래프의 양적, 질적 향상을 보여줍니다."
254,http://arxiv.org/abs/2202.09459 ,Interactive Visual Pattern Search on Graph Data via Graph Representation Learning,"Huan Song, Zeng Dai, Panpan Xu, Liu Ren",그래프는 광범위한 도메인의 프로세스와 관계를 모델링하는 유비쿼터스 데이터 구조입니다. 예를 들면 프로그램의 제어 흐름 그래프와 이미지의 의미론적 장면 그래프가 있습니다. 그래프에서 하위 그래프 패턴을 식별하는 것은 구조적 특성을 이해하는 데 중요한 접근 방식입니다. 우리는 많은 개별 그래프가 포함된 데이터베이스에서 인간 참여형(Human-In-The-Loop) 예제 기반 하위 그래프 패턴 검색을 지원하는 시각적 분석 시스템 GraphQ를 제안합니다. 빠른 대화형 쿼리를 지원하기 위해 그래프 신경망(GNN)을 사용하여 그래프를 고정 길이 잠재 벡터 표현으로 인코딩하고 잠재 공간에서 하위 그래프 일치를 수행합니다. 문제의 복잡성으로 인해 시각화 및 해석에 중요한 일치 결과에서 정확한 일대일 노드 대응을 얻는 것은 여전히 ​​어렵습니다. 따라서 우리는 쿼리 결과를 쉽게 검증하고 해석할 수 있도록 NeuroAlign이라는 노드 정렬을 위한 새로운 GNN을 제안합니다. GraphQ는 쿼리 편집기와 결과의 다중 규모 시각화뿐만 아니라 추가 제약 조건으로 결과를 구체화하기 위한 사용자 피드백 메커니즘을 갖춘 시각적 쿼리 인터페이스를 제공합니다. 우리는 프로그램 워크플로에서 재사용 가능한 서브루틴을 분석하고 이미지에서 의미론적 장면 그래프 검색이라는 두 가지 사용 시나리오 예시를 통해 GraphQ를 보여줍니다. 정량적 실험에 따르면 NeuroAlign은 기준선 GNN에 비해 노드 정렬 정확도가 19~29% 향상되었으며 조합 알고리즘에 비해 최대 100배의 속도 향상을 제공하는 것으로 나타났습니다. 도메인 전문가와의 정성적 연구를 통해 두 가지 사용 시나리오 모두에 대한 효율성이 확인되었습니다.
253,http://arxiv.org/abs/2202.09277 ,(2.5+1)D Spatio-Temporal Scene Graphs for Video Question Answering,"Anoop Cherian, Chiori Hori, Tim K. Marks, Jonathan Le Roux","비디오 질문 답변(QA)과 같은 비디오 기반 추론 작업에 대한 시공간 장면 그래프 접근 방식은 일반적으로 모든 비디오 프레임에 대해 이러한 그래프를 구성합니다. 이러한 접근 방식은 비디오가 본질적으로 3D 공간에서 발생하는 이벤트의 2D ""뷰"" 시퀀스이며 따라서 3D 장면의 의미가 프레임 간에 전달될 수 있다는 사실을 종종 무시합니다. 이러한 통찰력을 활용하여 비디오 내부의 시공간 정보 흐름을 더 잘 포착하기 위해 (2.5+1)D 장면 그래프 표현을 제안합니다. 구체적으로, 먼저 기성 2D-3D 변환 모듈을 사용하여 모든 2D 프레임을 추론된 3D 구조로 변환하여 2.5D(의사 3D) 장면 그래프를 생성한 다음 비디오 프레임을 공유된 (2.5+1)D 시공간 공간에 등록하고 그 안에 각 2D 장면 그래프를 접지합니다. 그런 다음 이러한 (2.5+1)D 그래프는 정적 하위 그래프와 동적 하위 그래프로 분리되며, 이는 그 안에 있는 개체가 일반적으로 세계에서 움직이는지 여부에 따라 결정됩니다. 동적 그래프의 노드에는 다른 그래프 노드와의 상호 작용을 캡처하는 모션 기능이 풍부해졌습니다. 다음으로, 비디오 QA 작업을 위해 (2.5+1)D 그래프를 시공간 계층적 잠재 공간에 삽입하는 새로운 변환기 기반 추론 파이프라인을 제시합니다. 여기서 하위 그래프와 해당 상호 작용은 다양한 세분성으로 캡처됩니다. 우리 접근 방식의 효율성을 입증하기 위해 NExT-QA 및 AVSD-QA 데이터 세트에 대한 실험을 제시합니다. 우리의 결과는 우리가 제안한 (2.5+1)D 표현이 더 빠른 훈련과 추론으로 이어지는 반면, 우리의 계층적 모델은 최신 기술에 비해 비디오 QA 작업에서 우수한 성능을 보여줍니다."
252,http://arxiv.org/abs/2202.07271 ,Hyper-relationship Learning Network for Scene Graph Generation,"Yibing Zhan, Zhi Chen, Jun Yu, BaoSheng Yu, Dacheng Tao, Yong Luo","이미지에서 유익한 장면 그래프를 생성하려면 다양한 그래프 구성 요소(예: 개체 및 관계)를 통합하고 추론해야 합니다. 그러나 Unbiased SGG 방법을 포함한 현재의 SGG(Scene Graph Generation) 방법은 1) 관계 간 전이적 추론과 같은 높은 수준의 추론과 2) 그래프 구성 요소의 모든 상호 작용을 통합할 수 있는 효율적인 메커니즘이 부족하여 여전히 유익한 관계를 예측하는 데 어려움을 겪고 있습니다. 위에서 언급한 문제를 해결하기 위해 우리는 SGG를 위해 HLN이라는 초관계 학습 네트워크를 고안했습니다. 구체적으로, 제안된 HLN은 하이퍼그래프에서 유래하고 두 개의 그래프 주의 네트워크(GAT)는 관계를 추론하도록 설계되었습니다. 1) 객체와 관계 사이의 상호 작용을 탐색하기 위한 객체-관계 GAT 또는 OR-GAT, 2) 초관계의 추이 추론, 즉 추이적 추론을 위한 세 객체 간의 순차적 관계를 통합하기 위한 초관계 GAT 또는 HR-GAT. 결과적으로 HLN은 객체 상호 작용, 관계 상호 작용 및 초관계의 전이적 추론을 통합하고 추론하여 장면 그래프 생성 성능을 크게 향상시킵니다. 우리는 가장 널리 사용되는 SGG 데이터세트, 즉 Visual Genome 데이터세트에서 HLN을 평가하며, 실험 결과는 최신 방법에 비해 큰 우월성을 입증합니다. 예를 들어 제안된 HLN은 관계당 재현율을 11.3\%에서 13.1\%로 향상시키고, 이미지당 재현율을 19.8\%에서 34.9\%로 유지합니다. GitHub에 소스 코드와 사전 훈련된 모델을 공개할 예정입니다."
251,http://arxiv.org/abs/2201.13360 ,Hydra: A Real-time Spatial Perception System for 3D Scene Graph Construction and Optimization,"Nathan Hughes, Yun Chang, Luca Carlone","3D 장면 그래프는 최근 3D 환경의 강력한 상위 수준 표현으로 등장했습니다. 3D 장면 그래프는 노드가 여러 수준의 추상화에서 공간 개념을 나타내고 가장자리가 개념 간의 관계를 나타내는 계층형 그래프로 환경을 설명합니다. 3D 장면 그래프는 로봇을 위한 고급 ""정신 모델"" 역할을 할 수 있지만 실시간으로 이러한 풍부한 표현을 구축하는 방법은 아직 미지의 영역입니다. 본 논문에서는 센서 데이터로부터 실시간으로 3D 장면 그래프를 구축하는 알고리즘 모음인 실시간 공간 인식 시스템에 대해 설명합니다. 우리의 첫 번째 기여는 로봇이 환경을 탐색할 때 장면 그래프의 레이어를 점진적으로 구성하는 실시간 알고리즘을 개발하는 것입니다. 이러한 알고리즘은 현재 로봇 위치 주변에 로컬 유클리드 부호 거리 함수(ESDF)를 구축하고, ESDF에서 장소의 토폴로지 지도를 추출한 다음, 커뮤니티 감지 기술에서 영감을 얻은 접근 방식을 사용하여 장소를 방으로 분할합니다. 두 번째 기여는 3D 장면 그래프에서 루프 폐쇄 감지 및 최적화를 조사하는 것입니다. 우리는 3D 장면 그래프를 통해 루프 폐쇄 감지를 위한 계층적 설명자를 정의할 수 있음을 보여줍니다. 설명자는 낮은 수준의 시각적 모양부터 개체 및 장소에 대한 요약 통계에 이르기까지 장면 그래프의 여러 레이어에 걸쳐 통계를 캡처합니다. 그런 다음 루프 폐쇄에 응답하여 3D 장면 그래프를 최적화하는 첫 번째 알고리즘을 제안합니다. 우리의 접근 방식은 장면 그래프의 모든 레이어를 동시에 수정하기 위해 내장된 변형 그래프에 의존합니다. 우리는 제안된 공간 인식 시스템을 빠른 초기 및 중간 수준 인식 프로세스와 느린 상위 수준 인식 프로세스를 결합한 Hydra라는 아키텍처에 구현합니다. 우리는 시뮬레이션된 데이터와 실제 데이터를 바탕으로 Hydra를 평가하고 온라인 실행에도 불구하고 배치 오프라인 방법과 비슷한 정확도로 3D 장면 그래프를 재구성할 수 있음을 보여줍니다."
250,http://arxiv.org/abs/2201.11697 ,Constrained Structure Learning for Scene Graph Generation,"Daqi Liu, Miroslaw Bober, Josef Kittler","구조화된 예측 작업인 장면 그래프 생성은 입력 이미지에서 개체와 개체 관계를 명시적으로 모델링하기 위해 시각적 기반 장면 그래프를 구축하는 것을 목표로 합니다. 현재 평균 필드 변형 베이지안 프레임워크는 기존 방법에서 사용되는 사실상의 방법론으로, 제약 없는 추론 단계는 종종 메시지 전달 신경망에 의해 구현됩니다. 그러나 이러한 공식은 다른 추론 전략을 탐색하는 데 실패하고 보다 일반적인 제한 최적화 모델을 대부분 무시합니다. 본 논문에서는 명시적 제약 변이 추론 목적을 제안하는 제약 구조 학습 방법을 제시한다. 유비쿼터스 메시지 전달 전략을 적용하는 대신 일반적인 제한된 최적화 방법인 엔트로피 미러 하강을 사용하여 제한된 변형 추론 단계를 해결합니다. 우리는 다양한 인기 있는 장면 그래프 생성 벤치마크에서 제안된 일반 모델을 검증하고 최신 방법보다 성능이 우수하다는 것을 보여줍니다."
249,http://arxiv.org/abs/2201.11460 ,RelTR: Relation Transformer for Scene Graph Generation,"Yuren Cong, Michael Ying Yang, Bodo Rosenhahn",동일한 장면에 있는 서로 다른 개체는 서로 어느 정도 관련되어 있지만 이러한 관계 중 제한된 수만 주목할 만합니다. 객체 감지에 탁월한 DETR에서 영감을 받아 장면 그래프 생성을 설정된 예측 문제로 보고 인코더-디코더 아키텍처를 갖는 엔드 투 엔드 장면 그래프 생성 모델 RelTR을 제안합니다. 인코더는 시각적 특징 컨텍스트에 대해 추론하는 반면 디코더는 결합된 주제 및 객체 쿼리와 함께 다양한 유형의 주의 메커니즘을 사용하여 주제-술어-객체의 고정 크기 세트를 추론합니다. 우리는 엔드투엔드 훈련을 위해 실제값과 예측된 삼중항 간의 매칭을 수행하는 세트 예측 손실을 설계합니다. 기존 대부분의 장면 그래프 생성 방법과 달리 RelTR은 개체를 결합하고 가능한 모든 조건에 레이블을 지정하지 않고 시각적 외관만 사용하여 직접 관계 집합을 예측하는 1단계 방법입니다. Visual Genome 및 Open Images V6 데이터 세트에 대한 광범위한 실험은 우리 모델의 뛰어난 성능과 빠른 추론을 보여줍니다.
248,http://arxiv.org/abs/2201.06794 ,Resistance Training using Prior Bias: toward Unbiased Scene Graph Generation,"Chao Chen, Yibing Zhan, Baosheng Yu, Liu Liu, Yong Luo, Bo Du","SGG(장면 그래프 생성)는 객체와 쌍별 관계를 사용하여 장면의 구조화된 표현을 구축하는 것을 목표로 하며 이는 다운스트림 작업에 도움이 됩니다. 그러나 현재 SGG 방법은 일반적으로 훈련 데이터의 긴 꼬리 분포로 인해 최적이 아닌 장면 그래프 생성으로 어려움을 겪습니다. 이 문제를 해결하기 위해 우리는 장면 그래프 생성을 위해 사전 바이어스(RTPB)를 사용한 저항 훈련을 제안합니다. 특히 RTPB는 분산 기반 사전 편향을 사용하여 훈련 중 덜 빈번한 관계에 대한 모델의 감지 능력을 향상시켜 꼬리 범주에 대한 모델 일반화 가능성을 향상시킵니다. 또한 객체와 관계의 상황별 정보를 더 자세히 탐색하기 위해 DTrans(Dual Transformer)라고 하는 상황별 인코딩 백본 네트워크를 설계합니다. 우리는 편견 없는 장면 그래프 생성을 위한 방법의 효율성을 입증하기 위해 매우 인기 있는 벤치마크인 VG150에 대한 광범위한 실험을 수행합니다. 구체적으로, 우리의 RTPB는 현재 SGG 방법에 적용될 때 평균 재현율에서 10% 이상의 개선을 달성합니다. 또한 RTPB를 갖춘 DTrans는 큰 마진으로 거의 모든 최첨단 방법보다 성능이 뛰어납니다."
247,http://arxiv.org/abs/2201.05977 ,Lightweight Object-level Topological Semantic Mapping and Long-term Global Localization based on Graph Matching,"Fan Wang, Chaofan Zhang, Fulin Tang, Hongkui Jiang, Yihong Wu, Yong Liu","매핑과 위치 파악은 실제 응용 분야에서 모바일 로봇의 두 가지 필수 작업입니다. 그러나 대규모의 역동적인 장면은 최신 성숙한 솔루션의 정확성과 견고성에 도전합니다. 컴퓨팅 리소스가 제한되면 이러한 상황은 더욱 악화됩니다. 본 논문에서는 높은 정확도와 견고성을 갖춘 새로운 경량 객체 수준 매핑 및 위치 파악 방법을 제시합니다. 이전 방법과 달리 우리 방법은 사전에 구성된 정확한 기하학적 지도가 필요하지 않으므로 특히 대규모 탐색의 경우 저장 부담이 크게 줄어듭니다. 우리는 의미론적 정보와 기하학적 정보가 모두 포함된 객체 수준 기능을 사용하여 환경의 랜드마크를 모델링합니다. 특히, 객체 수준의 랜드마크를 효율적으로 획득하고 구성하기 위해 학습 토폴로지 기본 요소를 먼저 제안합니다. 이를 기반으로 로봇 중심의 매핑 프레임워크를 사용하여 환경을 의미론적 토폴로지 그래프로 표현함과 동시에 전역적 일관성을 유지해야 하는 부담을 완화합니다. 또한, 제한된 계산 자원으로 온라인 매핑의 효율성을 높이기 위해 계층적 메모리 관리 메커니즘이 도입되었습니다. 제안된 맵을 기반으로 새로운 로컬 의미론적 장면 그래프 디스크립터를 구성하고 다중 제약 조건 그래프 매칭을 수행하여 장면 유사성을 비교함으로써 강력한 위치 파악을 달성합니다. 마지막으로 저비용 임베디드 플랫폼에서 방법을 테스트하여 장점을 보여줍니다. 대규모 및 다중 세션 실제 환경에 대한 실험 결과는 제안된 방법이 경량 및 견고성 측면에서 최신 기술을 능가한다는 것을 보여줍니다."
246,http://arxiv.org/abs/2201.01901 ,Incremental Object Grounding Using Scene Graphs,"John Seon Keun Yi, Yoonwoo Kim, Sonia Chernova",객체 접지 작업은 언어적 의사소통을 통해 이미지에서 대상 객체를 찾는 것을 목표로 합니다. 인간의 명령을 이해하는 것은 효과적인 인간-로봇 커뮤니케이션을 위해 필요한 중요한 과정입니다. 그러나 인간의 명령은 모호하고 오류가 있을 수 있기 때문에 이는 어려운 일입니다. 본 논문은 장면 그래프에서 얻은 의미 데이터를 기반으로 에이전트가 관련 질문을 하도록 하여 인간의 참조 표현을 명확화하는 것을 목표로 합니다. 에이전트가 장면 그래프의 객체 간 관계를 사용하여 원래 사용자 명령을 명확하게 할 수 있는 의미론적으로 관련된 질문을 할 수 있는지 테스트합니다. 본 논문에서는 이미지 장면 그래프의 의미 데이터와 언어 장면 그래프의 언어 구조를 인간 명령을 기반으로 지상 객체에 사용하는 명확화 모델인 장면 그래프를 이용한 증분 접지(IGSG)를 제시합니다. 기준선과 비교하여 IGSG는 동일한 대상 개체가 여러 개 있는 복잡한 실제 장면에서 유망한 결과를 보여줍니다. IGSG는 사용자에게 명확한 질문을 다시 던져 모호하거나 잘못된 참조 표현을 효과적으로 명확하게 할 수 있습니다.
245,http://arxiv.org/abs/2201.00443 ,Scene Graph Generation: A Comprehensive Survey,"Guangming Zhu, Liang Zhang, Youliang Jiang, Yixuan Dang, Haoran Hou, Peiyi Shen, Mingtao Feng, Xia Zhao, Qiguang Miao, Syed Afaq Ali Shah, Mohammed Bennamoun","딥 러닝 기술은 일반 객체 감지 분야에서 놀라운 혁신을 가져왔으며 최근 몇 년간 장면 이해 작업을 많이 탄생시켰습니다. 장면 그래프는 강력한 의미 표현과 장면 이해에 대한 적용으로 인해 연구의 초점이 되어 왔습니다. SGG(장면 그래프 생성)는 이미지를 의미론적 구조 장면 그래프에 자동으로 매핑하는 작업을 의미하며, 이를 위해서는 감지된 객체와 그 관계에 대한 올바른 라벨링이 필요합니다. 이는 어려운 작업이지만 커뮤니티에서는 SGG 접근 방식을 많이 제안했고 좋은 결과를 얻었습니다. 본 논문에서는 딥러닝 기술을 통해 이 분야에서 최근 달성한 성과에 대한 포괄적인 조사를 제공합니다. 다양한 입력 양식을 다루는 138개의 대표 연구를 검토하고, 특징 추출 및 융합의 관점에서 이미지 기반 SGG의 기존 방법을 체계적으로 요약합니다. 우리는 기존의 시각적 관계 탐지 방법을 연결하고 체계화하여 SGG의 메커니즘과 전략을 종합적으로 요약하고 해석하려고 합니다. 마지막으로, 현재 존재하는 문제점과 향후 연구 방향에 대한 심도 있는 논의를 마치며 본 설문조사를 마무리합니다. 이 설문조사는 독자들이 현재의 연구 상태와 아이디어를 더 잘 이해하는 데 도움이 될 것입니다."
244,http://arxiv.org/abs/2112.12970 ,SGTR: End-to-end Scene Graph Generation with Transformer,"Rongjie Li, Songyang Zhang, Xuming He","장면 그래프 생성(SGG)은 구성 특성으로 인해 여전히 어려운 시각적 이해 작업으로 남아 있습니다. 대부분의 이전 연구는 상향식 2단계 또는 포인트 기반 1단계 접근 방식을 채택했는데, 이는 종종 시간 복잡도가 높거나 최적이 아닌 설계로 인해 어려움을 겪습니다. 본 연구에서는 앞서 언급한 문제를 해결하기 위한 새로운 SGG 방법을 제안하고 작업을 이분 그래프 구성 문제로 공식화합니다. 문제를 해결하기 위해 우리는 먼저 엔터티와 술어 제안 세트를 생성한 다음 방향성 에지를 추론하여 관계 삼중항을 형성하는 변환기 기반 엔드 투 엔드 프레임워크를 개발합니다. 특히, 우리는 관계의 구성 속성을 활용하는 구조적 조건자 생성기를 기반으로 새로운 엔터티 인식 조건자 표현을 개발합니다. 또한, 엔터티 인식 구조를 기반으로 이분 장면 그래프의 연결성을 추론하는 그래프 조립 모듈을 설계하여 엔드 투 엔드 방식으로 장면 그래프를 생성할 수 있습니다. 광범위한 실험 결과에 따르면 우리의 설계는 두 가지 까다로운 벤치마크에서 최첨단 또는 이에 필적하는 성능을 달성하여 기존 접근 방식의 대부분을 능가하고 추론에서 더 높은 효율성을 누릴 수 있음을 보여줍니다. 우리 모델이 Transformer 기반 장면 그래프 생성을 위한 강력한 기준선 역할을 할 수 있기를 바랍니다. 코드는 https://github.com/Scarecrow0/SGTR에서 확인할 수 있습니다."
243,http://arxiv.org/abs/2112.11691 ,Comprehensive Visual Question Answering on Point Clouds through Compositional Scene Manipulation,"Xu Yan, Zhihao Yuan, Yuhao Du, Yinghong Liao, Yao Guo, Zhen Li, Shuguang Cui","3D 포인트 클라우드에 대한 시각적 질문 답변(VQA-3D)은 전체 포인트 클라우드 장면에서 다양한 유형의 텍스트 질문에 답하는 것을 목표로 하는 신흥 아직 도전적인 분야입니다. 이 문제를 해결하기 위해 우리는 8,771개의 3D 장면에서 나온 171,000개의 질문으로 구성된 대규모 VQA-3D 데이터 세트인 CLEVR3D를 제안합니다. 구체적으로, 우리는 3D 장면 그래프 구조를 활용하여 물체의 속성(예: 크기, 색상, 재료)과 공간적 관계에 대한 질문을 다루는 다양한 추론 질문을 생성하는 질문 엔진을 개발합니다. 이러한 방식을 통해 처음에는 1,333개의 실제 장면에서 44,000개의 질문을 생성했습니다. 또한 혼란스러운 편견을 제거하고 상식적인 레이아웃에서 컨텍스트를 조정하기 위해 더 어려운 설정이 제안되었습니다. 이러한 설정에서는 3D 장면이 일반적인 동시 발생 상황과 다를 때(예: 의자는 항상 테이블과 함께 존재함) 포괄적인 시각적 이해를 달성하기 위해 네트워크가 필요합니다. 이를 위해 구성 장면 조작 전략을 추가로 도입하고 7,438개의 증강 3D 장면에서 127K 질문을 생성하여 실제 이해를 위해 VQA-3D 모델을 향상시킬 수 있습니다. 제안된 데이터 세트를 기반으로 여러 VQA-3D 모델의 기준을 정하고 실험 결과를 통해 CLEVR3D가 다른 3D 장면 이해 작업을 크게 향상시킬 수 있음을 확인했습니다. 우리의 코드와 데이터 세트는 https://github.com/yanx27/CLEVR3D에서 공개적으로 제공됩니다."
242,http://arxiv.org/abs/2112.09828 ,Exploiting Long-Term Dependencies for Generating Dynamic Scene Graphs,"Shengyu Feng, Subarna Tripathi, Hesham Mostafa, Marcel Nassar, Somdeb Majumdar",비디오에서 동적 장면 그래프를 생성하는 것은 장면의 시간적 역학과 예측의 고유한 시간적 변동으로 인해 어렵습니다. 우리는 장기적인 시간적 의존성을 포착하는 것이 동적 장면 그래프를 효과적으로 생성하는 열쇠라고 가정합니다. 우리는 변환기를 사용하여 객체 수준 장기 트랙렛에 대한 객체 수준 일관성 및 객체 간 관계 역학을 캡처하여 비디오의 장기 종속성을 학습할 것을 제안합니다. 실험 결과는 DSG-DETR(Dynamic Scene Graph Protection Transformer)이 벤치마크 데이터 세트인 Action Genome에서 상당한 차이로 최첨단 방법보다 성능이 우수하다는 것을 보여줍니다. 우리의 절제 연구는 제안된 접근법의 각 구성 요소의 효율성을 검증합니다. 소스 코드는 https://github.com/Shengyu-Feng/DSG-DETR에서 확인할 수 있습니다.
241,http://arxiv.org/abs/2112.08587 ,SGEITL: Scene Graph Enhanced Image-Text Learning for Visual Commonsense Reasoning,"Zhecan Wang, Haoxuan You, Liunian Harold Li, Alireza Zareian, Suji Park, Yiqing Liang, Kai-Wei Chang, Shih-Fu Chang","이미지에 관한 복잡한 질문에 답하는 것은 머신 인텔리전스의 야심찬 목표이며, 이를 위해서는 이미지, 텍스트, 상식적 지식에 대한 공동 이해와 강력한 추론 능력이 필요합니다. 최근 다중 모드 변환기는 양식 간 주의 계층을 통해 시각적 개체와 텍스트 토큰을 공동으로 이해함으로써 VCR(시각적 상식 추론) 작업에서 큰 진전을 이루었습니다. 그러나 이러한 접근 방식은 복잡한 상식적인 질문에 대답하는 데 필수적인 장면의 풍부한 구조와 개체 간의 상호 작용을 활용하지 않습니다. 우리는 상식 추론에 시각적 장면 그래프를 통합하기 위해 장면 그래프 강화 이미지-텍스트 학습(SGEITL) 프레임워크를 제안합니다. 장면 그래프 구조를 활용하기 위해 모델 구조 수준에서 홉 간의 주의 상호 작용을 정규화하기 위한 다중 홉 그래프 변환기를 제안합니다. 사전 훈련에서는 시각적 장면 그래프에서 추출된 구조 지식을 활용하기 위해 장면 그래프 인식 사전 훈련 방법을 제안합니다. 또한 약한 감독 방식으로 텍스트 주석을 사용하여 도메인 관련 시각적 장면 그래프를 훈련하고 생성하는 방법을 소개합니다. VCR 및 기타 작업에 대한 광범위한 실험을 통해 최첨단 방법에 비해 성능이 크게 향상되었으며 제안된 각 구성 요소의 효율성이 입증되었습니다."
240,http://arxiv.org/abs/2112.05727 ,Neural Belief Propagation for Scene Graph Generation,"Daqi Liu, Miroslaw Bober, Josef Kittler","장면 그래프 생성은 잠재적 개체와 그 관계를 명시적으로 모델링하여 입력 이미지를 해석하는 것을 목표로 하며, 이는 주로 이전 방법의 메시지 전달 신경망 모델을 통해 해결됩니다. 현재 이러한 근사 모델은 일반적으로 출력 변수가 완전히 독립적이라고 가정하므로 유익한 구조적 고차 상호 작용을 무시합니다. 이로 인해 입력 이미지에 대한 해석이 일관되지 않을 수 있습니다. 본 논문에서는 결과 장면 그래프를 생성하기 위한 새로운 신경 신념 전파 방법을 제안합니다. 연관된 주변값을 추론하기 위해 평균 필드 근사보다는 구조적 베테 근사를 사용합니다. 더 나은 편향-분산 균형을 찾기 위해 제안된 모델은 쌍별 상호 작용뿐만 아니라 고차 상호 작용도 관련 점수 함수에 통합합니다. 다양한 인기 장면 그래프 생성 벤치마크에서 최첨단 성능을 달성합니다."
239,http://arxiv.org/abs/2112.04222 ,Classification-Then-Grounding: Reformulating Video Scene Graphs as Temporal Bipartite Graphs,"Kaifeng Gao, Long Chen, Yulei Niu, Jian Shao, Jun Xiao","오늘날의 VidSGG 모델은 모두 제안 기반 방법입니다. 즉, 먼저 여러 쌍의 주체-객체 조각을 제안으로 생성한 다음 각 제안에 대해 조건자 분류를 수행합니다. 본 논문에서 우리는 이 널리 퍼진 제안 기반 프레임워크에 세 가지 고유한 단점이 있다고 주장합니다. 1) 제안에 대한 실측 예측 레이블이 부분적으로 정확합니다. 2) 동일한 주체-객체 쌍의 서로 다른 술어 인스턴스 간의 상위 관계를 끊습니다. 3) VidSGG 성능은 제안의 품질에 따라 상한됩니다. 이를 위해 우리는 간과된 세 가지 단점을 모두 피할 수 있는 VidSGG에 대한 새로운 분류 후 접지 프레임워크를 제안합니다. 한편, 이 프레임워크에서 우리는 비디오 장면 그래프를 시간적 이분 그래프로 재구성합니다. 여기서 엔터티와 조건자는 시간 슬롯이 있는 두 가지 유형의 노드이고 가장자리는 이러한 노드 간의 서로 다른 의미론적 역할을 나타냅니다. 이 공식은 새로운 프레임워크를 최대한 활용합니다. 따라서 우리는 새로운 BIpartite Graph 기반 SGG 모델인 BIG를 제안합니다. 이는 분류 단계와 접지 단계로 구성되며, 전자는 모든 노드와 에지의 범주를 분류하는 것을 목표로 하고, 후자는 각 관계 인스턴스의 시간적 위치를 지역화하려고 시도합니다. 두 개의 VidSGG 데이터 세트에 대한 광범위한 제거를 통해 프레임워크와 BIG의 효율성이 입증되었습니다. 코드는 https://github.com/Dawn-LX/VidSGG-BIG에서 확인할 수 있습니다."
238,http://arxiv.org/abs/2112.00974 ,Consensus Graph Representation Learning for Better Grounded Image Captioning,"Wenqiao Zhang, Haochen Shi, Siliang Tang, Jun Xiao, Qiang Yu, Yueting Zhuang","현대의 시각적 캡션 모델은 시각적 정보와 대상 어휘 단어 사이의 의미적 불일치를 초래하는 사전에 대한 과도한 의존 또는 시각적 오분류로 인해 실제로 장면에 없는 개체를 환각화하는 경우가 많습니다. 가장 일반적인 방법은 캡션 모델이 생성된 개체 단어나 문구를 이미지의 적절한 영역, 즉 GIC(Grounded Image Captioning)에 동적으로 연결하도록 권장하는 것입니다. 그러나 GIC는 대상 환각의 핵심 문제인 의미 불일치를 해결하지 못한 보조 작업(접지 대상)을 활용합니다. 이 논문에서 우리는 위의 문제에 대해 시각적 양식과 언어 양식 간의 의미적 일관성을 활용하는 새로운 관점을 취합니다. 구체적으로 우리는 기반 캡션 파이프라인에 합의 표현을 통합하는 GIC용 합의 Rraph 표현 학습 프레임워크(CGRL)를 제안합니다. 그래프의 노드와 에지를 모두 고려한 언어 그래프에 시각적 그래프(예: 장면 그래프)를 정렬하여 합의를 학습합니다. 일치된 합의를 통해 캡션 모델은 올바른 언어적 특성과 시각적 관련성을 모두 포착한 다음 적절한 이미지 영역을 더욱 기반화할 수 있습니다. 우리는 Flickr30k Entities 데이터 세트에서 대상 환각(-9% CHAIRi)이 크게 감소하여 모델의 효율성을 검증했습니다. 게다가 우리의 CGRL은 여러 가지 자동 메트릭과 인간 평가를 통해 평가되었으며 결과는 제안된 접근 방식이 이미지 캡션(+2.9 Cider) 및 접지(+2.3 F1LOC)의 성능을 동시에 향상시킬 수 있음을 나타냅니다."
237,http://arxiv.org/abs/2112.00967 ,Relational Graph Learning for Grounded Video Description Generation,"Wenqiao Zhang, Xin Eric Wang, Siliang Tang, Haizhou Shi, Haocheng Shi, Jun Xiao, Yueting Zhuang, William Yang Wang","GVD(Grounded Video Description)는 캡션 모델이 적절한 비디오 영역(예: 개체)에 동적으로 참석하고 설명을 생성하도록 권장합니다. 이러한 설정은 캡션 모델의 결정을 설명하는 데 도움이 될 수 있으며 모델이 설명에서 목적어를 환각하는 것을 방지할 수 있습니다. 그러나 이러한 디자인은 주로 목적어 생성에 중점을 두기 때문에 세밀한 정보를 무시하고 시각적 개념이 누락되는 문제가 발생할 수 있습니다. 더욱이, 관계형 단어(예: ""왼쪽 또는 오른쪽으로 점프"")는 일반적인 시공간 추론 결과입니다. 즉, 이러한 단어는 특정 공간 영역에 근거할 수 없습니다. 위의 한계를 해결하기 위해 우리는 GVD를 위한 새로운 관계형 그래프 학습 프레임워크를 설계했습니다. 여기서는 언어로 개선된 장면 그래프 표현이 세밀한 시각적 개념을 탐색하도록 설계되었습니다. 또한, 정제된 그래프는 캡션 모델이 올바른 단어를 생성하는 데 필요한 관련 정보를 선택하는 데 도움이 되는 관계형 귀납적 지식으로 간주될 수 있습니다. 우리는 자동 측정과 사람의 평가를 통해 모델의 효율성을 검증합니다. 결과는 우리의 접근 방식이 보다 세밀하고 정확한 설명을 생성할 수 있으며 대상 환각 문제를 어느 정도 해결할 수 있음을 나타냅니다."
236,http://arxiv.org/abs/2111.13517 ,Not All Relations are Equal: Mining Informative Labels for Scene Graph Generation,"Arushi Goel, Basura Fernando, Frank Keller, Hakan Bilen",SGG(장면 그래프 생성)는 전체 장면 이해에 필수적인 개체 쌍 간의 다양한 상호 작용을 캡처하는 것을 목표로 합니다. 전체 관계 세트에 대해 훈련된 기존 SGG 방법은 훈련 데이터의 다양한 편향으로 인해 시각적 및 텍스트 상관관계에 대한 복잡한 추론을 획득하지 못합니다. 'parked on'과 같은 정보 관계 대신 'on'과 같은 일반적인 공간 구성을 나타내는 사소한 관계에 대한 학습은 이러한 복잡한 추론을 강요하지 않으며 일반화에 해를 끼칩니다. 이 문제를 해결하기 위해 우리는 정보성을 기반으로 관계 레이블을 활용하는 SGG 훈련을 위한 새로운 프레임워크를 제안합니다. 모델에 구애받지 않는 훈련 절차는 훈련 데이터에서 덜 유익한 샘플에 대해 누락된 정보 관계를 귀속시키고 기존 주석과 함께 귀속된 레이블에 대해 SGG 모델을 훈련합니다. 우리는 이 접근 방식이 최첨단 SGG 방법과 함께 성공적으로 사용될 수 있으며 표준 Visual Genome 벤치마크의 여러 측정 항목에서 성능이 크게 향상된다는 것을 보여줍니다. 더욱이 우리는 보다 까다로운 제로샷 설정에서 보이지 않는 삼중항에 대해 상당한 개선을 얻었습니다.
235,http://arxiv.org/abs/2111.13131 ,Scene Graph Generation with Geometric Context,"Vishal Kumar, Albert Mundu, Satish Kumar Singh","장면 그래프 생성은 시각적 질문 답변, 이미지 캡션 작성, 자율 주행 자동차, 군중 행동 분석, 활동 인식 등과 같은 이미지 이해 프로젝트의 수요가 증가함에 따라 컴퓨터 비전 연구에서 많은 주목을 받았습니다. 이미지의 시각적 기반 그래픽 구조인 장면 그래프는 이미지 이해 작업을 단순화하는 데 큰 도움이 됩니다. 본 작업에서는 시각적 장면을 기하학적으로 더 잘 이해하기 위해 기하학적 컨텍스트(Geometric Context)라는 후처리 알고리즘을 도입했습니다. 우리는 이 후처리 알고리즘을 사용하여 객체 쌍 간의 기하학적 관계를 이전 모델에 추가하고 개선합니다. 우리는 객체 쌍 사이의 방향과 거리를 계산하여 이러한 맥락을 활용합니다. 우리는 KERN(Knowledge Embedded Routing Network)을 기본 모델로 사용하고, 알고리즘으로 작업을 확장하며, 최신 최첨단 알고리즘에 대한 비교 가능한 결과를 보여줍니다."
234,http://arxiv.org/abs/2111.10196 ,Towards Traffic Scene Description: The Semantic Scene Graph,"Maximilian Zipfl, J. Marius Zöllner",교통 장면의 분류를 위해서는 영역에 관계없이 통일된 방식으로 장면을 설명할 수 있는 설명 모델이 필요합니다. 본 논문에서는 교통 상황을 의미론적으로 설명하는 모델을 설명합니다. 설명 모델을 사용하면 도로 형상 및 도로 토폴로지에 독립적으로 교통 상황을 설명할 수 있습니다. 여기서 교통 참여자는 도로망에 투영되어 그래프의 노드로 표시됩니다. 도로 토폴로지를 기준으로 두 교통 참여자 간의 상대적인 위치에 따라 해당 노드 사이에 의미적으로 분류된 에지가 생성됩니다. 구체화를 위해 가장자리 속성은 차선 코스와 관련하여 두 교통 참가자 간의 상대적 거리와 속도에 의해 확장됩니다. 설명의 중요한 측면은 기계가 읽을 수 있는 형식으로 쉽게 변환할 수 있다는 것입니다. 현재의 설명은 교통 현장의 동적 객체에 초점을 맞추고 보행자나 차량과 같은 교통 참여자를 고려합니다.
233,http://arxiv.org/abs/2111.06123 ,Spatio-Temporal Scene-Graph Embedding for Autonomous Vehicle Collision Prediction,"Arnav V. Malawade, Shih-Yuan Yu, Brandon Hsu, Deepan Muthirayan, Pramod P. Khargonekar, Mohammad A. Al Faruque","자율주행차(AV)에서 조기 경고 시스템은 탑승자의 안전을 보장하기 위해 충돌 예측에 의존합니다. 그러나 심층 컨벌루션 네트워크를 사용하는 최첨단 방법은 충돌 모델링에 실패하거나 너무 비싸거나 느리기 때문에 AV 에지 하드웨어에 배포하기에 적합하지 않습니다. 이러한 제한 사항을 해결하기 위해 GNN(Graph Neural Network) 및 LSTM(Long Short-Term Memory) 레이어를 사용하여 시각적 장면 인식을 통해 향후 충돌을 예측하는 시공간 장면 그래프 임베딩 방법인 sg2vec를 제안합니다. 우리는 sg2vec이 합성된 데이터 세트에 대한 최첨단 방법보다 8.11% 더 정확하고 39.07% 더 빨리 충돌을 예측하고, 까다로운 실제 충돌 데이터 세트에 대해 29.47% 더 정확하게 예측한다는 것을 보여줍니다. 또한 sg2vec이 합성 데이터 세트에서 실제 운전 데이터 세트로 지식을 전달하는 데 있어서 최첨단 기술보다 우수하다는 것을 보여줍니다. 마지막으로, 우리는 sg2vec이 업계 표준 Nvidia DRIVE PX 2 플랫폼의 최첨단 방법보다 88.0% 더 작은 모델, 32.4% 더 적은 전력, 92.8% 더 적은 에너지로 9.3배 더 빠른 추론을 수행하여 엣지 구현에 더 적합하다는 것을 보여줍니다."
232,http://arxiv.org/abs/2111.04785 ,Visual Question Answering based on Formal Logic,"Muralikrishnna G. Sethuraman, Ali Payani, Faramarz Fekri, J. Clayton Kerce","시각적 질문 답변(VQA)은 여러 양식(예: 이미지, 언어)에서 나오는 정보를 이해하는 데 따른 어려움으로 인해 최근 몇 년 동안 기계 학습 커뮤니티에서 많은 관심을 받고 있습니다. VQA에서는 일련의 이미지를 기반으로 일련의 질문이 제기되고 현재 작업은 답변에 도달하는 것입니다. 이를 달성하기 위해 우리는 형식논리의 프레임워크를 사용하여 상징적 추론 기반 접근 방식을 취합니다. 이미지와 질문은 명시적인 추론이 수행되는 상징적 표현으로 변환됩니다. 우리는 (i) 장면 그래프의 도움을 받아 이미지를 논리적 배경 사실로 변환하고, (ii) 변환기 기반 딥 러닝 모델을 사용하여 질문을 1차 술어 논리 절로 변환하고, (iii) 배경 지식과 술어 절의 근거를 사용하여 만족성 검사를 수행하여 답을 얻는 형식적 논리 프레임워크를 제안합니다. 우리가 제안한 방법은 해석 가능성이 높으며 파이프라인의 각 단계를 사람이 쉽게 분석할 수 있습니다. 우리는 CLEVR 및 GQA 데이터 세트에 대한 접근 방식을 검증합니다. 우리는 CLEVR 데이터세트에서 최첨단 모델에 필적하는 99.6%에 가까운 완벽한 정확도를 달성했으며, 이는 형식적 논리가 시각적 질문 답변을 해결하는 데 실행 가능한 도구임을 보여줍니다. 우리 모델은 또한 데이터 효율적이어서 훈련 데이터의 10%만 훈련했을 때 CLEVR 데이터 세트에서 99.1%의 정확도를 달성합니다."
231,http://arxiv.org/abs/2111.00312 ,3DP3: 3D Scene Perception via Probabilistic Programming,"Nishad Gothoskar, Marco Cusumano-Towner, Ben Zinberg, Matin Ghavamizadeh, Falk Pollok, Austin Garrett, Joshua B. Tenenbaum, Dan Gutfreund, Vikash K. Mansinghka","우리는 객체, 장면 및 이미지의 구조화된 생성 모델에서 추론을 사용하는 역그래픽용 프레임워크인 3DP3를 제시합니다. 3DP3은 (i) 물체의 3D 모양을 표현하기 위해 복셀 모델, (ii) 장면을 물체와 물체 사이의 접촉으로 분해하기 위한 계층적 장면 그래프, (iii) 실시간 그래픽을 기반으로 한 깊이 이미지 가능성을 사용합니다. 관찰된 RGB-D 이미지가 주어지면 3DP3의 추론 알고리즘은 빠른 상향식 포즈 제안, 장면 그래프 구조의 새롭고 포괄적인 MCMC 업데이트, 선택적으로 신경 객체 감지기 및 포즈 추정기를 사용하여 객체 포즈 및 이러한 포즈의 간결한 공동 매개변수화를 포함한 기본 잠재 3D 장면을 추론합니다. 우리는 3DP3가 3차원 형상, 폐색 및 접촉 구조를 인식하는 장면 이해를 가능하게 함을 보여줍니다. 우리의 결과는 3DP3가 딥 러닝 기준보다 실제 이미지로부터 6DoF 객체 포즈 추정에서 더 정확하고 새로운 관점, 접촉 및 부분 관찰 가능성을 통해 까다로운 장면에 대한 더 나은 일반화를 보여줍니다."
230,http://arxiv.org/abs/2110.13280 ,A Variational Graph Autoencoder for Manipulation Action Recognition and Prediction,"Gamze Akyol, Sanem Sariel, Eren Erdal Aksoy","수십 년간의 연구에도 불구하고 인간의 조작 활동을 이해하는 것은 컴퓨터 비전과 로봇공학에서 가장 매력적이고 도전적인 연구 주제 중 하나이며, 언제나 그랬습니다. 관찰된 인간 조작 행위의 인식과 예측은 예를 들어 인간-로봇 상호 작용 및 시연을 통한 로봇 학습과 관련된 응용 프로그램에 뿌리를 두고 있습니다. 현재 연구 동향은 RGB 카메라 이미지와 같은 구조화된 유클리드 데이터를 처리하기 위해 고급 컨볼루션 신경망에 크게 의존하고 있습니다. 그러나 이러한 네트워크에는 고차원 원시 데이터를 처리하기에는 엄청난 계산 복잡성이 있습니다.   관련 연구와 달리 구조화된 유클리드 데이터에 의존하는 대신 기호 장면 그래프에서 조작 작업의 인식 및 예측을 공동으로 학습하는 딥 그래프 오토인코더를 소개합니다. 우리 네트워크는 입력 그래프 유형을 식별하기 위한 분기와 미래 그래프를 예측하기 위한 분기로 구성된 변형 자동 인코더 구조를 가지고 있습니다. 제안된 네트워크의 입력은 장면의 주체와 객체 간의 공간 관계를 저장하는 의미 그래프 세트입니다. 네트워크 출력은 감지 및 예측 클래스 유형을 나타내는 레이블 세트입니다. 우리는 MANIAC 및 MSRC-9라는 두 가지 데이터 세트에 대한 다양한 최첨단 방법에 대해 새 모델을 벤치마킹하고 제안된 모델이 더 나은 성능을 달성할 수 있음을 보여줍니다. 또한 소스 코드 https://github.com/gamzeakyol/GNet을 공개합니다."
229,http://arxiv.org/abs/2110.11918 ,MIGS: Meta Image Generation from Scene Graphs,"Azade Farshad, Sabrina Musatian, Helisa Dhamo, Nassir Navab",장면 그래프에서 이미지를 생성하는 것은 명시적인 장면 생성 및 조작을 향한 유망한 방향입니다. 그러나 장면 그래프에서 생성된 이미지는 품질이 좋지 않습니다. 이는 부분적으로 데이터의 높은 난이도와 다양성으로 인해 발생합니다. 우리는 모델을 다양한 장면에 적용하고 다양한 작업 세트에 대한 훈련을 통해 이미지 품질을 향상시키는 그래프에서 몇 장의 이미지 생성을 위한 메타 학습 기반 접근 방식인 MIGS(Meta Image Generation from Scene Graphs)를 제안합니다. 작업 중심 방식으로 데이터를 샘플링함으로써 장면 속성에 따라 분류된 다양한 작업 세트에 대한 메타 학습을 사용하여 생성기를 훈련합니다. 우리의 결과는 장면 그래프에서 이미지를 생성하기 위해 이러한 메타 학습 접근 방식을 사용하면 이미지 품질 및 장면의 의미 관계 캡처 측면에서 최첨단 성능을 달성한다는 것을 보여줍니다. 프로젝트 웹사이트: https://migs2021.github.io/
228,http://arxiv.org/abs/2110.05731 ,Topic Scene Graph Generation by Attention Distillation from Caption,"W. Wang, R. Wang, X. Chen","이미지가 이야기를 전달하는 경우 이미지 캡션이 가장 간단한 설명이 됩니다. 일반적으로 장면 그래프는 전지적 일반주의자를 선호하는 반면, 이미지 캡션은 요점을 간략하게 설명하는 전문가를 선호합니다. 이전의 많은 연구에서는 장면 그래프가 사소한 내용과 노이즈를 줄일 수 없으면 예상만큼 실용적이지 않다는 사실이 밝혀졌습니다. 이런 점에서 이미지 캡션은 좋은 튜터입니다. 이를 위해 우리는 장면 그래프가 이미지 캡션의 기능을 빌려서 남은 만능을 기반으로 전문가가 될 수 있도록 하여 소위 주제 장면 그래프를 만들었습니다. 이미지 캡션이 주목하는 내용을 추출하여 부분적인 객체, 관계, 이벤트의 중요도를 추정하기 위한 장면 그래프로 전달합니다. 구체적으로, 캡션 생성 과정에서 각 시간 단계의 개별 객체에 대한 관심을 수집하고 모아서 관계에 대한 관심을 얻으며, 이는 추정된 관계 중요도 점수를 정규화하기 위한 약한 감독 역할을 합니다. 또한, 이러한 주의 증류 과정은 이미지 캡션과 장면 그래프의 생성을 함께 결합할 수 있는 기회를 제공하므로, 이미지 캡션과 단일 생성 모델을 공유하여 장면 그래프를 풍부하고 자유로운 형식의 표현을 갖춘 언어적 형식으로 더욱 변환합니다. 실험에 따르면 주의 집중 추출은 강력한 감독 없이 중요한 관계를 마이닝하는 데 상당한 개선을 가져오며 주제 장면 그래프는 후속 응용 프로그램에서 큰 잠재력을 보여줍니다."
227,http://arxiv.org/abs/2110.04494 ,SGMNet: Scene Graph Matching Network for Few-Shot Remote Sensing Scene Classification,"Baoquan Zhang, Shanshan Feng, Xutao Li, Yunming Ye, Rui Ye","FSRSSC(Few-Shot Remote Sensing Scene Classification)는 몇 가지 예를 사용하여 새로운 장면 클래스를 인식하는 것을 목표로 하는 중요한 작업입니다. 최근 몇몇 연구에서는 퓨샷 자연 이미지 분류 방법을 따라 FSRSSC 문제를 해결하려고 시도합니다. 이러한 기존 방법은 유망한 발전을 이루었고 우수한 성능을 달성했습니다. 그러나 그들은 모두 원격 탐사 이미지의 두 가지 독특한 특성, 즉 (i) 여러 객체가 장면 이미지에 함께 나타나는 경향이 있는 객체 동시 발생과 (ii) 이러한 동시 발생 객체가 일부 공간 구조 패턴을 따라 장면 이미지에 분포되어 있다는 객체 공간 상관 관계를 간과합니다. 이러한 고유한 특성은 FSRSSC에 매우 유용하며, 각 장면 클래스에 대해 보다 정교한 설명을 제공할 수 있으므로 레이블이 지정된 원격 감지 이미지의 부족 문제를 효과적으로 완화할 수 있습니다. 이러한 특성을 최대한 활용하기 위해 우리는 SGMNet이라는 FSRSSC용 새로운 장면 그래프 매칭 기반 메타 학습 프레임워크를 제안합니다. 이 프레임워크에서 장면 그래프 구성 모듈은 각 테스트 원격 감지 이미지 또는 각 장면 클래스를 장면 그래프로 나타내도록 신중하게 설계되었습니다. 여기서 노드는 이러한 동시 발생 개체를 반영하는 반면 가장자리는 이러한 동시 발생 개체 간의 공간적 상관 관계를 캡처합니다. 그런 다음 각 테스트 원격탐사 이미지와 각 장면 클래스 간의 유사성 점수를 평가하기 위해 장면 그래프 매칭 모듈을 추가로 개발합니다. 마지막으로 유사성 점수를 기반으로 가장 가까운 이웃 분류기를 통해 장면 클래스 예측을 수행합니다. 우리는 UCMerced LandUse, WHU19, AID 및 NWPU-RESISC45 데이터 세트에 대한 광범위한 실험을 수행합니다. 실험 결과는 우리의 방법이 기존의 최신 방법보다 우수한 성능을 얻는다는 것을 보여줍니다."
226,http://arxiv.org/abs/2110.00273 ,From SLAM to Situational Awareness: Challenges and Survey,"Hriday Bavle, Jose Luis Sanchez-Lopez, Claudio Cimarelli, Ali Tourani, Holger Voos","복잡한 임무를 효율적이고 안전하게 수행하는 모바일 로봇의 능력은 환경, 즉 상황에 대한 지식에 의해 제한됩니다. 고급 추론, 의사결정 및 실행 기술을 통해 지능형 에이전트는 알려지지 않은 환경에서도 자율적으로 행동할 수 있습니다. 상황인식(SA)은 심리학, 군사, 항공우주, 교육 등 다양한 분야에서 깊이 연구되어 온 인간의 기본 능력이다. 그럼에도 불구하고 감지, 공간 인식, 센서 융합, 상태 추정, SLAM(Simultaneous Localization and Mapping)과 같은 단일 구획화된 개념에 초점을 맞춘 로봇 공학에서는 아직 고려되지 않았습니다. 따라서 본 연구는 광범위한 다학문적 기존 지식을 연결하여 우리가 자율성을 위해 가장 중요하다고 생각하는 모바일 로봇을 위한 완전한 SA 시스템을 위한 길을 닦는 것을 목표로 합니다. 이를 위해 우리는 로봇 SA를 구성하는 주요 구성 요소와 해당 역량 영역을 정의합니다. 따라서 본 논문에서는 SA의 각 측면을 조사하고 이를 다루는 최첨단 로봇 알고리즘을 조사하고 현재 한계에 대해 논의합니다. 놀랍게도 현재 알고리즘 개발은 SA의 성능을 특정 환경으로만 제한하기 때문에 SA의 필수 측면은 아직 미성숙합니다. 그럼에도 불구하고 인공 지능(AI), 특히 딥 러닝(DL)은 배포와 실제 시나리오 사이에서 이러한 필드를 유지하는 격차를 해소하는 새로운 방법을 가져왔습니다. 또한, 잘 알려진 장면 그래프를 일반화한 상황 그래프(S-Graph)의 메커니즘을 통해 로봇 이해 알고리즘의 방대하게 파편화된 공간을 상호 연결할 수 있는 기회가 발견되었습니다. 따라서 우리는 최근 흥미로운 연구 방향을 논의함으로써 로봇 상황 인식의 미래에 대한 비전을 마침내 구체화합니다."
225,http://arxiv.org/abs/2109.11955 ,Visual Scene Graphs for Audio Source Separation,"Moitreya Chatterjee, Jonathan Le Roux, Narendra Ahuja, Anoop Cherian","시각적으로 안내되는 오디오 소스 분리를 ​​위한 최첨단 접근 방식은 일반적으로 악기와 같은 특징적인 사운드가 있는 소스를 가정합니다. 이러한 접근 방식은 종종 이러한 사운드 소스의 시각적 컨텍스트를 무시하거나 소스를 더 잘 특성화하는 데 유용할 수 있는 객체 상호 작용 모델링을 피하는 경우가 많습니다. 특히 동일한 객체 클래스가 서로 다른 상호 작용에서 다양한 사운드를 생성할 수 있는 경우 더욱 그렇습니다. 이 어려운 문제를 해결하기 위해 우리는 장면의 시각적 구조를 그래프로 포함하고 이 그래프를 하위 그래프로 분할하는 새로운 딥 러닝 모델인 AVSGS(Audio Visual Scene Graph Segmenter)를 제안합니다. 각 하위 그래프는 오디오 스펙트로그램을 공동 분할하여 얻은 고유한 사운드와 연결됩니다. 핵심적으로 AVSGS는 다중 헤드 주의를 사용하여 시각적 그래프의 상호 직교 하위 그래프 임베딩을 방출하는 재귀 신경망을 사용합니다. 이러한 임베딩은 소스 분리를 ​​위해 오디오 인코더-디코더를 조정하는 데 사용됩니다. 우리의 파이프라인은 인위적으로 혼합된 사운드에서 시각적 그래프를 사용하여 오디오 소스를 분리하는 자체 감독 작업을 통해 엔드투엔드(end-to-end) 교육을 받습니다. 이 문서에서는 ASIW(Audio Separation in the Wild)라고 하는 여러 비음악 소스를 포함하는 음원 분리를 위한 ""야생"" 비디오 데이터 세트를 소개합니다. 이 데이터 세트는 AudioCaps 데이터 세트에서 채택되었으며 소스 분리를 ​​위한 까다롭고 자연스럽고 일상적인 설정을 제공합니다. 제안된 ASIW 및 표준 MUSIC 데이터 세트에 대한 철저한 실험은 최근의 이전 접근 방식에 비해 우리 방법의 최첨단 사운드 분리 성능을 보여줍니다."
224,http://arxiv.org/abs/2109.11398 ,Scene Graph Generation for Better Image Captioning?,"Maximilian Mozes, Martin Schmitt, Vladimir Golkov, Hinrich Schütze, Daniel Cremers",우리는 감지된 객체와 자동 생성된 시각적 관계를 활용하여 자연어로 이미지를 설명하는 모델을 제안하여 감독된 이미지 캡션 생성 작업에 시각적 관계를 통합하는 방법을 조사합니다. 이를 위해 먼저 개별 개체와 개체 간의 시각적 관계를 식별하여 원시 이미지 픽셀에서 장면 그래프를 생성합니다. 이 장면 그래프는 최종 캡션을 생성하는 그래프-텍스트 모델에 대한 입력 역할을 합니다. 이전 접근 방식과 달리 우리 모델은 이미지의 객체 감지와 시각적 관계를 명시적으로 모델링합니다. 실험을 위해 우리는 Visual Genome과 MS COCO의 교차점에서 해당 골드 장면 그래프와 사람이 작성한 캡션이 모두 포함된 이미지로 구성된 새로운 데이터 세트를 구성했습니다. 우리의 결과는 BLEU 및 METEOR 평가 지표 측면에서 비교할 때 원시 입력 픽셀에서 직접 이미지 설명을 생성하는 기존의 최첨단 엔드 투 엔드 모델보다 성능이 우수하다는 것을 보여줍니다.
223,http://arxiv.org/abs/2109.10613 ,COVR: A test-bed for Visually Grounded Compositional Generalization with real images,"Ben Bogin, Shivanshu Gupta, Matt Gardner, Jonathan Berant",최근 몇 년 동안 테스트 시 새로운 구성으로 일반화하는 모델에 대한 관심이 높아졌지만 시각적 기반 영역의 벤치마크는 지금까지 합성 이미지로 제한되었습니다. 본 연구에서는 실제 이미지를 이용한 시각적 기반 구성 일반화를 위한 새로운 테스트베드인 COVR을 제안합니다. COVR을 생성하기 위해 우리는 장면 그래프로 주석이 달린 실제 이미지를 사용하고 일련의 컨텍스트 이미지와 함께 질문-답변 쌍을 생성하는 거의 완전 자동 절차를 제안합니다. COVR은 정량화 및 집계와 같은 고차원 작업을 포함하여 복잡한 추론이 필요한 질문에 중점을 둡니다. 자동 생성 프로세스로 인해 COVR은 테스트 시 모델이 제로 또는 소수 샷 설정에서 새로운 개념과 구성으로 일반화되어야 하는 구성 분할 생성을 용이하게 합니다. 우리는 COVR을 사용하여 구성 분할을 구성하고 최첨단 사전 학습된 언어 및 비전 모델이 구성적으로 일반화하는 데 어려움을 겪는 수많은 사례를 보여줍니다.
222,http://arxiv.org/abs/2109.07872 ,Knowledge-based Embodied Question Answering,"Sinan Tan, Mengmeng Ge, Di Guo, Huaping Liu, Fuchun Sun","본 논문에서는 에이전트가 지식을 바탕으로 다양한 질문에 답하기 위해 지능적으로 환경을 탐색하는 새로운 K-EQA(Knowledge-based Embodied Question Answering) 작업을 제안합니다. 기존 EQA 작업처럼 질문에 대상 객체를 명시적으로 지정하는 것과 달리, 에이전트는 ""음식을 자르는 데 칼이 사용됩니다""와 같은 지식을 알아야 하는 ""방에서 음식을 자르는 데 사용되는 물체가 무엇인지 알려주세요.""와 같은 좀 더 복잡한 질문을 이해하기 위해 외부 지식에 의존할 수 있습니다.   이러한 K-EQA 문제를 해결하기 위해 외부 지식과 3D 장면 그래프의 공동 추론을 수행하여 탐색 및 질문 응답을 구현하는 신경 프로그램 합성 추론을 기반으로 하는 새로운 프레임워크가 제안됩니다. 특히, 3D 장면 그래프는 방문한 장면의 시각적 정보를 저장할 수 있는 메모리를 제공할 수 있어 다단계 질의 응답의 효율성을 크게 향상시킵니다. 실험 결과는 제안된 프레임워크가 구현된 환경에서 보다 복잡하고 현실적인 질문에 답할 수 있음을 보여주었습니다. 제안된 방법은 다중 에이전트 시나리오에도 적용 가능합니다."
221,http://arxiv.org/abs/2109.06241 ,Incremental Abstraction in Distributed Probabilistic SLAM Graphs,"Joseph Ortiz, Talfan Evans, Edgar Sucar, Andrew J. Davison","장면 그래프는 간결하고 의미론적으로 풍부한 방식으로 장면의 주요 구성 요소를 나타내지만 추상 장면 요소를 확실하게 식별하고 지속적으로 변화하는 복잡한 그래프를 최적화해야 하는 과제로 인해 증분 SLAM 작업 중에 구축하기가 어렵습니다. 우리는 두 가지 새로운 구성 요소를 기반으로 장면 그래프를 점진적으로 구축하기 위한 분산형 그래프 기반 SLAM 프레임워크를 제시합니다. 첫째, 우리는 신경망이 특징 기반 단안 SLAM 시스템의 요인 그래프에 통합되는 추상 장면 요소를 제안하는 증분 추상화 프레임워크를 제안합니다. 장면 요소는 최적화를 통해 확인되거나 거부되며 점을 점진적으로 대체하여 보다 조밀하고 의미론적이며 간결한 표현을 생성합니다. 둘째, 새로운 라우팅 절차를 통해 그래프 프로세서에서 분산 추론을 위해 GBP(Gaussian Belief Propagation)를 사용합니다. GBP의 반복당 시간은 구조에 구애받지 않으며 이종 요인 그래프 추론을 위한 직접적인 방법에 비해 속도 이점을 보여줍니다. 우리는 평면 추상화를 사용하여 실제 실내 데이터 세트에서 시스템을 실행하고 상당한 압축을 통해 주요 평면을 복구합니다."
220,http://arxiv.org/abs/2109.05523 ,Constructing Phrase-level Semantic Labels to Form Multi-Grained Supervision for Image-Text Retrieval,"Zhihao Fan, Zhongyu Wei, Zejun Li, Siyuan Wang, Haijun Shan, Xuanjing Huang, Jianqing Fan",이미지 텍스트 검색을 위한 기존 연구는 쿼리 이미지에 대해 일치하는 문장과 불일치하는 문장을 구별하기 위해 주로 문장 수준 감독에 의존합니다. 그러나 이미지와 문장 사이의 의미적 불일치는 일반적으로 구문 수준과 같이 더 미세한 수준에서 발생합니다. 본 논문에서는 텍스트에서 불일치 단위를 더 잘 식별하기 위해 추가적인 구문 수준 감독을 도입하는 방법을 탐구합니다. 실제로 문장 수준과 구문 수준 모두에서 쿼리 이미지에 대해 다중 의미 의미 레이블이 자동으로 구성됩니다. 일치하는 문장에 대한 텍스트 장면 그래프를 구성하고 구문 수준 레이블로 엔터티와 트리플을 추출합니다. 문장 수준과 구문 수준의 감독을 통합하기 위해 다중 모드 표현 학습을 위한 SSAMT(Semantic Structure Aware Multimodal Transformer)를 제안합니다. SSAMT 내부에서는 다양한 종류의 주의 메커니즘을 활용하여 시각과 언어 측면 모두에서 다중 의미 의미 단위의 상호 작용을 시행합니다. 훈련을 위해 우리는 글로벌 관점과 로컬 관점 모두에서 다중 규모 매칭 손실을 제안하고 불일치 문구에 페널티를 적용합니다. MS-COCO 및 Flickr30K에 대한 실험 결과는 일부 최첨단 모델과 비교하여 우리 접근 방식의 효율성을 보여줍니다.
219,http://arxiv.org/abs/2109.05346 ,BGT-Net: Bidirectional GRU Transformer Network for Scene Graph Generation,"Naina Dhingra, Florian Ritter, Andreas Kunz","장면 그래프는 각각 객체와 객체-객체 관계로 구성된 노드와 에지입니다. SGG(장면 그래프 생성)는 개체와 개체 관계를 식별하는 것을 목표로 합니다. 이미지의 장면 그래프 생성을 위해 양방향 GRU(BiGRU) 변환기 네트워크(BGT-Net)를 제안합니다. 이 모델은 BiGRU 계층을 사용하여 객체 정보를 향상시키기 위해 새로운 객체-객체 통신을 구현합니다. 따라서 이미지의 모든 객체에 대한 정보는 다른 객체에 대해 사용 가능하며 나중에 객체 예측 단계에서 활용할 수 있습니다. 이 개체 정보는 변환기 인코더에서 개체 클래스를 예측하고 다른 변환기 인코더를 사용하여 개체별 가장자리 정보를 생성하는 데 사용됩니다. 긴 꼬리 관계 분포에 의해 유발된 데이터 세트 편향을 처리하기 위해 log-softmax 함수로 연화하고 편향 적응 항을 추가하여 모든 관계 예측에 대한 편향을 개별적으로 조절하는 것이 효과적인 접근 방식인 것으로 나타났습니다. 우리는 오픈 소스 데이터 세트(예: Visual Genome, Open-Images 및 Visual Relationship 탐지 데이터 세트)를 사용하여 실험 및 절제에 대한 정교한 연구를 수행하여 제안된 모델이 최신 기술에 비해 효과가 있음을 입증했습니다."
218,http://arxiv.org/abs/2109.04569 ,Open-World Distributed Robot Self-Localization with Transferable Visual Vocabulary and Both Absolute and Relative Features,"Mitsuki Yoshida, Ryogo Yamamoto, Daiki Iwata, Kanji Tanaka","시각적 로봇 자체 위치 파악은 시각적 로봇 탐색의 근본적인 문제이며 단안 및 순차 위치 파악을 포함한 다양한 문제 설정에서 연구되었습니다. 그러나 기존의 많은 연구는 주로 단일 로봇 시나리오에 초점을 맞추고 있으며 개방형 분산 로봇 시스템과 같이 통신 용량이 제한된 무선 네트워크를 통해 연결된 다양한 로봇과 관련된 일반 설정에 대한 탐색이 제한되어 있습니다. 특히, 로봇 간 시각적 설명, 시각적 어휘 등 핵심 지식의 전달 및 공유와 관련된 문제는 대체로 무시되어 왔습니다. 이 작업은 두 가지 주요 이점을 제공하면서 최첨단 성능을 유지하는 개방형 분산 로봇 시스템용으로 설계된 새로운 자체 위치 파악 프레임워크를 소개합니다. (1) 다중 모달, 경량 및 전송 가능한 시각적 기능에 매핑되는 감독되지 않은 시각적 어휘 모델을 사용하고 (2) 시각적 어휘 자체가 가볍고 의사소통 친화적인 모델입니다. 주요 초점은 단안 보기 이미지 인코딩에 있지만 프레임워크는 순차 현지화 애플리케이션으로 쉽게 확장될 수 있습니다. 절대 및 상대 모두 보완적인 유사성 보존 기능을 활용함으로써 프레임워크는 비지도, 다중 모드, 경량 및 전송 가능 요구 사항을 충족합니다. 모든 기능은 경량 그래프 신경망과 장면 그래프를 사용하여 학습되고 인식됩니다. 제안된 방법의 효율성은 수동 및 능동 자체 위치 파악 시나리오 모두에서 검증되었습니다."
217,http://arxiv.org/abs/2109.02227 ,Learning to Generate Scene Graph from Natural Language Supervision,"Yiwu Zhong, Jing Shi, Jianwei Yang, Chenliang Xu, Yin Li","이미지-텍스트 데이터로부터의 학습은 최근 많은 인식 작업에서 성공을 거두었지만 현재는 시각적 특징이나 객체와 같은 개별 시각적 개념으로 제한됩니다. 본 논문에서는 이미지-문장 쌍을 통해 학습하여 장면 그래프라고 알려진 이미지 내에서 지역화된 개체와 개체 관계의 그래픽 표현을 추출하는 첫 번째 방법 중 하나를 제안합니다. 이미지와 텍스트 사이의 간격을 메우기 위해 기성 객체 감지기를 활용하여 객체 인스턴스를 식별 및 위치화하고 감지된 영역의 레이블을 캡션에서 구문 분석된 개념과 일치시켜 장면 그래프 학습을 위한 ""의사"" 레이블을 생성합니다. 또한 마스킹된 토큰 예측 작업을 통해 이러한 ""의사"" 레이블을 예측하는 Transformer 기반 모델을 설계합니다. 이미지-문장 쌍만으로 학습한 우리 모델은 사람이 주석을 달고 현지화되지 않은 장면 그래프로 훈련한 최신 방법에 비해 30%의 상대적 이득을 달성합니다. 우리 모델은 또한 약하게 감독된 장면 그래프 생성과 완전 감독된 장면 그래프 생성에 대한 강력한 결과를 보여줍니다. 또한 장면 그래프 검출을 위한 개방형 어휘 설정을 탐색하고 개방형 장면 그래프 생성에 대한 첫 번째 결과를 제시합니다. 우리 코드는 https://github.com/YiwuZhong/SGG_from_NLS에서 확인할 수 있습니다."
216,http://arxiv.org/abs/2109.02226 ,GeneAnnotator: A Semi-automatic Annotation Tool for Visual Scene Graph,"Zhixuan Zhang, Chi Zhang, Zhenning Niu, Le Wang, Yuehu Liu","이 원고에서는 이미지용 반자동 장면 그래프 주석 도구인 GeneAnnotator를 소개합니다. 이 소프트웨어를 사용하면 인간 주석 작성자가 시각적 장면의 참가자 간의 기존 관계를 유향 그래프 형식으로 설명할 수 있으므로 이미지 캡션, VQA 및 장면 그래프 생성 등 시각적 관계에 대한 학습 및 추론이 가능합니다. 특정 이미지 데이터세트에 대한 주석은 단일 VG150 데이터 형식 파일로 병합되어 장면 그래프 학습을 위한 대부분의 기존 모델을 지원하거나 각 단일 이미지에 대해 별도의 주석 파일로 변환되어 맞춤형 데이터세트를 구축할 수 있습니다. 또한 GeneAnnotator는 과도한 주석 작업량을 줄이기 위해 규칙 기반 관계 추천 알고리즘을 제공합니다. GeneAnnotator를 통해 우리는 1000개의 다양한 교통 이미지가 포함된 포괄적인 장면 그래프 데이터 세트인 Traffic Genome을 제안하며, 이는 그 대가로 장면 그래프 주석을 위해 제안된 소프트웨어의 효율성을 검증합니다. 사용 예제 및 샘플 데이터가 포함된 프로젝트 소스 코드는 Apache 오픈 소스 라이선스에 따라 https://github.com/Milomilo0320/A-Semi-automatic-Annotation-Software-for-Scene-Graph에서 사용할 수 있습니다."
215,http://arxiv.org/abs/2109.01183 ,roadscene2vec: A Tool for Extracting and Embedding Road Scene-Graphs,"Arnav Vaibhav Malawade, Shih-Yuan Yu, Brandon Hsu, Harsimrat Kaeley, Anurag Karra, Mohammad Abdullah Al Faruque","최근에는 그래프 학습 기술과 함께 사용되는 도로 장면 그래프 표현이 동작 분류, 위험 평가 및 충돌 예측을 포함한 작업에서 최첨단 딥 러닝 기술을 능가하는 것으로 나타났습니다. 도로 장면 그래프 표현의 응용 프로그램을 탐색할 수 있도록 도로 장면 그래프를 추출하고 삽입하기 위한 오픈 소스 도구인 roadscene2vec를 소개합니다. roadscene2vec의 목표는 장면 그래프 생성 도구, 시공간 장면 그래프 임베딩을 생성하는 그래프 학습 모델, 장면 그래프 기반 방법론 시각화 및 분석 도구를 제공하여 도로 장면 그래프의 응용 및 기능에 대한 연구를 가능하게 하는 것입니다. roadscene2vec의 기능에는 (i) 비디오 클립 또는 CARLA 시뮬레이터의 데이터로부터 맞춤형 장면 그래프 생성, (ii) 구성 가능한 다중 시공간 그래프 임베딩 모델 및 기본 CNN 기반 모델, (iii) 위험 평가 및 충돌 예측 애플리케이션을 위해 그래프 및 시퀀스 임베딩을 사용하기 위한 내장 기능, (iv) 전이 학습 평가 도구, (v) 장면 그래프 시각화 및 그래프 학습 모델의 설명 가능성 분석을 위한 유틸리티가 포함됩니다. 우리는 그래프 학습 모델과 CNN 기반 모델 모두에 대한 실험 결과와 정성적 평가를 통해 이러한 사용 사례에 대한 roadscene2vec의 유용성을 보여줍니다. roadscene2vec는 https://github.com/AICPS/roadscene2vec에서 사용할 수 있습니다."
214,http://arxiv.org/abs/2108.13129 ,From General to Specific: Informative Scene Graph Generation via Balance Adjustment,"Yuyu Guo, Lianli Gao, Xuanhan Wang, Yuxuan Hu, Xing Xu, Xu Lu, Heng Tao Shen, Jingkuan Song","SGG(장면 그래프 생성) 작업은 이미지에서 주어, 술어, 객체 등의 시각적 관계 삼중항을 감지하여 장면 이해를 위한 구조적 비전 레이아웃을 제공하는 것을 목표로 합니다. 그러나 현재 모델은 ""~에 서다"", ""보다"" 등의 정보 제공용어가 아닌 ""on"", ""at"" 등의 공통 서술어에 갇혀 있어 정확한 정보와 전반적인 성능이 손실되는 문제가 있습니다. 모델이 이미지를 설명하기 위해 '차단'이 아닌 '길 위의 돌'만을 사용한다면 장면을 오해하기 쉽습니다. 우리는 이 현상이 유익한 술어와 일반적인 술어 사이의 두 가지 주요 불균형, 즉 의미 공간 수준 불균형과 훈련 샘플 수준 불균형으로 인해 발생한다고 주장합니다. 이 문제를 해결하기 위해 우리는 기존 분포 피팅이 아닌 균형 조정을 기반으로 하는 간단하면서도 효과적인 SGG 프레임워크인 BA-SGG를 제안합니다. 이러한 불균형을 조정하기 위해 각각 SA(의미 조정) 및 BPL(균형 예측 학습)이라는 두 가지 구성 요소를 통합합니다. 모델에 구애받지 않는 프로세스의 이점을 활용하여 우리의 방법은 최첨단 SGG 모델에 쉽게 적용되고 SGG 성능을 크게 향상시킵니다. 우리의 방법은 Visual Genome의 세 가지 장면 그래프 생성 하위 작업에서 Transformer 모델보다 각각 14.3%, 8.0%, 6.1% 더 높은 평균 재현율(mR)을 달성했습니다. 코드는 공개적으로 사용 가능합니다."
213,http://arxiv.org/abs/2108.09668 ,Learning of Visual Relations: The Devil is in the Tails,"Alakh Desai, Tz-Ying Wu, Subarna Tripathi, Nuno Vasconcelos",최근 시각적 관계를 모델링하는 데 상당한 노력이 기울여졌습니다. 이는 일반적으로 매개변수를 추가하고 모델 복잡성을 증가시켜 아키텍처 설계를 주로 다루었습니다. 그러나 시각적 관계 학습은 객체 그룹에 대한 공동 추론의 조합적 특성으로 인해 긴 꼬리 문제입니다. 일반적으로 모델 복잡성이 증가하면 과적합 경향이 있기 때문에 긴 꼬리 문제에는 적합하지 않습니다. 이 논문에서 우리는 악마가 꼬리에 있다는 대안 가설을 탐구합니다. 이 가설에 따르면 모델을 단순하게 유지하면서 긴 꼬리 분포에 대처하는 능력을 향상시킴으로써 더 나은 성능을 얻을 수 있습니다. 이 가설을 테스트하기 위해 우리는 최첨단 긴 꼬리 인식 문헌에서 영감을 받은 시각적 관계 모델을 훈련하기 위한 새로운 접근 방식을 고안했습니다. 이는 DT2(Decoupled Training for Devil in the Tails)라고 불리는 반복적인 분리 훈련 방식을 기반으로 합니다. DT2는 ACBS(Alternating Class-Balanced Sampling)라는 새로운 샘플링 접근 방식을 사용하여 긴 꼬리 엔터티와 시각적 관계의 조건자 분포 간의 상호 작용을 포착합니다. 결과는 매우 간단한 아키텍처를 갖춘 DT2-ACBS가 장면 그래프 생성 작업에서 훨씬 더 복잡한 최첨단 방법보다 훨씬 뛰어난 성능을 발휘한다는 것을 보여줍니다. 이는 정교한 모델의 개발이 문제의 장기적인 성격과 함께 고려되어야 함을 시사합니다.
212,http://arxiv.org/abs/2108.08841 ,Graph-to-3D: End-to-End Generation and Manipulation of 3D Scenes Using Scene Graphs,"Helisa Dhamo, Fabian Manhardt, Nassir Navab, Federico Tombari","제어 가능한 장면 합성은 기본 사양을 충족하는 3D 정보 생성으로 구성됩니다. 따라서 이러한 사양은 추상적이어야 합니다. 즉, 쉬운 사용자 상호 작용을 허용하는 동시에 세부적인 제어를 위한 충분한 인터페이스를 제공해야 합니다. 장면 그래프는 객체(노드)와 객체 간 관계(에지)로 구성된 장면의 표현으로, 생성된 콘텐츠에 대한 의미론적 제어를 허용하므로 이 작업에 특히 적합한 것으로 입증되었습니다. 이 작업을 다루는 이전 작업은 종종 합성 데이터에 의존하고 객체 메쉬를 검색하므로 생성 기능이 자연스럽게 제한됩니다. 이 문제를 회피하기 위해 우리는 대신 end-to-end 방식으로 장면 그래프에서 직접 모양을 생성하는 첫 번째 작업을 제안합니다. 또한, 해당 장면 그래프를 인터페이스로 사용하여 동일한 모델이 장면 수정을 지원함을 보여줍니다. GCN(Graph Convolutional Networks)을 활용하여 객체 및 가장자리 범주뿐만 아니라 3D 모양 및 장면 레이아웃에 대한 변형 자동 인코더를 훈련하여 나중에 새로운 장면과 모양을 샘플링할 수 있습니다."
211,http://arxiv.org/abs/2108.08600 ,Semantic Compositional Learning for Low-shot Scene Graph Generation,"Tao He, Lianli Gao, Jingkuan Song, Jianfei Cai, Yuan-Fang Li","장면 그래프는 많은 다운스트림 작업에 귀중한 정보를 제공합니다. 많은 장면 그래프 생성(SGG) 모델은 훈련을 위해 제한된 주석이 달린 관계 트리플만을 사용하므로 낮은 샷(몇 개 및 0개) 시나리오, 특히 드문 조건자에서 성능이 저하됩니다. 이 문제를 해결하기 위해 우리는 다른 이미지의 객체와 추가적이고 현실적인 관계 트리플을 구성할 수 있는 새로운 의미 구성 학습 전략을 제안합니다. 구체적으로, 우리의 전략은 불필요한 컴포넌트를 식별 및 제거하여 릴레이션 트리플을 분해하고, 새로 구성된 트리플의 현실성을 보장하면서 시각적 컴포넌트 사전에서 의미상 또는 시각적으로 유사한 객체와 융합하여 새로운 릴레이션 트리플을 구성합니다. 특히, 우리의 전략은 일반적이며 기존 SGG 모델과 결합하여 성능을 크게 향상시킬 수 있습니다. 우리는 벤치마크 데이터 세트인 Visual Genome에 대해 종합적인 평가를 수행했습니다. 세 가지 최신 SGG 모델에 대해 우리의 전략을 추가하면 성능이 50% 가까이 향상되며 모두 현재의 최첨단 모델을 크게 초과합니다."
210,http://arxiv.org/abs/2108.08584 ,Exploiting Scene Graphs for Human-Object Interaction Detection,"Tao He, Lianli Gao, Jingkuan Song, Yuan-Fang Li","HOI(Human-Object Interaction) 감지는 인간과 물체 사이의 상호 작용을 파악하고 인식하는 것을 목표로 하는 기본적인 시각적 작업입니다. 기존 작품들은 인간과 사물의 시각적, 언어적 특징에 초점을 맞추고 있다. 그러나 HOI 추론을 위한 중요한 맥락 및 세부 관계 지식을 제공하는 이미지에 존재하는 높은 수준의 의미론적 관계를 활용하지 않습니다. 우리는 SG2HOI(Human-Object Interaction) 탐지 작업을 위해 장면 그래프를 통해 이 정보를 활용하는 새로운 방법을 제안합니다. 우리의 방법인 SG2HOI는 두 가지 방법으로 SG 정보를 통합합니다. (1) 장면별 환경 컨텍스트 역할을 하는 전역 컨텍스트 단서에 장면 그래프를 삽입합니다. (2) 관계 인식 메시지 전달 모듈을 구축하여 객체 주변의 관계를 수집하고 이를 상호 작용으로 전환합니다. 경험적 평가에 따르면 SG2HOI 방법은 V-COCO 및 HICO-DET라는 두 가지 벤치마크 HOI 데이터 세트에서 최첨단 방법보다 성능이 뛰어납니다. 코드는 https://github.com/ht014/SG2HOI에서 확인할 수 있습니다."
209,http://arxiv.org/abs/2108.08121 ,Target Adaptive Context Aggregation for Video Scene Graph Generation,"Yao Teng, Limin Wang, Zhifeng Li, Gangshan Wu","본 논문은 높은 수준의 이해 작업을 위한 구조화된 비디오 표현 역할을 할 수 있는 비디오 장면 그래프 생성(VidSGG)의 어려운 작업을 다룹니다. 우리는 복잡한 하위 수준 엔터티 추적에서 관계 예측을 위한 컨텍스트 모델링을 분리하여 이 작업에 대한 새로운 {\em 감지-추적} 패러다임을 제시합니다. 특히 우리는 관계 인식을 위한 시공간 컨텍스트 정보 캡처에 중점을 두고 TRACE(Target Adaptive Context Aggregation Network)라고 하는 프레임 수준 VidSGG에 대한 효율적인 방법을 설계합니다. 우리의 TRACE 프레임워크는 모듈식 설계로 VidSGG 파이프라인을 간소화하고 HRTree(Hierarchical Relation Tree) 구성과 Target-adaptive Context Aggregation의 두 가지 고유한 블록을 제공합니다. 보다 구체적으로 HRTree는 먼저 가능한 관계 후보를 효율적으로 구성하기 위한 적응형 구조를 제공하고, 시공간 구조 정보를 효과적으로 캡처할 수 있도록 컨텍스트 집계 모듈을 안내합니다. 그런 다음 각 관계 후보에 대해 상황에 맞는 특징 표현을 얻고 해당 관계 카테고리를 인식하는 분류 헤드를 구축합니다. 마지막으로 TRACE 감지 결과를 추적하여 비디오 수준 VidSGG를 생성하는 간단한 시간적 연관 전략을 제공합니다. 우리는 ImageNet-VidVRD와 Action Genome이라는 두 가지 VidSGG 벤치마크에 대한 실험을 수행했으며 그 결과 TRACE가 최첨단 성능을 달성했음을 보여줍니다. 코드와 모델은 \url{https://github.com/MCG-NJU/TRACE}에서 확인할 수 있습니다."
208,http://arxiv.org/abs/2108.07073 ,ROSITA: Enhancing Vision-and-Language Semantic Alignments via Cross- and Intra-modal Knowledge Integration,"Yuhao Cui, Zhou Yu, Chunqi Wang, Zhongzhou Zhao, Ji Zhang, Meng Wang, Jun Yu",VLP(비전 및 언어 사전 훈련)는 대규모 이미지-텍스트 쌍에서 일반적인 다중 모드 표현을 학습하는 것을 목표로 합니다. 다양한 성공적인 시도가 제안되었지만 이미지-텍스트 쌍 간의 세분화된 의미 정렬을 학습하는 것이 접근 방식에서 핵심적인 역할을 합니다. 그럼에도 불구하고 대부분의 기존 VLP 접근 방식은 이미지-텍스트 쌍 내의 고유 지식을 완전히 활용하지 못하여 학습된 정렬의 효율성을 제한하고 모델 성능을 더욱 제한합니다. 이를 위해 우리는 의미론적 정렬을 향상시키기 위해 통합 장면 그래프에 교차 및 내부 모달 지식을 통합하는 ROSITA라는 새로운 VLP 방법을 소개합니다. 특히 장면 그래프 구조를 선험적으로 사용하여 마스킹된 언어(영역) 모델링을 수행하는 새로운 구조적 지식 마스킹(SKM) 전략을 소개합니다. 이는 양식 내 및 양식 전반에 걸쳐 간섭 정보를 제거하여 의미 정렬을 향상시킵니다. 광범위한 제거 연구와 포괄적인 분석을 통해 의미 체계 정렬에서 ROSITA의 효율성이 검증되었습니다. 도메인 내 및 도메인 외부 데이터 세트로 사전 훈련된 ROSITA는 6개의 벤치마크 데이터 세트에 대한 세 가지 일반적인 비전 및 언어 작업에서 기존의 최첨단 VLP 방법보다 훨씬 뛰어난 성능을 발휘합니다.
207,http://arxiv.org/abs/2108.05884 ,Unconditional Scene Graph Generation,"Sarthak Garg, Helisa Dhamo, Azade Farshad, Sabrina Musatian, Nassir Navab, Federico Tombari","단일 도메인 또는 단일 객체 이미지 생성의 최근 발전에도 불구하고 다양한 다중 객체와 상호 작용을 포함하는 복잡한 장면을 생성하는 것은 여전히 ​​​​어려운 일입니다. 객체로서의 노드와 객체 간의 관계인 방향성 에지로 구성된 장면 그래프는 이미지보다 의미론적으로 더 기초적인 장면의 대안적 표현을 제공합니다. 우리는 장면 그래프의 생성 모델이 이미지보다 실제 장면의 기본 의미 구조를 더 효과적으로 학습할 수 있으므로 장면 그래프 형태로 사실적이고 새로운 장면을 생성할 수 있다고 가정합니다. 본 연구에서는 의미론적 장면 그래프의 무조건 생성을 위한 새로운 작업을 탐색합니다. 우리는 계층적 순환 아키텍처를 사용하여 레이블이 지정된 그래프와 방향이 지정된 그래프에 대한 확률 분포를 직접 학습할 수 있는 SceneGraphGen이라는 심층 자동 회귀 모델을 개발합니다. 모델은 시드 개체를 입력으로 사용하고 일련의 단계로 장면 그래프를 생성합니다. 각 단계에서는 개체 노드를 생성한 다음 이전 노드에 연결되는 일련의 관계 가장자리가 이어집니다. SceneGraphGen에 의해 생성된 장면 그래프가 다양하고 실제 장면의 의미론적 패턴을 따른다는 것을 보여줍니다. 또한 생성된 그래프를 이미지 합성, 이상 탐지 및 장면 그래프 완성에 적용하는 방법을 보여줍니다."
206,http://arxiv.org/abs/2108.03554 ,Semantic-Based Explainable AI: Leveraging Semantic Scene Graphs and Pairwise Ranking to Explain Robot Failures,"Devleena Das, Sonia Chernova",구조화되지 않은 인간 환경에서 상호 작용할 때 가끔 로봇 오류가 발생하는 것은 불가피합니다. 이러한 장애가 발생하면 숙련된 기술자가 아닌 일반 사람들이 가장 먼저 대응하게 됩니다. 기존의 자연어 설명은 일상적인 사람들이 로봇 오류를 이해할 수 있도록 환경의 상황 정보에 손으로 주석을 추가합니다. 그러나 이 방법론에는 일반화성과 확장성이 부족합니다. 우리 작업에서는 보다 일반화 가능한 의미론적 설명 프레임워크를 소개합니다. 우리의 프레임워크는 장면의 의미 정보를 자동으로 캡처하여 일상적인 사용자를 위한 의미론적 설명을 생성합니다. 의미론적으로 기반을 둔 실패 중심 설명을 생성하기 위해 의미론적 장면 그래프를 모두 활용하여 환경에서 공간적 관계와 객체 속성을 추출하고 쌍별 순위를 매깁니다. 우리의 결과는 이러한 의미론적 설명이 기존의 최첨단 상황 기반 설명보다 오류를 식별하고 복구에 대한 지원을 제공하는 일상적인 사용자의 능력을 크게 향상한다는 것을 보여줍니다.
205,http://arxiv.org/abs/2108.03541 ,OSCAR-Net: Object-centric Scene Graph Attention for Image Attribution,"Eric Nguyen, Tu Bui, Vishy Swaminathan, John Collomosse","이미지는 강력한 이야기를 전달하지만 항상 신뢰할 수는 없습니다. 이미지를 신뢰할 수 있는 소스와 다시 일치시키면(속성) 사용자가 온라인에서 접하는 이미지에 대해 더 많은 정보를 바탕으로 판단할 수 있습니다. 우리는 이러한 매칭을 수행하기 위해 강력한 이미지 해싱 알고리즘을 제안합니다. 우리의 해시는 이미지가 전달하는 이야기를 실질적으로 바꿀 수 있는 미묘하고 두드러진 시각적 세부 사항을 조작하는 데 민감합니다. 그러나 해시는 온라인 재배포 중에 이미지가 겪는 양성 변환(품질, 코덱, 크기, 모양 등의 변화)에 대해 변하지 않습니다. 우리의 주요 기여는 OSCAR-Net(이미지 속성 네트워크를 위한 개체 중심 장면 그래프 주의)입니다. 시각적 영역에서 Transformers의 최근 성공에서 영감을 받은 강력한 이미지 해싱 모델입니다. OSCAR-Net은 모든 개체의 시각적 모양과 공간적 관계의 세밀한 변화에 주의를 기울이는 장면 그래프 표현을 구성합니다. 네트워크는 수백만 개의 이미지로 확장되는 콘텐츠 핑거프린팅을 위한 최첨단 이미지 해시를 생성하는 원본 이미지와 조작된 이미지의 데이터 세트에 대한 대조 학습을 통해 훈련됩니다."
204,http://arxiv.org/abs/2108.01176 ,Hierarchical Representations and Explicit Memory: Learning Effective Navigation Policies on 3D Scene Graphs using Graph Neural Networks,"Zachary Ravichandran, Lisa Peng, Nathan Hughes, J. Daniel Griffith, Luca Carlone","표현은 로봇이 효과적인 탐색 정책을 학습하는 데 매우 중요합니다. 최근 연구에 따르면 깊이 추정이나 2D 의미론적 분할과 같은 중간 수준의 지각 추상화가 원시 센서 데이터(예: RGB 이미지) 대신 관측값으로 제공될 때 더 효과적인 정책으로 이어지는 것으로 나타났습니다. 그러나 이러한 정책은 여전히 ​​중간 수준 추상화에서 잠재적인 3차원 장면 속성을 학습해야 합니다. 이와 대조적으로 3D 장면 그래프와 같은 상위 수준의 계층적 표현은 장면의 기하학적 구조, 토폴로지 및 의미를 명시적으로 제공하므로 탐색을 위한 강력한 표현이 됩니다. 이 연구에서는 높은 수준의 계층적 표현을 활용하여 탐색 정책을 학습하는 강화 학습 프레임워크를 제시합니다. 이 목표를 위해 그래프 신경망 아키텍처를 제안하고 3D 장면 그래프를 에이전트 중심 기능 공간에 삽입하는 방법을 보여줍니다. 이를 통해 로봇은 엔드투엔드 방식으로 낮은 수준의 작업에 대한 정책을 학습할 수 있습니다. 장면 그래프의 각 노드에 대해 우리 방법은 점유 및 의미 콘텐츠를 캡처하는 동시에 로봇 궤적의 메모리를 명시적으로 유지하는 기능을 사용합니다. 우리는 까다로운 개체 검색 작업에서 일반적으로 사용되는 시력 운동 정책에 대한 방법의 효율성을 보여줍니다. 이러한 실험과 지원 절제 연구는 우리의 방법이 보다 효과적인 객체 검색 동작으로 이어지고, 향상된 장기 기억을 나타내며, 계층적 정보를 성공적으로 활용하여 탐색 목표를 안내한다는 것을 보여줍니다."
203,http://arxiv.org/abs/2108.00316 ,Chest ImaGenome Dataset for Clinical Reasoning,"Joy T. Wu, Nkechinyere N. Agu, Ismini Lourentzou, Arjun Sharma, Joseph A. Paguio, Jasper S. Yao, Edward C. Dee, William Mitchell, Satyananda Kashyap, Andrea Giovannini, Leo A. Celi, Mehdi Moradi","최근 몇 년 동안 흉부 X선(CXR) 이미지에서 방사선학적 소견을 자동으로 감지하는 발전에도 불구하고 이러한 모델의 설명 가능성에 대한 정량적 평가는 다양한 소견에 대해 지역적으로 레이블이 지정된 데이터 세트가 부족하여 방해를 받습니다. 폐렴 및 기흉과 같은 특정 결과에 대해 전문가가 라벨을 붙인 소수의 소규모 데이터 세트를 제외하고 현재까지 대부분의 CXR 딥 러닝 모델은 텍스트 보고서에서 추출된 글로벌 ""약한"" 라벨에 대해 훈련되거나 공동 이미지 및 구조화되지 않은 텍스트 학습 전략을 통해 훈련됩니다. 컴퓨터 비전 커뮤니티의 Visual Genome 노력에 영감을 받아 $242,072$ 이미지를 설명하는 장면 그래프 데이터 구조를 갖춘 최초의 Chest ImaGenome 데이터 세트를 구성했습니다. 로컬 주석은 공동 규칙 기반 자연어 처리(NLP) 및 아틀라스 기반 경계 상자 감지 파이프라인을 사용하여 자동으로 생성됩니다. 방사선 전문의가 구축한 CXR 온톨로지를 통해 각 CXR에 대한 주석은 해부학 중심 장면 그래프로 연결되어 이미지 수준 추론 및 다중 모드 융합 애플리케이션에 유용합니다. 전반적으로 우리는 다음을 제공합니다: i) $29$ CXR 해부학적 위치(경계 상자 좌표가 있는 객체)와 이미지당 장면 그래프로 구조화된 해당 속성 사이의 $1,256$ 관계 주석 조합, ii) 순차 검사 전반에 걸쳐 해부학적 위치 간 $670,000$ 이상의 국지화된 비교 관계(개선, 악화 또는 변경 없음), ii) 수동으로 주석이 추가된 최적 표준 장면 그래프 데이터세트 $500$ 고유 환자."
202,http://arxiv.org/abs/2107.14178 ,ReFormer: The Relational Transformer for Image Captioning,"Xuewen Yang, Yingru Liu, Xin Wang","이미지 캡션은 장면 그래프를 사용하여 이미지 내 객체의 관계를 표현함으로써 더 나은 성능을 얻을 수 있는 것으로 나타났습니다. 현재 캡션 인코더는 일반적으로 GCN(Graph Convolutional Net)을 사용하여 관계 정보를 표현하고 이를 연결 또는 컨볼루션을 통해 객체 영역 특징과 병합하여 문장 디코딩을 위한 최종 입력을 얻습니다. 그러나 기존 방식의 GCN 기반 인코더는 두 가지 이유로 인해 자막 처리에 덜 효과적입니다. 첫째, 관계 중심 손실보다는 이미지 캡션을 목표(즉, 최대 가능성 추정)로 사용하면 인코더의 잠재력을 완전히 탐색할 수 없습니다. 둘째, 관계를 추출하기 위해 인코더 자체 대신 사전 훈련된 모델을 사용하는 것은 유연하지 않으며 모델의 설명 가능성에 기여할 수 없습니다. 이미지 캡션 품질을 향상시키기 위해 우리는 관계 정보가 포함된 특징을 생성하고 이미지의 개체 간의 쌍 관계를 명시적으로 표현하는 새로운 아키텍처 ReFormer(관계형 변환기)를 제안합니다. ReFormer는 하나의 수정된 Transformer 모델을 사용하여 장면 그래프 생성 목적과 이미지 캡션 작성 목적을 통합합니다. 이 설계를 통해 ReFormer는 강력한 관계형 이미지 특징을 추출하는 이점을 통해 더 나은 이미지 캡션을 생성할 수 있을 뿐만 아니라 쌍별 관계를 명시적으로 설명하는 장면 그래프도 생성할 수 있습니다. 공개적으로 사용 가능한 데이터 세트에 대한 실험에서는 우리 모델이 이미지 캡션 및 장면 그래프 생성에 대한 최첨단 방법보다 훨씬 뛰어난 성능을 보여줍니다."
201,http://arxiv.org/abs/2107.12604 ,Image Scene Graph Generation (SGG) Benchmark,"Xiaotian Han, Jianwei Yang, Houdong Hu, Lei Zhang, Jianfeng Gao, Pengchuan Zhang","객체 검출을 넘어 세밀한 영상 이해 모델 구축의 필요성으로 인해 영상 장면 그래프 생성(객체, 속성, 관계 검출)에 대한 관심이 급증하고 있습니다. 좋은 벤치마크가 부족하기 때문에 다양한 장면 그래프 생성 모델의 보고된 결과를 직접 비교할 수 없어 연구 진행이 지연됩니다. 우리는 Maskrcnn-benchmark와 여러 인기 모델을 기반으로 꼭 필요한 장면 그래프 생성 벤치마크를 개발했습니다. 이 문서에서는 벤치마크의 주요 기능과 Visual Genome 및 OpenImages 시각적 관계 감지 데이터 세트를 사용하는 장면 그래프 생성 모델에 대한 포괄적인 절제 연구를 제시합니다. 우리의 코드베이스는 https://github.com/microsoft/scene_graph_benchmark에서 공개적으로 제공됩니다."
200,http://arxiv.org/abs/2107.12309 ,Spatial-Temporal Transformer for Dynamic Scene Graph Generation,"Yuren Cong, Wentong Liao, Hanno Ackermann, Bodo Rosenhahn, Michael Ying Yang","동적 장면 그래프 생성은 주어진 비디오의 장면 그래프를 생성하는 것을 목표로 합니다. 이미지에서 장면 그래프를 생성하는 작업에 비해 객체 간의 동적 관계와 프레임 간의 시간적 종속성으로 인해 더 풍부한 의미 해석이 가능하기 때문에 더 어렵습니다. 본 논문에서는 두 가지 핵심 모듈로 구성된 신경망인 STTran(Spatial-Temporal Transformer)을 제안합니다. (1) 입력 프레임을 사용하여 공간 컨텍스트를 추출하고 프레임 내 시각적 관계를 추론하는 공간 인코더, (2) 프레임 간의 시간적 종속성을 캡처하고 동적 관계를 추론하기 위해 공간 인코더의 출력을 입력으로 사용하는 시간 디코더. 또한 STTran은 클리핑 없이 다양한 길이의 비디오를 입력으로 사용할 수 있는 유연성을 갖추고 있으며 이는 긴 비디오에 특히 중요합니다. 우리의 방법은 벤치마크 데이터세트인 Action Genome(AG)에서 검증되었습니다. 실험 결과는 동적 장면 그래프 측면에서 우리 방법의 우수한 성능을 보여줍니다. 또한 일련의 절제 연구가 수행되고 제안된 각 모듈의 효과가 정당화됩니다. 코드는 https://github.com/yrcong/STTran에서 확인할 수 있습니다."
199,http://arxiv.org/abs/2107.06325 ,Graphhopper: Multi-Hop Scene Graph Reasoning for Visual Question Answering,"Rajat Koner, Hang Li, Marcel Hildebrandt, Deepan Das, Volker Tresp, Stephan Günnemann","VQA(시각적 질문 응답)는 이미지에 대한 자유 형식 질문에 답변하는 것과 관련이 있습니다. 질문에 대한 깊은 의미론적, 언어적 이해와 이를 이미지에 존재하는 다양한 개체와 연관시키는 능력이 필요하기 때문에 야심찬 작업이며 컴퓨터 비전과 자연어 처리 모두의 다중 모드 추론이 필요합니다. 우리는 지식 그래프 추론, 컴퓨터 비전, 자연어 처리 기술을 통합하여 작업에 접근하는 새로운 방법인 Graphhopper를 제안합니다. 구체적으로, 우리의 방법은 장면 엔터티와 의미 및 공간적 관계를 기반으로 하는 상황 중심의 순차적 추론을 수행하는 데 기반을 두고 있습니다. 첫 번째 단계로, 우리는 이미지 속 물체와 그 속성, 상호 관계를 설명하는 장면 그래프를 도출합니다. 이후, 강화학습 에이전트는 추출된 장면 그래프를 멀티홉 방식으로 자율적으로 탐색하여 답변 도출의 기반이 되는 추론 경로를 생성하도록 훈련됩니다. 우리는 수동으로 선별된 장면 그래프와 자동으로 생성된 장면 그래프를 기반으로 까다로운 데이터 세트 GQA에 대한 실험적 연구를 수행합니다. 우리의 결과는 우리가 수동으로 선별한 장면 그래프에서 인간의 성능을 따라잡는다는 것을 보여줍니다. 또한 Graphhopper는 수동으로 선별된 장면 그래프와 자동으로 생성된 장면 그래프 모두에서 또 다른 최첨단 장면 그래프 추론 모델보다 훨씬 뛰어난 성능을 발휘한다는 사실을 발견했습니다."
198,http://arxiv.org/abs/2107.05448 ,Scenes and Surroundings: Scene Graph Generation using Relation Transformer,"Rajat Koner, Poulami Sinhamahapatra, Volker Tresp","이미지 속 객체와 상호 관계를 장면 그래프로 식별하면 이미지 내용에 대한 깊은 이해를 얻을 수 있습니다. 최근 딥러닝의 발전에도 불구하고 시각적 객체 관계를 감지하고 라벨링하는 것은 여전히 ​​어려운 작업으로 남아 있습니다. 이 작업은 복잡한 전역 개체를 개체로, 개체와 가장자리(관계) 상호 작용을 활용하는 관계 변환기라는 새로운 로컬 컨텍스트 인식 아키텍처를 제안합니다. 우리의 계층적 다중 헤드 주의 기반 접근 방식은 객체 간의 상황별 종속성을 효율적으로 포착하고 객체의 관계를 예측합니다. 최첨단 접근 방식과 비교하여 우리는 Visual Genome 데이터 세트의 모든 장면 그래프 생성 작업에 걸쳐 전반적인 평균 \textbf{4.85\%} 개선과 새로운 벤치마크를 달성했습니다."
197,http://arxiv.org/abs/2107.05080 ,Zero-Shot Scene Graph Relation Prediction through Commonsense Knowledge Integration,"Xuan Kan, Hejie Cui, Carl Yang","이미지 속 엔터티 간의 관계 예측은 장면 그래프 생성(SGG)의 중요한 단계이며, 이는 다양한 시각적 이해 및 추론 작업에 더욱 영향을 미칩니다. 그러나 기존 SGG 프레임워크는 과도한 훈련이 필요하지만 보이지 않는(즉, 제로샷) 삼중항을 모델링할 수 없습니다. 본 연구에서는 그러한 무능력이 상식적 추론, 즉 세상에 대한 일반적인 이해를 바탕으로 유사한 개체를 연관시키고 유사한 관계를 추론하는 능력이 부족하기 때문임을 강조한다. 이러한 격차를 메우기 위해 우리는 특히 제로샷 관계 예측을 위해 SGG에 대한 상식 지식을 통합하는 프레임워크인 CommOnsense-integrAted sCenegrapHrElation pRediction(COACHER)을 제안합니다. 구체적으로 우리는 외부 상식 지식 그래프에서 엔터티 주변의 이웃과 경로를 모델링하고 이를 최첨단 SGG 프레임워크 위에 통합하는 새로운 그래프 마이닝 파이프라인을 개발합니다. Visual Genome의 원본 데이터세트와 조작된 데이터세트 모두에 대한 광범위한 정량적 평가와 정성적 사례 연구는 제안된 접근 방식의 효율성을 입증합니다."
196,http://arxiv.org/abs/2107.02713 ,Predicate correlation learning for scene graph generation,"Leitian Tao, Li Mi, Nannan Li, Xianhang Cheng, Yaosi Hu, Zhenzhong Chen","일반적인 SGG(장면 그래프 생성) 방법의 경우 조건자의 헤드 클래스와 테일 클래스의 성능에 큰 차이가 있는 경우가 많습니다. 이러한 현상은 주로 서로 다른 술어 간의 의미적 중복과 긴 꼬리 데이터 분포로 인해 발생합니다. 본 논문에서는 술어 간의 상관관계를 고려하여 위의 두 가지 문제를 해결하기 위해 SGG를 위한 PCL(Predicate Correlation Learning) 방법을 제안합니다. 강한 상관 관계가 있는 조건자 클래스 간의 의미적 중복을 설명하기 위해 조건자 쌍 간의 관계를 정량화하기 위해 조건자 상관 행렬(PCM)이 정의됩니다. 이는 행렬의 긴 꼬리 편향을 제거하기 위해 동적으로 업데이트됩니다. 또한 PCM은 주석이 없는 클래스의 실망스러운 기울기를 줄이기 위해 Predicate Correlation Loss 함수($L_{PC}$)에 통합되었습니다. 제안된 방법은 Visual Genome 벤치마크에서 평가되었으며, 기존 방법을 기반으로 구축할 때 tail 클래스의 성능이 크게 향상되었습니다."
195,http://arxiv.org/abs/2107.02112 ,Recovering the Unbiased Scene Graphs from the Biased Ones,"Meng-Jiun Chiou, Henghui Ding, Hanshu Yan, Changhu Wang, Roger Zimmermann, Jiashi Feng","입력 이미지가 주어지면 장면 그래프 생성(SGG)은 주요 개체 간의 시각적 관계를 설명하는 포괄적인 그래픽 표현을 생성하는 것을 목표로 합니다. 최근 SGG에서는 롱테일 문제를 해결하기 위해 더 많은 노력을 기울이고 있습니다. 그러나 다양한 클래스의 누락된 라벨 비율의 불균형 또는 보고 편향으로 인해 롱테일을 악화시키는 문제는 거의 고려되지 않으며 기존 편향성 제거 방법으로는 해결할 수 없습니다. 이 논문에서 우리는 누락된 레이블로 인해 SGG가 ""긍정적이고 레이블이 지정되지 않은 데이터로부터 학습""(PU 학습) 문제로 볼 수 있음을 보여줍니다. 여기서 보고 편향은 레이블 빈도(예: 모든 긍정적인 예에서 레이블이 지정된 긍정적인 예의 클래스별 비율)를 활용하여 편향된 확률에서 편향되지 않은 확률을 복구하여 제거될 수 있습니다. 정확한 레이블 빈도 추정치를 얻기 위해 우리는 훈련 시간 데이터 증대를 활용하고 여러 훈련 반복에 대한 평균을 활용하여 더 유효한 예를 도입하는 DLFE(동적 레이블 빈도 추정)를 제안합니다. 광범위한 실험에 따르면 DLFE는 기존 추정의 순진한 변형보다 레이블 빈도를 추정하는 데 더 효과적이며 DLFE는 롱테일을 크게 완화하고 VG 데이터 세트에서 최첨단 편향성 제거 성능을 달성합니다. 또한 DLFE를 사용하는 SGG 모델이 더욱 균형있고 편향되지 않은 장면 그래프를 눈에 띄게 생성한다는 사실을 질적으로 보여줍니다."
194,http://arxiv.org/abs/2106.14476 ,Adventurer's Treasure Hunt: A Transparent System for Visually Grounded Compositional Visual Question Answering based on Scene Graphs,"Daniel Reich, Felix Putze, Tanja Schultz","VQA의 추론 과정에서 시스템 투명성과 시각적 기반을 향상시키겠다는 목표를 가지고 장면 그래프를 기반으로 하는 구성 VQA 작업을 위한 모듈형 시스템을 제시합니다. 우리 시스템은 ""모험가의 보물 찾기""(또는 ATH)라고 불리며, 답을 찾기 위한 모델의 검색 절차와 모험가의 보물 검색 사이에 유추한 이름을 따서 명명되었습니다. 우리는 세 가지 특징을 염두에 두고 ATH를 개발했습니다. 1. 설계상 ATH를 사용하면 전체 VQA 성능에 대한 각 하위 구성 요소의 영향과 개별 하위 작업에 대한 성능을 명시적으로 정량화할 수 있습니다. 2. ATH는 보물찾기 후 검색 작업을 모델링함으로써 본질적으로 처리된 질문에 대한 명시적이고 시각적으로 근거한 추론 경로를 생성합니다. 3. ATH는 미리 고정된 답변 어휘에 대해 특별히 학습된 분류기의 출력 분포에서 하나를 선택하는 대신 시각적 지식 기반을 직접 쿼리하여 답변을 동적으로 추출하는 최초의 GQA 훈련 VQA 시스템입니다. 우리는 모든 구성 요소에 대한 자세한 결과와 GQA 데이터 세트의 전체 VQA 성능에 대한 기여도를 보고하고 ATH가 검사된 모든 시스템 중에서 가장 높은 시각적 접지 점수를 달성했음을 보여줍니다."
193,http://arxiv.org/abs/2106.10936 ,TCIC: Theme Concepts Learning Cross Language and Vision for Image Captioning,"Zhihao Fan, Zhongyu Wei, Siyuan Wang, Ruize Wang, Zejun Li, Haijun Shan, Xuanjing Huang",이미지 캡션에 대한 기존 연구는 일반적으로 낮은 수준의 사실(객체 및 관계)을 포함하는 장면 그래프를 사용하여 이미지를 표현하므로 높은 수준의 의미를 포착하지 못합니다. 본 논문에서는 높은 수준의 교차 양식 의미론을 표현하기 위해 테마 개념을 통합하는 TCIC(테마 개념 확장 이미지 캡션) 프레임워크를 제안합니다. 실제로 우리는 테마 개념을 메모리 벡터로 모델링하고 해당 벡터를 이미지 캡션에 통합하기 위해 TTN(Transformer with Theme Nodes)을 제안합니다. 이미지와 캡션 모두에서 테마 개념을 학습할 수 있다는 점을 고려하여 TTN 기반 표현 학습을 위해 두 가지 설정을 제안합니다. 비전 측면에서 TTN은 장면 그래프 기반 기능과 테마 개념을 모두 시각적 표현 학습을 위한 입력으로 사용하도록 구성됩니다. 언어 측면에서 TTN은 텍스트 표현 재구성을 위한 입력으로 캡션과 테마 개념을 모두 사용하도록 구성됩니다. 두 설정 모두 동일한 변환기 기반 디코더를 사용하여 대상 캡션을 생성하는 것을 목표로 합니다. 훈련 중에 이미지와 해당 캡션에서 학습된 테마 개념의 표현을 추가로 정렬하여 교차 양식 학습을 강화합니다. MS COCO에 대한 실험 결과는 일부 최첨단 모델과 비교하여 우리 접근 방식의 효율성을 보여줍니다.
192,http://arxiv.org/abs/2106.10815 ,Structured Sparse R-CNN for Direct Scene Graph Generation,"Yao Teng, Limin Wang","장면 그래프 생성(SGG)은 이미지 내 관계를 통해 개체 쌍을 감지하는 것입니다. 기존 SGG 접근 방식은 종종 다단계 파이프라인을 사용하여 이 작업을 객체 감지, 관계 그래프 구성, 밀집 또는 밀집-희소 관계 예측으로 분해합니다. 대신, SGG를 직접 집합 예측으로 보는 관점에서 본 논문은 Structured Sparse R-CNN이라는 간단하고 희소하며 통합된 프레임워크를 제시합니다. 우리 방법의 핵심은 학습 가능한 삼중항 쿼리 세트와 엔드투엔드 방식으로 훈련 세트에서 공동으로 최적화할 수 있는 구조화된 삼중항 검출기입니다. 특히, 삼중항 쿼리는 객체 쌍의 일반적인 사전 관계를 해당 관계와 함께 인코딩하고 후속 개선을 위한 장면 그래프의 초기 추측을 제공합니다. 삼중항 검출기는 맞춤형 동적 헤드로 검출된 장면 그래프를 점진적으로 개선하기 위해 계단식 아키텍처를 제공합니다. 또한, 우리 방법의 훈련 어려움을 완화하기 위해 Siamese Sparse R-CNN의 지식 증류를 기반으로 완화되고 향상된 훈련 전략을 제안합니다. 우리는 Visual Genome과 Open Images V4/V6 등 여러 데이터 세트에 대해 실험을 수행했으며 결과는 우리의 방법이 최첨단 성능을 달성했음을 보여줍니다. 또한 삼중항 검출기 설계 및 훈련 전략의 구조적 모델링에 대한 통찰력을 제공하기 위해 심층적인 절제 연구도 수행합니다. 코드와 모델은 https://github.com/MCG-NJU/Structured-Sparse-RCNN에서 확인할 수 있습니다."
191,http://arxiv.org/abs/2106.08543 ,Tackling the Challenges in Scene Graph Generation with Local-to-Global Interactions,"Sangmin Woo, Junhyug Noh, Kangil Kim","이 작업에서 우리는 장면 그래프 생성(SGG) 작업의 기본 과제에 대한 새로운 통찰력을 추구합니다. Visual Genome 데이터 세트의 정량적, 정성적 분석은 다음을 의미합니다. 1) 모호성: 개체 간 관계에 동일한 개체(또는 술어)가 포함되어 있어도 시각적 또는 의미상 유사하지 않을 수 있음, 2) 비대칭성: 방향을 구현하는 관계의 특성에도 불구하고 이전 연구에서 잘 다루지 않았음, 3) 고차 컨텍스트: 특정 그래프 요소의 동일성을 활용하면 정확한 장면 그래프를 생성하는 데 도움이 될 수 있습니다. 분석을 바탕으로 우리는 새로운 SGG 프레임워크인 LOGIN(Local-to-Global Interaction Networks)을 설계했습니다. 지역적으로 상호 작용은 주체, 객체 및 배경의 세 가지 인스턴스 사이의 본질을 추출하는 동시에 주체와 객체의 입력 순서를 명시적으로 제한하여 방향 인식을 네트워크에 굽습니다. 전체적으로 상호 작용은 모든 그래프 구성 요소(예: 노드 및 에지) 간의 컨텍스트를 인코딩합니다. 마지막으로 Attract & Repel 손실은 조건자 임베딩의 분포를 미세 조정하는 데 활용됩니다. 설계상, 우리의 프레임워크는 가능한 보완성을 활용하여 상향식 방식으로 장면 그래프를 예측할 수 있습니다. LOGIN이 관계 방향을 얼마나 인식하는지 정량화하기 위해 BRC(양방향 관계 분류)라는 새로운 진단 작업도 제안되었습니다. 실험 결과는 LOGIN이 기존 방법(BRC 작업)과 관계 방향을 성공적으로 구별할 수 있음을 보여주며, Visual Genome 벤치마크(SGG 작업)에서는 최첨단 결과를 보여줍니다."
190,http://arxiv.org/abs/2106.15309 ,Multimodal Semantic Scene Graphs for Holistic Modeling of Surgical Procedures,"Ege Özsoy, Evin Pınar Örnek, Ulrich Eck, Federico Tombari, Nassir Navab","컴퓨터 과학의 관점에서 볼 때 수술 도메인 모델은 행동과 데이터를 모두 통합하는 개념적 모델이어야 합니다. 따라서 행위자, 장치, 도구, 이들의 복잡한 상호 작용 및 데이터 흐름을 모델링해야 합니다. 이를 캡처하고 모델링하기 위해 우리는 카메라 뷰에서 3D 장면 그래프를 생성하는 최신 컴퓨터 비전 방법론을 활용합니다. 그런 다음 수술 절차의 통합된 상징적, 시공간적 및 의미론적 표현을 제공하는 것을 목표로 하는 MSSG(Multimodal Semantic Scene Graph)를 소개합니다. 이 방법론은 의료진, 영상 시스템, 수술 장치 등 수술 영역의 다양한 구성 요소 간의 관계를 모델링하여 수술 절차에 대한 전체적인 이해와 모델링을 향한 길을 여는 것을 목표로 합니다. 그런 다음 MSSG를 사용하여 프로세스 최적화, OR 설계 및 자동 보고서 생성을 포함한 다양한 응용 프로그램에 사용할 수 있는 수술 절차 분석을 위해 동적으로 생성된 그래픽 사용자 인터페이스 도구를 소개합니다. 우리는 마침내 제안된 MSSG가 서로 다른 복잡한 수술 절차를 동기화하는 데 사용될 수 있음을 보여줍니다. 검증을 받기 전에 시스템을 실제 수술실에 통합해야 하지만, 이 컨퍼런스 문서는 주로 MVOR 데이터 세트를 기반으로 한 첫 번째 프로토타입 부분 구현을 통해 이 새로운 개념의 기본 원리를 커뮤니티에 제공하는 것을 목표로 합니다."
189,http://arxiv.org/abs/2106.03128 ,MOC-GAN: Mixing Objects and Captions to Generate Realistic Images,"Tao Ma, Yikang Li","조건부 설명을 사용하여 이미지를 생성하는 것은 최근 몇 년 동안 점점 더 많은 관심을 받고 있습니다. 그러나 기존 조건부 입력은 구조화되지 않은 형식(캡션)이나 제한된 정보 및 값비싼 라벨링(장면 그래프)으로 인해 어려움을 겪고 있습니다. 타겟 장면의 경우 핵심 항목인 개체는 일반적으로 명확하지만 상호 작용은 유연하고 명확하게 정의하기 어렵습니다. 따라서 우리는 객체와 캡션으로부터 사실적인 이미지를 생성하는 보다 합리적인 설정을 도입합니다. 이 설정에서 개체는 대상 이미지의 중요한 역할을 명시적으로 정의하고 캡션은 풍부한 속성과 연결을 암시적으로 설명합니다. 이에 따라 두 가지 양식의 입력을 혼합하여 사실적인 이미지를 생성하는 MOC-GAN이 제안되었습니다. 먼저 캡션에서 객체 쌍 간의 암시적 관계를 추론하여 숨겨진 상태 장면 그래프를 구축합니다. 따라서 객체, 관계 및 캡션을 포함하는 다층 표현이 구성되며, 여기서 장면 그래프는 장면의 구조를 제공하고 캡션은 이미지 수준 지침을 제공합니다. 그런 다음 계단식 주의 생성 네트워크는 캡션에서 가장 관련성이 높은 단어에 주의를 기울여 구문 패치를 대략적으로 생성하도록 설계되었습니다. 또한, 세분화된 구문 패치 일관성을 더 잘 감독하기 위해 구문별 DAMSM이 제안되었습니다. COCO 데이터 세트에서 우리의 방법은 높은 시각적 품질을 유지하면서 Inception Score와 FID 모두에서 최첨단 방법보다 성능이 뛰어납니다. 광범위한 실험을 통해 제안된 방법의 고유한 기능이 입증되었습니다."
188,http://arxiv.org/abs/2106.02400 ,A Deep Local and Global Scene-Graph Matching for Image-Text Retrieval,"Manh-Duy Nguyen, Binh T. Nguyen, Cathal Gurrin","이미지-텍스트 검색에 대한 기존 접근 방식은 주로 그림에 나타나는 시각적 개체를 색인화하는 데 중점을 두지만 이러한 개체 간의 상호 작용은 무시합니다. 이러한 개체 발생 및 상호 작용은 일반적으로 텍스트에서 언급되는 것처럼 이 분야에서도 유용하고 중요합니다. 장면 그래프 표현은 이미지-텍스트 매칭 과제에 적합한 방법이며, 상호 관계 정보를 캡처하는 능력으로 인해 좋은 결과를 얻었습니다. 이미지와 텍스트는 모두 장면 그래프 수준으로 표시되며 검색 문제를 장면 그래프 일치 문제로 공식화합니다. 본 논문에서는 그래프의 일반 정보를 캡처하기 위해 추가 그래프 컨볼루션 네트워크를 통합하여 최첨단 방법을 향상시키는 LGSGM(Local and Global Scene Graph Matching) 모델을 소개합니다. 특히 이미지와 캡션의 장면 그래프 쌍의 경우 각 그래프의 노드와 가장자리의 특징을 학습하기 위해 두 개의 별도 모델이 사용됩니다. 그런 다음 Siamese 구조 그래프 컨볼루션 모델을 사용하여 그래프를 벡터 형식에 삽입합니다. 마지막으로 그래프 수준과 벡터 수준을 결합하여 이 이미지-텍스트 쌍의 유사성을 계산합니다. 경험적 실험은 수준 조합을 통한 개선이 Flickr30k 데이터 세트에서 재현율을 10% 이상 증가시켜 기준 방법의 성능을 향상시킬 수 있음을 보여줍니다."
187,http://arxiv.org/abs/2106.01607 ,Grounding Complex Navigational Instructions Using Scene Graphs,"Michiel de Jong, Satyapriya Krishna, Anuva Agarwal","자연어 명령을 수행하기 위해 강화 학습 에이전트를 훈련하는 것은 사용 가능한 감독, 즉 명령이 수행된 시기를 아는 것에 의해 제한됩니다. 우리는 CLEVR 시각적 질문 응답 데이터 세트를 조정하여 복잡한 자연어 탐색 지침과 그에 따른 장면 그래프를 생성하여 환경에 구애받지 않는 감독 데이터 세트를 생성합니다. 이 데이터 세트의 사용을 시연하기 위해 장면을 VizDoom 환경에 매핑하고 \citet{gatedattention}의 아키텍처를 사용하여 에이전트가 이러한 보다 복잡한 언어 명령을 수행하도록 교육합니다."
186,http://arxiv.org/abs/2105.13994 ,Linguistic Structures as Weak Supervision for Visual Scene Graph Generation,"Keren Ye, Adriana Kovashka","장면 그래프 생성의 이전 작업에는 경계 상자 정보가 있든 없든 주제와 객체, 그리고 이들을 관련시키는 술어 등 삼중항 수준의 범주형 감독이 필요합니다. 그러나 장면 그래프 생성은 전체적인 작업입니다. 따라서 전체적인 상황별 감독은 직관적으로 성능을 향상시켜야 합니다. 이 연구에서는 캡션의 언어 구조가 장면 그래프 생성에 어떻게 도움이 될 수 있는지 살펴봅니다. 우리의 방법은 개별 세 쌍둥이 간의 관계와 주제 및 개체의 맥락(예: 시각적 속성이 언급됨)에 ​​대해 캡션에 제공된 정보를 캡처합니다. 캡션은 사람이 주석을 추가한 주제와 삼중 항목의 전체 목록과 캡션의 명사 간의 정렬이 약하기 때문에 삼중 항목보다 감독 유형이 약합니다. 그러나 웹상의 다양한 다중 모드 데이터 소스(예: 이미지 및 캡션이 포함된 블로그 게시물)를 고려할 때 언어 감독은 크라우드소싱된 세 가지 데이터보다 확장성이 더 높습니다. 우리는 인스턴스 및 이미지 수준 감독을 활용하는 이전 방법에 대한 광범위한 실험적 비교를 보여주고, 구문 및 순차 컨텍스트 활용의 영향을 보여주기 위해 방법을 제거하고, 주제와 객체의 위치 파악을 개선하는 기술을 보여줍니다."
185,http://arxiv.org/abs/2105.07391 ,Survey of Visual-Semantic Embedding Methods for Zero-Shot Image Retrieval,Kazuya Ueki,"시각적 의미 임베딩은 시각적 질문 답변(VQA), 이미지-텍스트 검색, 이미지 캡션 작성, 장면 그래프 생성 등 다양한 작업에 유용하기 때문에 흥미로운 연구 주제입니다. 본 논문에서는 문장을 쿼리로 사용하는 제로샷 이미지 검색에 중점을 두고 이 분야의 기술 동향에 대한 조사를 제시합니다. 먼저, 이미지-텍스트 매칭에 대한 초기 연구와 시간이 지남에 따라 기술이 어떻게 발전했는지에 대한 논의를 시작으로 기술의 역사에 대한 포괄적인 개요를 제공합니다. 또한 실험에 일반적으로 사용되는 데이터 세트에 대한 설명과 각 방법의 평가 결과를 비교하여 제시합니다. 또한 실험의 정확성을 확인하고 추가 개선을 위해 github에서 사용할 수 있는 구현을 소개합니다. 우리는 이 설문조사 보고서가 연구자들이 이미지와 언어를 연결하는 연구를 더욱 발전시키는 데 도움이 되기를 바랍니다."
184,http://arxiv.org/abs/2105.07264 ,Neural Trees for Learning on Graphs,"Rajat Talak, Siyi Hu, Lisa Peng, Luca Carlone","그래프 신경망(GNN)은 그래프를 통한 학습을 ​​위한 유연하고 강력한 접근 방식으로 등장했습니다. 이러한 성공에도 불구하고 기존 GNN은 로컬 메시지 전달 아키텍처의 제약을 받고 있으며 표현력도 제한되어 있습니다. 이 연구에서 우리는 새로운 GNN 아키텍처인 Neural Tree를 제안합니다. 신경 트리 아키텍처는 입력 그래프에서 메시지 전달을 수행하지 않고 입력 그래프에서 구성된 H-트리라는 트리 구조 그래프에서 메시지 전달을 수행합니다. H-트리의 노드는 입력 그래프의 하위 그래프에 해당하며, H-트리에 있는 노드의 부모가 항상 입력 그래프의 더 큰 하위 그래프에 해당하도록 계층적 방식으로 재구성됩니다. 우리는 신경 트리 구조가 방향이 지정되지 않은 그래프에 대해 매끄러운 확률 분포 함수를 근사화할 수 있음을 보여줍니다. 우리는 또한 분포 함수의 $ε$ 근사를 달성하는 데 필요한 매개변수의 수가 입력 그래프의 트리 너비에서는 지수적이지만 크기는 선형이라는 것을 증명했습니다. 우리는 모든 연속 $\mathcal{G}$-불변/등변 함수가 $\mathcal{G}$에 대한 확률 분포 함수의 비선형 조합에 의해 근사화될 수 있음을 증명합니다. 우리는 3D 장면 그래프의 반 감독 노드 분류에 신경 트리를 적용하고 이러한 이론적 속성이 보다 전통적인 GNN 아키텍처에 비해 예측 정확도가 크게 향상된다는 것을 보여줍니다. 또한 그래프 하위 샘플링 기술을 사용하여 큰 트리 폭을 가진 인용 네트워크에 신경 트리 아키텍처의 적용 가능성을 보여줍니다."
183,http://arxiv.org/abs/2105.01610 ,Reliving the Dataset: Combining the Visualization of Road Users' Interactions with Scenario Reconstruction in Virtual Reality,"Lars Töttel, Maximilian Zipfl, Daniel Bogdoll, Marc René Zofka, J. Marius Zöllner","자동화된 차량 개발의 핵심 과제 중 하나는 교통 참가자가 많고 예측하기 어려운 다수의 복잡한 교통 시나리오를 처리할 수 있는 능력입니다. 반복적인 개발 프로세스의 일환으로, 고도로 자동화된 운전(HAD) 기능을 개선하기 위해 중요한 시나리오를 감지하고 그로부터 지식을 생성하는 것이 필요합니다. 이 문제를 해결하기 위해 지난 몇 년 동안 이러한 알고리즘의 개발 및 테스트를 위한 기초 역할을 하는 수많은 데이터 세트가 출시되었습니다. 그럼에도 불구하고 남은 과제는 이러한 데이터 세트에서 안전에 중요한 코너 케이스와 같은 관련 장면을 찾고 이를 완전히 이해하는 것입니다. 따라서 이 문서에서는 두 가지 방법으로 자연주의적인 모션 데이터 세트를 처리하고 분석하는 방법론을 제시합니다. 높은 수준의 객관적인 분석. 여기서는 임의의 중요도 측정값을 사용합니다. TTC, RSS 또는 SFF를 설정하여 교통 참가자 간의 중요한 시나리오를 자동으로 감지할 수 있습니다. 한편, 시나리오는 현실적인 가상 현실(VR) 환경에서 재현되므로 여러 대화형 관점에서 주관적인 근접 분석이 가능합니다."
182,http://arxiv.org/abs/2104.14207 ,Segmentation-grounded Scene Graph Generation,"Siddhesh Khandelwal, Mohammed Suhail, Leonid Sigal","장면 그래프 생성은 컴퓨터 비전에서 중요한 문제로 등장했습니다. 장면 그래프는 이미지 내 개체, 위치 및 관계에 대한 기본 표현을 제공하지만 제안 경계 상자의 세부 수준에서만 이를 수행합니다. 이 연구에서 우리는 우리가 아는 한 픽셀 수준 분할 기반 장면 그래프 생성을 위한 첫 번째 프레임워크를 제안합니다. 우리의 프레임워크는 기본 장면 그래프 생성 방법에 무관하며 보조 데이터 세트(예: MS COCO)와의 전송 및 다중 작업 학습을 통해 대상 장면 그래프 데이터 세트(예: Visual Genome)의 분할 주석 부족을 해결합니다. 구체적으로, 감지되는 각 대상 개체에는 보조 데이터 세트에 주석이 있는 범주에 대한 언어 유사성 가중치 선형 조합으로 표현되는 분할 마스크가 부여됩니다. 이러한 추론된 마스크는 이미지 내의 픽셀 수준에서 관계를 기반으로 하는 새로운 가우스 주의 메커니즘과 함께 향상된 관계 예측을 허용합니다. 전체 프레임워크는 엔드투엔드 학습이 가능하며 대상 데이터세트와 보조 데이터세트를 모두 사용하여 다중 작업 방식으로 학습됩니다."
181,http://arxiv.org/abs/2104.11934 ,RelTransformer: A Transformer-Based Long-Tail Visual Relationship Recognition,"Jun Chen, Aniket Agarwal, Sherif Abdelkarim, Deyao Zhu, Mohamed Elhoseiny",VRR(시각적 관계 인식) 작업은 이미지에서 상호 작용하는 개체 간의 쌍별 시각적 관계를 이해하는 것을 목표로 합니다. 이러한 관계는 구성적 특성으로 인해 일반적으로 롱테일 분포를 갖습니다. 이 문제는 어휘가 커질수록 더욱 심각해지며 이 작업이 매우 어려워집니다. 이 문서에서는 어텐션 메커니즘을 통해 효과적인 메시지 전달 흐름을 모델링하는 것이 VRR의 구성성과 롱테일 문제를 해결하는 데 중요할 수 있음을 보여줍니다. RelTransformer라고 불리는 이 방법은 각 이미지를 완전히 연결된 장면 그래프로 표현하고 전체 장면을 관계 삼중항 및 전역 장면 컨텍스트로 재구성합니다. 이는 관계 삼중항 및 전역 장면 컨텍스트의 각 요소에서 self-attention을 통해 대상 관계로 메시지를 직접 전달합니다. 우리는 또한 롱테일 관계 표현 학습을 강화하기 위해 학습 가능한 메모리를 설계합니다. 광범위한 실험을 통해 우리 모델이 많은 VRR 벤치마크에서 잘 일반화된다는 사실을 발견했습니다. 우리 모델은 두 개의 대규모 롱테일 VRR 벤치마크인 VG8K-LT(+2.0% 전체 acc) 및 GQA-LT(+26.0% 전체 acc)에서 최고 성능 모델보다 성능이 뛰어납니다. 둘 다 꼬리 쪽으로 매우 치우친 분포를 가지고 있습니다. 또한 VG200 관계 탐지 작업에서도 강력한 결과를 얻었습니다. 우리 코드는 https://github.com/Vision-CAIR/RelTransformer에서 확인할 수 있습니다.
180,http://arxiv.org/abs/2104.10283 ,GraghVQA: Language-Guided Graph Neural Networks for Graph-based Visual Question Answering,"Weixin Liang, Yanhao Jiang, Zixuan Liu","이미지는 객체나 속성의 집합 그 이상입니다. 즉, 상호 연결된 객체 간의 관계망을 나타냅니다. 장면 그래프는 이미지의 구조화된 그래픽 표현을 위한 새로운 양식으로 등장했습니다. 장면 그래프는 개체를 쌍별 관계를 통해 가장자리로 연결된 노드로 인코딩합니다. 장면 그래프에 대한 질문 답변을 지원하기 위해 우리는 그래프 노드 간 메시지 전달의 여러 반복으로 자연어 질문을 번역하고 실행하는 언어 기반 그래프 신경망 프레임워크인 GraphVQA를 제안합니다. GraphVQA 프레임워크의 디자인 공간을 탐색하고 다양한 디자인 선택의 장단점에 대해 논의합니다. GQA 데이터 세트에 대한 실험에서는 GraphVQA가 최첨단 모델보다 큰 차이(88.43% 대 94.78%)로 성능이 뛰어난 것으로 나타났습니다."
179,http://arxiv.org/abs/2104.08541 ,TransVG: End-to-End Visual Grounding with Transformers,"Jiajun Deng, Zhengyuan Yang, Tianlang Chen, Wengang Zhou, Houqiang Li","본 논문에서는 시각적 접지를 위한 깔끔하고 효과적인 변환기 기반 프레임워크, 즉 TransVG를 제시하여 해당 영역에 대한 언어 쿼리를 이미지에 접지하는 작업을 해결합니다. 2단계 또는 1단계 방법을 포함한 최첨단 방법은 쿼리 추론 및 다중 모드 융합을 수행하기 위해 수동으로 설계된 메커니즘을 갖춘 복잡한 모듈에 의존합니다. 그러나 쿼리 분해 및 이미지 장면 그래프와 같은 융합 모듈 설계에 특정 메커니즘이 포함되면 모델이 특정 시나리오의 데이터 세트에 쉽게 과적합되고 시각적 언어 컨텍스트 간의 풍부한 상호 작용이 제한됩니다. 이러한 경고를 피하기 위해 우리는 변환기를 활용하여 다중 모드 대응을 구축할 것을 제안하고 복잡한 융합 모듈(예: 모듈식 주의 네트워크, 동적 그래프 및 다중 모드 트리)이 더 높은 성능을 갖춘 간단한 변환기 인코더 레이어 스택으로 대체될 수 있음을 경험적으로 보여줍니다. 또한 시각적 접지를 직접 좌표 회귀 문제로 다시 공식화하고 후보 집합(예: 지역 제안 또는 앵커 상자)에서 예측하는 것을 방지합니다. 널리 사용되는 5개의 데이터 세트에 대해 광범위한 실험이 수행되었으며 TransVG에 의해 일련의 최첨단 기록이 설정되었습니다. 우리는 변환기 기반 시각적 접지 프레임워크의 벤치마크를 구축하고 \url{https://github.com/djiajunustc/TransVG}에서 코드를 사용할 수 있도록 합니다."
178,http://arxiv.org/abs/2104.08194 ,Spatiotemporal Deformable Scene Graphs for Complex Activity Detection,"Salman Khan, Fabio Cuzzolin","스마트 자동차, 수술용 로봇과 같은 자율 시스템의 의사 결정에는 장기적인 복잡한 활동 인식 및 위치 파악이 중요할 수 있습니다. 여기서는 (i) 액션 튜브 감지, (ii) 변형 가능한 부품 형상 모델링, (iii) 그래프 컨볼루셔널 네트워크의 세 가지 주요 구성 요소로 구성된 새로운 변형 가능한 시공간 장면 그래프 접근 방식을 통해 문제를 해결합니다. 첫째, 일련의 스니펫에서 액션 튜브가 감지됩니다. 다음으로, 새로운 3D 변형 가능 RoI 풀링 레이어는 구성 동작 튜브의 유연하고 변형 가능한 형상을 학습하기 위해 설계되었습니다. 마지막으로 모든 부분을 노드로 간주하고 나타나는 순서 등 서로 다른 의미를 기반으로 연결하고 동일한 작업 레이블과 기능 유사성을 공유하여 장면 그래프를 구성합니다. 또한 최근 출시된 ROAD 자율 주행 및 SARAS-ESAD 수술 작업 데이터 세트에 대한 새로운 시간적 복합 활동 주석을 제공하고 다양한 영역에 대한 프레임워크의 적응성을 보여줍니다. 우리의 방법은 두 증강 데이터 세트 모두에서 그래프 기반 경쟁사보다 훨씬 뛰어난 성능을 보이는 것으로 나타났습니다."
177,http://arxiv.org/abs/2104.06008 ,Disentangled Motif-aware Graph Learning for Phrase Grounding,"Zongshen Mu, Siliang Tang, Jie Tan, Qiang Yu, Yueting Zhuang","본 논문에서는 이미지의 구문 기반을 위한 새로운 그래프 학습 프레임워크를 제안합니다. 순차적 그래프 모델에서 조밀한 그래프 모델로 발전한 기존 작업은 거친 맥락을 포착하지만 문구와 이미지 영역 간의 맥락의 다양성을 구별하지 못합니다. 대조적으로, 우리는 장면 그래프의 맥락에 암시된 다양한 모티프에 특별한 주의를 기울이고 모티프 인식 문맥 정보를 표현에 통합하기 위해 분리된 그래프 네트워크를 고안했습니다. 게다가 우리는 표현을 통합하고 일반화하기 위해 기능 및 구조 수준에서 중재 전략을 채택합니다. 마지막으로, 교차 모달 주의 네트워크는 모달 내 기능을 융합하는 데 활용되며, 여기서 각 구문은 가장 적합한 구문을 선택하기 위해 영역과의 유사성을 계산할 수 있습니다. 우리는 일련의 절제 연구를 통해 DIGN(Disentangled Interventional Graph Network)의 효율성을 검증하고, 우리 모델은 Flickr30K Entities 및 ReferIt Game 벤치마크에서 최첨단 성능을 달성합니다."
176,http://arxiv.org/abs/2104.02381 ,Scene Graph Embeddings Using Relative Similarity Supervision,"Paridhi Maheshwari, Ritwick Chaudhry, Vishwa Vinay","장면 그래프는 이미지의 기본 콘텐츠를 강력하게 구조적으로 표현하며, 여기에서 파생된 임베딩은 여러 다운스트림 작업에 유용한 것으로 나타났습니다. 이 작업에서는 장면 그래프의 구조를 활용하고 의미론적 이미지 검색에 유용한 이미지 임베딩을 생성하기 위해 그래프 컨벌루션 네트워크를 사용합니다. 이미지 표현 학습에 전통적으로 사용 가능한 분류 중심 감독과 달리 순위 컨텍스트에서 상대적 유사성 레이블을 통해 학습하는 작업을 다룹니다. 대조 학습 패러다임에 기반을 두고 유사하고 다른 이미지 쌍에 대해 작동하고 임베딩 공간에서 이들 사이에 상대적 순서를 적용하는 새로운 손실 함수를 제안합니다. 우리는 직관적인 삼중 샘플링 전략과 결합된 이 순위 손실이 검색 작업에서 잘 알려진 대조 손실을 능가하는 강력한 표현으로 이어진다는 것을 보여줍니다. 또한 구조화된 장면 정보를 활용한 검색 결과가 시각적 유사성 검색과 달리 장면의 전체적 맥락을 어떻게 포착하는지에 대한 질적 증거를 제공합니다."
175,http://arxiv.org/abs/2104.00356 ,Exploiting Relationship for Complex-scene Image Generation,"Tianyu Hua, Hongdong Zheng, Yalong Bai, Wei Zhang, Xiao-Ping Zhang, Tao Mei","GAN(Generative Adversarial Networks)의 상당한 발전으로 언어 입력을 기반으로 현실적인 단일 객체 이미지 생성이 가능해졌습니다. 그러나 복잡한 장면 생성(여러 개체 간의 다양한 상호 작용 포함)은 레이아웃과 모양의 다양한 구성으로 인해 여전히 지저분한 레이아웃과 개체 왜곡으로 인해 어려움을 겪고 있습니다. 이전 방법은 대부분 객체 중심이며 복잡한 장면 이미지에서 중요한 역할을 하는 상호 관계를 무시합니다. 이 작업은 여러 객체가 장면 그래프로 상호 연관되어 있는 관계 인식 복합 장면 이미지 생성을 탐구합니다. 관계의 도움으로 우리는 생성 프레임워크에서 세 가지 주요 업데이트를 제안합니다. 첫째, 객체 간의 의미와 관계를 종합적으로 고려하여 합리적인 공간 배치를 추론한다. 표준 위치 회귀와 비교하여 상대적인 규모와 거리가 더 안정적인 목표를 제공한다는 것을 보여줍니다. 둘째, 객체 간의 관계는 객체의 외관에 큰 영향을 미치므로 관계 기반 생성기를 설계하여 객체의 관계를 반영하는 객체를 생성합니다. 셋째, 생성된 영상과 입력된 장면 그래프 간의 일관성을 보장하기 위한 새로운 장면 그래프 판별기를 제안한다. 우리의 방법은 이미지에서 여러 개체의 상호 작용을 존중하면서 그럴듯한 레이아웃과 개체를 합성하는 경향이 있습니다. Visual Genome 및 HICO-DET 데이터세트에 대한 실험 결과는 우리가 제안한 방법이 IS 및 FID 측정항목 측면에서 이전 기술보다 훨씬 뛰어난 성능을 보인다는 것을 보여줍니다. 사용자 연구와 육안 검사를 기반으로 우리의 방법은 복잡한 장면의 논리적 레이아웃과 모양을 생성하는 데 더 효과적입니다."
174,http://arxiv.org/abs/2104.00308 ,Bipartite Graph Network with Adaptive Message Passing for Unbiased Scene Graph Generation,"Rongjie Li, Songyang Zhang, Bo Wan, Xuming He","장면 그래프 생성은 광범위한 비전 애플리케이션에서 중요한 시각적 이해 작업입니다. 최근의 엄청난 발전에도 불구하고 본질적인 긴 꼬리 클래스 분포와 클래스 내 큰 변동으로 인해 여전히 어려운 과제입니다. 이러한 문제를 해결하기 위해 편견 없는 장면 그래프 생성을 위한 적응형 메시지 전파 메커니즘을 갖춘 새로운 신뢰도 인식 이부분 그래프 신경망을 소개합니다. 또한, 그래프 네트워크 훈련 시 불균형 데이터 분포 문제를 완화하기 위한 효율적인 이중 수준 데이터 리샘플링 전략을 제안합니다. 우리의 접근 방식은 Visual Genome, Open Images V4/V6를 포함한 여러 까다로운 데이터 세트에서 이전 방법보다 우수하거나 경쟁력 있는 성능을 달성하여 효율성과 일반성을 입증합니다."
173,http://arxiv.org/abs/2103.16381 ,Free-form Description Guided 3D Visual Graph Network for Object Grounding in Point Cloud,"Mingtao Feng, Zhen Li, Qi Li, Liang Zhang, XiangDong Zhang, Guangming Zhu, Hui Zhang, Yaonan Wang, Ajmal Mian","3D 객체 접지는 자유 형식의 언어 설명을 기반으로 원시 포인트 클라우드 장면에서 가장 관련성이 높은 대상 객체를 찾는 것을 목표로 합니다. 복잡하고 다양한 설명을 이해하고 이를 포인트 클라우드로 직접 끌어올리는 것은 포인트 클라우드의 불규칙하고 희박한 특성으로 인해 새롭고 어려운 주제입니다. 3D 객체 접지에는 세 가지 주요 과제가 있습니다. 복잡하고 다양한 설명에서 주요 초점을 찾는 것입니다. 포인트 클라우드 장면을 이해합니다. 그리고 대상 물체를 찾는다. 이 문서에서는 세 가지 과제를 모두 해결합니다. 첫째, 풍부한 구조와 장거리 구문 상관관계를 포착하기 위한 언어 장면 그래프 모듈을 제안합니다. 둘째, 객체-객체 및 객체-장면 동시 발생 관계를 추출하고 초기 제안의 시각적 특성을 강화하기 위한 다단계 3D 제안 관계 그래프 모듈을 도입합니다. 마지막으로 노드 매칭 전략을 통해 문구와 제안의 글로벌 컨텍스트를 인코딩하는 설명 안내 3D 시각적 그래프 모듈을 개발합니다. 까다로운 벤치마크 데이터 세트(ScanRefer 및 Nr3D)에 대한 광범위한 실험을 통해 우리의 알고리즘이 기존 최첨단 기술보다 성능이 우수하다는 것을 보여줍니다. 우리 코드는 https://github.com/PNXD/FFL-3DOG에서 확인할 수 있습니다."
172,http://arxiv.org/abs/2103.16083 ,Fully Convolutional Scene Graph Generation,"Hengyue Liu, Ning Yan, Masood S. Mortazavi, Bir Bhanu","본 논문에서는 객체와 관계를 동시에 감지하는 FCSGG(Fully Convolutional Scene Graph Generation) 모델을 제시합니다. 대부분의 장면 그래프 생성 프레임워크는 Faster R-CNN과 같은 사전 훈련된 2단계 객체 감지기를 사용하고 경계 상자 기능을 사용하여 장면 그래프를 구축합니다. 이러한 파이프라인은 일반적으로 매개변수 수가 많고 추론 속도가 낮습니다. 이러한 접근 방식과 달리 FCSGG는 객체를 경계 상자 중심점으로 인코딩하고 관계를 RAF(Relation Affinity Field)라고 하는 2D 벡터 필드로 인코딩하는 개념적으로 우아하고 효율적인 상향식 접근 방식입니다. RAF는 의미론적 특징과 공간적 특징을 모두 인코딩하고 주체에서 객체를 가리키는 하위 영역의 적분으로 객체 쌍 간의 관계를 명시적으로 나타냅니다. FCSGG는 시각적 기능만 활용하며 장면 그래프 생성에 여전히 강력한 결과를 생성합니다. Visual Genome 데이터 세트에 대한 포괄적인 실험은 제안된 방법의 효능, 효율성 및 일반화 가능성을 보여줍니다. FCSGG는 추론 시간을 대폭 단축하여 회상 및 제로 샷 회상에서 매우 경쟁력 있는 결과를 달성합니다."
171,http://arxiv.org/abs/2103.15662 ,Unified Graph Structured Models for Video Understanding,"Anurag Arnab, Chen Sun, Cordelia Schmid","정확한 비디오 이해에는 종종 긴 시간 간격에 걸쳐 배우, 개체 및 환경 간의 관계에 대한 추론이 포함됩니다. 본 논문에서는 이러한 시공간 관계를 명시적으로 모델링하고 감독이 가능할 때 객체의 명시적 표현을 사용할 수 있고 그렇지 않은 경우 암시적 표현을 사용할 수 있는 메시지 전달 그래프 신경망을 제안합니다. 우리의 공식은 비디오 이해를 위해 이전의 구조화된 모델을 일반화하고 그래프 구조 및 표현의 다양한 디자인 선택이 모델 성능에 어떤 영향을 미치는지 연구할 수 있게 해줍니다. 우리는 비디오에서 관계형 추론이 필요한 두 가지 작업(AVA 및 UCF101-24의 시공간 동작 감지와 최근 Action Genome 데이터 세트의 비디오 장면 그래프 분류)에 대한 방법을 시연하고 세 가지 데이터 세트 모두에서 최첨단 결과를 얻습니다. 또한 우리의 방법이 장면에서 관련 엔터티 간의 관계를 보다 효과적으로 모델링할 수 있는 방법을 정량적, 질적으로 보여줍니다."
170,http://arxiv.org/abs/2103.15365 ,Visual Distant Supervision for Scene Graph Generation,"Yuan Yao, Ao Zhang, Xu Han, Mengdi Li, Cornelius Weber, Zhiyuan Liu, Stefan Wermter, Maosong Sun",장면 그래프 생성의 목표는 이미지에서 객체와 객체의 관계를 식별하여 컴퓨터 비전의 다양한 응용을 용이하게 할 수 있는 구조화된 이미지 표현을 제공하는 것입니다. 그러나 장면 그래프 모델에는 일반적으로 사람이 집중적으로 주석을 달고 레이블이 지정된 대량의 데이터에 대한 지도 학습이 필요합니다. 본 연구에서는 사람이 라벨링한 데이터 없이 장면 그래프 모델을 훈련할 수 있는 시각적 관계 학습의 새로운 패러다임인 시각적 원격 감독을 제안합니다. 상식적인 지식 기반과 이미지를 정렬함으로써 시각적 관계 학습에 대한 원격 감독을 제공하기 위해 자동으로 대규모 레이블이 지정된 데이터를 생성할 수 있다는 것이 직관입니다. 멀리 떨어진 레이블이 있는 데이터의 노이즈를 완화하기 위해 확률적 관계 레이블을 반복적으로 추정하고 노이즈가 있는 레이블을 제거하는 프레임워크를 추가로 제안합니다. 포괄적인 실험 결과에 따르면 우리의 원격 지도 모델은 약한 지도 기준선과 반 지도 기준선보다 성능이 뛰어납니다. 인간이 라벨링한 데이터를 반 감독 방식으로 추가 통합함으로써 우리 모델은 최첨단 완전 감독 모델보다 큰 차이로 성능이 뛰어납니다(예: 시각적 게놈 평가의 술어 분류에 대한 8.3 마이크로 리콜 및 7.8 매크로 리콜 @50 개선). 우리는 이 백서의 데이터와 코드를 https://github.com/thunlp/VisualDS에서 공개적으로 제공합니다.
169,http://arxiv.org/abs/2103.14898 ,SceneGraphFusion: Incremental 3D Scene Graph Prediction from RGB-D Sequences,"Shun-Cheng Wu, Johanna Wald, Keisuke Tateno, Nassir Navab, Federico Tombari",장면 그래프는 다양한 2D 장면 이해 작업에 성공적으로 사용되는 간결하고 명시적인 표현입니다. 이 연구에서는 일련의 RGB-D 프레임이 주어지면 3D 환경에서 의미론적 장면 그래프를 점진적으로 구축하는 방법을 제안합니다. 이를 위해 그래프 신경망을 통해 기본 장면 구성 요소의 PointNet 기능을 집계합니다. 또한 이러한 증분 재구성 시나리오에 존재하는 부분 및 누락 그래프 데이터에 매우 적합한 새로운 주의 메커니즘을 제안합니다. 제안된 방법은 장면의 서브맵에서 실행되도록 설계되었지만 전체 3D 장면으로의 전송도 보여줍니다. 실험에 따르면 우리의 접근 방식은 3D 장면 그래프 예측 방법보다 훨씬 뛰어나며 정확도는 35Hz에서 실행되는 동안 다른 3D 의미 체계 및 팬옵틱 분할 방법과 동등합니다.
168,http://arxiv.org/abs/2103.11537 ,How to Design Sample and Computationally Efficient VQA Models,"Karan Samel, Zelin Zhao, Binghong Chen, Kuan Wang, Robin Luo, Le Song","시각적 질문 답변(VQA)과 같은 다중 모드 추론 작업에서는 많은 모델링 및 교육 패러다임이 테스트되었습니다. 이전 모델은 비전 및 언어 작업에 대해 다양한 방법을 제안했지만, 샘플 및 계산 효율성을 유지하면서 가장 잘 수행되는 방법은 무엇입니까? 우리의 실험에 따르면 텍스트를 확률적 프로그램으로 표현하고 이미지를 객체 수준 장면 그래프로 표현하는 것이 이러한 요구 사항을 가장 잘 충족한다는 것을 알았습니다. 우리는 이러한 소프트 프로그램과 장면 그래프를 활용하여 엔드투엔드 방식으로 질문 답변 쌍을 학습하도록 기존 모델을 확장합니다. 경험적 결과는 이 미분 가능한 엔드-투-엔드 프로그램 실행기가 샘플 및 계산 효율성을 유지하면서 최첨단 정확도를 유지할 수 있음을 보여줍니다."
167,http://arxiv.org/abs/2104.01111 ,A Comprehensive Survey of Scene Graphs: Generation and Application,"Xiaojun Chang, Pengzhen Ren, Pengfei Xu, Zhihui Li, Xiaojiang Chen, Alex Hauptmann","장면 그래프는 장면 내 객체, 속성, 객체 간의 관계를 명확하게 표현할 수 있는 구조화된 장면 표현입니다. 컴퓨터 비전 기술이 계속해서 발전하면서 사람들은 더 이상 이미지 속 물체를 단순히 감지하고 인식하는 데 만족하지 않습니다. 대신 사람들은 시각적 장면에 대한 더 높은 수준의 이해와 추론을 기대합니다. 예를 들어, 이미지가 주어지면 이미지에 있는 객체를 감지하고 인식할 뿐만 아니라 객체 간의 관계(시각적 관계 감지)를 알고 이미지 콘텐츠를 기반으로 텍스트 설명(이미지 캡션)을 생성하려고 합니다. 또는 이미지 속 어린 소녀가 무엇을 하고 있는지(VQA(시각적 질문 응답)) 기계가 알려주거나 이미지에서 개를 제거하고 유사한 이미지를 찾는 등(이미지 편집 및 검색) 등을 할 수도 있습니다. 이러한 작업에는 이미지 비전 작업에 대한 더 높은 수준의 이해와 추론이 필요합니다. 장면 그래프는 장면을 이해하는 데 매우 강력한 도구입니다. 따라서 장면 그래프는 많은 연구자들의 관심을 끌었으며 관련 연구는 교차 양식적이고 복잡하며 빠르게 발전하는 경우가 많습니다. 그러나 현재로서는 장면 그래프에 대한 비교적 체계적인 조사가 존재하지 않습니다. 이를 위해 본 조사에서는 현재의 장면 그래프 연구를 종합적으로 조사한다. 보다 구체적으로 먼저 씬 그래프의 일반적인 정의를 요약한 후, 사전 지식을 바탕으로 씬 그래프(SGG)와 SGG의 생성 방법에 대해 포괄적이고 체계적인 논의를 진행했습니다. 그런 다음 장면 그래프의 주요 응용 프로그램을 조사하고 가장 일반적으로 사용되는 데이터 세트를 요약했습니다. 마지막으로, 장면 그래프의 향후 개발에 대한 몇 가지 통찰력을 제공합니다. 이는 향후 장면 그래프 연구에 매우 유용한 기반이 될 것이라고 믿습니다."
166,http://arxiv.org/abs/2103.09591 ,Automatic Generation of Contrast Sets from Scene Graphs: Probing the Compositional Consistency of GQA,"Yonatan Bitton, Gabriel Stanovsky, Roy Schwartz, Michael Elhadad","최근 연구에 따르면 지도 모델은 종종 좋은 테스트 점수를 얻기 위해 데이터 아티팩트를 활용하는 반면 훈련 분포 외부의 샘플에서는 성능이 심각하게 저하되는 것으로 나타났습니다. 대비 세트(Gardneret al., 2020)는 출력 레이블이 수정되도록 최소한의 방식으로 테스트 샘플을 교란하여 이 현상을 정량화합니다. 대부분의 대비 세트는 수동으로 생성되어 집중적인 주석 작업이 필요하지만 풍부한 의미론적 입력 표현을 활용하여 시각적 질문 응답 작업을 위한 대비 세트를 자동으로 생성하는 새로운 방법을 제시합니다. 우리의 방법은 교란된 질문에 대한 답변을 계산하여 주석 비용을 크게 줄이고 다양한 의미 측면(예: 공간 또는 관계 추론)에서 모델 성능을 철저히 평가할 수 있도록 합니다. 우리는 GQA 데이터 세트와 의미론적 장면 그래프 이미지 표현에 대한 접근 방식의 효율성을 보여줍니다. GQA의 구성성과 신중하게 균형 잡힌 라벨 분포에도 불구하고 두 개의 고성능 모델은 원래 테스트 세트에 비해 정확도가 13-17% 떨어지는 것으로 나타났습니다. 마지막으로, 성능 저하를 완화하기 위해 훈련 세트에 자동 섭동을 적용하여 보다 강력한 모델을 만들 수 있음을 보여줍니다."
165,http://arxiv.org/abs/2103.06422 ,Holistic 3D Scene Understanding from a Single Image with Implicit Representation,"Cheng Zhang, Zhaopeng Cui, Yinda Zhang, Bing Zeng, Marc Pollefeys, Shuaicheng Liu","우리는 단일 이미지에서 물체 모양, 물체 포즈 및 장면 레이아웃을 예측할 수 있는 전체적인 3D 장면 이해를 위한 새로운 파이프라인을 제시합니다. 이는 매우 잘못된 문제이기 때문에 기존 방법은 일반적으로 객체 간의 과도한 가려짐으로 인해 특히 어수선한 장면의 모양과 레이아웃 모두 부정확한 추정으로 인해 어려움을 겪습니다. 우리는 이 문제를 해결하기 위해 최신 심층 암시적 표현을 활용할 것을 제안합니다. 우리는 객체 모양 추정을 향상시키기 위해 이미지 기반 로컬 구조 암시적 네트워크를 제안할 뿐만 아니라 암시적 로컬 객체 특징을 활용하는 새로운 암시적 장면 그래프 신경망을 통해 3D 객체 포즈와 장면 레이아웃을 개선합니다. 객체 간의 잘못된 컨텍스트를 방지하기 위해 새로운 물리적 위반 손실도 제안되었습니다. 광범위한 실험을 통해 우리의 방법이 물체 모양, 장면 레이아웃 추정 및 3D 물체 감지 측면에서 최첨단 방법보다 성능이 우수하다는 것을 보여줍니다."
164,http://arxiv.org/abs/2103.05558 ,Exploiting Edge-Oriented Reasoning for 3D Point-based Scene Graph Analysis,"Chaoyi Zhang, Jianhui Yu, Yang Song, Weidong Cai","장면 이해는 컴퓨터 비전에서 중요한 문제입니다. 본 논문에서는 장면 그래프 구성, 추론, 추론의 세 가지 순차적 단계를 통해 장면 이해를 달성하기 위해 인식과 추론을 효과적으로 연결하는 3D 포인트 기반 장면 그래프 생성($\mathbf{SGG_{point}}$) 프레임워크를 제안합니다. 추론 단계 내에서 EDGE 지향 그래프 컨볼루션 네트워크($\texttt{EdgeGCN}$)는 명시적 관계 모델링을 위한 다차원 에지 기능을 활용하고 장면 그래프 표현의 독립적인 발전을 위해 노드와 에지 간의 두 가지 관련 트위닝 상호 작용 메커니즘을 탐색하기 위해 생성됩니다. 전반적으로 우리의 통합 $\mathbf{SGG_{point}}$ 프레임워크는 실제 및 합성 3D 포인트 기반 장면 모두에서 관심 있는 장면 구조를 찾고 추론하기 위해 구축되었습니다. 우리의 실험 결과는 장면 그래프 생성 연구에 대한 유망한 에지 지향 추론 효과를 보여줍니다. 또한 인용 네트워크의 노드별 분류 및 분자 분석을 위한 전체 그래프 인식 문제를 포함하여 벤치마크 데이터 세트를 학습하는 여러 가지 전통적인 그래프 표현에 대한 방법의 이점을 보여줍니다."
163,http://arxiv.org/abs/2103.05271 ,Probabilistic Modeling of Semantic Ambiguity for Scene Graph Generation,"Gengcong Yang, Jingyi Zhang, Yong Zhang, Baoyuan Wu, Yujiu Yang","""정확한"" 장면 그래프를 생성하기 위해 거의 모든 기존 방법은 결정론적인 방식으로 쌍별 관계를 예측합니다. 그러나 우리는 시각적 관계가 의미상 모호한 경우가 많다고 주장합니다. 구체적으로, 언어학적 지식에 영감을 받아 모호성을 동의어 모호함, 하명 모호함, 다시점 모호함의 세 가지 유형으로 분류합니다. 모호함은 자연스럽게 \emph{implicit multi-label} 문제로 이어지며, 이는 다양한 예측의 필요성을 불러일으킵니다. 본 연구에서는 새로운 플러그 앤 플레이 확률적 불확실성 모델링(PUM) 모듈을 제안합니다. 각 결합 영역을 가우스 분포로 모델링합니다. 이 분포의 분산은 해당 시각적 콘텐츠의 불확실성을 측정합니다. 이러한 불확실성 모델링은 기존 결정론적 방법에 비해 특징 표현의 확률성을 가져오며, 이는 자연스럽게 다양한 예측을 가능하게 합니다. 부산물로서 PUM은 더욱 세분화된 관계를 관리하여 빈번한 관계에 대한 편견 문제를 완화합니다. 대규모 Visual Genome 벤치마크에 대한 광범위한 실험에서는 PUM을 새로 제안된 ResCAGCN과 결합하면 특히 평균 재현율 지표에서 최첨단 성능을 달성할 수 있음을 보여줍니다. 또한, 우리는 PUM을 일부 기존 모델에 연결하여 PUM의 보편적 효율성을 입증하고 다양하면서도 그럴듯한 시각적 관계를 생성하는 능력에 대한 통찰력 있는 분석을 제공합니다."
162,http://arxiv.org/abs/2103.02221 ,Energy-Based Learning for Scene Graph Generation,"Mohammed Suhail, Abhay Mittal, Behjat Siddiquie, Chris Broaddus, Jayan Eledath, Gerard Medioni, Leonid Sigal","전통적인 장면 그래프 생성 방법은 개체와 관계를 독립적인 엔터티로 처리하는 교차 엔트로피 손실을 사용하여 학습됩니다. 그러나 이러한 공식은 본질적으로 구조화된 예측 문제에서 출력 공간의 구조를 무시합니다. 본 연구에서는 장면 그래프 생성을 위한 새로운 에너지 기반 학습 프레임워크를 소개합니다. 제안된 공식은 장면 그래프의 구조를 출력 공간에 효율적으로 통합할 수 있게 해줍니다. 학습 프레임워크의 이러한 추가 제약은 귀납적 편향으로 작용하여 모델이 적은 수의 레이블에서 효율적으로 학습할 수 있도록 합니다. 우리는 제안된 에너지 기반 프레임워크를 사용하여 기존 최첨단 모델을 훈련하고 Visual Genome 및 GQA 벤치마크 데이터 세트에서 각각 최대 21% 및 27%의 상당한 성능 개선을 얻었습니다. 또한, 데이터가 부족한 Zero-shot 및 Few-Shot 설정에서 우수한 성능을 보여 제안한 프레임워크의 학습 효율성을 보여줍니다."
161,http://arxiv.org/abs/2102.04990 ,In Defense of Scene Graphs for Image Captioning,"Kien Nguyen, Subarna Tripathi, Bang Du, Tanaya Guha, Truong Q. Nguyen","주류 이미지 캡션 모델은 CNN(Convolutional Neural Network) 이미지 기능을 사용하여 반복 모델을 통해 캡션을 생성합니다. 최근 이미지 장면 그래프는 캡션 모델을 강화하여 객체 엔터티, 관계 및 속성과 같은 구조적 의미를 활용하는 데 사용되었습니다. 여러 연구에 따르면 블랙박스 장면 그래프 생성기에서 장면 그래프를 순진하게 사용하는 것은 이미지 캡션 성능에 해를 끼치며 장면 그래프 기반 캡션 모델은 적절한 캡션을 생성하기 위해 이미지 특징을 명시적으로 사용하는 오버헤드를 발생시켜야 한다는 점에 주목했습니다. 이러한 문제를 해결하기 위해 우리는 경쟁력 있는 이미지 캡션 성능을 위해 장면 그래프 레이블만 활용하는 프레임워크인 \textbf{SG2Caps}를 제안합니다. 기본 아이디어는 두 장면 그래프(하나는 입력 이미지에서 파생되고 다른 하나는 캡션에서 파생됨) 사이의 의미적 격차를 줄이는 것입니다. 이를 달성하기 위해 우리는 객체의 공간적 위치와 HOI(Human-Object-Interaction) 레이블을 추가 HOI 그래프로 활용합니다. SG2Caps는 기존의 장면 그래프 전용 캡션 모델보다 훨씬 더 성능이 뛰어나며, 이는 장면 그래프가 이미지 캡션에 대한 유망한 표현임을 나타냅니다. 장면 그래프 레이블을 직접 활용하면 고차원 CNN 기능에 대한 비용이 많이 드는 그래프 컨볼루션을 방지하여 훈련 가능한 매개변수가 49% 줄어듭니다. 우리 코드는 https://github.com/Kien085/SG2Caps에서 확인할 수 있습니다."
160,http://arxiv.org/abs/2102.04760 ,Improving Scene Graph Classification by Exploiting Knowledge from Texts,"Sahand Sharifzadeh, Sina Moayed Baharlou, Martin Schmitt, Hinrich Schütze, Volker Tresp","훈련 장면 그래프 분류 모델에는 주석이 달린 대량의 이미지 데이터가 필요합니다. 한편, 장면 그래프는 텍스트나 지식 그래프의 기호 데이터로 모델링할 수 있는 관계형 지식을 나타낸다. 이미지 주석에는 많은 노력이 필요하지만 자연 장면에 대한 텍스트 설명을 수집하는 데는 더 적은 노력이 필요합니다. 본 연구에서는 텍스트로 된 장면 설명이 주석이 달린 이미지 데이터를 대체할 수 있는지 조사합니다. 이를 위해 우리는 주석이 달린 이미지뿐만 아니라 기호 데이터에서도 학습되는 장면 그래프 분류 프레임워크를 사용합니다. 우리 아키텍처에서 기호 엔터티는 먼저 해당 이미지 기반 표현에 매핑된 다음 관계형 추론 파이프라인에 공급됩니다. 지식 그래프의 형식과 같은 구조화된 형태의 지식을 항상 사용할 수 있는 것은 아니지만 변환기 기반 언어 모델을 사용하여 구조화되지 않은 텍스트에서 지식을 생성할 수 있습니다. 우리는 텍스트에서 추출된 지식으로 분류 파이프라인을 미세 조정함으로써 주석이 달린 이미지가 1%만 있는 지도 기준선에 비해 장면 그래프 분류에서 ~8배, 객체 분류에서 ~3배, 조건자 분류에서 ~1.5배 더 정확한 결과를 얻을 수 있음을 보여줍니다."
159,http://arxiv.org/abs/2102.04035 ,In-game Residential Home Planning via Visual Context-aware Global Relation Learning,"Lijuan Liu, Yin Yang, Yi Yuan, Tianjia Shao, He Wang, Kun Zhou","본 논문에서는 주거단지의 게임 내 맞춤화를 위해 건물 단위의 적절한 위치를 추천하는 효과적인 글로벌 관계 학습 알고리즘을 제안합니다. 건축 레이아웃이 주어지면 장면 구성 요소 간의 암시적 전역 관계를 학습하고 새 건물 단위의 위치를 ​​추론하는 시각적 상황 인식 그래프 생성 네트워크를 제안합니다. 제안된 네트워크는 장면 그래프와 해당 평면도 깊이 이미지를 입력으로 사용합니다. 기존 장면에 맞춰 자동 회귀 에지 분포를 학습하여 새로 추가된 건물 단위에 대한 위치 권장 사항을 제공합니다. 또한 사이트의 필수 기하학 의미에 대한 인식을 높이기 위해 전역 그래프-이미지 매칭 손실을 도입합니다. 정성적, 정량적 실험을 통해 권장 위치가 주거 단지 구성 요소의 암묵적인 공간 규칙을 잘 반영하고 있으며, 복합 건축의 3D 장면에서 건물 단위를 찾는 것이 유익하고 실용적이라는 사실이 입증되었습니다."
158,http://arxiv.org/abs/2101.06894 ,Kimera: from SLAM to Spatial Perception with 3D Dynamic Scene Graphs,"Antoni Rosinol, Andrew Violette, Marcus Abate, Nathan Hughes, Yun Chang, Jingnan Shi, Arjun Gupta, Luca Carlone","인간은 자신이 이동하는 환경에 대해 복잡한 정신 모델을 형성할 수 있습니다. 이 정신 모델은 장면의 기하학적 및 의미론적 측면을 포착하고, 여러 수준의 추상화(예: 물체, 방, 건물)에서 환경을 설명하고, 정적 및 동적 개체와 이들의 관계(예: 사람이 주어진 시간에 방에 있음)를 포함합니다. 대조적으로, 현재 로봇의 내부 표현은 여전히 ​​희박하거나 조밀한 기하학적 기본 요소(예: 점, 선, 평면, 복셀) 세트의 형태로 또는 객체 모음으로 환경에 대한 부분적이고 단편적인 이해를 제공합니다. 본 논문은 동적 환경의 측정 및 의미 측면을 원활하게 포착하는 새로운 표현인 3D 동적 장면 그래프(DSG)를 도입하여 로봇과 인간 인식 사이의 격차를 줄이려고 시도합니다. DSG는 노드가 다양한 추상화 수준에서 공간 개념을 나타내고 가장자리가 노드 간의 시공간 관계를 나타내는 계층형 그래프입니다. 우리의 두 번째 기여는 시각 관성 데이터로부터 DSG를 구축하는 최초의 완전 자동 방법인 Kimera입니다. Kimera에는 시각 관성 SLAM, 미터법 의미론적 3D 재구성, 객체 위치 파악, 인간 자세 및 모양 추정, 장면 구문 분석을 위한 최첨단 기술이 포함되어 있습니다. 우리의 세 번째 기여는 혼잡한 실내 및 실외 장면 모음을 시뮬레이션하는 새로 출시된 데이터 세트인 uHumans2를 포함하여 실제 데이터 세트와 사실적인 시뮬레이션에서 Kimera를 종합적으로 평가한 것입니다. 우리의 평가에 따르면 Kimera는 시각 관성 SLAM에서 최첨단 성능을 달성하고 정확한 3D 메트릭 의미 메시 모델을 실시간으로 추정하며 수십 개의 객체와 인간이 포함된 복잡한 실내 환경의 DSG를 몇 분 안에 구축하는 것으로 나타났습니다. 우리의 마지막 기여는 실시간 계층적 의미론적 경로 계획을 위해 DSG를 사용하는 방법을 보여줍니다. Kimera의 핵심 모듈은 오픈 소스입니다."
157,http://arxiv.org/abs/2101.05479 ,Understanding the Role of Scene Graphs in Visual Question Answering,"Vinay Damodaran, Sharanya Chakravarthy, Akshay Kumar, Anjana Umapathy, Teruko Mitamura, Yuta Nakashima, Noa Garcia, Chenhui Chu","시각적 질문 응답(VQA)은 시각 장애가 있는 사용자 지원 및 이미지 기반 검색과 같은 중요한 응용 프로그램을 통해 연구 커뮤니티에서 엄청난 관심을 끌고 있습니다. 이 작업에서는 VQA 작업을 해결하기 위해 장면 그래프를 사용하는 방법을 살펴봅니다. 우리는 계산, 구성 및 고급 추론 기능이 필요한 까다로운 질문 세트를 제시하고 수많은 이미지에 대한 장면 그래프를 제공하는 GQA 데이터 세트에 대한 실험을 수행합니다. 장면 그래프와 함께 사용할 이미지 + 질문 아키텍처를 채택하고, 보이지 않는 이미지에 대한 다양한 장면 그래프 생성 기술을 평가하고, 사람이 주석을 달고 자동 생성된 장면 그래프를 활용하는 교육 커리큘럼을 제안하고, 여러 이미지 표현에서 학습할 수 있는 후기 융합 아키텍처를 구축합니다. 우리는 VQA를 위한 장면 그래프 사용에 대한 다각적인 연구를 제시하여 이 작업을 최초로 수행합니다."
156,http://arxiv.org/abs/2012.14700 ,Image-to-Image Retrieval by Learning Similarity between Scene Graphs,"Sangwoong Yoon, Woo Young Kang, Sungwook Jeon, SeongEun Lee, Changjin Han, Jonghun Park, Eun-Sol Kim",장면 그래프는 이미지의 상위 내용을 구조적이고 상징적인 방식으로 간결하게 요약하므로 두 이미지의 장면 그래프 간의 유사성은 내용의 관련성을 반영합니다. 이 아이디어를 바탕으로 그래프 신경망으로 측정된 장면 그래프 유사성을 사용하여 이미지 간 검색을 위한 새로운 접근 방식을 제안합니다. 우리의 접근 방식에서 그래프 신경망은 사전 훈련된 문장 유사성 모델을 사용하여 사람이 주석을 추가한 캡션에서 계산된 프록시 이미지 관련성 측정을 예측하도록 훈련되었습니다. 검색 알고리즘을 평가하기 위해 인간 주석자가 측정한 이미지 관련성에 대한 데이터 세트를 수집하고 게시합니다. 수집된 데이터 세트는 우리의 방법이 다른 경쟁 기준보다 이미지 유사성에 대한 인간의 인식과 잘 일치한다는 것을 보여줍니다.
155,http://arxiv.org/abs/2012.11587 ,Object-Centric Diagnosis of Visual Reasoning,"Jianwei Yang, Jiayuan Mao, Jiajun Wu, Devi Parikh, David D. Cox, Joshua B. Tenenbaum, Chuang Gan","이미지에 대한 질문에 답할 때 이미지의 세밀한 내용(예: 개체, 관계)을 이해하는 것뿐만 아니라 질문에 대한 답을 도출하기 위해 기초적인 시각적 단서를 통해 추론하는 이유도 알아야 합니다. 지난 몇 년 동안 우리는 시각적 질문 답변에 있어 상당한 진전을 보았습니다. 정확도가 높아질수록 인상적이기는 하지만, 이러한 모델이 기초적인 시각적 추론을 수행하고 있는지 아니면 훈련 데이터에서 허위 상관 관계를 활용하고 있는지를 파악하는 것은 여전히 ​​뒤처져 있습니다. 최근에는 접지 및 견고성과 같은 관점에서 이 질문에 답하려는 많은 연구가 시도되었습니다. 하지만 대부분은 언어적인 측면에 집중하거나 픽셀 수준의 어텐션 맵을 대충 연구하고 있습니다. 본 논문에서는 GQA 데이터 세트에 제공된 단계별 객체 접지 주석을 활용하여 특히 비전 측면에서 접지 및 견고성에 대한 시각적 추론에 대한 체계적인 객체 중심 진단을 먼저 제시합니다. 다양한 모델에 대한 광범위한 비교에 따르면 정확도가 높은 모델이라도 물체를 정확하게 접지하는 데 좋지 않거나 시각적 콘텐츠 교란에 강력하지 않다는 것을 알 수 있습니다. 대조적으로, 기호 및 모듈식 모델은 정확성이 떨어지기는 하지만 상대적으로 더 나은 기반과 견고성을 갖습니다. 이러한 다양한 측면을 조정하기 위해 우리는 Graph Reasoning Machine이라는 진단 모델을 추가로 개발합니다. 우리 모델은 순전히 상징적인 시각적 표현을 확률적 장면 그래프로 대체한 다음 시각적 추론 모듈에 교사 강제 훈련을 적용합니다. 설계된 모델은 투명성을 상속하면서 바닐라 신경 기호 모델에 비해 세 가지 측정 항목 모두의 성능을 향상시킵니다. 추가 절제 연구에 따르면 이러한 개선은 주로 보다 정확한 이미지 이해와 적절한 중간 추론 감독으로 인한 것입니다."
154,http://arxiv.org/abs/2012.07277 ,Hierarchical Planning for Long-Horizon Manipulation with Geometric and Symbolic Scene Graphs,"Yifeng Zhu, Jonathan Tremblay, Stan Birchfield, Yuke Zhu",우리는 장거리 조작 작업을 위한 시각적 기반의 계층적 계획 알고리즘을 제시합니다. 우리의 알고리즘은 지정된 목표에 따라 신경 기호 작업 계획 및 낮은 수준 동작 생성의 공동 프레임워크를 제공합니다. 우리 접근 방식의 핵심은 기하학적 장면 그래프와 상징적 장면 그래프라는 두 가지 수준의 장면 그래프 표현입니다. 이 계층적 표현은 조작 장면의 구조화된 개체 중심 추상화 역할을 합니다. 우리 모델은 그래프 신경망을 사용하여 이러한 장면 그래프를 처리하여 높은 수준의 작업 계획과 낮은 수준의 동작을 예측합니다. 우리는 우리의 방법이 장거리 작업으로 확장되고 새로운 작업 목표에 잘 일반화된다는 것을 보여줍니다. 우리는 물리적 시뮬레이션과 실제 세계 모두에서 주방 보관 작업에 대한 방법을 검증합니다. 우리의 실험에 따르면 우리의 방법은 표준 검색 기반 작업 및 모션 플래너에 비해 계산 시간이 4배 더 빠르면서도 실제 로봇에서 70% 이상의 성공률과 거의 90%의 하위 목표 완료율을 달성한 것으로 나타났습니다.
153,http://arxiv.org/abs/2012.07192 ,Knowledge-Routed Visual Question Reasoning: Challenges for Deep Representation Embedding,"Qingxing Cao, Bailin Li, Xiaodan Liang, Keze Wang, Liang Lin","VQA(시각적 질문 응답) 모델이 이미지와 텍스트 컨텍스트를 넘어서 입력-출력 상관 관계를 활용하여 기본 지식을 발견하도록 장려하는 데 유용하지만 기존 지식 VQA 데이터 세트는 대부분 크라우드 소스 방식으로 주석이 추가됩니다(예: 인터넷을 통해 다양한 사용자로부터 질문 및 외부 이유 수집). 지식 추론의 과제 외에도 주석자 편향을 처리하는 방법도 해결되지 않은 상태로 남아 있으며, 이는 종종 질문과 답변 간의 피상적인 과적합 상관 관계로 이어집니다. 이 문제를 해결하기 위해 우리는 VQA 모델 평가를 위한 지식 라우팅 시각적 질문 추론이라는 새로운 데이터 세트를 제안합니다. 바람직한 VQA 모델은 이미지 컨텍스트를 올바르게 인식하고, 질문을 이해하고, 학습된 지식을 통합해야 한다는 점을 고려하여, 제안된 데이터 세트는 현재 딥 임베딩 모델에서 활용되는 지름길 학습을 차단하고 지식 기반 시각적 질문 추론의 연구 경계를 넓히는 것을 목표로 합니다. 특히, 우리는 시각적 게놈 장면 그래프와 제어된 프로그램이 포함된 외부 지식 기반을 기반으로 질문-답변 쌍을 생성하여 지식을 다른 편견으로부터 분리합니다. 프로그램은 장면 그래프나 지식 기반에서 하나 또는 두 개의 삼중항을 선택하여 다단계 추론을 추진하고, 답변의 모호성을 피하고, 답변 분포의 균형을 맞출 수 있습니다. 기존 VQA 데이터 세트와 달리 지식 추론을 통합하는 프로그램에 다음과 같은 두 가지 주요 제약 사항을 암시합니다. i) 여러 개의 지식 삼중항이 질문과 관련될 수 있지만 단 하나의 지식만이 이미지 객체와 관련됩니다. 이는 VQA 모델이 주어진 질문만을 토대로 지식을 추측하는 대신 이미지를 올바르게 인식하도록 할 수 있습니다. ii) 모든 질문은 서로 다른 지식을 기반으로 하지만 후보 답변은 훈련 세트와 테스트 세트 모두에서 동일합니다."
152,http://arxiv.org/abs/2012.04329 ,StacMR: Scene-Text Aware Cross-Modal Retrieval,"Andrés Mafla, Rafael Sampaio de Rezende, Lluís Gómez, Diane Larlus, Dimosthenis Karatzas",교차 모드 검색을 위한 최근 모델은 몇 가지 언급할 수 있는 장면 그래프 및 개체 상호 작용을 통해 시각적 장면에 대한 점점 더 풍부한 이해를 통해 이점을 얻었습니다. 이로 인해 이미지의 시각적 표현과 캡션의 텍스트 표현 간의 일치가 향상되었습니다. 그러나 현재의 시각적 표현은 검색을 위한 중요한 정보를 포함할 수 있는 이미지에 나타나는 텍스트라는 핵심 측면을 간과하고 있습니다. 본 논문에서는 이미지에 장면-텍스트 인스턴스가 포함된 교차 모달 검색을 탐색할 수 있는 새로운 데이터 세트를 먼저 제안합니다. 그런 다음 이 데이터 세트를 사용하여 캡션의 텍스트와 시각적 장면의 텍스트에 대한 특수 표현을 사용하고 공통 임베딩 공간에서 조정하는 더 나은 장면-텍스트 인식 교차 모달 검색 방법을 포함하여 장면 텍스트를 활용하는 여러 가지 접근 방식을 설명합니다. 광범위한 실험을 통해 교차 모달 검색 접근 방식이 장면 텍스트의 이점을 누리고 더 탐구할 가치가 있는 흥미로운 연구 질문을 강조한다는 것이 확인되었습니다. 데이터 세트 및 코드는 http://europe.naverlabs.com/stacmr에서 확인할 수 있습니다.
151,http://arxiv.org/abs/2012.04060 ,Semantic and Geometric Modeling with Neural Message Passing in 3D Scene Graphs for Hierarchical Mechanical Search,"Andrey Kurenkov, Roberto Martín-Martín, Jeff Ichnowski, Ken Goldberg, Silvio Savarese","집이나 사무실과 같이 조직화된 실내 환경에서 물건을 검색하는 것은 우리 일상 활동의 일부입니다. 대상 물체를 찾을 때 우리는 물체가 있을 가능성이 있는 방과 컨테이너에 대해 공동으로 추론합니다. 동일한 유형의 컨테이너는 그것이 있는 방에 따라 대상을 가질 확률이 다릅니다. 또한 기하학적 및 의미론적 정보를 결합하여 대상 개체가 보기에서 숨겨진 경우 어떤 컨테이너가 검색하기에 가장 좋은지 또는 다른 개체가 이동하기에 가장 좋은지 추론합니다. 우리는 이 문제의 계층적, 의미적, 기하학적 측면을 포착하기 위해 3D 장면 그래프 표현을 사용할 것을 제안합니다. 검색 프로세스에서 이 표현을 활용하기 위해 자연어 설명으로 지정된 대상 개체를 찾는 에이전트의 작업을 안내하는 방법인 HMS(Hierarchical Mechanical Search)를 도입합니다. HMS는 시각적, 기하학적, 언어적 정보가 포함된 벡터의 신경 메시지 전달을 사용하여 HMS가 의미론적 단서와 기하학적 단서를 결합하면서 그래프의 여러 레이어에 걸쳐 추론할 수 있도록 하는 새로운 신경망 아키텍처를 기반으로 합니다. HMS는 저장 위치에 의미론적으로 관련된 객체를 조밀하게 배치한 500개의 3D 장면 그래프로 구성된 새로운 데이터 세트에서 평가되었으며, 객체를 찾는 데 있어 여러 기준보다 훨씬 더 우수하고 필요한 작업 수의 중앙값 측면에서 오라클 정책에 가까운 것으로 나타났습니다. 추가적인 정성적 결과는 https://ai.stanford.edu/mech-search/hms에서 확인할 수 있습니다."
150,http://arxiv.org/abs/2012.04027 ,Generating unseen complex scenes: are we there yet?,"Arantxa Casanova, Michal Drozdzal, Adriana Romero-Soriano","최근의 복잡한 장면 조건 생성 모델이 점점 더 매력적인 장면을 생성하지만 어떤 모델이 더 잘 수행되고 그 이유는 무엇인지 평가하기가 매우 어렵습니다. 이는 모델이 다양한 데이터 분할에 맞게 훈련되고 자체 실험 설정을 정의하기 때문에 발생하는 경우가 많습니다. 본 논문에서는 복잡한 장면 조건부 생성 모델을 비교하는 방법론을 제안하고, 각 모델이 (1) 훈련 분포에 적합하여 보이는 조건화에서 잘 수행되고, (2) 보이는 개체 조합으로 구성된 보이지 않는 조건화로 일반화하고, (3) 보이지 않는 개체 조합으로 구성된 보이지 않는 조건화로 일반화하는 능력을 평가하는 심층 분석을 제공합니다. 결과적으로, 우리는 최근 방법이 보이는 조건화에 따라 인식 가능한 장면을 생성할 수 있고, 보이는 개체 조합을 사용하여 보이지 않는 조건화를 일반화하기 위해 구성성을 활용할 수 있음을 관찰합니다. 그러나 모든 방법은 보이지 않는 개체 조합으로 구성된 조건에서 이미지를 생성하도록 요청할 때 눈에 띄는 이미지 품질 저하를 겪습니다. 또한, 분석을 통해 다양한 파이프라인 구성 요소의 장점을 식별하고 (1) 인스턴스별 공간 조절 정규화를 통해 구성성을 장려하면 두 가지 유형의 보이지 않는 조절에 대한 견고성이 증가하고, (2) 장면 그래프 지각 유사성과 같은 의미상 인식 손실을 사용하여 생성 프로세스의 일부 차원을 개선하는 데 도움이 되며, (3) 생성된 마스크의 품질과 개별 개체의 품질을 향상시키는 것이 두 가지 유형의 보이지 않는 조절에 대한 견고성을 향상시키는 중요한 단계라는 것을 발견했습니다."
149,http://arxiv.org/abs/2011.14488 ,Self-Supervised Real-to-Sim Scene Generation,"Aayush Prakash, Shoubhik Debnath, Jean-Francois Lafleche, Eric Cameracci, Gavriel State, Stan Birchfield, Marc T. Law","합성 데이터는 지도 딥 러닝의 확장성 문제에 대한 유망한 솔루션으로 떠오르고 있으며, 특히 실제 데이터를 획득하기 어렵거나 주석을 달기가 어려울 때 더욱 그렇습니다. 그러나 도메인 전문가가 수동으로 힘들게 프로세스를 감독해야 하는 경우 합성 데이터 생성 자체는 엄청나게 비용이 많이 들 수 있습니다. 더욱이, 합성 데이터로 훈련된 신경망은 도메인 격차로 인해 실제 데이터에서 제대로 작동하지 않는 경우가 많습니다. 이러한 문제를 해결하기 위해 우리는 실제 데이터의 분포를 일치시키는 자기 지도 자동 장면 생성 기술인 Sim2SG를 제안합니다. 중요한 점은 Sim2SG는 실제 데이터 세트의 감독이 필요하지 않으므로 이러한 주석을 얻기 어려운 상황에 적용할 수 있다는 것입니다. Sim2SG는 실제 데이터의 내용을 일치시키고 소스 및 대상 도메인의 기능을 일치시켜 내용과 모양의 격차를 해소하도록 설계되었습니다. 레이블이 지정된 데이터세트의 가용성이 제한되어 있기 때문에 장면 그래프(SG) 생성을 다운스트림 작업으로 선택합니다. 실험은 실제 KITTI 데이터 세트뿐만 아니라 여러 합성 데이터 세트에서 도메인 격차를 질적, 양적으로 줄이는 데 있어 주요 기준선에 비해 상당한 개선이 있음을 보여줍니다."
148,http://arxiv.org/abs/2011.13588 ,Road Scene Graph: A Semantic Graph-Based Scene Representation Dataset for Intelligent Vehicles,"Yafu Tian, Alexander Carballo, Ruifeng Li, Kazuya Takeda","풍부한 의미 정보 추출은 차세대 지능형 차량에서 중요한 역할을 합니다. 현재 6D 자세 탐지, 도로 장면 의미론적 분할 등과 같은 기본 응용에 초점을 맞춘 많은 연구가 진행되고 있습니다. 이는 이러한 데이터를 어떻게 구성하고 활용해야 하는지 생각해 볼 수 있는 좋은 기회를 제공합니다.   본 논문에서는 지능형 자동차를 위한 특별한 장면 그래프인 도로 장면 그래프를 제안한다. 기존 데이터 표현과 달리 이 그래프는 객체 제안뿐만 아니라 쌍별 관계도 제공합니다. 이를 토폴로지 그래프로 구성함으로써 이러한 데이터는 설명 가능하고 완전히 연결되어 있으며 GCN(Graph Convolutional Networks)에서 쉽게 처리할 수 있습니다. 여기서는 기본 그래프 예측 모델을 포함하여 Road Scene Graph 데이터 세트를 사용하여 도로에 장면 그래프를 적용합니다. 이 작업에는 제안된 모델을 사용한 실험적 평가도 포함됩니다."
147,http://arxiv.org/abs/2011.11397 ,Imagination-enabled Robot Perception,"Patrick Mania, Franklin Kenghagho Kenfack, Michael Neumann, Michael Beetz","오늘날의 로봇 인식 시스템 중 다수는 너무 단순하고 어려운 인식 작업을 수행하는 것을 목표로 합니다. 조작 작업을 수행하는 데 필요한 모든 정보를 제공하기 위해 인식 시스템이 필요하지 않기 때문에 너무 단순합니다. 일반적으로 인식 결과에는 객체의 부품 구조, 관절 메커니즘 및 조작 동작을 조정하는 데 필요한 기타 속성에 대한 정보가 포함되지 않습니다. 반면에, 언급된 인식 문제는 인간과 달리 인식 시스템이 자신이 보게 될 것에 대한 기대를 최대한 활용할 수 없기 때문에 너무 어렵습니다. 따라서 우리는 가정용 로봇이나 소매점의 로봇과 같이 일상적인 조작 작업을 수행하는 로봇에 적합한 로봇 인식 작업의 변형을 조사합니다. 그러한 설정에서는 로봇이 대부분의 물체를 알고 있고 그에 대한 상세한 모델을 가지고 있다고 가정하는 것이 합리적입니다.   우리는 물리 시뮬레이션과 시각적 렌더링을 갖춘 장면 그래프로서 환경에 대한 믿음을 유지하는 인식 시스템을 제안합니다. 객체를 감지하면 인식 시스템은 객체의 모델을 검색하여 VR 기반 환경 모델의 해당 위치에 배치합니다. 물리 시뮬레이션은 물리적으로 불가능한 물체 감지를 거부하고 장면을 렌더링하여 이미지 수준에서 기대치를 생성할 수 있도록 보장합니다. 그 결과 조작 작업에 유용한 정보를 제공할 수 있는 인식 시스템이 탄생했습니다."
146,http://arxiv.org/abs/2011.10731 ,LRTA: A Transparent Neural-Symbolic Reasoning Framework with Modular Supervision for Visual Question Answering,"Weixin Liang, Feiyang Niu, Aishwarya Reganti, Govind Thattai, Gokhan Tur","시각적 질문 응답(VQA)에 대한 주요 접근 방식은 ""블랙박스"" 신경 인코더를 사용하여 이미지와 질문을 인코딩하고 단일 토큰을 ""예"" 또는 ""아니요""와 같은 대답으로 디코딩하는 것에 의존합니다. 이 접근 방식의 강력한 정량적 결과에도 불구하고 예측 프로세스에 대한 직관적이고 사람이 읽을 수 있는 형태의 정당화를 찾는 데 어려움을 겪습니다. 이러한 부족함을 해결하기 위해 우리는 VQA를 전체 답변 생성 작업으로 재구성합니다. 이를 위해서는 모델이 자연어로 예측을 정당화해야 합니다. 우리는 인간처럼 문제를 단계별로 해결하고 각 단계에서 인간이 읽을 수 있는 형태의 정당화를 제공하는 시각적 질문 답변을 위한 투명한 신경 기호 추론 프레임워크인 LRTA [보기, 읽기, 생각, 대답]를 제안합니다. 특히 LRTA는 먼저 이미지를 장면 그래프로 변환하고 질문을 여러 추론 지침으로 구문 분석하는 방법을 학습합니다. 그런 다음 순환 신경 기호 실행 모듈을 사용하여 장면 그래프를 탐색하여 한 번에 하나씩 추론 명령을 실행합니다. 마지막으로 자연어 정당화를 통해 주어진 질문에 대한 완전한 답변을 생성합니다. GQA 데이터 세트에 대한 실험에 따르면 LRTA는 전체 답변 생성 작업에서 큰 차이(43.1% 대 28.0%)로 최첨단 모델보다 성능이 뛰어납니다. 또한 모델이 피상적인 데이터 상관 관계를 통해 현명한 추측을 하는지 분석하기 위한 질문에서 언어적 단서(속성 및 관계)를 제거하여 교란된 GQA 테스트 세트를 만듭니다. 우리는 LRTA가 질문을 진정으로 이해하는 방향으로 나아가는 반면 최첨단 모델은 훈련 데이터로부터 피상적인 상관관계를 학습하는 경향이 있음을 보여줍니다."
145,http://arxiv.org/abs/2011.10452 ,Bridging Scene Understanding and Task Execution with Flexible Simulation Environments,"Zachary Ravichandran, J. Daniel Griffith, Benjamin Smith, Costas Frost","세계의 3D, 미터법 및 객체 지향 표현을 구축하려는 장면 이해에서 상당한 진전이 이루어졌습니다. 동시에 강화 학습은 시뮬레이션의 발전에 힘입어 인상적인 발전을 이루었습니다. 이에 비해 인식 알고리즘에 대한 시뮬레이션에는 초점이 덜 맞춰졌습니다. 미터법 의미 매핑 또는 3D 동적 장면 그래프 생성과 같은 정교한 인식 접근 방식에는 대화형 환경에서 정확한 3D, 2D 및 관성 정보가 필요하므로 시뮬레이션이 점점 더 중요해지고 있습니다. 이를 위해 장면 이해 및 작업 실행 알고리즘 개발을 위한 오픈 소스 시뮬레이터인 TESSE(Task Execution with Semantic Segmentation Environments)를 제시합니다. TESSE는 미터법 의미 매핑 및 3D 동적 장면 그래프 생성을 위한 최첨단 솔루션을 개발하는 데 사용되었습니다. 또한 TESSE는 강화 학습에 중점을 둔 객체 검색 대회인 ICRA(International Conference of Robotics and Automation) 2020에서 GOSEEK Challenge의 플랫폼 역할을 했습니다. TESSE 코드는 https://github.com/MIT-TESSE에서 확인할 수 있습니다."
144,http://arxiv.org/abs/2011.10379 ,Neural Scene Graphs for Dynamic Scenes,"Julian Ost, Fahim Mannan, Nils Thuerey, Julian Knodt, Felix Heide",최근의 암시적 신경 렌더링 방법은 RGB 이미지 세트에 의해서만 감독되는 체적 밀도와 색상을 예측함으로써 복잡한 장면에 대한 정확한 뷰 합성을 학습할 수 있음을 보여주었습니다. 그러나 기존 방법은 모든 장면 객체를 단일 신경망으로 인코딩하는 정적 장면의 효율적인 표현을 학습하는 데 제한되어 있으며 동적 장면을 표현하고 개별 장면 객체로 분해하는 기능이 부족합니다. 본 연구에서는 동적 장면을 장면 그래프로 분해하는 최초의 신경 렌더링 방법을 제시합니다. 우리는 장면의 새로운 배열과 뷰를 효율적으로 렌더링하기 위해 객체 변환과 광도를 인코딩하는 학습된 장면 그래프 표현을 제안합니다. 이를 위해 우리는 단일 암시적 기능을 가진 객체를 설명하기 위해 공동으로 학습된 잠재 표현과 결합된 암시적으로 인코딩된 장면을 학습합니다. 우리는 합성 및 실제 자동차 데이터에 대해 제안된 방법을 평가하여 우리의 접근 방식이 이 장면의 비디오를 관찰함으로써만 동적 장면을 학습하고 보이지 않는 포즈의 보이지 않는 개체 세트로 새로운 장면 구성의 새로운 사실적인 뷰를 렌더링할 수 있음을 검증합니다.
143,http://arxiv.org/abs/2011.10084 ,Classification by Attention: Scene Graph Classification with Prior Knowledge,"Sahand Sharifzadeh, Sina Moayed Baharlou, Volker Tresp","장면 그래프 분류의 주요 과제는 객체와 관계의 모양이 이미지마다 크게 다를 수 있다는 것입니다. 이전 연구에서는 이미지의 모든 객체에 대한 관계형 추론을 통해 이 문제를 해결하거나 사전 지식을 분류에 통합했습니다. 이전 연구와 달리 인식과 사전 지식에 대한 별도의 모델을 고려하지 않습니다. 대신에 우리는 분류를 주의 계층으로 구현하는 다중 작업 학습 접근 방식을 취합니다. 이를 통해 사전 지식이 인식 모델 내에서 나타나고 전파될 수 있습니다. 모델이 사전을 나타내도록 강제함으로써 우리는 강력한 귀납적 편향을 달성합니다. 우리는 우리 모델이 상식적인 지식을 정확하게 생성할 수 있으며 이 지식을 장면 표현에 반복적으로 주입하면 분류 성능이 훨씬 더 높아진다는 것을 보여줍니다. 또한, 우리 모델은 트리플로 제공된 외부 지식에 대해 미세 조정될 수 있습니다. 자기 지도 학습과 주석이 달린 이미지의 1%만 결합하면 객체 분류가 3% 이상, 장면 그래프 분류가 26%, 예측 예측 정확도가 36% 이상 향상됩니다."
142,http://arxiv.org/abs/2011.04779 ,"After All, Only The Last Neuron Matters: Comparing Multi-modal Fusion Functions for Scene Graph Generation",Mohamed Karim Belaid,"객체 분할부터 단어 벡터 표현까지 SGG(장면 그래프 생성)는 수많은 연구 결과를 바탕으로 구축된 복잡한 작업이 되었습니다. 본 논문에서는 이 모델의 마지막 모듈인 융합 기능에 중점을 둡니다. 후자의 역할은 세 가지 숨겨진 상태를 결합하는 것입니다. 다양한 구현을 비교하기 위해 제거 테스트를 수행합니다. 먼저 SUM, GATE 함수를 이용하여 최신 결과를 재현한다. 그런 다음 모델에 구애받지 않는 기능(DIST의 적응 버전과 MFB와 GATE의 혼합)을 추가하여 원래 솔루션을 확장합니다. DIST는 최첨단 구성을 기반으로 최고의 Recall @ K를 수행하여 이제 최첨단의 일부가 되었습니다."
141,http://arxiv.org/abs/2011.04234 ,Dual ResGCN for Balanced Scene GraphGeneration,"Jingyi Zhang, Yong Zhang, Baoyuan Wu, Yanbo Fan, Fumin Shen, Heng Tao Shen","시각적 장면 그래프 생성은 어려운 작업입니다. 이전 작업은 큰 진전을 이루었지만 대부분 장면 그래프 생성 시 클래스 불균형 문제를 명시적으로 고려하지 않습니다. 클래스 불균형을 고려하지 않고 학습된 모델은 다수 클래스를 예측하는 경향이 있으며, 이로 인해 사소한 빈번한 술어에서는 좋은 성능을 보이지만 정보가 풍부한 드문 술어에서는 성능이 좋지 않습니다. 그러나 소수 클래스의 술어는 종종 더 의미 있고 정확한 정보를 전달합니다~(\textit{e.g.}, \emph{`on'} v.s \emph{`parked on'}). %는 좋은 회상 점수로 이어지지만 평균 회상 점수는 낮습니다. 클래스 불균형의 영향을 완화하기 위해 우리는 객체 잔차 그래프 컨볼루션 네트워크와 관계 잔차 그래프 컨볼루션 네트워크로 구성된 \textit{dual ResGCN}이라는 새로운 모델을 제안합니다. 두 네트워크는 서로 보완적입니다. 전자는 객체 수준 컨텍스트 정보, 즉 객체 간의 연결을 캡처합니다. 우리는 교차 주의 방식으로 객체 특징을 향상시키는 새로운 ResGCN을 제안합니다. 게다가 불균형 문제를 완화하고 예측 다양성을 강화하기 위해 여러 상황별 계수를 쌓습니다. 후자는 관계 수준의 컨텍스트 정보 \textit{즉,} 관계 간의 연결을 명시적으로 캡처하도록 신중하게 설계되었습니다. 우리는 클래스 불균형 문제를 완화하는 데 더 도움이 되도록 관계 쌍의 동시 발생에 대한 사전 정보를 그래프에 통합할 것을 제안합니다. 제안된 방법의 우수성을 입증하기 위해 대규모 데이터베이스 VG에서 세 가지 작업에 대한 광범위한 평가를 수행합니다."
140,http://arxiv.org/abs/2010.04913 ,Interpretable Neural Computation for Real-World Compositional Visual Question Answering,"Ruixue Tang, Chao Ma","시각적 질문 응답(VQA)에 대한 연구에는 두 가지 주요 라인이 있습니다. 명시적 멀티 홉 추론을 사용하는 구성 모델과 잠재 기능 공간에서 암시적 추론을 사용하는 모놀리식 네트워크입니다. 전자는 해석 가능성과 구성성이 뛰어나지만 실제 이미지에서는 실패하는 반면, 후자는 일반적으로 모델 유연성과 매개변수 효율성으로 인해 더 나은 성능을 달성합니다. 우리는 두 가지를 결합하여 실제 구성 VQA를 위한 해석 가능한 프레임워크를 구축하는 것을 목표로 합니다. 우리의 프레임워크에서는 이미지와 질문이 장면 그래프와 프로그램으로 풀려나고 기호 프로그램 실행기가 완전히 투명하게 실행되어 주의 영역을 선택한 다음 시각적 언어 사전 훈련된 인코더에 반복적으로 전달되어 답변을 예측합니다. GQA 벤치마크에서 수행된 실험은 우리 프레임워크가 기존 구성 기술보다 성능이 뛰어나고 모놀리식 프레임워크 중에서 경쟁력 있는 정확성을 달성했음을 보여줍니다. 타당성, 타당성 및 분포 지표와 관련하여 우리 프레임워크는 다른 프레임워크보다 상당한 차이를 보입니다."
139,http://arxiv.org/abs/2010.03855 ,Dense Relational Image Captioning via Multi-task Triple-Stream Networks,"Dong-Jin Kim, Tae-Hyun Oh, Jinsoo Choi, In So Kweon","우리는 시각적 장면에서 개체 간의 관계 정보에 대해 여러 캡션을 생성하는 것을 목표로 하는 새로운 이미지 캡션 작업인 조밀한 관계형 캡션을 소개합니다. 관계형 캡션은 개체 조합 간의 각 관계에 대한 명시적인 설명을 제공합니다. 이 프레임워크는 정보의 다양성과 양 모두에서 유리하며, 관계형 제안 생성과 같은 관계 기반의 포괄적인 이미지 이해로 이어집니다. 객체 간의 관계 이해를 위해 품사(POS, 즉 주어-객체-술어 범주)는 캡션에서 단어의 인과적 순서를 안내하는 귀중한 사전 정보가 될 수 있습니다. 우리는 캡션을 생성하는 것뿐만 아니라 각 단어의 POS를 이해하는 방법을 배우기 위해 프레임워크를 강화합니다. 이를 위해 우리는 각 단어에 대한 올바른 캡션과 POS를 공동으로 예측하여 훈련된 각 POS를 담당하는 3개의 반복 단위로 구성된 MTTSNet(Multi-task Triple-Stream Network)을 제안합니다. 또한 명시적 관계형 모듈을 사용하여 객체 임베딩을 변조하면 MTTSNet의 성능이 향상될 수 있음을 발견했습니다. 우리는 제안된 모델이 대규모 데이터 세트와 여러 지표에 대한 광범위한 실험 분석을 통해 더 다양하고 풍부한 캡션을 생성할 수 있음을 보여줍니다. 그런 다음 전체적인 이미지 캡션 작성, 장면 그래프 생성 및 검색 작업에 프레임워크를 적용하는 방법을 제시합니다."
138,http://arxiv.org/abs/2010.03182 ,VICTR: Visual Information Captured Text Representation for Text-to-Image Multimodal Tasks,"Soyeon Caren Han, Siqu Long, Siwen Luo, Kunze Wang, Josiah Poon","주어진 텍스트 설명에서 이미지를 생성/검색하는 텍스트-이미지 다중 모달 작업은 원시 텍스트 설명이 시각적으로 사실적인 이미지를 완벽하게 설명하기 위해 상당히 제한된 정보를 다루기 때문에 매우 어려운 작업입니다. 우리는 텍스트 입력에서 개체의 풍부한 시각적 의미 정보를 캡처하는 텍스트-이미지 다중 모드 작업인 VICTR을 위한 새로운 시각적 상황별 텍스트 표현을 제안합니다. 먼저 텍스트 설명을 초기 입력으로 사용하고 종속성 구문 분석을 수행하여 구문 구조를 추출하고 객체 수량을 포함한 의미 측면을 분석하여 장면 그래프를 추출합니다. 그런 다음 Graph Convolutional Networks를 사용하여 장면 그래프에서 추출된 객체, 속성 및 관계와 해당 기하학적 관계 정보를 학습하고 텍스트 정보와 시각적 의미 정보를 통합한 텍스트 표현을 생성합니다. 텍스트 표현은 단어 수준 및 문장 수준 임베딩으로 집계되어 시각적 상황별 단어 및 문장 표현을 모두 생성합니다. 평가를 위해 텍스트-이미지 생성의 최첨단 모델에 VICTR을 연결했습니다. VICTR은 기존 모델에 쉽게 추가되며 정량적 측면과 정성적 측면 모두에서 향상됩니다."
137,http://arxiv.org/abs/2010.02591 ,Scene Graph Modification Based on Natural Language Commands,"Xuanli He, Quan Hung Tran, Gholamreza Haffari, Walter Chang, Trung Bui, Zhe Lin, Franck Dernoncourt, Nhan Dam",그래프 및 구문 분석 트리와 같은 구조화된 표현은 많은 자연어 처리 시스템에서 중요한 역할을 합니다. 최근 몇 년 동안 다중 턴 사용자 인터페이스의 발전으로 인해 새로운 정보 소스가 제공되면 이러한 구조적 표현을 제어하고 업데이트해야 할 필요성이 필요해졌습니다. 텍스트를 그래프나 구문 분석 트리에 매핑하는 구문 분석기의 성능을 향상시키는 데 초점을 맞춘 많은 노력이 있었지만 이러한 표현을 직접 조작하는 문제를 탐구한 사례는 거의 없습니다. 본 논문에서는 시스템이 새로운 사용자의 명령에 따라 기존 장면 그래프를 업데이트하는 방법을 학습해야 하는 그래프 수정의 새로운 문제를 탐구합니다. 그래프 기반 희소 변환기 및 교차 주의 정보 융합을 기반으로 하는 우리의 새로운 모델은 기계 번역 및 그래프 생성 문헌에서 채택된 이전 시스템보다 성능이 뛰어납니다. 우리는 이 새로운 문제에 대한 향후 연구를 장려하기 위해 대규모 그래프 수정 데이터 세트를 연구 커뮤니티에 추가로 제공합니다.
136,http://arxiv.org/abs/2010.01288 ,UNISON: Unpaired Cross-lingual Image Captioning,"Jiahui Gao, Yi Zhou, Philip L. H. Yu, Shafiq Joty, Jiuxiang Gu","이미지 캡션은 광범위한 적용 시나리오로 인해 최근 몇 년 동안 흥미로운 연구 분야로 부상했습니다. 이미지 캡션의 전통적인 패러다임은 지도 방식으로 모델을 훈련하기 위해 쌍을 이루는 이미지 캡션 데이터세트에 의존합니다. 그러나 모든 대상 언어에 대해 이러한 쌍을 이루는 데이터 세트를 생성하는 것은 엄청나게 비용이 많이 들며, 이는 캡션 기술의 확장성을 방해하고 전 세계 인구의 상당 부분에서 그 혜택을 박탈합니다. 이 연구에서는 소스 또는 대상 언어의 캡션 코퍼스에 의존하지 않고 이미지 캡션을 생성하는 새로운 짝이 없는 교차 언어 방법을 제시합니다. 구체적으로, 우리의 방법은 두 단계로 구성됩니다: (i) 문장 병렬(바이텍스트) 코퍼스를 활용하여 장면 그래프 인코딩 공간에서 소스에서 대상 언어로의 매핑을 학습하고 대상 언어의 문장을 디코딩하는 교차 언어 자동 인코딩 프로세스, (ii) 인코딩된 장면 그래프 기능을 이미지 양식에서 언어 양식으로 매핑하려는 교차 모달 비지도 기능 매핑. 제안한 방법이 중국 이미지 캡션 생성 작업에 미치는 효과를 검증한다. 여러 기존 방법과의 비교는 우리 접근 방식의 효율성을 보여줍니다."
135,http://arxiv.org/abs/2009.14558 ,Learning Object Detection from Captions via Textual Scene Attributes,"Achiya Jerbi, Roei Herzig, Jonathan Berant, Gal Chechik, Amir Globerson","개체 감지는 컴퓨터 비전의 기본 작업으로, 주석자가 개체와 해당 경계 상자에 레이블을 지정해야 하므로 수집하기 어려운 주석이 달린 대규모 데이터 세트가 필요합니다. 따라서 더 저렴한 형태의 감독을 효과적으로 사용하는 것은 중요한 과제입니다. 최근 작업에서는 약한 감독의 소스로 이미지 캡션을 탐색하기 시작했지만 현재까지 객체 감지의 맥락에서 캡션은 이미지에 있는 객체의 범주를 추론하는 데만 사용되었습니다. 이 연구에서 우리는 캡션이 사물의 속성과 그 관계를 포함하여 이미지에 대해 훨씬 더 풍부한 정보를 담고 있다고 주장합니다. 즉, 텍스트는 최근 문헌에 기술된 바와 같이 이미지의 한 장면을 나타낸다. 우리는 객체 감지기를 훈련시키기 위해 이 ""텍스트 장면 그래프""의 속성을 사용하는 방법을 제시합니다. 우리는 결과 모델이 여러 가지 까다로운 객체 감지 데이터 세트에서 최첨단 결과를 달성하여 최근 접근 방식을 능가한다는 것을 경험적으로 보여줍니다."
134,http://arxiv.org/abs/2009.13331 ,Addressing Class Imbalance in Scene Graph Parsing by Learning to Contrast and Score,"He Huang, Shunta Saito, Yuta Kikuchi, Eiichi Matsumoto, Wei Tang, Philip S. Yu","장면 그래프 구문 분석은 이미지 장면에서 객체를 감지하고 그 관계를 인식하는 것을 목표로 합니다. 최근 접근 방식은 일부 인기 있는 벤치마크에서 높은 평균 점수를 달성했지만 매우 긴 꼬리의 데이터 분포로 인해 학습이 빈번한 레이블로 편향되기 때문에 드문 관계를 탐지하는 데 실패했습니다. 이러한 희귀한 관계를 탐지하는 것이 실제 응용 프로그램에서 중요할 수 있다는 사실에 착안하여 이 논문에서는 장면 그래프 구문 분석의 클래스 불균형 문제를 해결하기 위해 분류 및 순위 지정의 새로운 통합 프레임워크를 소개합니다. 구체적으로 우리는 잘못된 빈번한 관계를 억제하여 희귀한 관계의 탐지를 촉진하는 새로운 Contrasting Cross-Entropy 손실을 설계합니다. 또한, 우리는 예측의 회상을 향상시키기 위해 이미지 특징과 관계 특징을 기반으로 관계의 순위를 매기는 방법을 학습하는 Scorer라는 새로운 채점 모듈을 제안합니다. 우리의 프레임워크는 간단하고 효과적이며 현재 장면 그래프 모델에 통합될 수 있습니다. 실험 결과는 제안된 접근 방식이 현재의 최첨단 방법을 개선하고 희귀한 관계를 탐지하는 분명한 이점이 있음을 보여줍니다."
133,http://arxiv.org/abs/2009.12395 ,SceneGen: Generative Contextual Scene Augmentation using Scene Graph Priors,"Mohammad Keshavarzi, Aakash Parikh, Xiyu Zhai, Melody Mao, Luisa Caldas, Allen Y. Yang","공간 컴퓨팅 경험은 사용자의 실제 환경에 의해 제한됩니다. 이러한 경험에서 가상 개체를 기존 장면에 추가하려면 기하학적 충돌을 피하고 다른 개체와의 기능적이고 그럴듯한 관계가 대상 환경에서 유지되는 상황별 접근 방식이 필요합니다. 그러나 사용자 환경의 복잡성과 다양성으로 인해 장면의 맥락에 맞는 가상 콘텐츠의 이상적인 위치를 자동으로 계산하는 것은 어려운 작업으로 간주됩니다. 이 문제에 착안하여 이 논문에서는 기존 장면 내에서 가상 개체 위치와 방향을 예측하는 생성적 상황 확대 프레임워크인 SceneGen을 소개합니다. SceneGen은 의미상으로 분할된 장면을 입력으로 사용하고 가상 콘텐츠 배치를 위한 위치 및 방향 확률 맵을 출력합니다. 우리는 객체, 객체 그룹 및 방 간의 명시적인 토폴로지 속성을 캡슐화하는 새로운 공간 장면 그래프 표현을 공식화합니다. 우리는 명시적이고 직관적인 기능을 제공하는 것이 유익한 콘텐츠 생성과 공간 컴퓨팅 설정의 사용자 상호 작용, 즉 암시적 모델에서 포착되지 않는 품질에 중요한 역할을 한다고 믿습니다. 우리는 커널 밀도 추정(KDE)을 사용하여 실제 3D 스캔 데이터에서 추출된 이전 공간 장면 그래프를 사용하여 훈련된 다변량 조건부 지식 모델을 구축합니다. 방향 속성을 추가로 캡처하기 위해 방향 레이블을 사용하여 현재 실제 데이터 세트를 확장하는 빠른 포즈 주석 도구를 개발합니다. 마지막으로 실제 시스템을 시연하기 위해 객체를 실시간으로 상황에 맞게 증강할 수 있는 증강 현실 애플리케이션을 개발합니다."
132,http://arxiv.org/abs/2009.12313 ,Are scene graphs good enough to improve Image Captioning?,"Victor Milewski, Marie-Francine Moens, Iacer Calixto",많은 최고 성능의 이미지 캡션 모델은 객체 감지 모델로 계산된 객체 특징에만 의존하여 이미지 설명을 생성합니다. 그러나 최근 연구에서는 개체 간의 상호 작용을 더 잘 설명하기 위해 장면 그래프를 직접 사용하여 개체 관계에 대한 정보를 캡션에 도입하는 것을 제안합니다. 본 연구에서는 이미지 캡션에 장면 그래프를 사용하는 방법을 철저히 조사합니다. 우리는 추가적인 장면 그래프 인코더를 사용하는 것이 더 나은 이미지 설명으로 이어질 수 있는지 여부를 경험적으로 연구하고 이미지 캡션 디코더 상태를 사용하여 그래프 업데이트를 조절하는 조건부 그래프 주의 네트워크(C-GAT)를 제안합니다. 마지막으로 예측된 ​​장면 그래프의 노이즈가 캡션 품질에 어느 정도 영향을 미치는지 확인합니다. 전반적으로 장면 그래프 기능을 사용하는 모델과 다양한 캡션 측정항목에서 객체 감지 기능만 사용하는 모델 간에는 큰 차이가 없습니다. 이는 기존 장면 그래프 생성 모델이 이미지 캡션에 유용하기에는 여전히 노이즈가 너무 많다는 것을 의미합니다. 또한 예측된 장면 그래프의 품질은 일반적으로 매우 낮지만 고품질 장면 그래프를 사용할 때 강력한 상향식 하향식 기준선에 비해 최대 3.3 CIDEr의 이득을 얻습니다. 우리는 모든 실험을 재현하기 위해 https://github.com/iacercalixto/butd-image-captioning에 소스 코드를 공개합니다.
131,http://arxiv.org/abs/2009.10939 ,Scene Graph to Image Generation with Contextualized Object Layout Refinement,"Maor Ivgi, Yaniv Benny, Avichai Ben-David, Jonathan Berant, Lior Wolf",장면 그래프에서 이미지를 생성하는 것은 최근 상당한 관심을 불러일으키는 어려운 작업입니다. 이전 작업에서는 대상 이미지의 중간 레이아웃 설명을 생성하여 이 작업에 접근했습니다. 그러나 레이아웃의 각 개체 표현은 독립적으로 생성되었으므로 겹치는 부분이 많고 적용 범위가 낮으며 전체적으로 흐릿한 레이아웃이 발생했습니다. 우리는 객체 간 종속성을 개선하기 위해 전체 레이아웃 설명을 점진적으로 생성하여 이러한 문제를 완화하는 새로운 방법을 제안합니다. 우리는 COCO-STUFF 데이터세트를 통해 우리의 접근 방식이 중간 레이아웃과 최종 이미지 모두의 품질을 향상시킨다는 것을 경험적으로 보여줍니다. 우리의 접근 방식은 레이아웃 적용 범위를 거의 20포인트까지 향상하고 개체 중복을 무시할 수 있는 양으로 줄였습니다.
130,http://arxiv.org/abs/2009.07526 ,CogTree: Cognition Tree Loss for Unbiased Scene Graph Generation,"Jing Yu, Yuan Chai, Yujing Wang, Yue Hu, Qi Wu","장면 그래프는 시각적 이해와 추론을 장려하는 이미지의 의미론적 추상화입니다. 그러나 실제 시나리오에서 편향된 데이터에 직면했을 때 장면 그래프 생성(SGG)의 성능은 만족스럽지 않습니다. 기존의 편향성 제거 연구는 주로 편향된 클래스 간의 상관 관계를 무시하고 데이터 분포의 균형을 맞추거나 편향되지 않은 모델 및 표현을 학습한다는 관점에서 연구합니다. 이 작업에서 우리는 새로운 인지 관점에서 이 문제를 분석합니다. 즉, 편향된 예측에서 계층적 인지 구조를 자동으로 구축하고 해당 계층을 탐색하여 관계를 찾아 꼬리 관계가 대략적인 모드에서 더 많은 관심을 받도록 만듭니다. 이를 위해 우리는 편견 없는 SGG를 위한 새로운 편향성 제거 인지 트리(CogTree) 손실을 제안합니다. 먼저 편향된 SGG 모델의 예측을 기반으로 관계를 구성하기 위해 인지 구조 CogTree를 구축합니다. CogTree는 처음에는 현저하게 다른 관계를 구별한 다음 쉽게 혼동되는 관계 중 작은 부분에 초점을 맞춥니다. 그런 다음, 올바른 관계에 대해 대략적인 구분을 지원하는 이 인지 구조에 대해 편향성 제거 손실을 특별히 제안합니다. 손실은 모델에 구애받지 않으며 여러 최첨단 모델의 성능을 지속적으로 향상시킵니다. 코드는 https://github.com/CYVincent/Scene-Graph-Transformer-CogTree에서 확인할 수 있습니다."
129,http://arxiv.org/abs/2009.05834 ,Exploring the Hierarchy in Relation Labels for Scene Graph Generation,"Yi Zhou, Shuyang Sun, Chao Zhang, Yikang Li, Wanli Ouyang","각 관계에 단일 레이블을 할당함으로써 현재 접근 방식은 관계 감지를 분류 문제로 공식화합니다. 이 공식에서는 술어 범주가 완전히 다른 클래스로 처리됩니다. 그러나 서로 다른 클래스에 명시적인 경계가 있는 객체 레이블과 달리 술어는 일반적으로 의미론적 의미가 중복됩니다. 예를 들어, sit\_on 및stand\_on은 수직 관계에서 공통된 의미를 갖지만 이 두 개체가 수직으로 배치되는 방식에 대한 세부 사항은 다릅니다. 술어 카테고리의 고유 구조를 활용하기 위해 먼저 언어 계층 구조를 구축한 다음 HGFL(Hierarchy Guided Feature Learning) 전략을 활용하여 거친 수준과 세분화된 수준 모두에서 더 나은 지역 기능을 학습할 것을 제안합니다. 또한, 우리는 세분화된 수준의 기능 학습을 안내하기 위해 대략적인 수준을 활용하는 HGM(Hierarchy Guided Module)을 제안합니다. 실험에 따르면 제안된 간단하면서도 효과적인 방법은 다양한 데이터 세트의 장면 그래프 생성 작업에 대한 Recall@50 측면에서 큰 차이(최대 $33\%$ 상대 이득)로 여러 최첨단 기준선을 개선할 수 있음을 보여줍니다."
128,http://arxiv.org/abs/2009.00893 ,PCPL: Predicate-Correlation Perception Learning for Unbiased Scene Graph Generation,"Shaotian Yan, Chen Shen, Zhongming Jin, Jianqiang Huang, Rongxin Jiang, Yaowu Chen, Xian-Sheng Hua","오늘날 장면 그래프 생성(SGG) 작업은 주로 조건자 주석 분포의 극도로 긴 꼬리 방향 편향으로 인해 현실적인 시나리오에서 크게 제한됩니다. 따라서 SGG의 클래스 불균형 문제를 해결하는 것은 중요하고 어려운 일입니다. 이 논문에서 우리는 술어 라벨이 서로 강한 상관관계를 가질 때 일반적인 재조정 전략(예: 재샘플링 및 재가중치)이 꼬리 데이터에 과적합을 초래하거나(예: 벤치가 보도 위에 앉아 있기보다는 보도에 앉아 있음) 여전히 원래의 고르지 않은 분포로 인해 역효과를 겪게 된다는 사실을 발견했습니다(예: 위에 주차된 다양한 위치/서 있는 위치/위에 앉아 있는 위치 집계). 우리는 주된 이유는 재조정 전략이 술어의 빈도에 민감하지만 관련성을 인식하지 못하기 때문에 술어 특징의 학습을 촉진하는 데 더 중요한 역할을 할 수 있다고 주장합니다. 따라서 본 논문에서는 술어 클래스 간의 상관관계를 직접 인지하고 활용하여 적응적으로 적절한 손실 가중치를 찾아내는 새로운 Predicate-Correlation Perception Learning(PCPL) 기법을 제안한다. 또한, 우리의 PCPL 프레임워크에는 컨텍스트 기능을 더 잘 추출하기 위한 그래프 인코더 모듈이 추가로 장착되어 있습니다. 벤치마크 VG150 데이터세트에 대한 광범위한 실험에서는 제안된 PCPL이 테일 클래스에서 훨씬 더 나은 성능을 발휘하는 동시에 헤드 클래스에서는 성능을 잘 보존하여 이전 최첨단 방법보다 훨씬 뛰어난 성능을 발휘한다는 것을 보여줍니다."
127,http://arxiv.org/abs/2009.06435 ,Scene-Graph Augmented Data-Driven Risk Assessment of Autonomous Vehicle Decisions,"Shih-Yuan Yu, Arnav V. Malawade, Deepan Muthirayan, Pramod P. Khargonekar, Mohammad A. Al Faruque","자율주행시스템(ADS)의 눈부신 발전에도 불구하고 복잡한 도로 상황에서의 내비게이션은 여전히 ​​어려운 문제로 남아 있습니다. 다양한 결정의 주관적인 위험 수준을 평가하면 정상 및 복잡한 운전 시나리오 모두에서 ADS의 안전성을 향상시킬 수 있다는 상당한 증거가 있습니다. 그러나 기존의 딥 러닝 기반 방법은 교통 참여자 간의 관계를 모델링하는 데 실패하는 경우가 많으며 복잡한 실제 시나리오에 직면할 때 어려움을 겪을 수 있습니다. 게다가 이러한 방법에는 전달성과 설명성이 부족합니다. 이러한 제한 사항을 해결하기 위해 장면 그래프를 중간 표현으로 사용하는 새로운 데이터 기반 접근 방식을 제안합니다. 우리의 접근 방식에는 다중 관계 그래프 컨볼루션 네트워크, 장단기 메모리 네트워크 및 운전 조작의 주관적 위험을 모델링하기 위한 주의 레이어가 포함됩니다. 모델을 훈련하기 위해 이 작업을 지도 장면 분류 문제로 공식화합니다. 모델의 기능을 보여주기 위해 일반적인 사용 사례인 차선 변경을 고려합니다. 우리는 우리의 접근 방식이 대규모(96.4% 대 91.2%) 및 소규모(91.8% 대 71.2%) 합성 데이터 세트 모두에서 최첨단 접근 방식보다 더 높은 분류 정확도를 달성한다는 것을 보여 주며, 또한 우리의 접근 방식이 더 작은 데이터 세트에서도 효과적으로 학습할 수 있음을 보여줍니다. 또한, 합성된 데이터세트로 훈련된 모델은 실제 데이터세트에서 테스트했을 때 평균 87.8%의 정확도를 달성했으며, 이는 동일한 합성된 데이터세트에서 훈련된 최첨단 모델이 달성한 70.3%의 정확도와 비교하여 우리의 접근 방식이 지식을 보다 효과적으로 전달할 수 있음을 보여줍니다. 마지막으로 공간적 및 시간적 주의 계층을 사용하면 모델 성능이 각각 2.7% 및 0.7% 향상되고 설명 가능성이 향상됨을 보여줍니다."
126,http://arxiv.org/abs/2008.11932 ,Attribute-guided image generation from layout,"Ke Ma, Bo Zhao, Leonid Sigal","최근 접근 방식은 의미론적 분할, 장면 그래프 또는 레이아웃과 같은 구조화된 입력을 통한 이미지 생성에서 큰 성공을 거두었습니다. 이러한 방법을 사용하면 이미지 수준에서 개체와 해당 위치를 지정할 수 있지만 인스턴스 수준에서 이러한 개체의 시각적 모양을 지정하는 충실도와 의미 제어가 부족합니다. 이러한 한계를 해결하기 위해 우리는 인스턴스 수준의 속성 제어가 가능한 새로운 이미지 생성 방법을 제안합니다. 구체적으로, 속성 기반 생성 모델에 대한 입력은 (1) 객체 경계 상자, (2) 객체 범주 및 (3) 각 객체에 대한 (선택적) 속성 세트를 포함하는 튜플입니다. 출력은 요청된 객체가 원하는 위치에 있고 규정된 속성을 갖는 생성된 이미지입니다. 여러 가지 손실이 협력하여 정확하고 일관되며 다양한 이미지 생성을 촉진합니다. Visual Genome 데이터 세트에 대한 실험은 생성된 이미지의 객체 수준 속성을 제어하고 레이아웃 작업의 이미지 생성에서 얽혀 있지 않은 객체 속성 표현의 타당성을 검증하는 모델의 능력을 보여줍니다. 또한, 우리 모델에서 생성된 이미지는 이전 최첨단 기술에 비해 더 높은 해상도, 객체 분류 정확도 및 일관성을 갖습니다."
125,http://arxiv.org/abs/2008.07832 ,Tackling the Unannotated: Scene Graph Generation with Bias-Reduced Models,"Tzu-Jui Julius Wang, Selen Pehlivan, Jorma Laaksonen","시각적 개체와 이미지의 상호 작용을 캡처하는 장면 그래프를 예측하는 것은 전체 장면 이해를 향한 중요한 단계로 간주되었습니다. 최근 SGG(장면 그래프 생성) 모델은 시각적 개체 간의 가장 빈번한 관계를 캡처하는 기능을 보여주었습니다. 그러나 최첨단 결과는 여전히 만족스럽지 않습니다. 모델은 전체 회상 R@100에서 31%를 얻을 수 있는 반면 마찬가지로 중요한 평균 클래스별 회상 mR@100은 Visual Genome(VG)에서 약 8%에 불과합니다. R과 mR 결과 사이의 불일치는 여전히 경쟁적인 R을 사용하여 높은 R을 추구하는 것에서 높은 mR로 초점을 이동하도록 촉구합니다. 관찰된 불일치는 VG의 주석 편향과 희소 주석 모두에서 비롯된 것으로 의심됩니다. 여기서 많은 시각적 엔터티 쌍은 전혀 주석이 추가되지 않거나 여러 항목이 유효할 수 있는 경우 단일 관계로만 표시됩니다. 이 특정 문제를 해결하기 위해 우리는 스스로 학습한 지식을 활용하는 새로운 SGG 훈련 계획을 제안합니다. 여기에는 두 개의 관계 분류기가 포함되며, 하나는 다른 하나가 기반으로 삼을 수 있는 덜 편향된 설정을 제공합니다. 제안된 방식은 대부분의 기존 SGG 모델에 적용할 수 있으며 구현이 간단합니다. 우리는 모든 표준 SGG 작업에서 mR(+6.6% ~ +20.4% 사이)과 경쟁적이거나 더 나은 R(-2.4% ~ 0.3% 사이)의 상당한 상대적 개선을 관찰했습니다."
124,http://arxiv.org/abs/2008.07817 ,Retargetable AR: Context-aware Augmented Reality in Indoor Scenes based on 3D Scene Graph,"Tomu Tahara, Takashi Seno, Gaku Narita, Tomoya Ishikawa","본 논문에서는 다양한 실제 환경에서 설정된 장면 맥락을 인식하여 가상 세계와 현실 세계 사이의 자연스러운 상호 작용을 달성하는 AR 경험을 제공하는 새로운 AR 프레임워크인 Retargetable AR을 제시합니다. 이를 위해 좌표 변환이 아닌 3D 공간의 개체 간의 관계를 통해 장면 컨텍스트를 특성화합니다. AR 콘텐츠가 가정하는 맥락과 사용자가 AR을 경험하는 실제 환경에서 형성되는 맥락을 추상적인 그래프 표현, 즉 장면 그래프로 표현한다. RGB-D 스트림에서 우리 프레임워크는 장면의 기하학적 정보와 의미 정보가 통합된 체적 맵을 생성합니다. 또한 의미론적 맵을 사용하여 장면 객체를 방향이 지정된 경계 상자로 추상하고 해당 방향을 추정합니다. 이러한 장면 표현을 통해 우리 프레임워크는 온라인 방식으로 AR에 대한 실제 환경의 맥락을 특성화하는 3D 장면 그래프를 구성합니다. 구성된 그래프와 AR 콘텐츠의 맥락을 나타내는 AR 장면 그래프의 대응은 의미적으로 등록된 콘텐츠 배열을 제공하여 가상 세계와 현실 세계 간의 자연스러운 상호 작용을 촉진합니다. 우리는 지향된 경계 상자 추정 성능에 대한 정량적 평가, 구성된 3D 장면 그래프를 기반으로 한 AR 콘텐츠 배열의 주관적 평가, 온라인 AR 시연을 통해 프로토타입 시스템에 대한 광범위한 평가를 수행했습니다. 이러한 평가 결과는 우리 프레임워크의 효율성을 보여 주며, 다양한 실제 장면에서 상황 인식 AR 경험을 제공할 수 있음을 입증했습니다."
123,http://arxiv.org/abs/2008.06651 ,Graph Edit Distance Reward: Learning to Edit Scene Graph,"Lichang Chen, Guosheng Lin, Shijie Wang, Qingyao Wu","장면 그래프는 언어 도메인과 이미지 도메인 간의 격차를 해소하는 중요한 도구로서 VQA와 같은 교차 양식 작업에 널리 채택되었습니다. 본 논문에서는 지금까지 탐구된 적이 없는 새로운 방법으로 사용자 지시에 따라 장면 그래프를 편집하는 방법을 제안한다. 구체적으로, 편집 장면 그래프를 텍스트가 제공하는 의미로 학습하기 위해 신경 기호 모델을 최적화하기 위해 정책 그라데이션 및 그래프 매칭 알고리즘을 기반으로 하는 그래프 편집 거리 보상을 제안합니다. 텍스트 편집 이미지 검색의 맥락에서 우리는 CSS 및 CRIR 데이터 세트에서 우리 방법의 효율성을 검증합니다. 게다가 CRIR은 우리가 생성한 새로운 합성 데이터세트로 향후 사용을 위해 곧 게시할 예정입니다."
122,http://arxiv.org/abs/2008.05156 ,HOSE-Net: Higher Order Structure Embedded Network for Scene Graph Generation,"Meng Wei, Chun Yuan, Xiaoyu Yue, Kuo Zhong","장면 그래프 생성은 이미지에 대한 구조화된 표현을 생성하는 것을 목표로 하며 이를 위해서는 객체 간의 관계를 이해해야 합니다. 심층신경망의 연속적인 특성으로 인해 장면 그래프의 예측은 객체 탐지와 관계 분류로 나누어집니다. 그러나 독립 관계 클래스는 시각적 특징을 잘 분리하지 못합니다. 일부 방법은 시각적 기능을 그래프 구조로 구성하고 메시지 전달을 사용하여 상황별 정보를 학습하지만 여전히 급격한 클래스 내 변형과 불균형한 데이터 분포로 인해 어려움을 겪습니다. 한 가지 중요한 요소는 장면 그래프의 고유 구조를 무시하는 구조화되지 않은 출력 공간을 학습한다는 것입니다. 따라서 본 논문에서는 이러한 문제를 완화하기 위해 HOSE-Net(Higher Order Structure Embedded Network)을 제안한다. 첫째, 관계의 로컬 및 글로벌 구조 정보를 모두 출력 공간에 통합하는 새로운 구조 인식 분류자 삽입(SEC) 모듈을 제안합니다. 구체적으로, 컨텍스트 임베딩 세트는 로컬 그래프 기반 메시지 전달을 통해 학습된 다음 글로벌 구조 기반 분류 공간에 매핑됩니다. 둘째, 문맥별 분류 부분공간을 너무 많이 학습하면 데이터 희소성 문제가 발생할 수 있으므로 고차 구조 정보를 도입하여 부분공간 수를 줄이는 HSA(Herarchical Semantic Aggregation) 모듈을 제안합니다. HSA는 또한 관계형 지식 그래프를 기반으로 의미 개체 계층 구조를 자동으로 검색하는 빠르고 유연한 도구입니다. 광범위한 실험을 통해 제안된 HOSE-Net은 Visual Genome과 VRD의 두 가지 인기 벤치마크에서 최첨단 성능을 달성하는 것으로 나타났습니다."
121,http://arxiv.org/abs/2008.03555 ,Assisting Scene Graph Generation with Self-Supervision,"Sandeep Inuganti, Vineeth N Balasubramanian","장면 그래프 생성에 대한 연구는 시각적 질문 답변, 이미지 캡션 등과 같은 다운스트림 작업에 도움이 될 수 있는 잠재력으로 인해 지난 몇 년 동안 빠르게 주목을 받았습니다. 이 문제를 해결하기 위해 많은 흥미로운 접근 방식이 제안되었습니다. 이들 연구의 대부분은 예비 특징 추출기로 사전 훈련된 객체 감지 모델을 가지고 있습니다. 따라서 객체 감지 모델에서 객체 경계 상자 제안을 얻는 것이 상대적으로 저렴합니다. 우리는 사전 훈련된 검출기에 의해 생성된 경계 상자 주석을 즉시 사용할 수 있다는 점을 활용합니다. 우리는 세 가지 참신하면서도 간단한 자기 감독 작업 세트를 제안하고 이를 메인 모델에 대한 보조 다중 작업으로 훈련합니다. 비교하는 동안 이러한 자체 감독 작업을 통해 기본 모델을 처음부터 훈련하고 모든 측정항목 및 재현율 설정에서 최첨단 결과를 달성합니다. 또한 제안된 자기 감독 손실로 모델을 훈련하여 기하학적 관계와 소유 관계라는 두 가지 유형의 관계 사이의 혼란을 일부 해결합니다. 우리는 벤치마크 데이터 세트인 Visual Genome을 사용하여 실험을 수행하고 결과를 보여줍니다."
120,http://arxiv.org/abs/2007.13262 ,"REXUP: I REason, I EXtract, I UPdate with Structured Compositional Reasoning for Visual Question Answering","Siwen Luo, Soyeon Caren Han, Kaiyuan Sun, Josiah Poon","시각적 질문 답변(VQA)은 이미지와 질문 모두에 대한 의미론적 이해뿐만 아니라 정답으로 이어지는 단계별 추론 프로세스에 대한 건전한 인식도 요구하는 까다로운 다중 모드 작업입니다. 지금까지 VQA에서 가장 성공적인 시도는 이미지의 시각적 픽셀 특징과 질문의 단어 특징의 상호 작용 또는 간단한 개체를 사용하여 이미지의 질문에 답하는 추론 과정 중 하나의 측면에만 집중되었습니다. 본 논문에서는 명시적인 시각적 구조 인식 텍스트 정보를 갖춘 심층 추론 VQA 모델을 제안하며, 이는 단계별 추론 프로세스를 캡처하고 사진처럼 사실적인 이미지에서 복잡한 객체 관계를 탐지하는 데 효과적입니다. REXUP 네트워크는 이미지 객체 지향과 장면 그래프 지향의 두 가지 분기로 구성되며 이는 초대각선 융합 구성 주의 네트워크와 공동으로 작동합니다. 우리는 GQA 데이터 세트에서 REXUP을 정량적, 정성적으로 평가하고 광범위한 절제 연구를 수행하여 REXUP의 효율성 뒤에 있는 이유를 탐구합니다. 우리의 최고의 모델은 검증 세트에서 92.7%, 테스트 개발 세트에서 73.1%를 제공하는 귀중한 최첨단 모델보다 훨씬 뛰어난 성능을 발휘합니다."
119,http://arxiv.org/abs/2007.11744 ,End-to-End Optimization of Scene Layout,"Andrew Luo, Zhoutong Zhang, Jiajun Wu, Joshua B. Tenenbaum",우리는 장면 그래프를 기반으로 한 장면 레이아웃 합성을 위한 엔드투엔드 변형 생성 모델을 제안합니다. 무조건적인 장면 레이아웃 생성과 달리 장면 그래프에 포함된 관계를 만족하는 다양한 장면 레이아웃의 합성을 안내하기 위해 추상적이지만 일반적인 표현으로 장면 그래프를 사용합니다. 이를 통해 합성 프로세스를 보다 유연하게 제어할 수 있어 문장에서 추출된 장면 레이아웃이나 단일 컬러 이미지에서 추론된 장면 레이아웃과 같은 다양한 형태의 입력이 가능해집니다. 조건부 레이아웃 합성기를 사용하면 입력 예제와 동일한 구조를 공유하는 다양한 레이아웃을 생성할 수 있습니다. 조건부 생성 설계 외에도 장면의 2D 투영만을 사용하여 레이아웃 개선을 가능하게 하는 차별화 가능한 렌더링 모듈도 통합했습니다. 깊이와 의미 맵이 주어지면 미분 가능 렌더링 모듈을 사용하면 합성별 분석 방식으로 주어진 입력에 맞게 합성된 레이아웃을 최적화할 수 있습니다. 실험에 따르면 우리 모델은 조건부 장면 합성에서 더 높은 정확도와 다양성을 달성하고 다양한 입력 형식에서 예시 기반 장면 생성을 허용합니다.
118,http://arxiv.org/abs/2007.11731 ,Comprehensive Image Captioning via Scene Graph Decomposition,"Yiwu Zhong, Liwei Wang, Jianshu Chen, Dong Yu, Yin Li","우리는 이미지 장면 그래프 표현을 재검토하여 이미지 캡션 작성의 어려운 문제를 해결합니다. 우리 방법의 핵심은 장면 그래프를 일련의 하위 그래프로 분해하는 것입니다. 각 하위 그래프는 입력 이미지의 의미 구성 요소를 캡처합니다. 중요한 하위 그래프를 선택하고 선택된 각 하위 그래프를 단일 대상 문장으로 디코딩하기 위한 심층 모델을 설계합니다. 하위 그래프를 사용함으로써 모델은 이미지의 다양한 구성 요소에 주의를 기울일 수 있습니다. 따라서 우리의 방법은 정확하고 다양하며 근거 있고 제어 가능한 캡션을 동시에 설명합니다. 우리는 포괄적인 캡션 모델의 이점을 입증하기 위해 광범위한 실험을 제시합니다. 우리의 방법은 캡션 다양성, 접지 및 제어 가능성에 대한 새로운 최첨단 결과를 확립하고 캡션 품질의 최신 방법과 유리하게 비교됩니다. 우리 프로젝트 웹사이트는 http://pages.cs.wisc.edu/~yiwuzhong/Sub-GC.html에서 찾을 수 있습니다."
117,http://arxiv.org/abs/2007.08760 ,Sketching Image Gist: Human-Mimetic Hierarchical Scene Graph Generation,"Wenbin Wang, Ruiping Wang, Shiguang Shan, Xilin Chen","장면 그래프는 이미지 내용에 대한 인간의 인식을 충실하게 드러내는 것을 목표로 합니다. 인간은 장면을 분석할 때 일반적으로 이미지 요점, 즉 장면 그래프의 주요 개체와 주요 관계를 먼저 설명하는 것을 선호합니다. 이러한 인간 고유의 지각 습관은 장면 분석 과정에서 인간의 선호에 대한 계층적 구조가 존재함을 의미한다. 따라서 우리는 바람직한 장면 그래프도 계층적으로 구성되어야 한다고 주장하고, 장면 그래프를 모델링하기 위한 새로운 방식을 도입한다. 구체적으로 장면은 일련의 이미지 영역으로 구성된 인간 모방 계층 개체 트리(HET)로 표현됩니다. HET를 기반으로 장면 그래프를 생성하기 위해 HET에 포함된 구조화된 정보를 캡처하기 위해 계층 구조 및 형제 컨텍스트를 특별히 인코딩하는 하이브리드 장단기 메모리(Hybrid-LSTM)로 HET를 구문 분석합니다. 장면 그래프에서 주요 관계의 우선순위를 더욱 높이기 위해 우리는 객관적인 엔터티의 돌출성과 크기로부터 인간의 주관적인 지각 습관을 포착하는 방법을 학습하여 순위를 동적으로 조정하는 관계 순위 모듈(RRM)을 고안했습니다. 실험에 따르면 우리의 방법은 장면 그래프 생성을 위한 최첨단 성능을 달성할 뿐만 아니라 다운스트림 작업을 제공하는 데 큰 역할을 하는 이미지별 관계 마이닝에도 능숙하다는 것을 나타냅니다."
116,http://arxiv.org/abs/2007.05756 ,Generative Compositional Augmentations for Scene Graph Prediction,"Boris Knyazev, Harm de Vries, Cătălina Cangea, Graham W. Taylor, Aaron Courville, Eugene Belilovsky","장면 그래프 형태의 이미지에서 객체와 객체의 관계를 추론하는 것은 시각과 언어가 교차하는 많은 응용 프로그램에서 유용합니다. 우리는 롱테일 데이터 분포로 인해 이 작업에서 나타나는 구성 일반화의 어려운 문제를 고려합니다. 현재 장면 그래프 생성 모델은 가장 빈번한 구성에 해당하는 분포의 작은 부분에 대해 훈련됩니다. <컵, 온, 테이블>. 그러나 테스트 이미지에는 개체와 관계의 제로 샷 및 소수 샷 구성이 포함될 수 있습니다. <컵, 온, 서핑보드>. 학습 데이터에서 각 객체 카테고리와 조건자(예: 'on')가 자주 등장함에도 불구하고 모델은 이러한 보이지 않거나 희귀한 구성을 제대로 이해하지 못하는 경우가 많습니다. 일반화를 개선하기 위해 훈련 분포의 다양성을 높이려는 시도는 당연합니다. 그러나 그래프 영역에서는 이는 사소한 일이 아닙니다. 이를 위해 우리는 실제 장면 그래프를 섭동시켜 드물지만 그럴듯한 장면 그래프를 합성하는 방법을 제안합니다. 그런 다음 교란된 장면 그래프의 시각적 특징을 생성하고 공동 방식으로 학습할 수 있는 조건부 생성 적대 신경망(GAN)을 기반으로 하는 모델을 제안하고 경험적으로 연구합니다. Visual Genome 데이터 세트를 평가할 때 우리의 접근 방식은 제로 샷 및 소수 샷 측정 항목에서 미미하지만 일관된 개선을 가져왔습니다. 우리는 미래 연구의 유망한 방향을 나타내는 우리 접근 방식의 한계를 분석합니다."
115,http://arxiv.org/abs/2007.03848 ,Dynamic Graph Representation Learning for Video Dialog via Multi-Modal Shuffled Transformers,"Shijie Geng, Peng Gao, Moitreya Chatterjee, Chiori Hori, Jonathan Le Roux, Yongfeng Zhang, Hongsheng Li, Anoop Cherian","입력 비디오, 관련 오디오 및 간략한 캡션이 주어지면 AVSD(시청각 장면 인식 대화) 작업을 수행하려면 에이전트가 시청각 콘텐츠에 대해 사람과 질문 답변 대화를 해야 합니다. 따라서 이 작업은 여러 가지 인간-기계 상호 작용 응용 프로그램에 영향을 미칠 수 있는 발전된 다중 모드 표현 학습 및 추론 시나리오를 제시합니다. 이 작업을 해결하기 위해 우리는 각각 양식을 입력으로 취하고 입력 질문에 따라 표현을 생성하는 일련의 Transformer 모듈로 구성된 의미론 제어 다중 모드 셔플된 Transformer 추론 프레임워크를 소개합니다. 우리가 제안한 Transformer 변형은 다중 헤드 출력에 셔플링 방식을 사용하여 더 나은 정규화를 보여줍니다. 세분화된 시각적 정보를 인코딩하기 위해 우리는 모든 프레임에 대해 공간 의미 그래프 표현을 생성하는 프레임 내 추론 계층과 시간적 단서를 캡처하는 프레임 간 집계 모듈로 구성된 새로운 동적 장면 그래프 표현 학습 파이프라인을 제시합니다. 우리의 전체 파이프라인은 엔드 투 엔드로 훈련되었습니다. 답변 생성 및 선택 작업 모두에서 벤치마크 AVSD 데이터 세트에 대한 실험을 제시합니다. 우리의 결과는 모든 평가 지표에서 최고 수준의 성과를 보여줍니다."
114,http://arxiv.org/abs/2007.03357 ,Learning and Reasoning with the Graph Structure Representation in Robotic Surgery,"Mobarakol Islam, Lalithkumar Seenivasan, Lim Chwee Ming, Hongliang Ren",복잡한 수술 환경에서 그래프 표현을 추론하고 공간 추론을 수행하는 방법을 배우는 것은 로봇 수술의 수술 장면을 이해하는 데 중요한 역할을 할 수 있습니다. 이를 위해 우리는 장면 그래프를 생성하고 로봇 보조 수술 중 기구와 수술 관심 영역(ROI) 간의 수술 상호 작용을 예측하는 접근 방식을 개발합니다. 우리는 주의 연결 기능을 설계하고 그래프 분석 네트워크와 통합하여 수술 상호 작용을 인식합니다. 각 노드에 해당하는 이웃 노드 기능을 포함시키기 위해 SageConv를 네트워크에 추가로 통합합니다. 장면 그래프 생성 및 활성 에지 분류는 주로 복잡한 이미지 표현에서 노드 및 에지 특징의 임베딩 또는 특징 추출에 따라 달라집니다. 여기서는 라벨 스무딩 가중 손실을 사용하여 특징 추출 방법을 경험적으로 보여줍니다. 하드 라벨을 평활화하면 모델의 과도한 예측을 피할 수 있으며 두 번째 레이어에서 학습한 특징 표현을 향상시킬 수 있습니다. 그래프 장면 레이블을 얻기 위해 우리는 로봇 수술 분야의 숙련된 임상 전문가와 함께 로봇 장면 분할 과제 2018 데이터 세트에 대한 경계 상자 및 기기-ROI 상호 작용에 주석을 달고 이를 사용하여 제안을 평가했습니다.
113,http://arxiv.org/abs/2007.01072 ,Scene Graph Reasoning for Visual Question Answering,"Marcel Hildebrandt, Hang Li, Rajat Koner, Volker Tresp, Stephan Günnemann","시각적 질문 답변은 이미지에 대한 자유 형식 질문에 답변하는 것과 관련이 있습니다. 질문에 대한 깊은 언어적 이해와 이를 이미지에 존재하는 다양한 개체와 연관시키는 능력이 필요하기 때문에 야심찬 작업이며 컴퓨터 비전과 자연어 처리 기술이 모두 필요합니다. 우리는 장면에 존재하는 객체와 객체의 의미 및 공간적 관계를 기반으로 상황 중심, 순차적 추론을 수행하여 작업에 접근하는 새로운 방법을 제안합니다. 첫 번째 단계로, 우리는 이미지 속 물체와 그 속성, 상호 관계를 설명하는 장면 그래프를 도출합니다. 그런 다음 강화 에이전트는 추출된 장면 그래프를 자율적으로 탐색하여 경로를 생성하는 방법을 학습하며, 이는 답변 도출의 기초가 됩니다. 우리는 수동으로 선별된 장면 그래프를 사용하여 까다로운 GQA 데이터 세트에 대한 첫 번째 실험 연구를 수행합니다. 여기서 우리의 방법은 거의 인간 성능 수준에 도달합니다."
112,http://arxiv.org/abs/2006.16934 ,ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph,"Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang","우리는 장면 그래프에서 얻은 구조화된 지식을 통합하여 비전 언어의 공동 표현을 학습하는 지식 강화 접근 방식인 ERNIE-ViL을 제안합니다. ERNIE-ViL은 비전-언어 교차 모달 작업에 필수적인 비전과 언어 전반에 걸쳐 상세한 의미 연결(객체, 객체의 속성 및 객체 간의 관계)을 구축하려고 합니다. ERNIE-ViL은 시각적 장면의 장면 그래프를 활용하여 사전 훈련 단계에서 장면 그래프 예측 작업, 즉 개체 예측, 속성 예측 및 관계 예측 작업을 구성합니다. 구체적으로 이러한 예측 작업은 문장에서 파싱된 장면 그래프에서 다양한 유형의 노드를 예측하여 구현됩니다. 따라서 ERNIE-ViL은 비전과 언어 전반에 걸쳐 세부 의미 체계의 정렬을 특징으로 하는 결합 표현을 학습할 수 있습니다. 대규모 이미지-텍스트 정렬 데이터 세트에 대한 사전 훈련을 마친 후 5가지 크로스 모달 다운스트림 작업에서 ERNIE-ViL의 효율성을 검증합니다. ERNIE-ViL은 이러한 모든 작업에서 최첨단 성능을 달성하고 3.7%의 절대 향상으로 VCR 리더보드에서 1위를 차지했습니다."
111,http://arxiv.org/abs/2006.12373 ,Learning Physical Graph Representations from Visual Scenes,"Daniel M. Bear, Chaofei Fan, Damian Mrowca, Yunzhu Li, Seth Alter, Aran Nayebi, Jeremy Schwartz, Li Fei-Fei, Jiajun Wu, Joshua B. Tenenbaum, Daniel L. K. Yamins","CNN(Convolutional Neural Networks)은 시각적 개체 분류를 위한 표현 학습에 탁월한 것으로 입증되었습니다. 그러나 CNN은 객체, 부품 및 물리적 속성을 명시적으로 인코딩하지 않으므로 시각적 장면에 대한 구조적 이해가 필요한 작업에서 CNN의 성공이 제한되었습니다. 이러한 한계를 극복하기 위해 장면을 계층적 그래프로 표현하는 물리적 장면 그래프(PSG)의 아이디어를 소개합니다. 계층의 노드는 서로 다른 규모의 객체 부분에 직관적으로 대응하고 가장자리는 부분 간의 물리적 연결에 해당합니다. 각 노드에는 표면 모양, 질감과 같은 개체 속성을 직관적으로 나타내는 잠재 속성 벡터가 바인딩되어 있습니다. 또한 PSG 구조의 병목 현상을 통해 장면을 재구성하여 PSG를 추출하는 방법을 학습하는 네트워크 아키텍처인 PSGNet에 대해서도 설명합니다. PSGNet은 다음을 포함하여 표준 CNN을 강화합니다. 낮은 수준과 높은 수준의 이미지 정보를 결합하는 반복 피드백 연결; 공간적으로 균일한 특징 맵을 객체 중심 그래프 구조로 변환하는 그래프 풀링 및 벡터화 작업 의미 있는 장면 요소의 식별을 장려하는 지각적 그룹화 원칙. 우리는 PSGNet이 장면 분할 작업, 특히 복잡한 실제 이미지에서 대안적인 자체 감독 장면 표현 알고리즘보다 성능이 뛰어나고 보이지 않는 객체 유형 및 장면 배열에 잘 일반화된다는 것을 보여줍니다. PSGNet은 또한 물리적 동작을 통해 학습할 수 있으므로 정적 이미지에 대한 장면 추정도 향상됩니다. 우리는 PSGNet 아키텍처의 각 구성 요소의 중요성을 설명하는 일련의 절제 연구를 제시하고, 학습된 잠재 속성이 직관적인 장면 속성을 포착한다는 것을 보여주는 분석을 제시하며, 구성 장면 추론을 위한 PSG의 사용을 설명합니다."
110,http://arxiv.org/abs/2006.11524 ,"Neuro-Symbolic Visual Reasoning: Disentangling ""Visual"" from ""Reasoning""","Saeed Amizadeh, Hamid Palangi, Oleksandr Polozov, Yichen Huang, Kazuhito Koishida",시각적 질문 응답(VQA)과 같은 시각적 추론 작업에는 시각적 인식과 인식에 기초한 질문 의미론에 대한 추론의 상호 작용이 필요합니다. 그러나 이 분야의 최근 발전은 여전히 ​​추론보다는 인식 개선(예: 장면 그래프 생성)에 의해 주도되고 있습니다. 신경 모듈 네트워크와 같은 신경 기호 모델은 구성 추론의 이점을 VQA에 제공하지만 여전히 시각적 표현 학습과 얽혀 있으므로 신경 추론은 자체적으로 개선하고 평가하기 어렵습니다. 이 문제를 해결하기 위해 우리는 (1) VQA의 추론 측면을 인식과 별도로 분리하고 평가하는 프레임워크와 (2) 모델이 불완전한 인식으로도 추론 질문에 답할 수 있도록 하는 새로운 하향식 보정 기술을 제안합니다. 이를 위해 시각적 인식에서 질문 답변을 명시적으로 분리하는 VQA에 대한 미분 가능한 1차 논리 형식을 소개합니다. 까다로운 GQA 데이터세트에서 이 프레임워크는 잘 알려진 VQA 모델 간의 심층적이고 얽힌 비교를 수행하여 참여 모델과 작업에 관한 유익한 통찰력을 얻는 데 사용됩니다.
109,http://arxiv.org/abs/2006.09623 ,Learning Visual Commonsense for Robust Scene Graph Generation,"Alireza Zareian, Zhecan Wang, Haoxuan You, Shih-Fu Chang","장면 그래프 생성 모델은 객체 및 술어 인식을 통해 장면을 이해하지만 실제 인식 문제로 인해 실수가 발생하기 쉽습니다. 인식 오류는 출력 장면 그래프에서 실제 규칙과 패턴을 따르지 않는 무의미한 구성으로 이어지는 경우가 많으며 상식적인 지식을 사용하여 수정할 수 있습니다. 우리는 데이터로부터 어포던스, 직관적인 물리학과 같은 시각적 상식을 자동으로 획득하고 이를 활용하여 장면 이해의 견고성을 향상시키는 첫 번째 방법을 제안합니다. 이를 위해 Transformer 모델을 확장하여 장면 그래프의 구조를 통합하고 장면 그래프 코퍼스에서 Global-Local Attention Transformer를 훈련시킵니다. 일단 훈련되면 우리 모델은 모든 장면 그래프 생성 모델에 적용되고 명백한 실수를 수정하여 의미상 더 그럴듯한 장면 그래프를 얻을 수 있습니다. 광범위한 실험을 통해 우리 모델이 어떤 대안보다 상식을 더 잘 학습하고 최첨단 장면 그래프 생성 방법의 정확성을 향상시키는 것을 보여줍니다."
108,http://arxiv.org/abs/2006.07585 ,Learning from the Scene and Borrowing from the Rich: Tackling the Long Tail in Scene Graph Generation,"Tao He, Lianli Gao, Jingkuan Song, Jianfei Cai, Yuan-Fang Li",최근 몇 년 동안 장면 그래프 생성이 크게 발전했음에도 불구하고 객체 관계의 롱테일 분포는 여전히 까다롭고 골치 아픈 문제로 남아 있습니다. 기존 방법은 이 문제를 완화하기 위해 외부 지식이나 통계적 편향 정보에 크게 의존합니다. 본 논문에서는 또 다른 두 가지 측면에서 이 문제를 다룹니다. (1) 추가 주의 메커니즘을 통해 장면에서 특정 지식을 학습하는 것을 목표로 하는 장면-객체 상호 작용; (2) 머리에서 배운 풍부한 지식을 꼬리로 전달하려는 롱테일 지식 전달. 세 가지 작업에 대한 벤치마크 데이터 세트 Visual Genome에 대한 광범위한 실험은 우리의 방법이 현재 최첨단 경쟁사보다 성능이 우수하다는 것을 보여줍니다.
107,http://arxiv.org/abs/2006.02174 ,CompGuessWhat?!: A Multi-task Evaluation Framework for Grounded Language Learning,"Alessandro Suglia, Ioannis Konstas, Andrea Vanzo, Emanuele Bastianelli, Desmond Elliott, Stella Frank, Oliver Lemon",기반 언어 학습에 대한 접근 방식은 일반적으로 두드러진 속성을 예측하거나 보이지 않는 상황에 일반화하는 능력과 같은 학습된 숨겨진 표현의 바람직한 속성에 의존하지 않을 수 있는 단일 작업 기반 최종 성능 측정에 중점을 둡니다. 이 문제를 해결하기 위해 우리는 세 가지 하위 작업이 있는 속성 기반 언어 학습을 위한 평가 프레임워크인 GROLLA를 제시합니다. 1) 목표 지향 평가; 2) 객체 속성 예측 평가; 및 3) 제로샷 평가. 우리는 또한 새로운 데이터 세트 CompGuessWhat?! 특히 속성 접지와 관련하여 학습된 신경 표현의 품질을 평가하기 위한 이 프레임워크의 예입니다. 이를 위해 원래의 GuessWhat?! 지각 계층 위에 의미 계층을 포함하여 데이터 세트를 만듭니다. 특히 GuessWhat?!와 관련된 VisualGenome 장면 그래프를 강화합니다. 추상적이고 상황적인 속성을 지닌 이미지. 진단 분류기를 사용하여 현재 모델이 객체 속성을 인코딩할 만큼 표현력이 충분하지 않은 표현(평균 F1 44.27)을 학습한다는 것을 보여줍니다. 또한 게임플레이에 새로운 장면이나 사물이 포함될 때 잘 수행될 만큼 견고한 전략이나 표현을 배우지 않습니다(제로샷 최고 정확도 50.06%).
106,http://arxiv.org/abs/2005.08230 ,Graph Density-Aware Losses for Novel Compositions in Scene Graph Generation,"Boris Knyazev, Harm de Vries, Cătălina Cangea, Graham W. Taylor, Aaron Courville, Eugene Belilovsky","SGG(장면 그래프 생성)는 객체 및 객체 간의 관계 형태로 입력 이미지의 그래프 구조 설명을 예측하는 것을 목표로 합니다. 이 작업은 시각과 언어 인터페이스의 발전에 점점 더 유용해지고 있습니다. 여기서는 사물과 관계의 새로운(제로 샷) 또는 드문(몇 샷) 구성에서 잘 수행하는 것이 중요하지만 도전적입니다. 본 논문에서는 그러한 일반화를 제한하는 두 가지 주요 문제를 식별합니다. 첫째, 이 작업에 사용된 표준 손실은 의도치 않게 장면 그래프 밀도의 함수라는 것을 보여줍니다. 이로 인해 일반화에 중요한 다양한 소수의 예제가 포함되어 있음에도 불구하고 훈련 중에 큰 희소 그래프의 개별 가장자리가 무시됩니다. 둘째, 관계의 빈도는 이 작업에서 강한 편향을 생성할 수 있으므로 가장 빈번한 관계를 예측하는 블라인드 모델이 좋은 성능을 달성합니다. 결과적으로 일부 최첨단 모델은 이러한 편향을 활용하여 결과를 개선합니다. 우리는 이러한 모델이 Visual Genome 데이터 세트와 최신 개선 버전인 GQA에 대한 두 가지 다른 모델을 평가하여 희귀한 구성으로 일반화하는 능력에서 가장 큰 어려움을 겪을 수 있음을 보여줍니다. 이러한 문제를 해결하기 위해 특정 일반화 측정 항목에서 2배 이상의 개선을 제공하는 밀도 정규화 에지 손실을 도입했습니다. 이 방향의 다른 작업과 비교할 때 우리의 개선 사항에는 몇 줄의 코드만 필요하고 추가 계산 비용도 필요하지 않습니다. 또한 기존 측정항목, 특히 0개/몇 개의 샷에서 모델을 정확하게 평가하는 것이 어렵다는 점을 강조하고 새로운 가중치 측정항목을 도입합니다."
105,http://arxiv.org/abs/2005.08045 ,Visual Relationship Detection using Scene Graphs: A Survey,"Aniket Agarwal, Ayush Mangal,  Vipul","이미지에 묘사된 시각적 관계를 디코딩하여 장면을 이해하는 것은 오랫동안 연구된 문제였습니다. 최근 딥 러닝의 발전과 심층 신경망의 사용으로 인해 많은 작업에서 인간 수준에 가까운 정확도가 달성되었지만, 다양한 시각적 관계 감지 작업에 있어서는 인간 수준과 기계 수준 성능 사이에 여전히 큰 격차가 존재합니다. 비교적 대략적인 이미지 이해에 초점을 맞춘 객체 인식, 분할 및 캡션과 같은 이전 작업을 개발하면서 최근에는 더 미세한 수준의 이미지 이해를 처리하기 위한 새로운 작업이 도입되었습니다. 장면 그래프는 장면과 그 안에 존재하는 다양한 관계를 더 잘 표현하는 기술 중 하나입니다. 시각적 질문 응답, 의미론적 이미지 검색, 이미지 생성 등과 같은 다양한 작업에 대한 광범위한 응용 프로그램을 통해 더 깊고 더 나은 시각적 관계 이해를 위한 유용한 도구임이 입증되었습니다. 본 논문에서는 장면 그래프 생성을 위한 다양한 기술, 시각적 관계를 표현하는 효율성, 다양한 다운스트림 작업을 해결하는 데 어떻게 사용되었는지에 대한 자세한 조사를 제시합니다. 또한 이 분야가 앞으로 나아갈 수 있는 다양한 미래 방향에 대한 분석을 시도합니다. 이 주제에 대한 자세한 조사를 제공하는 최초의 논문 중 하나로서 우리는 또한 장면 그래프에 대한 간결한 소개를 제공하고 실무자들이 응용 프로그램에 대한 접근 방식을 개발하는 데 도움이 되기를 바랍니다."
104,http://arxiv.org/abs/2005.06653 ,Structured Query-Based Image Retrieval Using Scene Graphs,"Brigit Schroeder, Subarna Tripathi","구조화된 쿼리는 단일 개체(예: '여성' 또는 '오토바이')와 달리 개체 상호 작용(예: '여성이 오토바이를 탄다')의 복잡성을 포착할 수 있습니다. 따라서 구조화된 쿼리를 사용한 검색은 단일 개체 검색보다 훨씬 유용하지만 훨씬 더 어려운 문제입니다. 본 논문에서는 이미지 검색 접근 방식의 기초로 장면 그래프 임베딩을 사용하는 방법을 제시합니다. 장면 그래프에서 파생된 시각적 관계가 구조화된 쿼리로 어떻게 사용될 수 있는지 살펴봅니다. 시각적 관계는 술어 관계로 연결된 노드로서 주제와 객체를 갖는 장면 그래프의 방향성 하위 그래프입니다. 특히, 우리는 긴 꼬리의 COCO-Stuff 데이터 세트에서 발견된 낮거나 중간 빈도의 객체에 대해서도 높은 재현율을 달성할 수 있었고, 시각적 관계에서 영감을 받은 손실을 추가하면 최상의 경우 재현율이 10% 향상된다는 것을 발견했습니다."
103,http://arxiv.org/abs/2004.08814 ,Graph-Structured Referring Expression Reasoning in The Wild,"Sibei Yang, Guanbin Li, Yizhou Yu","참조 표현을 접지하는 것은 자연어 표현이 참조하는 객체를 이미지에서 찾는 것을 목표로 합니다. 지시 표현의 언어적 구조는 시각적 내용에 대한 추론의 레이아웃을 제공하며, 이미지와 지시 표현을 정렬하고 공동으로 이해하는 것이 종종 중요합니다. 본 논문에서는 표현의 언어적 구조에 따라 신경 모듈을 이용한 의미 그래프와 장면 그래프에 대한 추론을 수행하는 장면 그래프 유도 모듈러 네트워크(SGMN)를 제안한다. 특히 이미지를 구조화된 의미 그래프로 모델링하고 표현을 언어 장면 그래프로 구문 분석합니다. 언어 장면 그래프는 표현의 언어적 구조를 해독할 뿐만 아니라, 이미지 의미 그래프와 일관성 있게 표현됩니다. 참조 표현을 기반으로 하는 구조화된 솔루션을 탐색하는 것 외에도 구조적 참조 표현 추론을 위한 대규모 실제 데이터 세트인 Ref-Reasoning도 제안합니다. 다양한 표현 템플릿과 기능적 프로그램을 사용하여 이미지의 장면 그래프에 대한 참조 표현을 자동으로 생성합니다. 이 데이터세트에는 실제 시각적 콘텐츠와 다양한 추론 레이아웃을 갖춘 의미적으로 풍부한 표현이 포함되어 있습니다. 실험 결과에 따르면 SGMN은 새로운 Ref-Reasoning 데이터 세트에서 기존의 최첨단 알고리즘보다 훨씬 뛰어난 성능을 발휘할 뿐만 아니라 일반적으로 사용되는 벤치마크 데이터 세트에서 최첨단 구조적 방법도 능가합니다. 또한 추론에 대한 해석 가능한 시각적 증거를 제공할 수도 있습니다. 데이터와 코드는 https://github.com/sibeiyang/sgmn에서 확인할 수 있습니다."
102,http://arxiv.org/abs/2004.06193 ,Relation Transformer Network,"Rajat Koner, Suprosanna Shit, Volker Tresp","객체를 노드로, 상호 관계를 에지로 하는 장면 그래프의 추출은 이미지 내용에 대한 깊은 이해의 기초가 됩니다. 메시지 전달 및 공동 분류와 같은 최근의 발전에도 불구하고 시각적 개체 간의 상호 상호 작용에 대한 차선책 탐색으로 인해 시각적 관계 검색은 여전히 ​​어려운 작업으로 남아 있습니다. 본 연구에서는 장면 그래프 생성 및 관계 예측을 위한 새로운 변환기 공식을 제안합니다. 우리는 노드와 에지의 풍부한 기능 임베딩을 위해 변환기의 인코더-디코더 아키텍처를 활용합니다. 구체적으로 우리는 변환기 인코더의 self-attention을 사용한 노드 간 상호 작용과 변환기 디코더의 교차 주의를 사용한 에지-노드 상호 작용을 모델링합니다. 또한 디코더의 에지를 처리하는 데 적합한 새로운 위치 임베딩을 소개합니다. 마지막으로, 우리의 관계 예측 모듈은 학습된 노드와 에지 임베딩으로부터 방향성 관계를 분류합니다. 우리는 이 아키텍처를 RTN(Relation Transformer Network)이라고 부릅니다. Visual Genome 및 GQA 데이터 세트에서 최신 방법에 비해 전체 평균 4.85% 및 3.1% 포인트 개선을 달성했습니다. 우리의 실험에서는 Relation Transformer가 소규모, 중간 규모, 대규모 관계 분류를 통해 다양한 데이터 세트에서 컨텍스트를 효율적으로 모델링할 수 있음을 보여줍니다."
101,http://arxiv.org/abs/2004.03967 ,Learning 3D Semantic Scene Graphs from 3D Indoor Reconstructions,"Johanna Wald, Helisa Dhamo, Nassir Navab, Federico Tombari","장면 이해는 컴퓨터 비전에서 높은 관심을 받아왔습니다. 이는 장면에서 객체를 식별하는 것뿐만 아니라 주어진 컨텍스트 내에서 객체의 관계도 포함합니다. 이 목표를 위해 최근 작업 라인에서는 3D 의미론적 분할 및 장면 레이아웃 예측을 다루고 있습니다. 우리 작업에서는 그래프에서 장면의 엔터티를 구성하는 데이터 구조인 장면 그래프에 중점을 둡니다. 여기서 개체는 노드이고 해당 관계는 가장자리로 모델링됩니다. 우리는 3D 장면 이해, 개체 및 개체 관계 매핑을 수행하는 방법으로 장면 그래프에 대한 추론을 활용합니다. 특히, 장면의 포인트 클라우드로부터 장면 그래프를 회귀하는 학습된 방법을 제안합니다. 우리의 새로운 아키텍처는 PointNet 및 GCN(Graph Convolutional Networks)을 기반으로 합니다. 또한 3D 장면의 의미론적으로 풍부한 장면 그래프가 포함된 반자동 생성 데이터 세트인 3DSSG를 소개합니다. 우리는 그래프가 3D-3D 및 2D-3D 매칭을 위한 중간 표현 역할을 하는 도메인 독립적 검색 작업에서 우리 방법의 적용을 보여줍니다."
100,http://arxiv.org/abs/2004.03708 ,Context-Aware Group Captioning via Self-Attention and Contrastive Features,"Zhuowan Li, Quan Tran, Long Mai, Zhe Lin, Alan Yuille",이미지 캡션이 빠르게 발전하는 동안 기존 작업은 주로 단일 이미지를 설명하는 데 중점을 둡니다. 본 논문에서는 관련 참조 이미지의 다른 그룹의 맥락에서 대상 이미지 그룹을 설명하는 것을 목표로 하는 새로운 작업인 상황 인식 그룹 캡션을 소개합니다. 상황 인식 그룹 캡션에는 대상 이미지 그룹과 참조 이미지 그룹의 정보를 요약하는 것뿐만 아니라 둘 사이의 대조도 필요합니다. 이 문제를 해결하기 위해 우리는 각 이미지 그룹의 공통 정보를 효과적으로 요약하는 동시에 이미지 그룹 간의 차별적인 정보를 캡처하기 위해 self-attention 메커니즘과 대조 특징 구성을 결합한 프레임워크를 제안합니다. 이 작업을 위한 데이터세트를 구축하기 위해 우리는 이미지를 그룹화하고 장면 그래프 매칭을 사용하여 단일 이미지 캡션을 기반으로 그룹 캡션을 생성하는 것을 제안합니다. 우리의 데이터 세트는 공개 개념 캡션 데이터 세트와 새로운 스톡 캡션 데이터 세트를 기반으로 구성되었습니다. 두 데이터 세트에 대한 실험은 이 새로운 작업에 대한 우리 방법의 효율성을 보여줍니다. 관련 데이터 세트 및 코드는 https://lizw14.github.io/project/groupcap에서 공개됩니다.
99,http://arxiv.org/abs/2004.03677 ,Semantic Image Manipulation Using Scene Graphs,"Helisa Dhamo, Azade Farshad, Iro Laina, Nassir Navab, Gregory D. Hager, Federico Tombari, Christian Rupprecht",이미지 조작은 생성할 이미지가 기존 이미지를 수정하는 이미지 생성의 특별한 경우로 간주될 수 있습니다. 이미지 생성 및 조작은 대부분 원시 픽셀에서 작동하는 작업이었습니다. 그러나 풍부한 이미지와 객체 표현을 학습하는 데 있어 놀라운 발전으로 주로 의미론에 의해 주도되는 텍스트-이미지 또는 레이아웃-이미지 생성과 같은 작업의 길이 열렸습니다. 우리 작업에서는 사용자가 이미지에서 생성된 의미 그래프의 노드나 가장자리에 변경 사항을 적용하기만 하면 이미지를 편집할 수 있는 장면 그래프의 이미지 조작이라는 새로운 문제를 해결합니다. 우리의 목표는 주어진 집합의 이미지 정보를 인코딩하고 거기에서 원본 이미지의 의미와 스타일을 존중하면서 개체를 교체하거나 개체 간의 관계를 변경하는 등 새로운 집합을 생성하는 것입니다. 성상도 변경이나 이미지 편집에 대한 직접적인 감독이 필요하지 않은 공간 의미론적 장면 그래프 네트워크를 소개합니다. 이를 통해 추가 주석 작업 없이 기존 실제 데이터 세트에서 시스템을 교육할 수 있습니다.
98,http://arxiv.org/abs/2003.12962 ,GPS-Net: Graph Property Sensing Network for Scene Graph Generation,"Xin Lin, Changxing Ding, Jinquan Zeng, Dacheng Tao","SGG(장면 그래프 생성)는 쌍별 관계와 함께 이미지의 객체를 감지하는 것을 목표로 합니다. 최근 연구에서 충분히 탐구되지 않은 장면 그래프의 세 가지 주요 속성은 가장자리 방향 정보, 노드 간 우선순위 차이, 관계의 긴 꼬리 분포입니다. 따라서 본 논문에서는 SGG의 세 가지 속성을 완벽하게 탐색하는 그래프 속성 감지 네트워크(GPS-Net)를 제안합니다. 먼저, 우리는 노드별 상황 정보로 노드 특징을 강화하고 삼선형 모델을 통해 에지 방향 정보를 인코딩하는 새로운 메시지 전달 모듈을 제안합니다. 둘째, 훈련 중 노드 간 우선순위 차이를 반영하기 위해 노드 우선순위 민감 손실을 도입합니다. 이는 초점 손실의 초점 매개변수를 조정하는 매핑 기능을 설계함으로써 달성됩니다. 셋째, 관계의 빈도는 긴 꼬리 분포 문제의 영향을 받기 때문에 먼저 분포를 부드럽게 한 다음 시각적 모양에 따라 각 주체-객체 쌍에 대해 조정할 수 있도록 하여 이 문제를 완화합니다. 체계적인 실험은 제안된 기술의 효율성을 보여줍니다. 또한, GPS-Net은 다양한 설정 및 측정 기준에서 상당한 이득을 얻어 VG, OI 및 VRD의 세 가지 인기 데이터베이스에서 최첨단 성능을 달성합니다. 코드와 모델은 \url{https://github.com/taksau/GPS-Net}에서 확인할 수 있습니다."
97,http://arxiv.org/abs/2003.07449 ,Object-Centric Image Generation from Layouts,"Tristan Sylvain, Pengchuan Zhang, Yoshua Bengio, R Devon Hjelm, Shikhar Sharma","단일 개체 및 단일 도메인 이미지 생성에 대한 최근의 인상적인 결과에도 불구하고 여러 개체가 포함된 복잡한 장면을 생성하는 것은 여전히 ​​어려운 일입니다. 본 논문에서는 복잡한 장면을 잘 생성하기 위해서는 모델이 개별 객체와 객체 간의 관계를 이해할 수 있어야 한다는 아이디어에서 시작합니다. 객체 중심 생성 적대 네트워크(또는 OC-GAN)라고 부르는 레이아웃-이미지 생성 방법은 새로운 장면 그래프 유사성 모듈(SGSM)을 사용합니다. SGSM은 장면에 있는 객체 간의 공간 관계 표현을 학습하여 모델의 레이아웃 충실도를 향상시킵니다. 우리는 또한 객체 인스턴스 인식을 향상시키는 생성기의 컨디셔닝 메커니즘에 대한 변경을 제안합니다. 이미지 품질 개선 외에도 우리의 기여는 이전 접근 방식의 두 가지 실패 모드를 완화합니다. (1) 레이아웃에서 해당 경계 상자 없이 생성되는 가짜 객체, (2) 레이아웃에서 경계 상자가 겹쳐서 이미지의 객체가 병합됩니다. 광범위한 정량적 평가 및 절제 연구는 COCO-Stuff 및 Visual Genome 데이터 세트 모두에 대한 이전의 최첨단 접근 방식을 능가하는 모델을 통해 우리의 기여의 영향을 보여줍니다. 마지막으로, 다중 객체 이미지에 더 적합한 널리 사용되는 Fr{é}chet Inception Distance 측정항목을 객체 중심으로 적용한 SceneFID를 도입하여 이전 작업에서 사용된 평가 측정항목의 중요한 제한 사항을 해결합니다."
96,http://arxiv.org/abs/2003.00387 ,Say As You Wish: Fine-grained Control of Image Caption Generation with Abstract Scene Graphs,"Shizhe Chen, Qin Jin, Peng Wang, Qi Wu","인간은 원하는 대로 이미지 내용을 거친 것부터 미세한 것까지 묘사할 수 있습니다. 그러나 대부분의 이미지 캡션 모델은 의도에 구애받지 않으므로 다양한 사용자 의도에 따라 다양한 설명을 먼저 생성할 수 없습니다. 본 연구에서는 사용자 의도를 세분화된 수준으로 표현하고 생성된 설명의 내용과 세부 사항을 제어하기 위한 추상 장면 그래프(ASG) 구조를 제안합니다. ASG는 구체적인 의미 라벨 없이 이미지에 기반을 둔 세 가지 유형의 \textbf{추상 노드}(객체, 속성, 관계)로 구성된 방향성 그래프입니다. 따라서 수동으로든 자동으로든 쉽게 얻을 수 있습니다. ASG에서 우리는 그래프에서 사용자 의도와 의미를 인식하여 그래프 구조에 따라 원하는 캡션을 생성할 수 있는 새로운 ASG2Caption 모델을 제안합니다. 우리 모델은 VisualGenome 및 MSCOCO 데이터 세트 모두에서 신중하게 설계된 기준선보다 ASG에서 더 나은 제어 가능성 조건을 달성합니다. 또한 다양한 ASG를 제어 신호로 자동 샘플링하여 캡션 다양성을 크게 향상시킵니다."
95,http://arxiv.org/abs/2002.11949 ,Unbiased Scene Graph Generation from Biased Training,"Kaihua Tang, Yulei Niu, Jianqiang Huang, Jiaxin Shi, Hanwang Zhang","오늘날의 장면 그래프 생성(SGG) 작업은 주로 심각한 훈련 편견(예: 다양한 ""해변 위의 인간 걷기/앉기/눕기""를 ""해변 위의 인간""으로 축소)으로 인해 여전히 실용적이지 않습니다. 이러한 SGG가 주어지면 VQA와 같은 다운스트림 작업은 단순한 객체 가방보다 더 나은 장면 구조를 추론할 수 없습니다. 그러나 전통적인 편향성 제거 방법은 좋은 편향과 나쁜 편향을 구별할 수 없기 때문에 SGG에서의 편향 제거는 사소한 것이 아닙니다. 예를 들어 좋은 맥락 이전(예: ""먹기""보다 ""사람이 책을 읽는다"")과 나쁜 긴 꼬리 편향(예: ""뒤/앞""을 지배하는 ""가까운"" 편향)입니다. 본 논문에서는 인과 추론을 기반으로 하지만 기존 우도가 아닌 새로운 SGG 프레임워크를 제시합니다. 먼저 SGG에 대한 인과 그래프를 구축하고 그래프를 사용하여 기존의 편향된 학습을 수행합니다. 그런 다음 훈련된 그래프에서 반사실적 인과성을 도출하여 제거해야 하는 나쁜 편향의 효과를 추론할 것을 제안합니다. 특히 우리는 편향되지 않은 SGG에 대해 제안된 최종 예측 점수로 TDE(Total Direct Effect)를 사용합니다. 우리의 프레임워크는 모든 SGG 모델에 불가지론적이므로 편견 없는 예측을 추구하는 커뮤니티에 널리 적용될 수 있습니다. SGG 벤치마크 Visual Genome과 여러 널리 사용되는 모델에서 제안된 장면 그래프 진단 툴킷을 사용하여 이전 최첨단 방법에 비해 상당한 개선이 관찰되었습니다."
94,http://arxiv.org/abs/2002.08945 ,Spatiotemporal Relationship Reasoning for Pedestrian Intent Prediction,"Bingbin Liu, Ehsan Adeli, Zhangjie Cao, Kuan-Hui Lee, Abhijeet Shenoi, Adrien Gaidon, Juan Carlos Niebles","시각적 데이터에 대한 추론은 로봇 공학 및 비전 기반 애플리케이션에 바람직한 기능입니다. 이러한 추론을 통해 동영상의 다음 이벤트나 동작을 예측할 수 있습니다. 최근에는 예측이나 예측을 위해 컨볼루션 연산을 기반으로 다양한 모델이 개발되었지만 시공간 데이터를 추론하고 장면 내 다양한 ​​객체의 관계를 추론하는 능력이 부족합니다. 본 논문에서는 보행자 의도를 추론하기 위해 장면의 시공간 관계를 밝히기 위해 그래프 컨볼루션을 기반으로 하는 프레임워크를 제시합니다. 장면 그래프는 비디오 프레임 내에서 그리고 비디오 프레임 전체에 걸쳐 분할된 개체 인스턴스 위에 구축됩니다. 길을 건너거나 건너지 않는 미래의 행동으로 정의되는 보행자 의도는 자율주행차가 안전하고 원활하게 주행하는 데 매우 중요한 정보입니다. 우리는 두 가지 다른 관점에서 의도 예측 문제에 접근하고 보행자 중심 및 위치 중심 시나리오 모두에서 교차 의도를 예측합니다. 또한 보행자 인구가 밀집된 지역의 자율 주행 시나리오를 위해 특별히 설계된 새로운 데이터 세트인 STIP(Stanford-TRI Intent Prediction) 데이터 세트를 소개합니다. STIP 및 다른 벤치마크 데이터 세트에 대한 실험에서는 그래프 모델링 프레임워크가 보행자의 횡단 의도를 STIP에서 79.10%, \rev{Joint Attention for Autonomous Driving(JAAD) 데이터 세트에서 79.28%의 정확도로 실제 횡단이 발생하는 시점보다 최대 1초 빠르게 예측할 수 있음을 보여줍니다. 이러한 결과는 기준선과 이전 작업을 능가합니다. 데이터세트와 코드는 http://stip.stanford.edu/를 참조하세요."
93,http://arxiv.org/abs/2002.08417 ,Table-Top Scene Analysis Using Knowledge-Supervised MCMC,"Ziyuan Liu, Dong Chen, Kai M. Wurm, Georg von Wichert",본 논문에서는 6차원 객체 포즈 추정으로부터 테이블탑 장면에 대한 추상 장면 그래프를 생성하는 확률론적 방법을 제안합니다. 우리는 Markov 논리 네트워크에서 이 지식을 설명 규칙으로 인코딩하여 작업별 컨텍스트 지식을 명시적으로 활용합니다. 장면 그래프를 생성하는 우리의 접근 방식은 확률적입니다. 물체 포즈의 불확실성은 데이터 기반 MCMC 프로세스에 내장된 확률적 센서 모델을 통해 해결됩니다. 우리는 숨겨진 객체에 대한 추론과 객체 포즈의 잘못된 추정을 탐지하기 위해 Markov 논리 추론을 적용합니다. 우리 접근 방식의 효율성은 실제 실험에서 입증되고 평가됩니다.
92,http://arxiv.org/abs/2002.06289 ,"3D Dynamic Scene Graphs: Actionable Spatial Perception with Places, Objects, and Humans","Antoni Rosinol, Arjun Gupta, Marcus Abate, Jingnan Shi, Luca Carlone","우리는 실행 가능한 공간 인식을 위한 통합 표현인 3D 동적 장면 그래프를 제시합니다. 장면 그래프는 노드가 장면의 개체(예: 개체, 벽, 방)를 나타내고 가장자리가 노드 간의 관계(예: 포함, 인접)를 나타내는 방향 그래프입니다. 동적 장면 그래프(DSG)는 이 개념을 확장하여 움직이는 에이전트(예: 인간, 로봇)가 있는 동적 장면을 표현하고 계획 및 의사 결정을 지원하는 실행 가능한 정보(예: 시공간 관계, 다양한 추상화 수준의 토폴로지)를 포함합니다. 우리의 두 번째 기여는 시각 관성 데이터로부터 DSG를 구축하기 위한 최초의 완전 자동 공간 인식 엔진(SPIN)을 제공하는 것입니다. 물체와 인간의 감지 및 자세 추정을 위한 최첨단 기술을 통합하고 혼잡한 장면에서 물체, 로봇 및 인간 노드를 강력하게 추론하는 방법을 설명합니다. 우리가 아는 한, 이 논문은 시각적 관성 SLAM과 조밀한 휴먼 메시 추적을 조화시킨 최초의 논문입니다. 또한 실내 환경(예: 장소, 구조물, 방)과 그 관계에 대한 계층적 표현을 얻기 위한 알고리즘을 제공합니다. 우리의 세 번째 기여는 제안된 공간 인식 엔진을 사실적인 Unity 기반 시뮬레이터에서 시연하여 견고성과 표현력을 평가하는 것입니다. 마지막으로, 우리는 현대 로봇공학 응용에 대한 우리 제안의 의미를 논의합니다. 3D 동적 장면 그래프는 계획 및 의사 결정, 인간-로봇 상호 작용, 장기적인 자율성 및 장면 예측에 큰 영향을 미칠 수 있습니다. 동영상 요약은 https://youtu.be/SWbofjhyPzI에서 볼 수 있습니다."
91,http://arxiv.org/abs/2002.00176 ,Unbiased Scene Graph Generation via Rich and Fair Semantic Extraction,"Bin Wen, Jie Luo, Xianglong Liu, Lei Huang","이미지에서 시각적 장면의 그래프 표현을 추출하는 것은 컴퓨터 비전에서 어려운 작업입니다. 지난 10년 동안 장면 그래프 생성이 고무적으로 진행되었지만 놀랍게도 기존 접근 방식의 성능은 주로 (1) 대칭과 같은 특정 의미론적 속성과의 관계를 무의식적으로 가정하고 (2) 서로 다른 관계에 대한 불균형 주석에서 비롯되는 강한 편향에 의해 크게 제한된다는 사실을 발견했습니다. 이러한 편향의 부정적인 영향을 완화하기 위해 우리는 관계의 풍부한 의미론적 속성을 캡처할 뿐만 아니라 다양한 규모의 주석을 사용하여 관계를 공정하게 예측하기 위해 Rich and Fair 의미론적 추출 네트워크(줄여서 RiFa)라는 새롭고 간단한 아키텍처를 제안했습니다. 의사 샴 네트워크를 사용하여 RiFa는 주체와 객체를 각각 내장하여 의미적 차이를 구별하는 동시에 기본 의미적 속성을 보존합니다. 그런 다음 특정 맥락 영역에서 개체의 시각적 및 의미적 특징을 기반으로 주체-객체 관계를 추가로 예측하고 주석이 적은 개체에 대한 관계 예측의 순위를 공정하게 지정합니다. 인기 있는 Visual Genome 데이터 세트에 대한 실험은 RiFa가 장면 그래프 작업의 여러 가지 까다로운 설정에서 최첨단 성능을 달성한다는 것을 보여줍니다. 특히, 관계의 다양한 의미론적 속성을 포착하는 데 훨씬 더 나은 성능을 발휘하고, 관계당 전체적으로 최고의 성능을 얻습니다."
90,http://arxiv.org/abs/2001.04735 ,NODIS: Neural Ordinary Differential Scene Understanding,"Cong Yuren, Hanno Ackermann, Wentong Liao, Michael Ying Yang, Bodo Rosenhahn","의미론적 이미지 이해는 컴퓨터 비전에서 어려운 주제입니다. 이미지 속의 모든 객체를 감지해야 할 뿐만 아니라 객체 간의 모든 관계를 식별해야 합니다. 감지된 객체, 해당 라벨 및 발견된 관계를 사용하여 이미지의 추상적 의미 해석을 제공하는 장면 그래프를 구성할 수 있습니다. 이전 연구에서는 혼합 정수 선형 프로그램(Mixed-Integer Linear Program)으로 공식화된 할당 문제를 해결하여 관계를 식별했습니다. 이 연구에서 우리는 해당 공식을 상미분 방정식(ODE)으로 해석합니다. 제안된 아키텍처는 end-to-end 학습을 통해 ODE의 신경 변형을 해결하여 장면 그래프 추론을 수행합니다. Visual Genome 벤치마크에서 장면 그래프 생성(SGGen), 분류(SGCls) 및 시각적 관계 감지(PredCls)의 세 가지 벤치마크 작업 모두에서 최첨단 결과를 달성합니다."
89,http://arxiv.org/abs/2001.02359 ,Weakly Supervised Visual Semantic Parsing,"Alireza Zareian, Svebor Karaman, Shih-Fu Chang","SGG(장면 그래프 생성)는 이미지에서 엔터티, 조건자 및 의미 구조를 추출하여 시각적 추론 및 이미지 검색과 같은 다양한 응용 프로그램을 통해 시각적 콘텐츠에 대한 깊은 이해를 가능하게 하는 것을 목표로 합니다. 그럼에도 불구하고 기존 SGG 방법은 훈련을 위해 수동으로 주석이 달린 수백만 개의 경계 상자가 필요하며 조건자를 탐지하기 위해 모든 객체 제안 쌍을 철저하게 처리하므로 계산적으로 비효율적입니다. 본 논문에서는 엔터티와 술어 인식을 분리하고 2차 성능을 가능하게 하는 SGG의 일반화된 공식, 즉 Visual Semantic Parsing을 먼저 제안하여 이러한 두 가지 제한 사항을 해결합니다. 그런 다음 반복 프로세스를 통해 그래프 노드와 에지를 공동으로 추론하는 동적, 주의 기반, 이분 메시지 전달 프레임워크를 기반으로 하는 시각적 의미 구문 분석 네트워크(VSPNet)를 제안합니다. 또한 경계 상자 주석 없이 훈련을 가능하게 하는 새로운 그래프 정렬 알고리즘을 기반으로 하는 최초의 그래프 기반 약한 지도 학습 프레임워크를 제안합니다. 광범위한 실험을 통해 우리는 VSPNet이 약하게 감독된 기준선보다 훨씬 뛰어난 성능을 발휘하고 완전 감독된 성능에 접근하면서도 몇 배 더 빠르다는 것을 보여줍니다. 우리는 우리 방법의 소스 코드를 공개적으로 공개합니다."
88,http://arxiv.org/abs/2001.02314 ,Bridging Knowledge Graphs to Generate Scene Graphs,"Alireza Zareian, Svebor Karaman, Shih-Fu Chang","장면 그래프는 이미지를 추상적 의미 요소, 즉 객체와 객체의 상호 작용으로 구문 분석하여 시각적 이해와 설명 가능한 추론을 용이하게 하는 강력한 표현입니다. 반면, 상식 지식 그래프는 세상이 어떻게 구성되어 있는지, 그리고 일반적인 개념이 어떻게 상호 작용하는지를 인코딩하는 풍부한 저장소입니다. 본 논문에서는 장면 그래프가 상식 지식 그래프의 이미지 조건 인스턴스화로 표시되는 이 두 구성의 통합된 공식을 제시합니다. 이 새로운 관점을 바탕으로 우리는 장면 그래프 생성을 장면 그래프와 상식 그래프 사이의 연결 추론으로 재구성합니다. 여기서 장면 그래프의 각 엔터티 또는 조건자 인스턴스는 상식 그래프의 해당 엔터티 또는 조건자 클래스에 연결되어야 합니다. 이를 위해 우리는 두 그래프 사이와 각 그래프 내에서 정보를 반복적으로 전파하는 동시에 각 반복에서 브리지를 점차적으로 개선하는 새로운 그래프 기반 신경망을 제안합니다. 우리의 그래프 브리징 네트워크인 GB-Net은 에지와 노드를 연속적으로 추론하여 상호 연결된 장면과 상식적인 그래프의 풍부하고 이질적인 구조를 동시에 활용하고 개선할 수 있습니다. 광범위한 실험을 통해 최신 방법에 비해 GB-Net의 뛰어난 정확도를 선보이며 새로운 최첨단 기술을 탄생시켰습니다. 우리는 우리 방법의 소스 코드를 공개적으로 공개합니다."
87,http://arxiv.org/abs/1912.07414 ,Learning Canonical Representations for Scene Graph to Image Generation,"Roei Herzig, Amir Bar, Huijuan Xu, Gal Chechik, Trevor Darrell, Amir Globerson","생성된 이미지의 구조를 제어하려는 경우 복잡한 시각적 장면의 사실적인 이미지를 생성하는 것이 어려워집니다. 이전 접근 방식에서는 장면 그래프를 사용하여 개체 수가 적은 장면을 제어할 수 있음을 보여 주었지만 이 접근 방식은 그래프의 복잡성(객체 및 가장자리 수)이 증가함에 따라 어려움을 겪습니다. 이 연구에서 우리는 현재 방법의 한 가지 한계가 그래프에서 의미론적 동등성을 포착할 수 없다는 점을 보여줍니다. 우리는 데이터에서 표준 그래프 표현을 학습하여 이러한 문제를 해결하는 새로운 모델을 제시하고 결과적으로 복잡한 시각적 장면에 대한 이미지 생성이 향상됩니다. 우리 모델은 대규모 장면 그래프의 향상된 경험적 성능, 입력 장면 그래프의 노이즈에 대한 견고성 및 의미상 동등한 그래프의 일반화를 보여줍니다. 마지막으로 Visual Genome, COCO 및 CLEVR의 세 가지 벤치마크에서 모델의 향상된 성능을 보여줍니다."
86,http://arxiv.org/abs/1912.06992 ,Action Genome: Actions as Composition of Spatio-temporal Scene Graphs,"Jingwei Ji, Ranjay Krishna, Li Fei-Fei, Juan Carlos Niebles","동작 인식은 일반적으로 동작과 활동을 비디오에서 발생하는 모놀리식 이벤트로 처리했습니다. 그러나 인지과학과 신경과학에서는 사람들이 활동을 일관된 계층적 부분 구조로 적극적으로 인코딩한다는 증거가 있습니다. 그러나 Computer Vision에서는 이벤트 부분법을 인코딩하는 표현에 대한 탐색이 거의 이루어지지 않았습니다. 이벤트의 원형 단위가 동작-객체 상호작용이라는 증거에서 영감을 받아 동작을 시공간 장면 그래프로 분해하는 표현인 Action Genome을 소개합니다. Action Genome은 동작이 발생하는 동안 개체와 쌍 관계 사이의 변화를 포착합니다. 여기에는 0.4M개의 개체와 170만개의 시각적 관계에 주석이 달린 10K 비디오가 포함되어 있습니다. Action Genome을 사용하면 장면 그래프를 시공간 특징 뱅크로 통합하여 Charades 데이터 세트에서 더 나은 성능을 달성함으로써 기존 동작 인식 모델을 확장합니다. 다음으로, 행동을 초래하는 시각적 관계의 시간적 변화를 분해하고 학습함으로써 몇 번의 행동 인식을 가능하게 하여 계층적 이벤트 분해의 유용성을 입증하고, 단 10개의 예를 사용하여 42.7% mAP를 달성했습니다. 마지막으로, 시공간 장면 그래프 예측이라는 새로운 작업에 대해 기존 장면 그래프 모델을 벤치마킹합니다."
85,http://arxiv.org/abs/1912.00501 ,Interpreting Context of Images using Scene Graphs,"Himangi Mittal, Ajith Abraham, Anuja Arora","시각적 장면을 이해하는 것은 사물, 관계, 맥락을 통합합니다. 이미지 작업을 수행하는 기존 방법은 대부분 객체 감지에 중점을 두고 객체 간의 관계를 캡처하는 데 실패합니다. 관계는 장면의 개체에 대한 풍부한 의미 정보를 제공할 수 있습니다. 맥락은 대상 간의 관계를 인식하는 데 도움이 되고 이미지에 대한 더 깊은 통찰력을 제공하므로 이미지를 이해하는 데 도움이 될 수 있습니다. 이 아이디어를 통해 우리 프로젝트는 이미지를 그래프로 표현하여 이미지에 존재하는 맥락을 찾는 데 초점을 맞춘 모델을 제공합니다. 여기서 노드는 개체가 되고 가장자리는 이들 간의 관계가 됩니다. 컨텍스트는 두 개체 간의 관계를 감지하기 위해 추가로 연결되고 SVM(Support Vector Machine)에 제공되는 시각적 및 의미적 단서를 사용하여 발견됩니다. 이는 유사한 이미지 검색, 이미지 캡션 또는 스토리 생성과 같은 애플리케이션에서 추가로 사용될 수 있는 이미지의 맥락을 제공합니다."
84,http://arxiv.org/abs/1911.10115 ,TPsgtR: Neural-Symbolic Tensor Product Scene-Graph-Triplet Representation for Image Captioning,Chiranjib Sur,"그래픽 표현의 구조를 개념적 위치 바인딩으로 공식화할 수 있으면 이미지 캡션이 향상될 수 있습니다. 이 작업에서는 이미지의 지역적 시각적 정보에서 파생된 장면 그래프의 신경 기호 인코딩을 사용하여 캡션 생성을 위한 새로운 기술을 도입했으며 이를 Tensor Product Scene-Graph-Triplet Representation(TP$_{sgt}$R)이라고 합니다. 이전 연구의 대부분은 이미지의 객체 특징 식별에 집중했지만, 우리는 임의/모든 조합을 구성하기 위해 모델에 의존하는 대신 이미지의 여러 영역 간의 식별된 관계를 구체적인 형태로 임베드할 수 있는 신경 기호 임베딩을 소개합니다. 이러한 신경 기호 표현은 신경 기호 주의를 위한 신경 기호 공간을 더 잘 정의하는 데 도움이 되며 더 나은 캡션으로 변환될 수 있습니다. 이 접근 방식을 통해 우리는 비교를 위해 두 가지 새로운 아키텍처(TP$_{sgt}$R-TDBU 및 TP$_{sgt}$R-sTDBU)를 도입했으며 실험 결과는 우리 접근 방식이 다른 모델보다 성능이 뛰어나고 생성된 캡션이 더 포괄적이고 자연스럽다는 것을 보여줍니다."
83,http://arxiv.org/abs/1911.07527 ,SOGNet: Scene Overlap Graph Network for Panoptic Segmentation,"Yibo Yang, Hongyang Li, Xia Li, Qijie Zhao, Jianlong Wu, Zhouchen Lin","Panoptic 분할 작업에는 중복이 포함될 수 있는 의미론적 및 인스턴스 분할 출력의 통합 결과가 필요합니다. 그러나 현재 연구에서는 모델링 중복을 광범위하게 무시합니다. 본 연구에서는 인스턴스 간의 중첩 관계를 모델링하고 Panoptic 분할을 위해 이를 해결하는 것을 목표로 합니다. 장면 그래프 표현에서 영감을 받아 중첩 문제를 장면 중첩 그래프라는 단순화된 사례로 공식화합니다. 각 객체의 카테고리, 기하학, 모양 특징을 활용하여 관계형 임베딩을 수행하고 중첩 관계를 인코딩하는 관계 매트릭스를 출력합니다. 감독 부족을 극복하기 위해 인스턴스 쌍 간의 중복을 해결하는 미분 가능 모듈을 도입합니다. 겹침을 제거한 후의 마스크 로짓은 픽셀당 인스턴스 \verb|id|에 공급됩니다. 분류는 범감시 감독을 활용하여 중첩 관계 모델링을 지원합니다. 게다가, 우리는 우리의 방법으로 예측된 ​​중첩 관계의 정확성을 정량화하기 위해 약한 감독으로서 중첩 관계의 대략적인 근거 진실을 생성합니다. COCO 및 Cityscapes에 대한 실험은 우리의 방법이 중첩 관계를 정확하게 예측할 수 있고 팬옵틱 분할에 대한 최첨단 성능을 능가할 수 있음을 보여줍니다. 우리의 방법은 COCO 2019 챌린지에서 혁신상을 수상하기도 했습니다."
82,http://arxiv.org/abs/1911.00850 ,Scene Graph based Image Retrieval -- A case study on the CLEVR Dataset,"Sahana Ramnath, Amrita Saha, Soumen Chakrabarti, Mitesh M. Khapra",다양한 영역에서 다중 모드 상호 작용이 확산됨에 따라 최근 컴퓨터 비전 커뮤니티에서는 텍스트 기반 이미지 검색에 많은 관심이 생겼습니다. 그러나 대부분의 최신 기술은 이 문제를 순수 신경 방식으로 모델링하므로 특히 검색 요구 사항이 불충분하고 모델이 여러 번의 질문 답변을 통해 대화형 검색 프로세스에 의존해야 하는 경우 대규모 카탈로그 검색 시 실용적인 전략을 통합하기가 어렵습니다. 이에 동기를 부여하여 우리는 캡션 설명이 주어지면 대규모 카탈로그에서 이미지를 한 번에 검색하기 위한 신경 기호 접근 방식을 제안합니다. 이를 용이하게 하기 위해 우리는 카탈로그와 캡션을 장면 그래프로 표현하고 검색 작업을 REINFORCE 알고리즘으로 엔드투엔드 훈련된 학습 가능한 그래프 일치 문제로 모델링합니다. 또한 대화형 질문 및 답변을 기반으로 하는 반복 검색 프레임워크로의 이 파이프라인 확장에 대해 간략하게 설명합니다.
81,http://arxiv.org/abs/1910.12324 ,Leveraging Auxiliary Text for Deep Recognition of Unseen Visual Relationships,"Gal Sadeh Kenigsfield, Ran El-Yaniv","장면 이해에서 가장 어려운 작업 중 하나는 이미지 속 개체 간의 상호 작용을 인식하는 것입니다. 이 작업을 흔히 VRD(시각적 관계 감지)라고 합니다. VRD 모델 훈련에 사용되는 표준 시각적 데이터 외에 보조 텍스트 데이터가 제공되면 VRD 성능이 향상될 수 있는지 여부에 대한 질문을 고려합니다. 추가적인 텍스트 데이터를 활용할 수 있는 새로운 심층 모델을 제시합니다. 우리 모델은 텍스트에 나타나는 주어-동사-목적어 관계의 공유 텍스트, 이미지 표현과 이미지의 개체 상호 작용에 의존합니다. 우리의 방법은 시각적 훈련 데이터에서 누락되고 보조 텍스트에만 나타나는 시각적 관계를 인식할 수 있는 최초의 방법입니다. 우리는 두 가지 서로 다른 텍스트 소스, 즉 이미지에서 가져온 텍스트와 책에서 가져온 텍스트에 대한 접근 방식을 테스트합니다. VRD와 장면 그래프 생성이라는 두 가지 대규모 인식 작업을 사용하여 접근 방식을 테스트하고 검증합니다. 우리는 놀라운 결과를 보여줍니다. 우리의 접근 방식은 책에서 나온 텍스트에 더 잘 작동하고, 보이지 않는 관계 인식 작업에서 이미지에서 나온 텍스트보다 성능이 뛰어납니다. 보이는 관계 인식 작업에 이미지에서 유래된 텍스트를 활용하는 모델과 유사합니다."
80,http://arxiv.org/abs/1910.09119 ,"Generative Hierarchical Models for Parts, Objects, and Scenes","Fei Deng, Zhuo Zhi, Sungjin Ahn",부분과 사물 사이의 구성적 구조는 자연스러운 장면에 내재되어 있습니다. 비지도 학습을 통해 이러한 구성 계층을 모델링하면 많은 다운스트림 작업에서 중요한 해석 가능성 및 전달 가능성과 같은 다양한 이점을 얻을 수 있습니다. 본 논문에서는 해석 가능한 구성 계층 표현 학습을 위한 RICH라는 최초의 심층 잠재 변수 모델을 제안합니다. RICH의 핵심에는 장면의 개체를 구성 관계에 따라 트리 구조로 구성하는 잠재 장면 그래프 표현이 있습니다. 추론 중에 하향식 접근 방식을 취하는 RICH는 상위 수준 표현을 사용하여 하위 수준 분해를 안내할 수 있습니다. 이는 상향식 접근 방식에서 직면하는 부품과 객체 간 라우팅의 어려운 문제를 방지합니다. 부품 구성이 서로 다른 여러 개체가 포함된 이미지에 대한 실험에서 우리는 RICH가 잠재적 구성 계층을 학습하고 상상의 장면을 생성할 수 있음을 보여줍니다.
79,http://arxiv.org/abs/1910.05134 ,Cross-modal Scene Graph Matching for Relationship-aware Image-Text Retrieval,"Sijin Wang, Ruiping Wang, Ziwei Yao, Shiguang Shan, Xilin Chen","자연 장면의 이미지-텍스트 검색은 인기 있는 연구 주제였습니다. 이미지와 텍스트는 이질적인 교차 모드 데이터이므로, 핵심 과제 중 하나는 다중 모드 데이터를 표현하기 위해 포괄적이면서도 통일된 표현을 학습하는 방법입니다. 자연 장면 이미지는 주로 이미지-텍스트 검색에 똑같이 필수적인 두 가지 종류의 시각적 개념인 객체와 객체의 관계를 포함합니다. 그러므로 좋은 표현은 이 두 가지를 모두 설명해야 합니다. 복잡한 자연 장면을 설명하기 위한 많은 CV 및 NLP 작업에서 최근 장면 그래프의 성공을 고려하여 우리는 시각적 장면 그래프(VSG)와 텍스트 장면 그래프(TSG)라는 두 종류의 장면 그래프를 사용하여 이미지와 텍스트를 표현할 것을 제안합니다. 각 장면 그래프는 해당 양식에서 개체와 관계를 공동으로 특성화하는 데 활용됩니다. 이미지-텍스트 검색 작업은 자연스럽게 크로스 모달 장면 그래프 매칭으로 공식화됩니다. 특히 우리는 VSG 및 TSG 모델에서 두 개의 특정 장면 그래프 인코더를 설계합니다. 이 인코더는 이웃 정보를 집계하여 그래프의 각 노드 표현을 개선할 수 있습니다. 결과적으로 객체 수준과 관계 수준의 교차 모달 특성을 모두 얻을 수 있으며, 이를 통해 두 수준에서 이미지와 텍스트의 유사성을 보다 그럴듯한 방식으로 평가할 수 있습니다. 우리는 이미지-텍스트 검색을 위한 그래프 매칭 기반 접근 방식의 장점을 검증하는 Flickr30k 및 MSCOCO에서 최첨단 결과를 얻었습니다."
78,http://arxiv.org/abs/1910.02527 ,"3D Scene Graph: A Structure for Unified Semantics, 3D Space, and Camera","Iro Armeni, Zhi-Yang He, JunYoung Gwak, Amir R. Zamir, Martin Fischer, Jitendra Malik, Silvio Savarese","장면에 대한 포괄적인 의미론적 이해는 많은 응용 분야에서 중요합니다. 하지만 다양한 의미론적 정보(예: 개체, 장면 범주, 재료 유형, 질감 등)는 어떤 공간에 기반을 두어야 하며 그 구조는 무엇이어야 합니까? 다양한 유형의 의미를 호스팅하는 하나의 통합 구조를 갖기를 열망하면서 우리는 3D 장면 그래프 패러다임을 따라 3D 장면 그래프를 생성합니다. 3D 메시와 등록된 파노라마 이미지가 주어지면 건물 전체에 걸쳐 객체(예: 클래스, 재료 및 기타 속성), 방(예: 장면 카테고리, 볼륨 등), 카메라(예: 위치 등)에 대한 의미는 물론 이러한 개체 간의 관계를 포함하는 그래프를 구성합니다.   그러나 이 프로세스를 수동으로 수행하면 엄청나게 노동력이 많이 듭니다. 이를 완화하기 위해 우리는 기존 감지 방법을 사용하고 두 가지 주요 제약 조건을 사용하여 이를 향상시키는 반자동 프레임워크를 고안했습니다. I. 2D 감지기의 성능을 최대화하기 위해 파노라마에서 샘플링된 쿼리 이미지의 프레이밍 II. 다양한 카메라 위치에서 발생하는 2D 감지 전반에 걸쳐 다중 뷰 일관성을 적용합니다."
77,http://arxiv.org/abs/1910.01198 ,Using Image Priors to Improve Scene Understanding,"Brigit Schroeder, Hanlin Tang, Alexandre Alahi","여러 카메라 시점에서 객체를 강력하게 분할할 수 있는 의미론적 분할 알고리즘은 자율 주행과 같은 새로운 애플리케이션에서 탐색 및 안전을 보장하는 데 매우 중요합니다. 기존 알고리즘은 각 이미지를 개별적으로 처리하지만 자율주행차는 동일한 위치를 다시 방문하거나 직전 과거의 정보를 유지하는 경우가 많습니다. 우리는 순차적 구동 데이터세트에서 이미지의 의미론적 분할을 개선하기 위해 이러한 사전 이미지를 활용하는 간단하면서도 효과적인 방법을 제안합니다. 우리는 이러한 시간적 장면 사전 융합을 위한 여러 가지 방법을 검토하고, 이 정보를 전송하는 방법을 학습할 수 있는 사전 융합 네트워크를 소개합니다. 사전 융합 모델은 동적 클래스의 경우 비사전 기준에 비해 정확도를 69.1%에서 73.3%로, 정적 클래스의 경우 88.2%에서 89.1%로 향상시킵니다. FCN-8과 같은 모델과 비교하여 이전 방법은 5배 더 적은 매개변수로 동일한 정확도를 달성했습니다. 우리는 간단한 인코더 디코더 백본을 사용했지만 이 일반적인 사전 융합 방법은 더 복잡한 의미 분할 백본에 적용될 수 있습니다. 또한 장면 그래프 형태의 구조화된 장면 표현을 사전 분석으로 활용하여 장면 이해를 더욱 향상시킬 수 있는 방법에 대해서도 논의합니다."
76,http://arxiv.org/abs/1909.10128 ,Explainable High-order Visual Question Reasoning: A New Benchmark and Knowledge-routed Network,"Qingxing Cao, Bailin Li, Xiaodan Liang, Liang Lin","설명 및 고차 추론 기능은 다양한 수준의 추론 복잡성(예: 함께 놀고 있는 소녀 근처에 있는 개는 무엇입니까?)을 사용하여 실제 시각적 질문에 대답하는 데 중요하며 사용자가 시스템의 신뢰성을 이해하고 진단하는 데 중요합니다. 정확도 측정항목만 사용하는 자연 이미지에 대한 현재 VQA 벤치마크는 결국 모델이 데이터세트 편향을 활용하도록 강요하고 해석 가능한 정당성을 제공할 수 없으며, 이는 높은 수준의 질문 답변의 발전을 여러 측면에서 방해합니다. 이 작업에서 우리는 세 가지 장점으로 설명 가능한 고차원 시각적 질문 추론 능력을 평가하기 위한 새로운 HVQR 벤치마크를 제안합니다. 1) 질문에는 종종 하나 또는 두 개의 관계 삼중항이 포함되어 있으며, 이를 위해서는 모델이 그럴듯한 답변을 예측하기 위한 다단계 추론 능력이 필요합니다. 2) 이미지 장면 그래프와 상식 지식 기반으로 구성된 다단계 추론 프로세스에 대한 명시적인 평가를 제공합니다. 3) 대규모 지식 기반의 각 관계 삼중항은 모든 질문 중에서 한 번만 나타납니다. 이는 훈련 세트에 이미 나타나는 지식 기반을 과적합하려고 시도하고 모델이 보이지 않는 질문과 지식 사실 사용을 처리하도록 강제하는 기존 네트워크에 문제를 제기합니다. 또한 대규모 지식 기반에 대한 다단계 추론 프로세스를 시각적 질문 추론에 통합하는 새로운 지식 라우팅 모듈러 네트워크(KM-net)를 제안합니다. HVQR 벤치마크에 대한 광범위한 데이터 세트 분석 및 기존 모델과의 비교를 통해 우리 벤치마크는 설명 가능한 평가, 포괄적인 추론 요구 사항 및 VQA 시스템의 현실적인 과제를 제공할 뿐만 아니라 정확성 및 설명 능력 측면에서 KM-net의 우수성을 보여줍니다."
75,http://arxiv.org/abs/1909.09953 ,Learning Visual Relation Priors for Image-Text Matching and Image Captioning with Neural Scene Graph Generators,"Kuang-Huei Lee, Hamid Palangi, Xi Chen, Houdong Hu, Jianfeng Gao",시각적 관계에 대한 언어의 접지는 다양한 언어 및 시각 적용에 매우 중요합니다. 이 작업에서 우리는 이미지-텍스트 일치와 이미지 캡션이라는 두 가지 기본 언어 및 비전 작업을 다루고 신경 장면 그래프 생성기가 효과적인 시각적 관계 기능을 학습하여 시각적 관계에 대한 언어 기반을 촉진하고 이후 두 최종 애플리케이션을 개선할 수 있음을 보여줍니다. 관계 기능을 최첨단 모델과 결합함으로써 우리의 실험은 표준 Flickr30K 및 MSCOCO 벤치마크에 대한 상당한 개선을 보여줍니다. 우리의 실험 결과 및 분석에 따르면 관계 기능은 최종 비전 및 언어 응용 프로그램에서 시각적 관계를 캡처하는 다운스트림 모델의 기능을 향상시킵니다. 또한 관계 기능의 효율성과 시각적으로 관련된 관계가 있는 장면 그래프 생성기를 학습하는 것의 중요성을 보여줍니다.
74,http://arxiv.org/abs/1909.09256 ,Triplet-Aware Scene Graph Embeddings,"Brigit Schroeder, Subarna Tripathi, Hanlin Tang","장면 그래프는 이미지 생성, 시각적 관계 감지, 시각적 질문 답변, 이미지 검색과 같은 작업에 대한 구조화된 지식의 중요한 형태가 되었습니다. 단어 임베딩을 시각화하고 해석하는 것은 잘 이해되었지만 장면 그래프 임베딩은 완전히 탐색되지 않았습니다. 이 작업에서는 다양한 형태의 감독을 사용하여 레이아웃 생성 작업에서 장면 그래프 임베딩을 훈련하며, 특히 삼중 슈퍼비전 및 데이터 증대를 도입합니다. 삼중항 감독 및 데이터 확대를 추가한 후 레이아웃 예측의 우수성을 측정하는 두 측정항목, 평균 교차-결합(mIoU)(52.3% 대 49.2%) 및 관계 점수(61.7% 대 54.1%)에서 상당한 성능 향상을 확인했습니다. 이러한 다양한 방법이 장면 그래프 표현에 어떤 영향을 미치는지 이해하기 위해 몇 가지 새로운 시각화 및 평가 방법을 적용하여 장면 그래프 임베딩의 진화를 탐색합니다. 우리는 삼중항 감독이 임베딩 분리성을 크게 향상시키며 이는 레이아웃 예측 모델의 성능과 높은 상관관계가 있음을 발견했습니다."
73,http://arxiv.org/abs/1909.06273 ,Scene Graph Parsing by Attention Graph,"Martin Andrews, Yew Ken Chia, Sam Witteveen","속성 및 관계와 함께 시각적 개체 노드의 그래프를 형성하는 장면 그래프 표현은 다양한 비전 및 언어 응용 프로그램에서 유용한 것으로 입증되었습니다. 이 분야의 최근 작업에서는 자연어 처리 종속성 트리 방법을 사용하여 장면 그래프를 자동으로 작성했습니다.   본 연구에서는 end-to-end 학습이 가능한 'Attention Graph' 메커니즘을 제시하고, 표준 Transformer 모델의 최상위 레이어에서 직접 리프트할 수 있는 장면 그래프 구조를 생성합니다.   우리 모델에 의해 생성된 장면 그래프는 SPICE 메트릭을 사용하는 평가 세트의 실측 그래프와 52.21%의 F-점수 유사성을 달성하여 이전 최고의 접근 방식을 2.5% 능가합니다."
72,http://arxiv.org/abs/1909.05379 ,Specifying Object Attributes and Relations in Interactive Scene Generation,"Oron Ashual, Lior Wolf","입력 장면 그래프에서 이미지를 생성하는 방법을 소개합니다. 이 방법은 레이아웃 임베딩과 모양 임베딩을 구분합니다. 이중 임베딩을 통해 장면 그래프와 더 잘 일치하고 시각적 품질이 더 높으며 더 복잡한 장면 그래프를 지원하는 이미지가 생성됩니다. 또한 임베딩 방식은 장면 그래프당 여러 개의 다양한 출력 이미지를 지원하며, 이는 사용자가 추가로 제어할 수 있습니다. 우리는 객체별 제어의 두 가지 모드를 보여줍니다. (i) 다른 이미지에서 요소를 가져오는 것과 (ii) 모양 원형을 선택하여 객체 공간에서 탐색하는 것입니다. 우리 코드는 https://www.github.com/ashual/scene_ Generation에서 공개적으로 제공됩니다."
71,http://arxiv.org/abs/1909.00640 ,Relationship-Aware Spatial Perception Fusion for Realistic Scene Layout Generation,"Hongdong Zheng, Yalong Bai, Wei Zhang, Tao Mei","GAN(Generative Adversarial Networks)의 상당한 발전으로 자연어 설명을 기반으로 단일 개체에 대한 놀랍도록 사실적인 이미지를 생성할 수 있게 되었습니다. 그러나 명시적인 상호 작용이 있는 여러 엔터티에 대한 제어된 이미지 생성은 장면 레이아웃 생성이 다양성 객체 크기 조정 및 공간 위치로 인해 크게 어려움을 겪기 때문에 여전히 달성하기 어렵습니다. 본 논문에서는 텍스트 장면 그래프로부터 사실적인 이미지 레이아웃을 생성하기 위한 새로운 프레임워크를 제안했습니다. 우리 프레임워크에서 공간 제약 모듈은 개체 쌍 간의 관계를 고려하여 개체 쌍의 합리적인 크기 조정 및 공간 레이아웃에 맞도록 설계되었습니다. 또한 장면 그래프의 개체 종속성 측면에서 쌍별 공간 정보를 융합하기 위한 상황별 융합 모듈이 도입되었습니다. 이 두 모듈을 사용함으로써 우리가 제안한 프레임워크는 현실적인 이미지 생성에 도움이 되는 보다 상식적인 레이아웃을 생성하는 경향이 있습니다. 두 개의 서로 다른 장면 그래프 데이터 세트에 대한 정량적 결과, 정성적 결과 및 사용자 연구를 포함한 실험 결과는 제안된 프레임워크가 장면 그래프의 여러 개체로 복잡하고 논리적인 레이아웃을 생성하는 능력을 보여줍니다."
70,http://arxiv.org/abs/1908.10700 ,Explainable Video Action Reasoning via Prior Knowledge and State Transitions,"Tao Zhuo, Zhiyong Cheng, Peng Zhang, Yongkang Wong, Mohan Kankanhalli","비디오에서 인간의 행동을 분석하고 이해하는 것은 중요하면서도 어려운 작업입니다. 지난 몇 년 동안 상당한 진전이 있었지만 기존 방법의 설명 가능성은 여전히 ​​제한적입니다. 본 연구에서는 사전 지식을 사용하여 비디오 상태 변화에 대한 의미 수준의 관찰을 설명하는 새로운 동작 추론 프레임워크를 제안합니다. 우리의 방법은 고전적인 추론과 현대적인 딥러닝 접근 방식을 모두 활용합니다. 구체적으로, 사전 지식은 대상 비디오 도메인의 객체, 속성 및 관계 집합뿐만 아니라 시간적 속성 및 관계 변화(예: 상태 전환)에 의해 정의된 관련 작업을 포함하는 대상 비디오 도메인의 정보로 정의됩니다. 비디오 시퀀스가 ​​주어지면 먼저 각 프레임에 장면 그래프를 생성하여 관련 객체, 속성 및 관계를 나타냅니다. 그런 다음 해당 장면 그래프는 프레임 전체에서 개체를 추적하여 연결되어 의미 수준의 비디오 상태를 나타내는 시공간 그래프(비디오 그래프라고도 함)를 형성합니다. 마지막으로, 비디오 그래프의 각 상태 전환을 순차적으로 검사함으로써 우리의 방법은 인간의 논리적 사고 방식과 마찬가지로 사전 지식을 통해 해당 동작이 어떻게 실행되는지 감지하고 설명할 수 있습니다. 이전 연구와 비교하여, 우리 방법의 행동 추론 결과는 비디오 콘텐츠 변화에 대한 논리적 규칙과 의미 수준 관찰로 설명될 수 있습니다. 또한, 제안된 방법은 누가(특정 객체), 언제(시간), 어디서(객체 위치), 어떻게(어떤 변화)와 같은 세부 정보를 통해 여러 동시 동작을 탐지하는 데 사용될 수 있습니다. 다시 주석이 달린 데이터세트 CAD-120에 대한 실험은 우리 방법의 효율성을 보여줍니다."
69,http://arxiv.org/abs/1908.06592 ,Seq-SG2SL: Inferring Semantic Layout from Scene Graph Through Sequence to Sequence Learning,"Boren Li, Boyu Zhuang, Mingyang Li, Jian Gu",장면 그래프에서 의미 체계 레이아웃을 생성하는 것은 텍스트와 이미지를 연결하는 중요한 중간 작업입니다. 우리는 이 작업을 위해 시퀀스 간(seq-to-seq) 학습을 사용하여 개념적으로 간단하고 유연하며 일반적인 프레임워크를 제시합니다. Seq-SG2SL이라는 프레임워크는 두 가지 양식에 대한 시퀀스 프록시를 파생하고 Transformer 기반 seq-to-seq 모델은 하나를 다른 것으로 변환하는 방법을 학습합니다. 장면 그래프는 각 관계마다 하나씩 일련의 의미론적 조각(SF)으로 분해됩니다. 의미 체계 레이아웃은 일련의 BACS(브릭 작업 코드 세그먼트)의 결과로 표시되며 레이아웃에 있는 각 개체 경계 상자의 위치와 크기를 지정합니다. 두 가지 빌딩 블록인 SF와 BACS를 서로 다른 두 어휘의 해당 용어로 보면 seq-to-seq 모델이 번역에 적합하게 사용됩니다. BLEU에서 영감을 받은 의미 체계 레이아웃 예측 작업을 평가하기 위해 새로운 측정 기준인 의미 체계 레이아웃 평가 언더스터디(SLEU)가 고안되었습니다. SLEU는 레이아웃 내의 관계를 유니그램으로 정의하고 n-그램의 공간 분포를 살펴봅니다. BLEU의 이진 정밀도와 달리 SLEU는 Jaccard Index의 임계값을 통해 공간적으로 일부 허용 오차를 허용하므로 결과적으로 작업에 더 적합합니다. 까다로운 Visual Genome 데이터세트에 대한 실험 결과는 그래프 컨볼루션을 기반으로 한 비순차적 접근 방식에 비해 개선된 점을 보여줍니다.
68,http://arxiv.org/abs/1908.04929 ,3-D Scene Graph: A Sparse and Semantic Representation of Physical Environments for Intelligent Agents,"Ue-Hwan Kim, Jin-Man Park, Taek-Jin Song, Jong-Hwan Kim","지능형 에이전트는 주어진 작업을 수행하기 전에 환경 내에서 정보를 수집하고 의미를 인식합니다. 에이전트는 수집된 정보를 주변 환경을 간결하게 표현하는 환경 모델 형태로 저장합니다. 그러나 효율적이고 효과적인 환경 모델이 없으면 에이전트는 제한된 작업만 수행할 수 있습니다. 따라서 이러한 환경 모델은 지능형 에이전트의 자율 시스템에 중요한 역할을 합니다. 우리는 다목적 환경 모델의 정확성, 적용성, 유용성 및 확장성이라는 특성을 주장합니다. 많은 연구자들이 환경을 어느 정도 정확하게 표현하는 모델을 개발하려고 시도했지만 광범위한 적용성, 직관적인 사용성 및 만족스러운 확장성이 부족합니다. 이러한 한계를 해결하기 위해 우리는 환경 모델로서 3차원 장면 그래프와 3차원 장면 그래프 구축 프레임워크를 제안한다. 간결하고 널리 사용되는 그래프 구조는 3차원 장면 그래프의 유용성과 확장성을 쉽게 보장합니다. 3차원 장면 그래프의 실제 응용 사례를 전시하여 3차원 장면 그래프의 정확성과 적용 가능성을 보여줍니다. 또한 다양한 조건에서 일련의 종합적인 실험을 수행하여 제안된 3차원 장면 그래프와 프레임워크의 성능을 검증한다."
67,http://arxiv.org/abs/1908.02962 ,CRIC: A VQA Dataset for Compositional Reasoning on Vision and Commonsense,"Difei Gao, Ruiping Wang, Shiguang Shan, Xilin Chen","또는 시각적 사실과 상식을 추론하는 것은 고급 VQA 시스템의 기본입니다. 이 기능을 사용하려면 모델이 상식에 대한 문자 그대로의 이해를 뛰어넘어야 합니다. 시스템은 객체를 배경 지식에 대한 입구로 취급할 뿐만 아니라 시각적 세계에 대한 상식을 완전히 기반으로 하고 객체 간의 가능한 관계(예: ""포크, 들어올릴 수 있음, 음식"")를 상상해야 합니다. 이러한 능력을 종합적으로 평가하기 위해 우리는 비전과 상식에 대한 구성추론에 대한 새로운 유형의 질문과 답변의 정확성과 상식적 근거를 통합한 평가 지표를 도입하는 VQA 벤치마크인 CRIC를 제안합니다. 이러한 질문과 메트릭을 지원하는 풍부한 추가 주석을 수집하기 위해 이미지와 관련된 장면 그래프 및 관련 지식 그래프에서 질문 샘플을 생성하는 자동 알고리즘도 제안합니다. 우리는 CRIC 데이터 세트에서 VQA 모델의 몇 가지 대표적인 유형을 추가로 분석합니다. 실험 결과에 따르면 이미지 영역에 상식을 기반으로 하는 것과 비전과 상식에 대한 공동 추론이 현재 접근 방식으로는 여전히 어려운 것으로 나타났습니다. 데이터 세트는 https://cricvqa.github.io에서 사용할 수 있습니다."
66,http://arxiv.org/abs/1908.02436 ,Continuous Graph Flow,"Zhiwei Deng, Megha Nawhal, Lili Meng, Greg Mori","본 논문에서는 그래프 구조의 데이터의 복잡한 분포를 모델링하는 것을 목표로 하는 생성적 연속 흐름 기반 방법인 연속 그래프 흐름(Continuous Graph Flow)을 제안합니다. 일단 학습되면 모델은 임의의 그래프에 적용되어 그래프로 표시되는 무작위 변수에 대한 확률 밀도를 정의할 수 있습니다. 이는 그래프에서 작동하는 공유되고 재사용 가능한 함수를 갖춘 상미분 방정식 시스템으로 공식화됩니다. 이는 시간이 지남에 따라 연속적인 메시지 전달을 수행하는 새로운 유형의 신경 그래프 메시지 전달 방식으로 이어집니다. 이 모델 클래스는 다음과 같은 여러 가지 장점을 제공합니다. 가변 데이터 차원으로 일반화할 수 있는 유연한 표현; 복잡한 데이터 분포에서 종속성을 모델링하는 능력; 가역적이고 메모리 효율적입니다. 데이터의 가능성을 정확하고 효율적으로 계산합니다. 우리는 그래프 생성, 이미지 퍼즐 생성, 장면 그래프의 레이아웃 생성 등 다양한 도메인에 걸친 다양한 생성 작업 세트에서 모델의 효율성을 보여줍니다. 우리가 제안한 모델은 최신 모델에 비해 훨씬 더 나은 성능을 달성합니다."
65,http://arxiv.org/abs/1907.12133 ,An Empirical Study on Leveraging Scene Graphs for Visual Question Answering,"Cheng Zhang, Wei-Lun Chao, Dong Xuan","시각적 질문 답변(Visual QA)은 최근 몇 년 동안 큰 관심을 끌었습니다. 다양한 알고리즘이 제안되었지만 대부분은 다중 모드 주의 및 융합뿐만 아니라 이미지와 언어 기능의 다양한 조합을 기반으로 구축되었습니다. 본 논문에서는 지식 그래프에서 작동하는 기존 QA 시스템에서 영감을 받은 대체 접근 방식을 조사합니다. 특히, 우리는 시각적 QA를 위해 이미지에서 파생된 장면 그래프의 사용을 조사합니다. 이미지는 객체 엔터티에 해당하는 노드와 객체 관계에 대한 가장자리가 있는 그래프로 추상적으로 표현됩니다. 최근 제안된 그래프 네트워크(GN)를 적용하여 장면 그래프를 인코딩하고 입력 질문에 따라 구조화된 추론을 수행합니다. 우리의 경험적 연구에 따르면 장면 그래프는 이미 이미지의 필수 정보를 캡처할 수 있으며 그래프 네트워크는 최첨단 Visual QA 알고리즘보다 성능이 뛰어나지만 훨씬 깔끔한 아키텍처를 사용할 수 있는 잠재력이 있습니다. GN에 의해 ​​생성된 기능을 분석함으로써 추론 프로세스를 추가로 해석하여 설명 가능한 시각적 QA를 향한 유망한 방향을 제안할 수 있습니다."
64,http://arxiv.org/abs/1906.08707 ,The Limited Multi-Label Projection Layer,"Brandon Amos, Vladlen Koltun, J. Zico Kolter",우리는 엔드투엔드 학습 시스템을 위한 새로운 기본 작업으로 LML(Limited Multi-Label) 프로젝션 레이어를 제안합니다. LML 계층은 정확히 k개의 레이블을 갖는 것으로 제한된 다중 레이블 예측을 모델링하는 확률론적 방법을 제공합니다. 우리는 이 레이어에 대해 효율적인 정방향 및 역방향 전달을 유도하고 불완전한 라벨 정보가 있는 다중 라벨 작업에 대한 top-k 재현율을 최적화하는 데 레이어를 사용할 수 있는 방법을 보여줍니다. Top-k CIFAR-100 분류 및 장면 그래프 생성에 대한 LML 레이어를 평가합니다. 우리는 LML 레이어가 무시할 만큼의 계산 오버헤드를 추가하고 모델의 표현 능력을 엄격하게 향상시키며 정확도를 향상시킨다는 것을 보여줍니다. 또한 Top-K 분류를 위한 경쟁 기준으로 잘린 Top-K 엔트로피 방법을 다시 검토합니다.
63,http://arxiv.org/abs/1906.04876 ,Learning Predicates as Functions to Enable Few-shot Scene Graph Prediction,"Apoorva Dornadula, Austin Narcomey, Ranjay Krishna, Michael Bernstein, Li Fei-Fei",장면 그래프 예측(시각적 장면에서 객체 및 조건자 집합을 분류하려면) 상당한 훈련 데이터가 필요합니다. 그러나 대부분의 술어는 몇 번만 발생하므로 배우기가 어렵습니다. 술어의 퓨샷 학습을 지원하는 최초의 장면 그래프 예측 모델을 소개합니다. 기존 장면 그래프 생성 모델은 사전 훈련된 개체 감지기 또는 제공되는 관계에 대한 정보를 인코딩하는 대신 의미 개체 정보를 캡처하는 단어 임베딩을 사용하여 개체를 나타냅니다. 따라서 이러한 객체 표현은 새로운 퓨샷 관계로 일반화할 수 없습니다. 시각적 관계에 따라 구조화된 객체 표현을 유도하는 프레임워크를 소개합니다. 이전 방법과 달리 우리 프레임워크는 유사한 관계를 더 가깝게 유지하는 개체를 포함합니다. 이 속성을 사용하면 모델이 몇 번의 샷 설정에서 잘 작동할 수 있습니다. 예를 들어 'riding' 술어 변환을 'person'에 적용하면 라이딩을 가능하게 하는 'skateboard' 및 'horse'와 같은 객체에 대한 표현이 수정됩니다. 우리는 새로운 그래프 컨볼루션 프레임워크 내에서 메시지 전달 기능으로 훈련된 조건자를 학습하여 객체 표현을 생성합니다. 개체 표현은 레이블이 지정된 예가 1개 미만인 희귀 조건자에 대한 소수 조건자 분류자를 구축하는 데 사용됩니다. 우리는 강력한 전이 학습 기준과 비교할 때 3.7 증가한 22.70 Recall@50의 5샷 성능을 달성했습니다.
62,http://arxiv.org/abs/1906.03561 ,Joint Visual Grounding with Language Scene Graphs,"Daqing Liu, Hanwang Zhang, Zheng-Jun Zha, Meng Wang, Qianru Sun","시각적 접지는 이미지의 참조 표현을 접지하는 작업입니다(예: ""노란색 트럭 앞에 흰색 트럭이 있다""). 이 작업을 근본적으로 해결하려면 모델은 먼저 상황별 개체(예: ""노란색"" 트럭)를 찾은 다음 이를 활용하여 속성 및 관계(예: ""흰색"", ""노란색"", ""앞"")를 사용하여 다른 유사한 개체와 지시대상을 명확하게 구분해야 합니다. 그러나 기존 방법들은 문맥적 대상과 그 관계에 대한 주석이 부족하여 위의 공동접근 과정을 표현과 영역 간의 전체적인 연관으로 변질시켜 성능이 만족스럽지 못하고 해석 가능성이 제한되는 문제를 겪고 있다. 본 논문에서는 레이블이 지정된 참조 컨텍스트와 레이블이 지정되지 않은 컨텍스트(기타 객체, 속성 및 관계)를 모두 포함하는 언어 장면 그래프를 활용하여 주석 누락 문제를 완화하고 공동 추론을 가능하게 합니다. 특히, 언어 장면 그래프는 노드가 속성을 가진 개체이고 가장자리가 관계인 그래픽 표현입니다. 이를 기반으로 요인 그래프를 구성한 다음 그래프에 대해 주변화를 수행하여 JVG(Joint Visual Grounding)를 달성하기 위해 해당 이미지 영역에 참조 대상과 컨텍스트를 모두 접지할 수 있습니다. 실험 결과는 제안된 접근 방식이 효과적이고 해석 가능하다는 것을 보여줍니다. 예를 들어 세 가지 벤치마크에서 이는 참조 표현에 언급된 모든 개체에 대한 완전한 기반을 제공하는 동시에 최첨단 방법보다 성능이 뛰어납니다."
61,http://arxiv.org/abs/1905.11624 ,Contextual Translation Embedding for Visual Relationship Detection and Scene Graph Generation,"Zih-Siou Hung, Arun Mallya, Svetlana Lazebnik","개체 간의 관계는 이미지를 이해하는 데 핵심적인 역할을 합니다. (주어, 술어, 목적어) 관계 삼중항 모델링의 복잡성으로 인해 보이는 관계를 인식할 수 있을 뿐만 아니라 보이지 않는 경우에도 일반화할 수 있는 방법을 개발하는 것이 중요합니다. 이전에 제안된 시각적 번역 임베딩 모델(VTransE)에서 영감을 받아 우리는 공통 관계와 희귀 관계를 모두 포착할 수 있는 상황에 맞는 번역 임베딩 모델을 제안합니다. 이전 VTransE 모델은 엔터티와 조건자를 저차원 임베딩 벡터 공간에 매핑합니다. 여기서 조건자는 주제와 개체의 경계 상자 영역에 포함된 기능 간의 변환 벡터로 해석됩니다. 우리 모델은 주어와 객체의 합집합의 경계 상자에 의해 캡처된 상황별 정보를 추가로 통합하고 제약 조건 조건 $\about$ Union (subject, object) $-$ subject $-$ object에 의해 안내되는 임베딩을 학습합니다. 여러 가지 까다로운 벤치마크에 대한 포괄적인 평가에서 우리의 접근 방식은 이전 번역 기반 모델보다 성능이 뛰어나며 소규모에서 대규모 데이터 세트, 공통 관계에서 이전에 볼 수 없었던 관계까지 다양한 설정에 걸쳐 최첨단에 가깝거나 이를 능가합니다. 또한 최근 도입된 장면 그래프 생성 작업에서도 유망한 결과를 얻었습니다."
60,http://arxiv.org/abs/1905.09891 ,Adding Intuitive Physics to Neural-Symbolic Capsules Using Interaction Networks,"Michael Kissner, Helmut Mayer","직관적인 물리학을 학습하는 현재의 많은 방법은 상호작용 네트워크 및 유사한 접근 방식을 기반으로 합니다. 그러나 과거에는 이미지 데이터에서 직접 추정하기 어려웠던 정보에 의존합니다. 우리는 장면 그래프 형태의 원시 픽셀 데이터에서 필요한 모든 의미 정보를 추론하여 이러한 격차를 줄이는 것을 목표로 합니다. 우리의 접근 방식은 장면의 어떤 객체가 정적, 동적, 탄성 또는 강성인지, 객체 사이의 가능한 관절 및 충돌 정보를 식별하는 신경 기호 캡슐을 기반으로 합니다. 이 모든 것을 상호 작용 네트워크와 통합함으로써 우리는 우리의 방법이 이미지 시퀀스에서 직접 직관적인 물리학을 학습하고 그 지식을 새로운 장면과 객체에 적용하여 역시뮬레이션 파이프라인을 생성할 수 있는 방법을 보여줍니다."
59,http://arxiv.org/abs/1905.05143 ,VideoGraph: Recognizing Minutes-Long Human Activities in Videos,"Noureldien Hussein, Efstratios Gavves, Arnold W. M. Smeulders","많은 인간 활동이 전개되는 데 몇 분이 걸립니다. 이를 대표하기 위해 관련 작품들은 시간적 구조를 무시하는 통계적 풀링을 선택한다. 다른 사람들은 CNN 및 Non-Local과 같은 컨볼루션 방법을 선택합니다. 시간적 개념을 학습하는 데는 성공했지만 몇 분 동안의 시간적 종속성을 모델링하는 데는 부족합니다. 우리는 몇 분 동안의 인간 활동을 표현하고 그 기본 시간 구조를 학습하는 두 가지 세계의 장점을 달성하는 방법인 VideoGraph를 제안합니다. VideoGraph는 인간 활동에 대한 그래프 기반 표현을 학습합니다. 그래프, 그래프의 노드 및 에지는 비디오 데이터세트에서 완전히 학습되므로 VideoGraph를 노드 수준 주석 없이 문제에 적용할 수 있습니다. 그 결과 벤치마크 관련 작업인 Epic-Kitchen 및 Breakfast에 비해 개선되었습니다. 게다가, 우리는 VideoGraph가 몇 분 길이의 비디오에서 인간 활동의 시간적 구조를 학습할 수 있음을 보여줍니다."
58,http://arxiv.org/abs/1905.03743 ,Interactive Image Generation Using Scene Graphs,"Gaurav Mittal, Shubham Agrawal, Anuva Agarwal, Sushant Mehta, Tanya Marwah",최근 몇 년 동안 장면 기반 텍스트 설명에서 이미지를 생성하는 영역에서 몇 가지 흥미로운 발전이 있었습니다. 이러한 접근 방식은 주로 정적 텍스트 설명에서 이미지를 생성하는 데 중점을 두었으며 단일 패스에서 이미지를 생성하는 것으로 제한됩니다. 점진적으로 추가되는 텍스트 설명(보다 직관적이고 이미지를 설명하는 방식과 유사한 설명)을 기반으로 대화형으로 이미지를 생성할 수 없습니다. 장면 설명 그래프(장면 그래프)의 시퀀스를 기반으로 점진적으로 이미지를 생성하는 방법을 제안합니다. 우리는 이전 단계에서 생성된 이미지 콘텐츠를 보존하고 새로 제공되는 장면 정보에 따라 누적 이미지를 수정하는 순환 네트워크 아키텍처를 제안합니다. 우리 모델은 GCN(Graph Convolutional Networks)을 활용하여 Generative Adversarial 이미지 변환 네트워크와 함께 가변 크기 장면 그래프를 제공하여 훈련 중에 중간 감독 없이 현실적인 다중 객체 이미지를 생성합니다. 우리는 시각적 장면을 설명하는 주석과 함께 다중 객체 이미지가 있는 Coco-Stuff 데이터 세트를 실험하고 점진적으로 성장하는 장면 그래프에 대해 시각적으로 일관된 이미지를 생성하는 데 있어 동일한 데이터 세트에 대한 다른 접근 방식보다 우리 모델이 훨씬 뛰어난 성능을 보인다는 것을 보여줍니다.
57,http://arxiv.org/abs/1905.01608 ,PasteGAN: A Semi-Parametric Method to Generate Image from Scene Graph,"Yikang Li, Tao Ma, Yeqi Bai, Nan Duan, Sining Wei, Xiaogang Wang","구조화된(장면 그래프) 또는 자유 형식(문장) 설명에서 고품질 이미지 생성에 대한 흥미로운 진전에도 불구하고 대부분은 이미지 수준의 의미적 일관성, 즉 생성된 이미지가 설명의 의미론적 의미와 일치하는 것만 보장합니다. 모든 물체의 시각적 모양을 미세하게 조작하는 등 보다 제어 가능한 방식으로 이미지를 합성하는 방법에 대한 조사가 여전히 부족합니다. 따라서 선호하는 개체와 풍부한 상호 작용이 있는 이미지를 생성하기 위해 장면 그래프와 이미지 자르기에서 이미지를 생성하는 반모수적 방법인 PasteGAN을 제안합니다. 여기서 개체의 공간 배열과 쌍별 관계는 장면 그래프에 의해 정의되고 개체 모양은 주어진 개체 자르기에 의해 결정됩니다. 출력에서 객체의 상호 작용을 향상시키기 위해 자르기 정제 네트워크와 객체-이미지 융합기를 설계하여 객체와 객체의 관계를 하나의 맵에 포함합니다. 여러 손실이 협력하여 생성된 이미지가 작물을 매우 존중하고 장면 그래프를 준수하는 동시에 탁월한 이미지 품질을 유지하도록 보장합니다. 작물이 제공되지 않는 경우 장면 그래프에서 객체 주변의 상호 작용을 인코딩하여 외부 개체 탱크에서 가장 호환되는 작물을 선택하는 작물 선택기도 제안됩니다. Visual Genome 및 COCO-Stuff 데이터세트를 평가한 결과, 제안된 방법은 Inception Score, Diversity Score 및 Fréchet Inception Distance에서 SOTA 방법보다 훨씬 뛰어난 성능을 보였습니다. 광범위한 실험은 또한 주어진 물체로 복잡하고 다양한 이미지를 생성하는 우리 방법의 능력을 보여줍니다."
56,http://arxiv.org/abs/1904.11622 ,Scene Graph Prediction with Limited Labels,"Vincent S. Chen, Paroma Varma, Ranjay Krishna, Michael Bernstein, Christopher Re, Li Fei-Fei","Visual Genome과 같은 시각적 지식 기반은 시각적 질문 답변 및 캡션을 포함하여 컴퓨터 비전의 수많은 애플리케이션을 지원하지만 희박하고 불완전한 관계로 인해 어려움을 겪습니다. 현재까지의 모든 장면 그래프 모델은 각각 수천 개의 훈련 레이블이 있는 작은 시각적 관계 세트에 대한 훈련으로 제한됩니다. 인간 주석자를 고용하는 것은 비용이 많이 들고, 텍스트 기반 지식 기반 완성 방법을 사용하는 것은 시각적 데이터와 호환되지 않습니다. 본 논문에서는 몇 가지 레이블이 지정된 예제를 사용하여 레이블이 지정되지 않은 다수의 이미지에 확률적 관계 레이블을 할당하는 준지도 방법을 소개합니다. 우리는 시각적 관계를 분석하여 잡음이 있는 휴리스틱을 생성하는 데 사용되는 두 가지 유형의 이미지 독립적 특징을 제안합니다. 그 출력은 요인 그래프 기반 생성 모델을 사용하여 집계됩니다. 관계당 10개의 레이블이 지정된 예시를 사용하여 생성 모델은 기존의 최첨단 장면 그래프 모델을 교육하는 데 충분한 교육 데이터를 생성합니다. 우리는 우리의 방법이 PREDCLS에 대해 5.16 Recall@100만큼 장면 그래프 예측에 대한 모든 기본 접근 방식보다 성능이 우수하다는 것을 보여줍니다. 제한된 레이블 설정에서는 제한된 레이블을 사용한 교육을 위한 사실상의 접근 방식인 전이 학습에 비해 우리 방법이 성공하는 조건에 대한 지표(R^2 = 0.778) 역할을 하는 관계에 대한 복잡성 메트릭을 정의합니다."
55,http://arxiv.org/abs/1904.11621 ,Meta-Sim: Learning to Generate Synthetic Datasets,"Amlan Kar, Aayush Prakash, Ming-Yu Liu, Eric Cameracci, Justin Yuan, Matt Rusiniak, David Acuna, Antonio Torralba, Sanja Fidler","최고 수준의 성능을 발휘하도록 모델을 훈련하려면 레이블이 지정된 대규모 데이터 세트를 사용할 수 있어야 하며, 이를 얻는 데 비용이 많이 듭니다. 우리 작업의 목표는 다운스트림 작업과 관련된 레이블이 지정된 데이터 세트를 자동으로 합성하는 것입니다. 우리는 합성 장면의 생성 모델을 학습하고 그래픽 엔진을 통해 이미지와 그에 따른 실제 정보를 얻는 Meta-Sim을 제안합니다. 우리는 확률적 장면 문법에서 얻은 장면 그래프의 속성을 수정하는 방법을 학습하여 렌더링된 출력과 대상 데이터 사이의 분포 격차를 최소화하는 신경망을 사용하여 데이터세트 생성기를 매개변수화합니다. 실제 데이터 세트에 작은 레이블이 지정된 검증 세트가 함께 제공되는 경우 추가로 메타 목표, 즉 다운스트림 작업 성능을 최적화하는 것을 목표로 합니다. 실험은 제안된 방법이 다운스트림 작업의 성능으로 측정할 때 질적, 양적으로 인간이 설계한 확률적 장면 문법에 비해 콘텐츠 생성 품질을 크게 향상시킬 수 있음을 보여줍니다."
54,http://arxiv.org/abs/1904.09626 ,Deep Metric Learning Beyond Binary Supervision,"Sungyeon Kim, Minkyo Seo, Ivan Laptev, Minsu Cho, Suha Kwak","시각적 유사성을 위한 메트릭 학습은 한 쌍의 이미지가 동일한 클래스인지 여부를 나타내는 이진 감독을 대부분 채택했습니다. 이러한 이진 표시기는 이미지 관계의 제한된 하위 집합만을 다루며 객체 포즈, 이미지 캡션 및 장면 그래프와 같은 연속 및/또는 구조화된 레이블로 설명되는 이미지 간의 의미론적 유사성을 표현하는 데 충분하지 않습니다. 이에 동기를 부여하여 연속 레이블을 사용한 심층 메트릭 학습을 위한 새로운 방법을 제시합니다. 먼저, 학습된 메트릭 공간에서 레이블 공간의 거리 비율이 보존되도록 하는 새로운 삼중항 손실을 제안합니다. 따라서 제안된 손실을 통해 우리 모델은 단순한 순서가 아닌 유사성 정도를 학습할 수 있습니다. 또한 연속 레이블을 사용하여 메트릭 학습에 적합한 삼중 마이닝 전략을 설계합니다. 우리는 인간 포즈, 방 레이아웃 및 이미지 캡션 측면에서 연속 레이블을 사용하여 세 가지 다른 이미지 검색 작업을 다루고 이전 방법에 비해 우리 접근 방식의 우수한 성능을 보여줍니다."
53,http://arxiv.org/abs/1904.09447 ,An Unsupervised Joint System for Text Generation from Knowledge Graphs and Semantic Parsing,"Martin Schmitt, Sahand Sharifzadeh, Volker Tresp, Hinrich Schütze","지식 그래프(KG)는 도메인마다 크게 다를 수 있습니다. 따라서 그래프-텍스트 생성 및 텍스트-그래프 지식 추출(의미론적 구문 분석) 모두에 대한 감독 방식은 항상 도메인별 병렬 그래프-텍스트 데이터 부족으로 어려움을 겪습니다. 동시에, 다른 영역에서 훈련된 모델을 적용하는 것은 엔터티와 관계의 중첩이 거의 또는 전혀 없기 때문에 불가능한 경우가 많습니다. 이러한 상황에서는 (1) 주석이 달린 대량의 데이터가 필요하지 않으므로 (2) 다양한 도메인에서 잘 작동하기 위해 도메인 적응 기술에 의존할 필요가 없는 접근 방식이 필요합니다. 이를 위해 우리는 KG의 비지도 텍스트 생성에 대한 첫 번째 접근 방식을 제시하고 동시에 비지도 의미 분석에 어떻게 사용될 수 있는지 보여줍니다. 우리는 WebNLG v2.1에 대한 접근 방식과 Visual Genome의 장면 그래프를 활용하는 새로운 벤치마크를 평가합니다. 우리 시스템은 한 데이터 세트에서 다른 데이터 세트로의 수동 조정 없이 text$\leftrightarrow$graph 변환 작업 모두에 대해 강력한 기준을 능가합니다. 추가 실험에서는 다양한 비지도 목표 사용의 영향을 조사합니다."
52,http://arxiv.org/abs/1904.09348 ,Compact Scene Graphs for Layout Composition and Patch Retrieval,"Subarna Tripathi, Sharath Nittur Sridhar, Sairam Sundaresan, Hanlin Tang","장면 그래프와 같은 구조화된 표현은 다운스트림 렌더링 또는 검색 작업에 사용할 수 있는 효율적이고 간결한 표현 역할을 합니다. 그러나 장면 그래프에서 사실적인 이미지를 생성하려는 기존 노력은 어수선하거나 복잡한 장면에 대한 장면 구성에서는 제대로 수행되지 않습니다. 우리는 장면 구성을 개선하기 위해 두 가지 기여를 제안합니다. 첫째, 최소한의 스토리지 오버헤드를 추가하는 경험적 기반 관계로 장면 그래프 표현을 향상합니다. 둘째, 장면 구성 네트워크의 학습을 감독하기 위해 극점 표현을 사용합니다. 이러한 방법은 기존 작업에 비해 훨씬 더 높은 성능을 달성합니다(관계 점수 지표에서 69.0% 대 51.2%). 또한 장면 그래프를 사용하여 소스 쿼리와 의미상 유사한 포즈가 제한된 이미지 패치를 검색하는 방법을 보여줍니다. 렌더링 또는 검색을 위해 구조화된 장면 그래프 표현을 개선하는 것은 사실적인 이미지 생성을 향한 중요한 단계입니다."
51,http://arxiv.org/abs/1904.03177 ,Structured agents for physical construction,"Victor Bapst, Alvaro Sanchez-Gonzalez, Carl Doersch, Kimberly L. Stachenfeld, Pushmeet Kohli, Peter W. Battaglia, Jessica B. Hamrick","물리적 구성(물리적 역학에 따라 특정 기능을 수행하기 위해 물체를 구성하는 능력)은 인간 지능의 기본입니다. 목표 구성 맞추기, 블록 쌓아서 개체 연결하기, 목표 개체 위에 피난처와 같은 구조 만들기 등 어린이들이 블록을 가지고 노는 방식에서 영감을 받은 일련의 도전적인 물리적 구성 작업을 소개합니다. 우리는 다양한 심층 강화 학습 에이전트가 이러한 문제를 어떻게 해결하는지 조사하고 우수한 성능을 제공하는 몇 가지 새로운 접근 방식을 소개합니다. 우리의 결과는 구조화된 표현(예: 객체 및 장면 그래프)과 구조화된 정책(예: 객체 중심 작업)을 사용하는 에이전트가 덜 구조화된 표현을 사용하는 에이전트보다 성능이 뛰어나고 더 큰 장면에 대해 추론하도록 요청받을 때 훈련 이상으로 더 잘 일반화한다는 것을 보여줍니다. Monte-Carlo Tree Search를 사용하는 모델 기반 에이전트는 가장 까다로운 구성 문제에서 모델이 없는 에이전트보다 성능이 뛰어납니다. 우리는 구조화된 표현과 추론을 강력한 학습과 결합하는 접근 방식이 풍부한 직관적인 물리학, 장면 이해 및 계획을 갖춘 에이전트를 향한 핵심 경로라고 결론지었습니다."
50,http://arxiv.org/abs/1904.02225 ,Revisiting Visual Grounding,"Erik Conser, Kennedy Hahn, Chandler M. Watson, Melanie Mitchell","우리는 특정 시각적 기반 방법인 Johnson et al의 ""장면 그래프를 사용한 이미지 검색""(IRSG) 시스템을 다시 살펴봅니다. (2015). 우리의 실험은 시스템이 학습된 객체 관계 모델을 효과적으로 사용하지 않는다는 것을 나타냅니다. 또한 IRSG 데이터 세트와 이를 적용한 널리 사용되는 VRD(Visual Relationship Dataset)를 자세히 살펴봅니다. 우리는 이러한 데이터 세트가 관계를 무시하는 방법이 상대적으로 잘 수행되도록 허용하는 편향을 나타냄을 발견했습니다. 또한 IRSG 데이터 세트의 몇 가지 다른 문제를 설명하고 편견 및 기타 문제가 제거된 데이터 세트의 하위 세트를 사용한 실험에 대해 보고합니다. 우리의 연구는 언어와 시각을 결합한 기계 학습 방법이 실제로 학습하는 것과 인기 있는 데이터 세트가 실제로 테스트하는 것이 무엇인지 더 잘 이해하는 보다 일반적인 노력에 기여합니다."
49,http://arxiv.org/abs/1904.02104 ,Target-Tailored Source-Transformation for Scene Graph Generation,"Wentong Liao, Cuiling Lan, Wenjun Zeng, Michael Ying Yang, Bodo Rosenhahn","장면 그래프 생성의 목적은 객체(노드 포함)와 객체 관계(가장자리 포함)를 표시하여 이미지의 의미론적 및 구조적 설명을 제공하는 것입니다. 현재까지 가장 뛰어난 성과를 거둔 작품은 예를 들어 사물 간에 정보를 전달하는 등 사물이나 관계를 둘러싼 맥락을 활용하는 데 기반을 두고 있습니다. 이러한 접근 방식에서 소스 개체의 표현을 변환하는 것은 대상 개체에서 사용할 정보를 추출하는 데 중요한 프로세스입니다. 이 작업에서 우리는 소스 객체가 대상 객체에 필요한 것을 제공해야 하며 모든 대상에 공통 정보를 제공하기보다는 서로 다른 객체에 서로 다른 정보를 제공해야 한다고 주장합니다. 이 목표를 달성하기 위해 객체 제안과 관계 간에 정보를 효율적으로 전파하는 TTST(Target-TailoredSource-Transformation) 방법을 제안합니다. 특히, 다른 대상 객체에 정보를 제공할 소스 객체 제안의 경우 소스와 대상을 동시에 고려하여 소스 객체 특징을 대상 객체 특징 도메인으로 변환합니다. 우리는 장면 그래프 생성을 위한 변환에서 시각적 컨텍스트와 사전 언어를 통합하여 더욱 강력한 표현을 탐색합니다. 이를 통해 대상 개체는 소스 개체 및 소스 관계에서 대상별 정보를 추출하여 표현을 구체화할 수 있습니다. 우리의 프레임워크는 Visual Genome 벤치마크에서 검증되었으며 장면 그래프 생성을 위한 최첨단 성능을 입증했습니다. 실험 결과는 우리의 방법에 의해 객체 탐지와 시각적 관계 탐지의 성능이 상호 향상된다는 것을 보여줍니다."
48,http://arxiv.org/abs/1904.00560 ,Scene Graph Generation with External Knowledge and Image Reconstruction,"Jiuxiang Gu, Handong Zhao, Zhe Lin, Sheng Li, Jianfei Cai, Mingyang Ling","장면 그래프 생성은 객체 감지, 속성 및 관계 예측 등 이미지 이해 작업의 발전으로 주목을 받고 있습니다. 그러나 기존 데이터 세트는 개체 및 관계 레이블 측면에서 편향되어 있거나 종종 노이즈가 있거나 주석이 누락되어 신뢰할 수 있는 장면 그래프 예측 모델을 개발하는 것이 매우 어렵습니다. 본 논문에서는 이러한 데이터 세트 문제를 극복하기 위해 외부 지식과 이미지 재구성 손실을 갖춘 새로운 장면 그래프 생성 알고리즘을 제안합니다. 특히, 장면 그래프 생성의 일반화 가능성을 높이기 위해 외부 지식 베이스에서 상식적인 지식을 추출하여 객체 및 구문 특징을 정제합니다. 노이즈가 있는 객체 주석의 편향을 해결하기 위해 장면 그래프 생성 네트워크를 정규화하는 보조 이미지 재구성 경로를 도입합니다. 광범위한 실험을 통해 우리 프레임워크가 더 나은 장면 그래프를 생성하여 시각적 관계 감지 및 시각적 게놈 데이터세트라는 두 가지 벤치마크 데이터세트에서 최첨단 성능을 달성할 수 있음을 보여줍니다."
47,http://arxiv.org/abs/1903.10658 ,Unpaired Image Captioning via Scene Graph Alignments,"Jiuxiang Gu, Shafiq Joty, Jianfei Cai, Handong Zhao, Xu Yang, Gang Wang","현재 이미지 캡션 모델의 대부분은 쌍을 이루는 이미지 캡션 데이터 세트에 크게 의존합니다. 그러나 대규모 이미지-캡션 쌍 데이터를 얻는 것은 노동 집약적이고 시간 소모적입니다. 본 논문에서는 짝이 없는 이미지 캡션을 위한 장면 그래프 기반 접근 방식을 제시합니다. 우리의 프레임워크는 이미지 장면 그래프 생성기, 문장 장면 그래프 생성기, 장면 그래프 인코더 및 문장 디코더로 구성됩니다. 구체적으로, 먼저 텍스트 양식에 대해 장면 그래프 인코더와 문장 디코더를 훈련합니다. 이미지와 문장 간의 장면 그래프를 정렬하기 위해 이미지의 장면 그래프 특징을 문장 양식으로 매핑하는 비지도 특징 정렬 방법을 제안합니다. 실험 결과는 우리가 제안한 모델이 이미지 캡션 훈련 쌍을 사용하지 않고도 매우 유망한 결과를 생성할 수 있으며 기존 방법보다 훨씬 뛰어난 성능을 발휘할 수 있음을 보여줍니다."
46,http://arxiv.org/abs/1903.05434 ,Visual Semantic Information Pursuit: A Survey,"Daqi Liu, Miroslaw Bober, Josef Kittler","시각적 의미 정보는 두 가지 중요한 부분, 즉 각 시각적 의미 단위의 의미와 이러한 시각적 의미 단위에 의해 전달되는 일관된 시각적 의미 관계로 구성됩니다. 본질적으로 전자는 시각적 인식 작업이고 후자는 시각적 맥락 추론에 해당합니다. 딥러닝의 성공으로 인해 시각적 인식이 눈부시게 발전했습니다. 이에 비해, 시각적 인식과 시각적 맥락 추론을 결합한 시각적 장면 의미 해석 작업인 시각적 의미 정보 추구는 아직 초기 단계입니다. 이는 객체 감지, 시각적 의미 분할, 시각적 관계 감지 또는 장면 그래프 생성과 같은 다양한 컴퓨터 비전 애플리케이션의 핵심 작업입니다. 결과 해석의 정확성과 일관성을 향상시키는 데 도움이 되므로 시각적 맥락 추론은 현재의 심층 엔드투엔드 시각적 의미 정보 추적 방법에서 시각적 인식과 통합되는 경우가 많습니다. 그러나 이 흥미로운 분야에 대한 포괄적인 검토는 아직 부족합니다. 이 설문 조사에서 우리는 이러한 모든 방법에 대한 통일된 이론적 패러다임을 제시하고 각 잠재적 방향의 주요 개발 개요와 미래 추세를 제시합니다. 공통 벤치마크 데이터 세트, 평가 지표 및 해당 방법의 비교도 소개됩니다."
45,http://arxiv.org/abs/1903.03326 ,Knowledge-Embedded Routing Network for Scene Graph Generation,"Tianshui Chen, Weihao Yu, Riquan Chen, Liang Lin","장면을 심층적으로 이해하려면 개별 개체를 찾고 인식하는 것뿐만 아니라 개체 간의 관계와 상호 작용을 추론해야 합니다. 그러나 실제 관계의 분포는 심각하게 불균형하기 때문에 기존 방법은 빈도가 낮은 관계에 대해 성능이 매우 낮습니다. 이 연구에서 우리는 객체 쌍과 그 관계 사이의 통계적 상관관계가 의미 공간을 효과적으로 정규화하고 예측을 덜 모호하게 만들어 불균형 분포 문제를 잘 해결할 수 있음을 발견했습니다. 이를 달성하기 위해 우리는 지식 내장 라우팅 네트워크를 개발하여 장면 그래프 생성을 용이하게 하기 위해 이러한 통계 상관 관계를 심층 신경망에 통합합니다. 보다 구체적으로, 우리는 이미지에 나타나는 객체와 그 관계 사이의 통계적 상관관계가 구조화된 지식 그래프로 명시적으로 표현될 수 있으며 그래프를 통해 메시지를 전파하여 상호 작용을 탐색하는 라우팅 메커니즘을 학습한다는 것을 보여줍니다. 대규모 Visual Genome 데이터 세트에 대한 광범위한 실험은 제안된 방법이 현재 최첨단 경쟁자보다 우수함을 보여줍니다."
44,http://arxiv.org/abs/1903.03166 ,CLEVR-Dialog: A Diagnostic Dataset for Multi-Round Reasoning in Visual Dialog,"Satwik Kottur, José M. F. Moura, Devi Parikh, Dhruv Batra, Marcus Rohrbach","시각적 대화는 대화 기록을 컨텍스트로 사용하여 이미지를 바탕으로 일련의 질문에 답하는 다중 모드 작업입니다. 이는 비전, 언어, 추론 및 기초에 대한 도전을 수반합니다. 그러나 이러한 하위 작업을 대규모의 실제 데이터 세트에서 분리하여 연구하는 것은 모든 이미지와 대화 상자의 '상태'에 대해 엄청나게 비용이 많이 드는 완전한 주석이 필요하기 때문에 실행 불가능합니다.   우리는 시각적 대화에서 다단계 추론을 연구하기 위한 대규모 진단 데이터세트인 CLEVR-Dialog를 개발합니다. 구체적으로 우리는 CLEVR 데이터 세트의 이미지 장면 그래프를 기반으로 하는 대화 문법을 구성합니다. 이 조합을 통해 시각적 대화 상자의 모든 측면에 완전히 주석이 달린 데이터 세트가 생성됩니다. 전체적으로 CLEVR-Dialog에는 약 85,000개의 CLEVR 이미지에 대한 10라운드 대화 상자 5개가 포함되어 있으며 총 425만 개의 질문-답변 쌍이 있습니다.   우리는 표준 시각적 대화 모델의 성능을 벤치마킹하기 위해 CLEVR-Dialog를 사용합니다. 특히 시각적 상호참조 해상도(상호참조 거리의 함수)에 대해 설명합니다. 이는 이 데이터 세트 없이는 불가능했던 시각적 대화 모델에 대한 최초의 분석입니다. CLEVR-Dialog의 결과가 시각적 대화를 위한 미래 모델 개발에 도움이 되기를 바랍니다. 우리의 데이터세트와 코드는 공개적으로 이용 가능합니다."
43,http://arxiv.org/abs/1903.02728 ,Graphical Contrastive Losses for Scene Graph Parsing,"Ji Zhang, Kevin J. Shih, Ahmed Elgammal, Andrew Tao, Bryan Catanzaro","대부분의 장면 그래프 파서는 시각적 관계를 감지하기 위해 2단계 파이프라인을 사용합니다. 첫 번째 단계에서는 엔터티를 감지하고 두 번째 단계에서는 소프트맥스 분포를 사용하여 각 엔터티 쌍에 대한 조건자를 예측합니다. 우리는 조건자 클래스에 대한 교차 엔트로피 손실만으로 훈련된 이러한 파이프라인이 두 가지 일반적인 오류로 인해 어려움을 겪는다는 것을 발견했습니다. 첫 번째인 엔터티 인스턴스 혼란은 모델이 동일한 유형의 엔터티(예: 여러 컵)의 여러 인스턴스를 혼동할 때 발생합니다. 두 번째, 근접 관계 모호함은 여러 개의 주어-술어-목적어 삼중항이 동일한 술어와 근접하게 나타나고 모델이 올바른 주어-객체 쌍을 추론하는 데 어려움을 겪을 때 발생합니다(예: 음악가와 악기의 잘못된 쌍). 우리는 장면 그래프 구문 분석 문제 내에서 이러한 유형의 오류를 구체적으로 대상으로 하는 일련의 대비 손실 공식을 제안합니다. 이를 통칭하여 그래픽 대비 손실이라고 합니다. 이러한 손실은 모델이 각 혼란 유형에 특정한 마진 제약을 통해 관련 인스턴스와 관련 없는 인스턴스를 명확하게 구분하도록 명시적으로 강제합니다. 우리는 제안된 손실의 효율성을 입증하기 위해 앞서 언급한 파이프라인을 사용하여 RelDN이라는 관계 감지기를 추가로 구성합니다. 우리 모델은 테스트 세트에서 OpenImages 관계 감지 챌린지의 승리 방법보다 4.7\%(16.5\% 상대) 성능이 뛰어납니다. 또한 시각적 게놈 및 시각적 관계 감지 데이터 세트에 대한 이전 최고의 방법보다 향상된 결과를 보여줍니다."
42,http://arxiv.org/abs/1902.10200 ,Differentiable Scene Graphs,"Moshiko Raboh, Roei Herzig, Gal Chechik, Jonathan Berant, Amir Globerson","복잡한 시각적 장면에 대한 추론에는 개체와 그 관계에 대한 인식이 포함됩니다. 장면 그래프는 엔터티(노드)와 관계(에지) 모두에 레이블을 할당하여 추론 작업에 대한 자연스러운 표현을 제공합니다. 불행하게도 SG 기반 추론 시스템은 일반적으로 2단계 절차로 훈련됩니다. 첫째, 이미지에서 SG를 예측하는 모델을 훈련합니다. 그런 다음 예측된 SG를 기반으로 추론하기 위해 별도의 모델이 생성됩니다. 많은 영역에서 엔드투엔드 방식으로 시스템을 공동으로 교육하는 것이 바람직하지만 SG는 일반적으로 시각적 추론 시스템에서 중간 구성 요소로 사용되지 않습니다. 왜냐하면 이산적이고 희박한 장면 그래프 표현은 차별화할 수 없고 최적화하기 어렵기 때문입니다. 여기에서는 차별화 가능한 엔드투엔드 최적화가 가능하고 다운스트림 작업에서만 감독이 필요한 이미지 표현인 DSG(미분 가능 장면 그래프)를 제안합니다. DSG는 모든 지역 및 지역 쌍에 대해 조밀한 표현을 제공하며 관심 개체나 관계를 포함하지 않는 이미지 영역에 모델링 용량을 소비하지 않습니다. 우리는 Visual Genome, VRD 및 CLEVR의 세 가지 벤치마크 데이터 세트에서 참조 관계(RR)를 식별하는 어려운 작업에 대해 모델을 평가합니다. 우리는 다중 작업 목표를 설명하고 다운스트림 RR 작업이 감독하는 엔드투엔드 방식으로 훈련합니다. DSG를 중간 표현으로 사용하면 새로운 최첨단 성능이 탄생합니다."
41,http://arxiv.org/abs/1902.09506 ,GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering,"Drew A. Hudson, Christopher D. Manning","이전 VQA 데이터 세트의 주요 단점을 해결하기 위해 실제 시각적 추론 및 구성 질문 답변을 위한 새로운 데이터 세트인 GQA를 소개합니다. 우리는 장면 그래프 구조를 활용하여 2,200만 개의 다양한 추론 질문을 생성하는 강력하고 강력한 질문 엔진을 개발했으며, 모두 해당 의미를 나타내는 기능적 프로그램과 함께 제공됩니다. 우리는 프로그램을 사용하여 답변 분포를 엄격하게 제어하고 질문 편향을 완화하기 위해 조정 가능한 새로운 평활화 기술을 제시합니다. 데이터 세트에는 일관성, 근거 및 타당성과 같은 필수 품질을 평가하는 새로운 측정항목 모음이 함께 제공됩니다. 기준선은 물론 최첨단 모델에 대한 광범위한 분석이 수행되어 다양한 질문 유형 및 토폴로지에 대한 세부적인 결과를 제공합니다. 블라인드 LSTM은 42.1%에 불과하고 강력한 VQA 모델은 54.1%를 달성하는 반면, 인간 성능은 89.3%로 최고로 새로운 연구를 탐구할 수 있는 충분한 기회를 제공합니다. 우리는 GQA가 향상된 견고성, 향상된 일관성, 이미지와 언어에 대한 더 깊은 의미론적 이해를 통해 차세대 모델을 위한 지원 리소스를 제공할 수 있기를 바랍니다."
40,http://arxiv.org/abs/1902.05715 ,Generating Natural Language Explanations for Visual Question Answering using Scene Graphs and Visual Attention,"Shalini Ghosh, Giedrius Burachas, Arijit Ray, Avi Ziskind","본 논문에서는 XQA(eXplainable Question Answering) 작업에 대한 새로운 접근 방식을 제시합니다. 즉, VQA(Visual Question Answering) 문제에 대한 자연어(NL) 설명을 생성하는 것입니다. 우리는 두 가지 정보 소스를 사용하여 이미지에 대한 질문에 대한 답변을 뒷받침하는 증거로 구성된 NL 설명을 생성합니다. (a) 이미지의 장면 그래프에서 생성된 이미지의 엔터티 주석(예: 개체 레이블, 영역 설명, 관계 구문) 및 (b) 질문에 답할 때 VQA 모델에 의해 생성된 주의 지도. 언어 모델을 사용하여 신중하게 선택한 관련 장면 그래프 개체의 NL 표현과 시각적 주의 맵을 결합하면 추가로 수집된 데이터(설명 캡션 등) 없이도 합리적인 텍스트 설명을 제공할 수 있는 방법을 보여줍니다. 우리는 VG(Visual Genome) 데이터 세트에서 알고리즘을 실행하고 내부 사용자 연구를 수행하여 강력한 기준에 대한 접근 방식의 효율성을 입증합니다. 또한 장면 그래프와 시각적 주의를 사용하여 VQA 및 텍스트 설명 생성을 보여주는 라이브 웹 데모도 출시했습니다."
39,http://arxiv.org/abs/1902.00313 ,VrR-VG: Refocusing Visually-Relevant Relationships,"Yuanzhi Liang, Yalong Bai, Wei Zhang, Xueming Qian, Li Zhu, Tao Mei","관계는 개별 인스턴스 간의 상호 작용을 인코딩하고 심층적인 시각적 장면 이해에 중요한 역할을 합니다. 비시각적 정보의 높은 예측 가능성으로 인해 기존 방법은 이미지로부터 관계를 '추론'하는 '학습'보다는 통계적 편향에 맞는 경향이 있습니다. 시각적 관계의 추가 개발을 장려하기 위해 시각적으로 관련 없는 관계를 잘라내어 더 가치 있는 관계를 자동으로 마이닝하는 새로운 방법을 제안합니다. 우리는 Visual Genome을 기반으로 VrR-VG(Visually-Relevant Relationships Dataset)라는 새로운 장면 그래프 데이터 세트를 구성합니다. VrR-VG에서는 기존 데이터 세트와 비교하여 학습 가능한 방법과 통계 방법 간의 성능 격차가 더 크고 빈도 기반 분석이 더 이상 작동하지 않습니다. 또한 인스턴스, 속성 및 관계를 공동으로 고려하여 관계 인식 표현을 학습할 것을 제안합니다. VrR-VG에서 학습된 표현 인식 기능을 적용함으로써 이미지 캡션 및 시각적 질문 응답 성능이 큰 폭으로 체계적으로 향상되었으며, 이는 데이터 세트 및 기능 내장 스키마의 이점을 입증합니다. VrR-VG는 http://vrr-vg.com/을 통해 제공됩니다."
38,http://arxiv.org/abs/1901.10124 ,Adversarial Adaptation of Scene Graph Models for Understanding Civic Issues,"Shanu Kumar, Shubham Atreja, Anjali Singh, Mohit Jain","시민 참여와 기술 사용은 스마트 시티 이니셔티브에 의해 주도되는 두 가지 새로운 트렌드입니다. 전 세계 정부는 시민 문제를 더 빠르게 해결하기 위해 기술을 채택하고 있습니다. 일반적으로 시민들은 도로 ​​파손, 쓰레기 처리 등의 문제를 웹 포털과 모바일 앱을 통해 신고하고 정부 당국이 적절한 조치를 취하도록 합니다. 이러한 문제를 보고하기 위해 텍스트, 이미지, 오디오, 비디오 등 여러 매체가 사용됩니다. 13명의 시민과 3개의 당국을 대상으로 한 사용자 연구를 통해 우리는 이미지가 시민 문제를 보고하는 데 가장 선호되는 매체라는 것을 발견했습니다. 그러나 시민 문제 관련 이미지를 분석하는 것은 당국의 수작업이 필요하기 때문에 어려운 작업입니다. 더욱이 이전 작업은 이미지에서 특정 문제 집합을 식별하는 것으로 제한되었습니다. 이 작업에서 우리는 이미지가 주어지면 근본적인 시민 문제를 대표하는 개체 집합과 개체 간의 의미 관계로 구성된 시민 문제 그래프를 생성할 것을 제안합니다. 또한 이미지를 통해 시민 문제를 추가로 분석하는 데 도움이 될 수 있는 두 가지 다중 모드(텍스트 및 이미지) 데이터세트를 출시합니다. 우리는 레이블이 지정된 훈련 데이터가 없는 경우 새로운 애플리케이션에 장면 그래프를 사용할 수 있는 기존 장면 그래프 모델의 적대적 훈련을 위한 새로운 접근 방식을 제시합니다. 우리는 접근 방식의 효율성을 분석하기 위해 여러 실험을 수행하고 인간의 평가를 사용하여 다양한 시민 문제를 대표하는 모델의 적합성을 확립합니다."
37,http://arxiv.org/abs/1901.03762 ,Using Scene Graph Context to Improve Image Generation,"Subarna Tripathi, Anahita Bhiwandiwalla, Alexei Bastidas, Hanlin Tang",장면 그래프에서 사실적인 이미지를 생성하려면 신경망이 객체 관계와 구성성을 추론할 수 있어야 합니다. 상대적으로 새로운 작업으로서 생성된 이미지가 장면 그래프를 준수하는지 확인하는 방법이나 작업 성능을 측정하는 방법은 여전히 ​​열려 있는 질문으로 남아 있습니다. 본 논문에서는 장면 그래프에서 이미지 생성을 개선하기 위해 장면 그래프 컨텍스트를 활용하는 방법을 제안합니다. 그래프 컨벌루션 신경망에서 생성된 특징을 모아서 이미지 생성 네트워크와 적대적 손실 모두에 제공하는 장면 그래프 컨텍스트 네트워크를 소개합니다. 컨텍스트 네트워크를 통해 우리 모델은 사실적으로 보이는 이미지를 생성할 뿐만 아니라 비공간 개체 관계를 더 잘 보존하도록 훈련되었습니다. 또한 장면 그래프 준수 여부를 직접 평가하는 이 작업에 대해 두 가지 새로운 평가 지표인 관계 점수와 평균 의견 관계 점수를 정의합니다. 우리는 제안된 모델이 이 도전적인 작업에서 최첨단 모델보다 성능이 우수하다는 것을 입증하기 위해 정량적 및 질적 연구를 모두 사용합니다.
36,http://arxiv.org/abs/1812.09681 ,Scene Graph Reasoning with Prior Visual Relationship for Visual Question Answering,"Zhuoqian Yang, Zengchang Qin, Jing Yu, Yue Hu","VQA(Visual Question Answering)의 주요 이슈 중 하나는 질문의 안내에 따라 시각적 콘텐츠의 의미론적 단서를 추론하는 것인데, 관계형 의미론을 어떻게 모델링할지는 여전히 큰 과제로 남아 있습니다. 시각적 의미를 완전히 포착하기 위해 우리는 구조화된 시각적 표현(장면 그래프, 내장된 개체 및 개체 간 관계)에 대한 추론을 제안합니다. 이는 바닐라 벡터 표현 및 암시적 시각적 관계 학습에 비해 큰 이점을 보여줍니다. 기존의 시각적 관계 모델을 기반으로 우리는 시각적 맥락과 언어 사전에 의해 제한된 학습된 깊은 의미 공간에 시각적 관계를 투영하는 시각적 관계 인코더를 제안합니다. 구성된 그래프에 대해 객체 속성과 관계형 의미를 공동으로 추론하여 정답을 찾는 Scene Graph Convolutional Network(SceneGCN)를 제안합니다. 우리는 까다로운 GQA 데이터 세트와 기존 VQA 2.0 데이터 세트에 대한 모델의 효율성과 해석 가능성을 입증하여 기존 최고의 모델과 비교하여 GQA에 대해 54.56%의 최첨단 정확도를 현저하게 달성했습니다."
35,http://arxiv.org/abs/1812.08434 ,Graph Neural Networks: A Review of Methods and Applications,"Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, Maosong Sun","많은 학습 과제에는 요소 간의 풍부한 관계 정보가 포함된 그래프 데이터를 처리해야 합니다. 물리 시스템 모델링, 분자 지문 학습, 단백질 인터페이스 예측, 질병 분류에는 그래프 입력을 통해 학습할 수 있는 모델이 필요합니다. 텍스트 및 이미지와 같은 비구조적 데이터로부터의 학습과 같은 다른 영역에서 추출된 구조(예: 문장의 종속성 트리 및 이미지의 장면 그래프)에 대한 추론은 그래프 추론 모델이 필요한 중요한 연구 주제입니다. 그래프 신경망(GNN)은 그래프 노드 간 메시지 전달을 통해 그래프의 종속성을 캡처하는 신경 모델입니다. 최근 몇 년 동안 그래프 컨벌루션 네트워크(GCN), 그래프 주의 네트워크(GAT), 그래프 순환 네트워크(GRN)와 같은 GNN의 변형은 많은 딥 러닝 작업에서 획기적인 성능을 보여주었습니다. 본 설문조사에서 우리는 GNN 모델에 대한 일반적인 설계 파이프라인을 제안하고 각 구성 요소의 변형을 논의하며 응용 프로그램을 체계적으로 분류하고 향후 연구를 위한 4가지 미해결 문제를 제안합니다."
34,http://arxiv.org/abs/1812.07524 ,Learning Direct Optimization for Scene Understanding,"Lukasz Romaszko, Christopher K. I. Williams, John Winn","우리는 입력 이미지 x를 설명하는 잠재 변수 모델의 개선을 위한 LiDO(Learning Direct Optimization) 방법을 개발합니다. 우리의 목표는 장면 그래프 잠재 변수 z(객체 모양, 카메라 위치 등)가 있는 해석 가능한 3D 컴퓨터 그래픽 모델로 단일 이미지 x를 설명하는 것입니다. z의 현재 추정값이 주어지면 이미지 x와 비교할 수 있는 이미지 g(z)의 예측을 렌더링할 수 있습니다. 진행하는 표준 방법은 둘 사이의 오류 E(x, g(z))를 측정하고 최적화 프로그램을 사용하여 오류를 최소화하는 것입니다. 그러나 잘못 정렬된 객체, 폐색, 텍스처 등과 같은 문제를 동시에 해결하는 데 어떤 오류 측정 E가 가장 효과적인지는 알 수 없습니다. 이와 대조적으로 LiDO 접근 방식은 z에 대한 오류를 최소화하는 대신 z를 수정하기 위해 업데이트를 직접 예측하도록 예측 네트워크를 훈련합니다. 실험에 따르면 우리의 LiDO 방법은 오류 환경에 대한 검색을 수행할 필요가 없고 오류 기반 경쟁사보다 더 나은 솔루션을 생성하며 데이터와 피팅된 장면 모델 간의 불일치를 처리할 수 있기 때문에 빠르게 수렴됩니다. 우리는 LiDO를 현실적인 합성 데이터 세트에 적용하고 이 방법이 실제 이미지에서도 잘 작동하도록 전송된다는 것을 보여줍니다."
33,http://arxiv.org/abs/1812.02378 ,Auto-Encoding Scene Graphs for Image Captioning,"Xu Yang, Kaihua Tang, Hanwang Zhang, Jianfei Cai","우리는 보다 인간과 유사한 캡션을 위해 언어 귀납적 편견을 인코더-디코더 이미지 캡션 프레임워크에 통합하는 장면 그래프 자동 인코더(SGAE)를 제안합니다. 직관적으로 우리 인간은 담론에서 연어와 맥락적 추론을 구성하기 위해 귀납적 편견을 사용합니다. 예를 들어, '자전거를 탄 사람'이라는 관계를 볼 때 'on'을 'ride'로 바꾸고 '도로'가 분명하지 않더라도 '도로에서 자전거를 타는 사람'을 추론하는 것이 자연스럽습니다. 따라서 이러한 편향을 언어 사전으로 활용하면 기존 인코더-디코더 모델이 데이터 세트 편향에 과적합될 가능성을 줄이고 추론에 집중하는 데 도움이 될 것으로 예상됩니다. 구체적으로, 우리는 장면 그래프(객체 노드가 형용사 노드와 관계 노드로 연결된 방향성 그래프($\mathcal{G}$))를 사용하여 이미지($\mathcal{I}$)와 문장($\mathcal{S}$)의 복잡한 구조 레이아웃을 나타냅니다. 텍스트 영역에서는 $\mathcal{S}\rightarrow \mathcal{G} \rightarrow \mathcal{D} \rightarrow \mathcal{S}$ 파이프라인에서 문장을 재구성하는 데 도움이 되는 사전($\mathcal{D}$)을 학습하기 위해 SGAE를 사용합니다. 여기서 $\mathcal{D}$는 원하는 언어를 먼저 인코딩합니다. 비전 언어 영역에서는 $\mathcal{I}\rightarrow \mathcal{G}\rightarrow \mathcal{D} \rightarrow \mathcal{S}$ 파이프라인에서 인코더-디코더를 안내하기 위해 공유된 $\mathcal{D}$를 사용합니다. 장면 그래프 표현과 공유 사전 덕분에 귀납적 바이어스는 원칙적으로 도메인 전체에 전달됩니다. 우리는 까다로운 MS-COCO 이미지 캡션 벤치마크에서 SGAE의 효율성을 검증합니다. 예를 들어 SGAE 기반 단일 모델은 Karpathy 분할에서 새로운 최첨단 $127.8$ CIDEr-D를 달성하고 다른 앙상블 모델과 비교해도 공식 서버에서 경쟁력 있는 $125.5$ CIDEr-D(c40)를 달성합니다."
32,http://arxiv.org/abs/1812.02347 ,Counterfactual Critic Multi-Agent Training for Scene Graph Generation,"Long Chen, Hanwang Zhang, Jun Xiao, Xiangnan He, Shiliang Pu, Shih-Fu Chang","장면 그래프(노드로서의 개체 및 가장자리로서의 시각적 관계)는 포괄적인 장면 이해를 위해 이미지에 있는 사물의 위치와 상호 작용을 설명합니다. 일관된 장면 그래프를 생성하기 위해 거의 모든 기존 방법은 개체 간 메시지 전달을 모델링하여 유익한 시각적 컨텍스트를 활용하고 추론의 동적 특성을 시각적 컨텍스트와 일치시킵니다. 예를 들어 ""자전거""의 ""사람""은 ""라이드"" 관계를 결정하는 데 도움이 될 수 있으며 이는 결국 두 개체의 카테고리 신뢰도에 기여합니다. 그러나 우리는 그래프 불일치에 민감하지 않은 일반적인 교차 엔트로피 기반 지도 학습 패러다임을 사용하여 장면 역학이 제대로 학습되지 않는다고 주장합니다. 허브 또는 비허브 노드의 오류는 불행하게도 동일하게 처벌됩니다. 이를 위해 우리는 불일치를 해결하기 위한 Counterfactual Critic Multi-Agent Training(CMAT) 접근 방식을 제안합니다. CMAT는 객체를 협력 에이전트로 구성한 다음 보상으로 그래프 수준 메트릭을 직접 최대화하는 다중 에이전트 정책 그라디언트 방법입니다. 특히, 각 에이전트에게 보상을 적절하게 할당하기 위해 CMAT는 다른 에이전트의 역학을 수정하여 에이전트별 보상을 분리하는 반사실 기준선을 사용합니다. 까다로운 Visual Genome 벤치마크에 대한 광범위한 검증을 통해 CMAT는 다양한 설정 및 지표에서 상당한 성능 향상을 통해 최첨단 기술을 달성한 것으로 나타났습니다."
31,http://arxiv.org/abs/1812.01880 ,Learning to Compose Dynamic Tree Structures for Visual Contexts,"Kaihua Tang, Hanwang Zhang, Baoyuan Wu, Wenhan Luo, Wei Liu","우리는 장면 그래프 생성 및 시각적 Q&A와 같은 시각적 추론 작업을 돕기 위해 이미지의 개체를 시각적 컨텍스트에 배치하는 동적 트리 구조를 구성할 것을 제안합니다. VCTree라고 불리는 시각적 컨텍스트 트리 모델은 체인 및 완전히 연결된 그래프를 포함한 기존의 구조화된 객체 표현에 비해 두 가지 주요 이점을 가지고 있습니다. 1) 효율적이고 표현력이 풍부한 이진 트리는 객체 간의 고유한 병렬/계층 관계를 인코딩합니다. 예를 들어 ""옷""과 ""바지""는 일반적으로 동시에 발생하고 ""사람""에 속합니다. 2) 동적 구조는 이미지마다, 작업마다 다르므로 개체 간에 더 많은 콘텐츠/작업별 메시지 전달이 가능합니다. VCTree를 구성하기 위해 우리는 각 개체 쌍 간의 작업 종속 유효성을 계산하는 점수 함수를 설계하고 트리는 점수 행렬의 최대 스패닝 트리의 이진 버전입니다. 그런 다음 시각적 컨텍스트는 양방향 TreeLSTM으로 인코딩되고 작업별 모델로 디코딩됩니다. 우리는 최종 작업 지도 학습과 트리 구조 강화 학습을 통합한 하이브리드 학습 절차를 개발합니다. 여기서 전자의 평가 결과는 후자의 구조 탐색에 대한 자기 비판 역할을 합니다. 컨텍스트에 대한 추론이 필요한 두 가지 벤치마크(장면 그래프 생성을 위한 Visual Genome 및 시각적 Q&A를 위한 VQA2.0)에 대한 실험 결과는 VCTree가 해석 가능한 시각적 컨텍스트 구조를 발견하는 동시에 최첨단 결과를 능가한다는 것을 보여줍니다."
30,http://arxiv.org/abs/1812.01855 ,Explainable and Explicit Visual Reasoning over Scene Graphs,"Jiaxin Shi, Hanwang Zhang, Juanzi Li","우리는 복잡한 시각적 추론 작업에 사용되는 널리 사용되는 블랙박스 신경 아키텍처를 제안된 eXplainable 및 eXplicit 신경 모듈(XNM)로 해체하는 것을 목표로 합니다. 이 모듈은 구조화된 지식을 통한 설명 가능하고 명시적인 추론을 위해 장면 그래프(객체를 노드로, 쌍 관계를 모서리로)를 사용하는 방향으로 기존 신경 모듈 네트워크를 발전시킵니다. XNM을 사용하면 기계가 ""보는"" 것과 상관없이 ""생각하는"" 방법을 기계에게 가르치는 데 더 많은 주의를 기울일 수 있습니다. 논문에서 보여주듯이, 장면 그래프를 귀납적 바이어스로 사용함으로써 1) XNM을 간결하고 유연한 방식으로 설계할 수 있습니다. 즉, XNM은 단지 4개의 메타 유형으로 구성되어 매개변수 수를 10~100배 크게 줄입니다. 2) 그래프 주의 측면에서 추론 흐름을 명시적으로 추적할 수 있습니다. XNM은 매우 일반적이므로 다양한 품질의 광범위한 장면 그래프 구현을 지원합니다. 예를 들어, 그래프가 완벽하게 감지되면 XNM은 CLEVR 및 CLEVR CoGenT 모두에서 100% 정확도를 달성하여 시각적 추론을 위한 경험적 성능 상한선을 설정합니다. 실제 이미지에서 그래프가 시끄럽게 감지되는 경우 XNM은 여전히 ​​강력하여 VQAv2.0에서 67.5%의 경쟁력 있는 정확도를 달성하여 그래프 구조가 없는 인기 있는 Bag-of-Objects Attention 모델을 능가합니다."
29,http://arxiv.org/abs/1811.10696 ,Attentive Relational Networks for Mapping Images to Scene Graphs,"Mengshi Qi, Weijian Li, Zhengyuan Yang, Yunhong Wang, Jiebo Luo","장면 그래프 생성은 이미지를 의미 구조 그래프에 자동으로 매핑하는 작업을 의미하며, 이를 위해서는 추출된 각 객체와 상호 작용 관계에 올바르게 레이블을 지정해야 합니다. 최근 딥러닝 기술을 사용한 객체 감지의 성공에도 불구하고 시각적 데이터에서 복잡한 상황별 관계와 구조화된 그래프 표현을 추론하는 것은 여전히 ​​어려운 주제로 남아 있습니다. 본 연구에서는 이 문제에 접근하기 위해 객체 감지 백본을 갖춘 두 개의 핵심 모듈로 구성된 새로운 Attentive Relational Network를 제안합니다. 첫 번째 모듈은 시각적 특징과 언어적 특징을 공통 의미 공간으로 변환하여 의미 내재된 관계 특징을 캡처하는 데 활용되는 의미 변환 모듈입니다. 다른 모듈은 이웃 노드에 다양한 중요도 가중치를 할당하여 결합 그래프 표현을 내장하기 위해 도입된 그래프 self-attention 모듈입니다. 마지막으로 관계 추론 모듈을 통해 정확한 장면 그래프가 생성되어 모든 개체와 해당 관계를 인식합니다. 우리는 널리 채택된 Visual Genome Dataset에서 우리가 제안한 방법을 평가했으며, 그 결과는 우리 모델의 효율성과 우수성을 보여줍니다."
28,http://arxiv.org/abs/1811.09543 ,An Interpretable Model for Scene Graph Generation,"Ji Zhang, Kevin Shih, Andrew Tao, Bryan Catanzaro, Ahmed Elgammal","우리는 효율적이고 해석 가능한 장면 그래프 생성기를 제안합니다. 우리는 시각적, 공간적, 의미적이라는 세 가지 유형의 기능을 고려하고 각 기능의 기여도를 명시적으로 조사할 수 있도록 후기 융합 전략을 사용합니다. 우리는 성능에 가장 큰 영향을 미치는 이러한 기능에 대한 핵심 요소를 연구하고 관계에 대해 학습된 시각적 기능을 시각화하고 모델의 효율성을 조사합니다. 우리는 Kaggle의 OpenImages 시각적 관계 탐지 챌린지에서 우승을 차지하여 2위보다 5\%(상대적으로 20\%) 더 나은 성과를 거두었습니다. 우리는 정확한 장면 그래프 생성기가 픽셀과 객체를 넘어 이미지에 대한 의미론적이고 구조화된 이해를 제공하기 때문에 이미지 캡션 작성 및 시각적 QA와 같은 더 높은 수준의 비전 언어 작업을 위한 근본적인 디딤돌이라고 믿습니다."
27,http://arxiv.org/abs/1811.08075 ,Scene Graph Generation via Conditional Random Fields,"Weilin Cong, William Wang, Wang-Chien Lee","객체 감지 및 분할 모델이 이미지의 개별 객체를 인식하는 데 큰 성공을 거두었음에도 불구하고 이미지 캡션, 의미론적 이미지 검색, 시각적 QA와 같은 인지 작업의 성능은 만족스럽지 않습니다. 이러한 인지 작업에서 더 나은 성능을 얻으려면 개별 개체 인스턴스를 인식하는 것만으로는 충분하지 않습니다. 대신, 이미지의 시각적 장면에 대한 추론과 이해를 용이하게 하기 위해 객체 인스턴스 간의 상호 작용을 캡처해야 합니다. 객체 인스턴스와 그 관계를 포착하여 이미지를 그래프로 표현한 장면 그래프를 통해 이미지에 대한 포괄적인 이해를 제공합니다. 그러나 기존의 장면 그래프 생성 기술은 이미지의 시각적 장면에서 피사체와 객체를 구분하지 못하므로 모호한 객체 인스턴스가 존재하는 실제 데이터 세트에서는 제대로 작동하지 않습니다. 본 연구에서는 이미지에서 객체 인스턴스와 해당 관계를 예측하기 위한 새로운 장면 그래프 생성 모델을 제안합니다. 우리 모델인 SG-CRF는 관계 삼중항에서 주체와 객체의 순차적 순서와 장면 그래프에서 객체 인스턴스 노드와 관계 노드의 의미적 호환성을 효율적으로 학습합니다. 실험에 따르면 SG-CRF는 CLEVR, VRD 및 Visual Genome의 세 가지 데이터 세트에서 최첨단 방법보다 성능이 뛰어나 Recall@100을 각각 24.99%에서 49.95%, 41.92%에서 50.47%, 54.69%에서 54.77%로 높였습니다."
26,http://arxiv.org/abs/1811.06410 ,LinkNet: Relational Embedding for Scene Graph,"Sanghyun Woo, Dahun Kim, Donghyeon Cho, In So Kweon",객체와 그 관계는 이미지 이해에 중요한 콘텐츠입니다. 장면 그래프는 이미지의 이러한 속성을 포착하는 구조화된 설명을 제공합니다. 그러나 객체 간의 관계를 추론하는 것은 매우 어려운 일이며 최근 몇 가지 연구에서만 이미지에서 장면 그래프를 생성하는 문제를 해결하려고 시도했습니다. 본 논문에서는 전체 객체 인스턴스 간의 상호 의존성을 명시적으로 모델링하여 장면 그래프 생성을 향상시키는 방법을 제시합니다. 우리는 모델이 고립된 개체에 초점을 맞추는 대신 모든 관련 개체 간의 연결을 공동으로 표현할 수 있도록 하는 간단하고 효과적인 관계형 임베딩 모듈을 설계합니다. 우리의 방법은 장면 그래프 생성 작업의 주요 부분인 관계 분류에 큰 이점을 제공합니다. 기본 Faster R-CNN 위에 이를 사용하여 우리 모델은 Visual Genome 벤치마크에서 최첨단 결과를 달성합니다. 글로벌 컨텍스트 인코딩 모듈과 기하학적 레이아웃 인코딩 모듈을 도입하여 성능을 더욱 향상시킵니다. 우리는 광범위한 절제 연구를 통해 최종 모델인 LinkNet을 검증하고 장면 그래프 생성에서의 효율성을 입증합니다.
25,http://arxiv.org/abs/1811.03830 ,Image-Level Attentional Context Modeling Using Nested-Graph Neural Networks,"Guillaume Jaume, Behzad Bozorgtabar, Hazim Kemal Ekenel, Jean-Philippe Thiran, Maria Gabrani","이미지 수준 주의 맥락 모델링(ILAC)이라는 새로운 장면 그래프 생성 방법을 소개합니다. 우리 모델에는 이미지 수준 기능을 사용하여 그래프 전체에 상황별 정보를 효과적으로 전파하는 주의 그래프 네트워크가 포함되어 있습니다. 이전 작업에서는 객체 중심 컨텍스트를 사용하는 반면, 우리는 장면 속성을 인코딩하기 위해 이미지 수준 컨텍스트 에이전트를 구축합니다. 제안된 방법은 중첩된 그래프 신경망을 사용하여 장면 그래프를 반복적으로 개선하는 단일 스트림 네트워크로 구성됩니다. 우리는 우리의 접근 방식이 Visual Genome 데이터세트의 장면 그래프 생성을 위한 최첨단 기술을 통해 경쟁력 있는 성능을 달성하는 동시에 다른 방법보다 더 적은 매개변수를 필요로 함을 보여줍니다. 또한 ILAC가 관계형 이미지 수준 정보를 통합하여 일반 객체 감지기를 향상시킬 수 있음을 보여줍니다."
24,http://arxiv.org/abs/1810.00912 ,Visual Curiosity: Learning to Ask Questions to Learn Visual Recognition,"Jianwei Yang, Jiasen Lu, Stefan Lee, Dhruv Batra, Devi Parikh","오픈 월드 환경에서는 지능형 에이전트(예: 로봇)가 인식하지 못하는 시각적 개체, 속성 또는 관계를 만나는 것이 불가피합니다. 이 작업에서 우리는 시각적 호기심, 즉 이미지의 내용(예: 빨간색 큐브의 왼쪽에 있는 객체는 무엇입니까?)에 대해 오라클(예: 인간)에게 질문하고 수신된 답변(예: 실린더)을 기반으로 시각적 인식 모델을 구축할 수 있는 에이전트를 개발합니다. 이를 위해 에이전트는 (1) 인식하는 것과 인식하지 못하는 것을 이해하고, (2) Oracle에 질문하기 위한 유효하고 명확하며 유익한 언어 쿼리(질문)를 공식화하고, (3) Oracle 응답에서 시각적 분류자의 매개변수를 파생하고, (4) 업데이트된 시각적 분류자를 활용하여 보다 명확한 질문을 해야 합니다. 구체적으로, 우리는 새로운 프레임워크를 제안하고 시각적 호기심 학습을 강화 학습 문제로 공식화합니다. 이 프레임워크에서는 에이전트의 모든 구성 요소인 시각적 인식 모듈(보기), 질문 생성 정책(묻기), 답변 소화 모듈(이해하기) 및 그래프 메모리 모듈(기억하기)을 완전히 엔드투엔드 학습하여 에이전트가 Oracle과의 대화의 결과로 얻은 장면 그래프에서 파생된 보상을 최대화합니다. 중요한 것은 질문 생성 정책이 시각적 인식 시스템 및 환경의 특성과 분리되어 있다는 것입니다. 결과적으로 우리는 일종의 이중 일반화를 보여줍니다. 우리의 질문 생성 정책은 새로운 환경과 새로운 눈, 즉 새로운 시각 시스템에 일반화됩니다. 합성 데이터 세트를 기반으로 훈련된 결과에 따르면 에이전트는 새로운 객체가 포함된 합성 환경과 실제 환경에서 테스트할 때에도 여러 휴리스틱 기준보다 훨씬 빠르게 새로운 시각적 개념을 학습하는 것으로 나타났습니다."
23,http://arxiv.org/abs/1809.06213 ,Context-Dependent Diffusion Network for Visual Relationship Detection,"Zhen Cui, Chunyan Xu, Wenming Zheng, Jian Yang","시각적 관계 감지는 이미지의 장면 이해를 위해 컴퓨터 비전과 자연어 간의 격차를 해소할 수 있습니다. 순수 객체 인식 작업과 달리 주어-술어-객체의 관계 삼중항은 \textit{person-behind-person} 및 \textit{car-behind-building}과 같은 극도의 다양성 공간에 놓여 있으면서 조합 폭발의 문제를 겪습니다. 본 논문에서는 시각적 관계 탐지를 처리하기 위한 CDDN(Context-dependent Diffusion Network) 프레임워크를 제안합니다. 서로 다른 개체 인스턴스의 상호 작용을 캡처하기 위해 단어 의미 그래프와 시각적 장면 그래프라는 두 가지 유형의 그래프를 구성하여 전역 컨텍스트 상호 의존성을 인코딩합니다. 의미 그래프는 객체 간의 의미 상관 관계를 모델링하기 위해 사전에 언어를 통해 구축되고, 시각적 장면 그래프는 주변 장면 정보를 활용할 수 있도록 장면 객체 간의 연결을 정의합니다. 그래프 구조 데이터의 경우, 우리는 시각적 관계의 잠재 표현을 효과적으로 학습하고 그래프에 대한 동형 불변성을 고려하여 시각적 관계 감지를 잘 충족할 수 있는 컨텍스트에서 정보를 적응적으로 집계하는 확산 네트워크를 설계합니다. 널리 사용되는 두 가지 데이터 세트에 대한 실험은 제안된 방법이 더 효과적이며 최첨단 성능을 달성한다는 것을 보여줍니다."
22,http://arxiv.org/abs/1808.00191 ,Graph R-CNN for Scene Graph Generation,"Jianwei Yang, Jiasen Lu, Stefan Lee, Dhruv Batra, Devi Parikh","우리는 이미지에서 객체와 객체의 관계를 탐지하는 데 효과적이고 효율적인 Graph R-CNN이라는 새로운 장면 그래프 생성 모델을 제안합니다. 우리 모델에는 이미지의 객체 간 잠재적 관계의 2차 수를 효율적으로 처리하는 RePN(Relation Proposal Network)이 포함되어 있습니다. 또한 객체와 관계 사이의 맥락 정보를 효과적으로 캡처하는 aGCN(attentional Graph Convolutional Network)을 제안합니다. 마지막으로, 기존 지표보다 더 총체적이고 현실적인 새로운 평가 지표를 도입합니다. 우리는 기존 측정항목과 제안된 측정항목을 모두 사용하여 평가된 장면 그래프 생성에 대한 최첨단 성능을 보고합니다."
21,http://arxiv.org/abs/1807.05933 ,Visual Graphs from Motion (VGfM): Scene understanding with object geometry reasoning,"Paul Gay, Stuart James, Alessio Del Bue","시각적 장면 이해에 대한 최근 접근 방식은 장면 그래프(객체와 객체의 쌍별 관계를 계산적으로 표현)를 구축하려고 시도합니다. 이러한 풍부한 의미 표현은 매우 매력적이지만 특히 장면의 복잡한 공간 배열을 고려할 때 단일 이미지에서 얻기가 어렵습니다. 이와 달리 이미지 시퀀스는 카메라 동작으로 인해 발생하는 다중 시점 기하학적 관계를 사용하여 유용한 정보를 전달합니다. 실제로 이러한 경우 개체 관계는 자연스럽게 3D 장면 구조와 관련됩니다. 이를 위해 본 논문에서는 일반적인 장면에서 객체의 기하학적 위치를 먼저 계산한 후, 이러한 기하학적 추론을 내장하여 비디오로부터 장면 그래프를 효율적으로 구성하는 시스템을 제안합니다. 이러한 강력한 표현은 RNN 프레임워크를 사용하여 기하학적 특징과 시각적 특징을 병합하는 새로운 모델을 사용하여 얻습니다. 우리는 3D 장면 그래프 생성 작업을 위해 생성한 데이터 세트에 대한 결과를 여러 보기에서 보고합니다."
20,http://arxiv.org/abs/1806.11538 ,Factorizable Net: An Efficient Subgraph-based Framework for Scene Graph Generation,"Yikang Li, Wanli Ouyang, Bolei Zhou, Jianping Shi, Chao Zhang, Xiaogang Wang",이미지 내부의 모든 관계를 설명하기 위해 장면 그래프를 생성하는 것은 최근 점점 더 많은 관심을 받고 있습니다. 그러나 이전 방법의 대부분은 추론 속도가 느린 복잡한 구조를 사용하거나 외부 데이터에 의존하므로 실제 시나리오에서 모델의 사용이 제한됩니다. 장면 그래프 생성의 효율성을 높이기 위해 추론 시 장면 그래프를 간결하게 표현할 수 있는 하위 그래프 기반 연결 그래프를 제안합니다. 상향식 클러스터링 방법은 먼저 전체 장면 그래프를 하위 그래프로 분해하는 데 사용됩니다. 각 하위 그래프에는 여러 개체와 해당 관계의 하위 집합이 포함됩니다. 장면 그래프의 수많은 관계 표현을 더 적은 수의 하위 그래프 및 객체 특징으로 대체함으로써 중간 단계의 계산이 크게 줄어듭니다. 또한 공간 정보는 관계 인식을 용이하게 하기 위해 제안된 공간 가중 메시지 전달(SMP) 구조와 공간 감지 관계 추론(SRI) 모듈을 활용하는 하위 그래프 기능에 의해 유지됩니다. 최근 시각적 관계 감지 및 시각적 게놈 데이터 세트에서 우리의 방법은 정확성과 속도 모두에서 최첨단 방법보다 성능이 뛰어납니다.
19,http://arxiv.org/abs/1804.10660 ,Large-Scale Visual Relationship Understanding,"Ji Zhang, Yannis Kalantidis, Marcus Rohrbach, Manohar Paluri, Ahmed Elgammal, Mohamed Elhoseiny","<주체, 관계, 대상> 삼중의 광범위하고 불균형한 분포를 처리하려면 모델이 필요하기 때문에 대규모 시각적 이해가 어렵습니다. 많은 수의 개체와 관계가 있는 실제 시나리오에서 일부는 매우 일반적으로 표시되는 반면 다른 일부는 거의 표시되지 않습니다. 우리는 식별 능력과 의미 친화력이 모두 보존되는 두 개의 벡터 공간에 개체와 관계를 포함하는 새로운 관계 탐지 모델을 개발합니다. 우리는 두 양식의 특징을 공유 공간으로 매핑하는 시각적 및 의미론적 모듈을 모두 배웁니다. 여기서 일치하는 특징 쌍은 일치하지 않는 특징을 구별해야 하지만 의미상 유사한 특징과 가까운 거리를 유지해야 합니다. 이를 통해 우리 모델은 시각적 엔터티 범주가 극도로 왜곡된 클래스 분포를 통해 80,000개 이상으로 확장되는 경우에도 탁월한 성능을 달성할 수 있습니다. 우리는 이전 작업에서 평가된 적이 없는 규모인 53,000개 이상의 개체와 29,000개 이상의 관계로 구성된 Visual Genome을 기반으로 하는 대규모 불균형 벤치마크에서 모델의 효율성을 입증합니다. 우리는 80,000개 이상의 범주가 포함된 원본 Visual Genome 데이터세트에서 신중하게 설계된 기준선에 비해 우리 모델의 우수성을 보여줍니다. 또한 VRD 데이터셋과 Visual Genome의 하위 집합인 200개 카테고리의 장면 그래프 데이터셋에 대해서도 최첨단 성능을 보여줍니다."
18,http://arxiv.org/abs/1804.01622 ,Image Generation from Scene Graphs,"Justin Johnson, Agrim Gupta, Li Fei-Fei","시각적 세계를 진정으로 이해하려면 모델이 이미지를 인식할 수 있을 뿐만 아니라 이미지를 생성할 수도 있어야 합니다. 이를 위해 최근 자연어 설명에서 이미지를 생성하는 흥미로운 진전이 있었습니다. 이러한 방법은 새나 꽃에 대한 묘사와 같은 제한된 영역에서는 놀라운 결과를 제공하지만, 많은 객체와 관계가 있는 복잡한 문장을 충실하게 재현하는 데 어려움을 겪습니다. 이러한 한계를 극복하기 위해 우리는 장면 그래프에서 이미지를 생성하여 객체와 객체의 관계에 대해 명시적으로 추론할 수 있는 방법을 제안합니다. 우리 모델은 그래프 컨볼루션을 사용하여 입력 그래프를 처리하고, 객체에 대한 경계 상자 및 분할 마스크를 예측하여 장면 레이아웃을 계산하고, 계단식 세분화 네트워크를 사용하여 레이아웃을 이미지로 변환합니다. 네트워크는 현실적인 출력을 보장하기 위해 한 쌍의 판별자에 대해 적대적으로 훈련됩니다. 우리는 정성적 결과, 절제 및 사용자 연구를 통해 여러 개체로 복잡한 이미지를 생성하는 방법의 능력을 입증하는 Visual Genome 및 COCO-Stuff에 대한 접근 방식을 검증합니다."
17,http://arxiv.org/abs/1803.09189 ,Scene Graph Parsing as Dependency Parsing,"Yu-Siang Wang, Chenxi Liu, Xiaohui Zeng, Alan Yuille","본 논문에서는 텍스트 설명에서 구조화된 지식 그래프를 구문 분석하는 문제를 연구합니다. 특히, 우리는 객체의 속성 및 관계와 함께 객체를 고려하는 장면 그래프 표현을 고려합니다. 이 표현은 다양한 비전 및 언어 응용 프로그램에서 유용한 것으로 입증되었습니다. 먼저 종속성 구문 분석에 연결되는 장면 그래프의 대안적이지만 동등한 에지 중심 보기를 소개합니다. 레이블과 작업 공간을 신중하게 재설계하면서 이전 작업에서 사용된 2단계 파이프라인(일반 종속성 구문 분석에 이어 간단한 후처리)을 하나로 결합하여 엔드투엔드 교육을 가능하게 합니다. 학습된 신경 의존성 파서에 의해 생성된 장면 그래프는 평가 세트의 실제 그래프와 49.67%의 F-점수 유사성을 달성하여 이전 최고의 접근 방식을 5% 능가합니다. 우리는 이미지 검색 애플리케이션에서 학습된 파서의 효율성을 추가로 보여줍니다."
16,http://arxiv.org/abs/1803.05401 ,Approximate Query Matching for Image Retrieval,"Abhijit Suprem, Polo Chau","전통적인 이미지 인식에는 단일 객체 초점(ILSVRC, AlexNet 및 VGG)을 사용하여 인물 유형 이미지에서 주요 객체를 식별하는 작업이 포함됩니다. 보다 최근의 접근법은 조밀한 이미지 인식을 고려합니다. 즉, 적절한 경계 상자로 이미지를 분할하고 이러한 경계 상자 내에서 이미지 인식을 수행합니다(의미론적 분할). Visual Genome 데이터 세트[5]는 경계 상자 생성, 이미지 인식, 캡션 및 새로운 작업인 장면 그래프 생성 등 각 하위 작업에 대한 응집력 있는 데이터 세트에 이러한 다양한 접근 방식을 연결하려는 시도입니다. 우리의 초점은 이러한 장면 그래프를 사용하여 이미지 데이터베이스에서 그래프 검색을 수행하여 검색 기준에 따라 이미지를 전체적으로 검색하는 것입니다. 장면 그래프와 메타데이터를 그래프 데이터베이스에 저장하고(Neo4J 사용), 그래프 검색 쿼리를 기반으로 이미지의 대략적인 빠른 검색을 수행하는 방법을 개발합니다. 우리는 단일 개체 검색보다 더 복잡한 쿼리를 처리합니다. ""케이크를 먹는 소녀""는 지정된 관계와 변형을 포함하는 이미지를 검색합니다."
15,http://arxiv.org/abs/1802.05451 ,Mapping Images to Scene Graphs with Permutation-Invariant Structured Prediction,"Roei Herzig, Moshiko Raboh, Gal Chechik, Jonathan Berant, Amir Globerson","복잡한 이미지를 기계가 이해하는 것은 인공지능의 핵심 목표입니다. 이 작업의 기본 과제 중 하나는 시각적 장면에 상호 관련된 여러 개체가 포함되어 있고 전역 컨텍스트가 장면을 해석하는 데 중요한 역할을 한다는 것입니다. 이러한 효과를 포착하기 위한 자연스러운 모델링 프레임워크는 복잡한 레이블을 최적화하는 동시에 레이블 내 상호 작용을 모델링하는 구조화된 예측입니다. 그러나 딥 러닝 구성 요소의 힘을 활용하는 구조화된 예측 모델의 설계를 안내해야 하는 원칙이 무엇인지는 불분명합니다. 여기서 우리는 순열 불변성의 자연스러운 요구 사항을 따르는 아키텍처에 대한 설계 원칙을 제안합니다. 우리는 이러한 불변성을 따르는 아키텍처에 대한 필요하고 충분한 특성화를 증명하고 모델 설계에 미치는 영향을 논의합니다. 마지막으로, 결과 모델이 Visual Genome 장면 그래프 라벨링 벤치마크에서 새로운 최첨단 결과를 달성하여 최근의 모든 접근 방식을 능가한다는 것을 보여줍니다."
14,http://arxiv.org/abs/1802.02598 ,Generating Triples with Adversarial Networks for Scene Graph Construction,"Matthew Klawonn, Eric Heim","딥 러닝의 성공에 힘입어 컴퓨터 비전 연구는 객체 감지 및 이미지 분류를 넘어 이미지 캡션 작성 또는 시각적 질문 답변과 같은 보다 정교한 작업으로 이동하기 시작했습니다. 그러한 노력에 동기를 부여하는 것은 모델이 이미지에 존재하는 물체뿐만 아니라 물체와 그 속성 간의 관계와 같은 장면의 보다 세밀한 측면을 포착하려는 욕구입니다. 장면 그래프는 이미지의 이러한 측면을 캡처하기 위한 공식적인 구성을 제공합니다. 그럼에도 불구하고, 이미지로부터 장면 그래프를 생성하려는 노력은 최근 몇 가지에 불과합니다. 이전 작업에서는 경계 상자 정보를 훈련 시간에 사용할 수 있는 설정으로 제한하고 속성이 있는 장면 그래프를 생성하려고 시도하지 않았습니다. 본 논문에서는 생성적 적대 신경망(Generative Adversarial Networks)의 최근 발전을 기반으로 이러한 결함을 극복하는 방법을 제안합니다. 우리는 먼저 주의 메커니즘을 사용하여 선택된 입력 이미지의 특정 영역에서 장면에 대한 단일 설명을 설명하는 작은 하위 그래프를 생성하는 접근 방식을 취합니다. 그렇게 함으로써 우리의 방법은 경계 상자 레이블 없이도 속성 정보가 있는 장면 그래프의 일부를 생성할 수 있습니다. 그런 다음 이러한 하위 그래프로부터 완전한 장면 그래프가 구성됩니다. 우리는 최첨단 데이터 세트 및 허용된 측정 항목에 대한 장면 그래프 생성의 이전 작업을 통해 모델이 개선되었음을 보여줍니다. 또한 우리 모델이 이전 작업에서 시도한 것보다 더 큰 어휘 크기를 처리할 수 있음을 보여줍니다."
13,http://arxiv.org/abs/1711.06640 ,Neural Motifs: Scene Graph Parsing with Global Context,"Rowan Zellers, Mark Yatskar, Sam Thomson, Yejin Choi",우리는 시각적 장면의 구조화된 그래프 표현을 생성하는 문제를 조사합니다. 우리 작업에서는 장면 그래프에 정기적으로 나타나는 하위 구조인 모티프의 역할을 분석합니다. 우리는 Visual Genome 데이터세트에서 이러한 반복 구조에 대한 새로운 정량적 통찰력을 제시합니다. 우리의 분석에 따르면 객체 레이블은 관계 레이블을 잘 예측하지만 그 반대는 아닙니다. 또한 더 큰 하위 그래프에서도 반복되는 패턴이 있음을 발견했습니다. 그래프의 50% 이상이 적어도 두 가지 관계를 포함하는 모티프를 포함합니다. 우리의 분석은 새로운 기준에 동기를 부여합니다. 주어진 객체 감지를 통해 훈련 세트에서 볼 수 있듯이 객체 쌍과 주어진 레이블 사이의 가장 빈번한 관계를 예측합니다. 이 기준선은 평가 설정 전체에 걸쳐 평균 3.6% 상대적 개선으로 이전 최첨단 기술을 향상시킵니다. 그런 다음 장면 그래프에서 고차 모티프를 캡처하도록 설계된 새로운 아키텍처인 Stacked Motif Networks를 소개합니다. 이는 강력한 기준보다 평균 7.1% 상대 게인을 더욱 향상시킵니다. 우리 코드는 github.com/rowanz/neural-motifs에서 확인할 수 있습니다.
12,http://arxiv.org/abs/1711.00088 ,Semantic Image Retrieval via Active Grounding of Visual Situations,"Max H. Quinn, Erik Conser, Jordan M. Witte, Melanie Mitchell","우리는 의미론적 이미지 검색, 특히 시각적 상황 인스턴스 검색을 위한 새로운 아키텍처를 설명합니다. 시각적 상황은 ""권투 경기"", ""개 산책"", ""버스를 기다리는 군중"" 또는 ""탁구 게임""과 같은 개념으로, 이미지의 인스턴스화는 낮은 수준의 시각적 유사성보다는 공통 공간 및 의미 구조에 의해 더 많이 연결됩니다. 쿼리 상황 설명이 주어지면 Situate라고 불리는 우리의 아키텍처는 예상되는 개체의 시각적 특징과 개체 간 관계의 예상되는 공간 구성을 캡처하는 모델을 학습합니다. 새로운 이미지가 주어지면 Situate는 활성 검색 절차를 통해 이미지에서 예상되는 각 상황 구성 요소를 접지(즉, 위치를 지정하는 경계 상자 생성)하려는 시도에서 이러한 모델을 사용합니다. Situate는 결과 접지를 사용하여 새 이미지가 상황의 인스턴스를 포함하는 것으로 판단되는 정도를 나타내는 점수를 계산합니다. 이러한 점수는 검색 시스템의 일부로 컬렉션의 이미지 순위를 매기는 데 사용될 수 있습니다. 여기에 설명된 예비 연구에서 우리는 Situate의 성능을 두 가지 기본 방법 및 ""장면 그래프""를 기반으로 하는 관련 의미 이미지 검색 시스템과 비교하여 이 시스템의 가능성을 보여줍니다."
11,http://arxiv.org/abs/1707.09700 ,"Scene Graph Generation from Objects, Phrases and Region Captions","Yikang Li, Wanli Ouyang, Bolei Zhou, Kun Wang, Xiaogang Wang","서로 다른 의미론적 수준에서 세 가지 장면 이해 작업인 객체 감지, 장면 그래프 생성 및 영역 캡션은 서로 연결되어 있습니다. 장면 그래프는 쌍 관계가 예측된 이미지에서 감지된 객체 위에 생성되는 반면, 영역 캡션은 객체, 해당 속성, 관계 및 기타 컨텍스트 정보에 대한 언어 설명을 제공합니다. 이 작업에서는 의미 수준 전반의 상호 연결을 활용하기 위해 세 가지 비전 작업을 엔드투엔드 방식으로 공동으로 해결하기 위해 MSDN으로 표시되는 다단계 장면 설명 네트워크라고 하는 새로운 신경망 모델을 제안합니다. 개체, 구문 및 캡션 영역은 먼저 공간 및 의미 연결을 기반으로 동적 그래프에 정렬됩니다. 그런 다음 기능 정제 구조를 사용하여 그래프를 통해 의미론적 작업의 세 가지 수준에 걸쳐 메시지를 전달합니다. 우리는 세 가지 작업에 대해 학습된 모델을 벤치마킹하고 제안된 방법을 사용하여 세 가지 작업에 대한 공동 학습이 이전 모델에 비해 상호 개선을 가져올 수 있음을 보여줍니다. 특히 장면 그래프 생성 작업에서는 제안한 방법이 기존 방법에 비해 3% 이상의 마진을 보이는 것으로 나타났다."
10,http://arxiv.org/abs/1706.07365 ,Pixels to Graphs by Associative Embedding,"Alejandro Newell, Jia Deng",그래프는 이미지 콘텐츠의 유용한 추상화입니다. 그래프는 장면의 개별 개체에 대한 세부 정보를 나타낼 수 있을 뿐만 아니라 개체 쌍 간의 상호 작용을 캡처할 수 있습니다. 우리는 입력 이미지를 가져와 전체 그래프 정의를 생성하도록 컨볼루션 신경망을 훈련하는 방법을 제시합니다. 이는 연관 임베딩을 사용하여 단일 단계에서 엔드 투 엔드로 수행됩니다. 네트워크는 그래프를 구성하는 모든 요소를 ​​동시에 식별하고 이를 하나로 묶는 방법을 학습합니다. 우리는 Visual Genome 데이터 세트를 벤치마킹하고 장면 그래프 생성이라는 어려운 작업에 대한 최첨단 성능을 보여줍니다.
9,http://arxiv.org/abs/1705.01661 ,Learning Hierarchical Shape Segmentation and Labeling from Online Repositories,"Li Yi, Leonidas Guibas, Aaron Hertzmann, Vladimir G. Kim, Hao Su, Ersin Yumer","우리는 기하학적 모양을 부품 라벨이 있는 계층적으로 분할된 부품으로 변환하는 방법을 제안합니다. 우리의 핵심 아이디어는 공개 저장소의 3D 모양과 함께 제공되는 장면 그래프와 부품 이름을 바탕으로 카테고리별 모델을 훈련시키는 것입니다. 무료로 사용할 수 있는 이러한 주석은 기하학에 대한 아직 개발되지 않은 막대한 정보 소스를 나타냅니다. 그러나 모델과 해당 장면 그래프는 다양한 수준의 전문 지식, 모델링 도구 및 목표를 가진 광범위한 모델러에 의해 생성되기 때문에 이러한 모델은 희박하고 잡음이 많은 텍스트 태그로 인해 매우 일관되지 않은 분할 및 계층 구조를 갖습니다. 우리의 방법에는 두 가지 분석 단계가 포함됩니다. 먼저, 표준 태그 사전과 부품 계층 구조를 추론하는 동시에 데이터베이스의 부품을 클러스터링하고 라벨을 지정하는 공동 최적화를 수행합니다. 그런 다음 이 레이블이 지정된 데이터를 사용하여 새로운 3D 모양의 계층적 분할 및 레이블 지정 방법을 교육합니다. 우리는 우리의 방법이 복잡한 정보를 마이닝하고, 인공 물체와 그 구성 부분의 계층 구조를 감지하고, 기존 대안보다 더 미세한 규모의 세부 정보를 얻을 수 있음을 보여줍니다. 또한 몇 가지 감독 예제를 사용하여 도메인 전송을 수행함으로써 우리 기술이 수백 개의 수동으로 레이블이 지정된 모델이 필요한 완전 감독 기술보다 성능이 우수하다는 것을 보여줍니다."
8,http://arxiv.org/abs/1704.01189 ,Semantic Robot Programming for Goal-Directed Manipulation in Cluttered Scenes,"Zhen Zeng, Zheming Zhou, Zhiqiang Sui, Odest Chadwicke Jenkins",우리는 시연과 의미 매핑을 통한 로봇 프로그래밍의 융합으로서 의미 로봇 프로그래밍(SRP) 패러다임을 제시합니다. SRP에서는 사용자가 작업 공간에서 의도한 목표 장면의 스냅샷을 보여줌으로써 로봇 매니퓰레이터를 직접 프로그래밍할 수 있습니다. 그런 다음 로봇은 이 목표를 알려진 객체 형상을 가정하여 객체 포즈와 객체 간 관계로 구성된 장면 그래프로 분석합니다. 그런 다음 작업 및 모션 계획을 사용하여 임의의 초기 장면 구성에서 사용자의 목표를 실현합니다. 다양한 초기 장면 구성에 직면하더라도 SRP를 사용하면 로봇이 사용자가 입증한 목표에 도달하도록 원활하게 적응할 수 있습니다. 장면 인식을 위해 RGBD 이미지로부터 세계의 초기 상태와 목표 상태를 추론하는 DIGEST(Discriminatively-Informed Generative Estimation of Scenes and Transforms) 방법을 제안합니다. DIGEST 인식 기능을 갖춘 SRP의 효율성은 Michigan Progress Fetch 로봇을 사용한 트레이 설정 작업에서 입증되었습니다. 장면 인식 및 작업 실행은 공개 가구 폐색 데이터 세트와 복잡한 장면 데이터 세트를 사용하여 평가됩니다.
7,http://arxiv.org/abs/1701.02426 ,Scene Graph Generation by Iterative Message Passing,"Danfei Xu, Yuke Zhu, Christopher B. Choy, Li Fei-Fei",시각적 장면을 이해하는 것은 개별 개체를 개별적으로 인식하는 것 이상입니다. 객체 간의 관계도 장면에 대한 풍부한 의미 정보를 구성합니다. 이 작업에서 우리는 시각적으로 기반을 둔 이미지의 그래픽 구조인 장면 그래프를 사용하여 객체와 객체의 관계를 명시적으로 모델링합니다. 우리는 입력 이미지로부터 구조화된 장면 표현을 생성하는 새로운 엔드투엔드 모델을 제안합니다. 이 모델은 표준 RNN을 사용하여 장면 그래프 추론 문제를 해결하고 메시지 전달을 통해 예측을 반복적으로 개선하는 방법을 학습합니다. 우리의 공동 추론 모델은 상황별 단서를 활용하여 개체와 개체 관계에 대해 더 나은 예측을 할 수 있습니다. 실험은 우리 모델이 Visual Genome 데이터 세트를 사용하여 장면 그래프를 생성하고 NYU Depth v2 데이터 세트와의 지원 관계를 추론하는 이전 방법보다 훨씬 뛰어난 성능을 보여줍니다.
6,http://arxiv.org/abs/1609.05834 ,On Support Relations and Semantic Scene Graphs,"Michael Ying Yang, Wentong Liao, Hanno Ackermann, Bodo Rosenhahn",장면 이해는 컴퓨터 비전과 사진 측량 모두에서 인기 있고 어려운 주제입니다. 장면 그래프는 이러한 장면 이해를 위한 풍부한 정보를 제공합니다. 본 논문에서는 이러한 관계를 추론하고 장면 그래프를 구성하는 새로운 접근 방식을 제시합니다. 지원 관계는 이전에 무시되었던 중요한 정보(물리적 안정성 및 객체 클래스 간의 사전 지원 지식)를 고려하여 추정됩니다. 지원 관계를 추출하는 이전 방법과 달리 제안된 접근 방식은 더 정확한 결과를 생성하며 장면의 픽셀 단위 의미 체계 라벨링이 필요하지 않습니다. 이 정보를 이용하여 장면 내의 모든 맥락적 관계를 설명하는 의미론적 장면 그래프가 구성됩니다. 이러한 그래프의 정확성을 평가하기 위해 여러 가지 다른 측정값이 공식화됩니다. 제안된 알고리즘은 NYUv2 데이터베이스를 사용하여 평가됩니다. 결과는 추론된 지원 관계가 최신 기술보다 더 정확하다는 것을 보여줍니다. 장면 그래프는 실제 그래프와 비교됩니다.
5,http://arxiv.org/abs/1607.08822 ,SPICE: Semantic Propositional Image Caption Evaluation,"Peter Anderson, Basura Fernando, Mark Johnson, Stephen Gould","이미지 캡션을 자동으로 생성하는 작업에 상당한 관심이 쏠리고 있습니다. 그러나 평가는 어렵다. 기존 자동 평가 지표는 주로 n-gram 중복에 민감하며 이는 인간 판단을 시뮬레이션하는 작업에 필요하지도 충분하지도 않습니다. 우리는 의미론적 명제 콘텐츠가 인간 캡션 평가의 중요한 구성 요소라는 가설을 세우고 SPICE라는 장면 그래프에 대해 정의된 새로운 자동 캡션 평가 메트릭을 제안합니다. 다양한 모델 및 데이터 세트에 대한 광범위한 평가에 따르면 SPICE는 다른 자동 측정 항목보다 모델 생성 캡션에 대한 인간의 판단을 더 잘 포착합니다(예: MS COCO 데이터 세트에 대한 인간 판단과 시스템 수준 상관 관계는 0.88, CIDEr의 경우 0.43, METEOR의 경우 0.53). 게다가 SPICE는 '어떤 캡션 생성기가 색상을 가장 잘 이해하는가?'나 '캡션 생성기가 셀 수 있나요?'와 같은 질문에 답할 수 있습니다."
4,http://arxiv.org/abs/1607.05910 ,Visual Question Answering: A Survey of Methods and Datasets,"Qi Wu, Damien Teney, Peng Wang, Chunhua Shen, Anthony Dick, Anton van den Hengel","VQA(시각적 질문 응답)는 컴퓨터 비전과 자연어 처리 커뮤니티 모두에서 점점 더 많은 관심을 받고 있는 어려운 작업입니다. 이미지와 자연어로 된 질문이 주어지면 정답을 추론하기 위해서는 이미지의 시각적 요소에 대한 추론과 일반 지식이 필요합니다. 이 설문 조사의 첫 번째 부분에서는 문제에 대한 현대적인 접근 방식을 비교하여 최신 기술을 조사합니다. 우리는 시각적 양식과 텍스트 양식을 연결하는 메커니즘을 기준으로 방법을 분류합니다. 특히, 이미지와 질문을 공통 특징 공간에 매핑하기 위해 컨벌루션 신경망과 순환 신경망을 결합하는 일반적인 접근 방식을 조사합니다. 또한 구조화된 지식 기반과 인터페이스하는 메모리 확장 및 모듈식 아키텍처에 대해서도 논의합니다. 이 설문조사의 두 번째 부분에서는 VQA 시스템 교육 및 평가에 사용할 수 있는 데이터 세트를 검토합니다. 다양한 데이터 세트에는 다양한 수준의 복잡성에 대한 질문이 포함되어 있으며, 여기에는 다양한 추론 유형과 기능이 필요합니다. 우리는 Visual Genome 프로젝트의 질문/답변 쌍을 심층적으로 조사하고 VQA용 장면 그래프를 사용하여 이미지의 구조화된 주석의 관련성을 평가합니다. 마지막으로, 우리는 이 분야의 유망한 미래 방향, 특히 구조화된 지식 기반과의 연결 및 자연어 처리 모델의 사용에 대해 논의합니다."
3,http://arxiv.org/abs/1408.0200 ,Towards a Domain Specific Language for a Scene Graph based Robotic World Model,"Sebastian Blumenthal, Herman Bruyninckx","로봇 세계 모델 표현은 로봇 응용 프로그램의 중요한 부분입니다. 그러나 모델 기반 엔지니어링 도구 체인에서는 이러한 표현이 지원되지 않습니다. 이 작업은 로봇 장면 그래프(RSG) 접근 방식을 기반으로 하는 로봇 세계 모델을 위한 새로운 도메인 특정 언어(DSL)를 제안합니다. RSG-DSL은 (a) 애플리케이션별 장면 구성, (b) 의미론적 장면 구조, (c) 세계 모델의 인스턴스에 로드되는 계산 엔터티에 대한 입력 및 출력을 표현할 수 있습니다."
2,http://arxiv.org/abs/1308.5843 ,Affordable Virtual Reality System Architecture for Representation of Implicit Object Properties,"Stoyan Maleshkov, Dimo Chotrov","유연하고 확장 가능하며 저렴한 가상 현실 소프트웨어 시스템 아키텍처가 제안됩니다. 이 솔루션은 단일 컴퓨터 또는 컴퓨터 클러스터 등 다양한 하드웨어 구성에서 쉽게 구현할 수 있습니다. 이 아키텍처는 엔지니어링 작업을 해결하기 위한 워크플로에 통합되고 여러 감각 채널(시각, 오디오 및 촉각)을 통해 암시적인 개체 속성을 제시하는 것을 목표로 합니다. 암시적 속성은 관찰자가 감각을 통해 인지할 수 없지만 관찰자의 감각 능력을 확장하기 위해 특수 장비가 필요한 숨겨진 개체 기능(예: 자화, 방사선, 습도, 독성 등)을 나타냅니다. 우리의 접근 방식은 암시적 속성 표현을 위한 추가 효과 노드를 통합하는 기본 일반 장면 그래프 구조를 확장합니다."
1,http://arxiv.org/abs/1105.2890 ,Improving Usability of Interactive Graphics Specification and Implementation with Picking Views and Inverse Transformations,Stéphane Conversy,"그래픽 상호 작용을 지정하고 프로그래밍하는 것은 어려운 작업입니다. 특히 디자이너가 상호 작용의 역동성을 표현하는 데 어려움을 겪기 때문입니다. 이 문서에서는 MDPC 아키텍처가 사양의 유용성과 그래픽 상호 작용 구현을 어떻게 향상시키는지 보여줍니다. 아키텍처는 뷰 선택 및 그래픽에서 데이터로의 역변환 사용을 기반으로 합니다. 그래픽 상호작용의 세 가지 예를 통해 이를 아키텍처로 표현하는 방법과 이를 구현하는 방법, 프로그래밍 사용성을 어떻게 향상시키는지 보여줍니다. 또한 장면 그래프 없이 그래픽 상호 작용을 구현할 수 있음을 보여줍니다. 이러한 종류의 코드는 캐시 일관성 관리로 인한 오류를 방지합니다."
